var tipuesearch = {"pages":[{"title":"About","text":"Personalia My name is Sven Vermeulen, born in Bruges (Belgium) on the 5th of September in 1982. I am currently employed as an enterprise architect with responsibility on the infrastructure domain (strategy, evolution and standardization within both on premise and cloud) at KBC Group and am living happily in Mechelen, a city near Brussels (also Belgium). I'm a heavy proponent of the Free Software movement. I'm an active Gentoo Linux user (a Linux meta-distribution) and have published a few books about SELinux. Most of my hobbies are computing-related: security (both mathematical approach and conceptual), system-level programming, processes and automation development. I like to be a know-it-all, checking out how stuff works and why it sometimes doesn't ;-) Sven Vermeulen sven.vermeulen@siphos.be Curriculum Vitae Work Experience Year Activity 2020 - present Enterprise Architect at KBC Group with responsibility on the infrastructure domain. 2015 - 2019 IT Architect at KBC Group where I am working on the design of an analytics and data discovery environment, meant to strive in a big data environment with high flexibility demands and modern technological approaches. 2010 - 2015 ICT Architect at KBC Group where I maintain the strategy on database & scheduling technologies, watch over the information system & infrastructure enterprise architectures and help in programs and projects that want to integrate one or more database technologies into their own architecture. 2005 - 2009 WebSphere System Engineer at the KBC Group where I develop and maintain a framework for automated deployment and maintenance of JEE applications on WebSphere systems and, of course, work in projects to represent the WebSphere group. 2001 - 2006, 2007 - 2008, 2011 - 2018 Active in the Gentoo project, first as a documentation translator (English - Dutch), later as author (English), project lead (documentation, public relations), council member and foundation trustee. Now active as a SELinux policy developer, SELinux userspace package maintainer and documentation developer. 2000 - 2005 Graduated from the University of Ghent as Master of Science in Engineering: Computer Science, major in Software Engineering (in Dutch 'Burgerlijk Ingenieur in de Computerwetenschappen - major Softwareontwikkeling'). My thesis was about making software tamperproof by embedding control regions, checksum algorithms and more. 1992 - 2005 Board member of the ACCB, a computerclub located in Bruges 2004 Summerjob at Philips : Search and repair faults in television sets 2003 Summerjob at K.U. Leuven : Authoring 'Programming for Realtime Systems', redesign of Orocos website 2000 - 2003 Board member of the LugWV, the Linux User Group for West-Vlaanderen (one of Belgium's 10 provinces) 2000 Summerjob at Tyco Electronics : Camera verification of electromechanical components 1994 - 2000 Graduated from the secundary high school K.A. Brugge where I attended the Science / Math (8h) courses. Additional Courses Year Description 2014 - 2016 Master in Enterprise Architecture 2009 IVPV Courses on Software Engineering 2008 IVPV Courses on Networking and Security 05/2008 dynaTrace Advanced Training 2005 'WebSphere Administration' at ABIS 1998 'Intel PC Technical Repairs' at IVO Brugge 1996 - 1997 'Programming in Turbo Pascal' at SNT De Bogaerde Technical Knowledge Operating systems Linux (Administration and engineering) Unix (Sun Solaris 8-10, HP-UX, AIX) (Administration) Windows 9x and higher (Advanced use) Enterprise Applications Apache (Administration and engineering) OpenLDAP (Administration) Programming Languages C (Advanced) Java and J2EE (Advanced) Python (Basic) Certifications Product Description SAFe Scaled Agile Frameworks for Architects v5.0 dynaTrace Certified for dynaTrace Diagnostics 2.5.4 Publications Date Publication December 2016 SELinux System Administration, 2nd Edition September 2014 SELinux Cookbook September 2013 SELinux System Administration","tags":"pages","url":"https://blog.siphos.be/pages/about.html","loc":"https://blog.siphos.be/pages/about.html"},{"title":"SELinux System Administration 3rd Edition","text":"As I mentioned previously, recently my latest installment of \"SELinux System Administration\" has been released by Packt Publishing. This is already the third edition of the book, after the first (2013) and second (2016) editions have gotten reasonable success given the technical and often hard nature of full SELinux administration. Like with the previous editions, this book remains true to the public of system administrators, rather than SELinux policy developers. Of course, SELinux policy development is not ignored in the book. What has changed First and foremost, it of course updates the content of the previous edition to be up to date with the latest evolutions within SELinux. There are no earth shattering changes, so the second edition isn't suddenly deprecated. The examples are brought up to date with a recent distribution setup, for which I used Gentoo Linux and CentOS. The latter is, given the recent announcement of CentOS stopping support for CentOS version 8 in general, a bit confrontational, although it doesn't really matter that much for the scope of the book. I hope that Rocky Linux will get the focus and support it deserves. Anyway, I digress. A significant part of the updates on the existing content is on SELinux-enabled applications, applications that act as a so-called object manager themselves. While quite a few were already covered in the past, these applications continue to enhance their SELinux support, and in the third edition a few of these receive a full dedicated chapter. There are also a small set of SELinux behavioral changes, like SELinux' NNP support, as well as SELinux userspace changes like specific extended attributes for restorecon. Most of the book though isn't about changes, but about new content. What has been added As administrators face SELinux-aware applications more and more, the book goes into much more detail on how to tune SELinux with those SELinux-aware applications. If we look at the book's structure, you'll find that it has roughly three parts: Using SELinux, which covers the fundamentals of using SELinux and understanding what SELinux is. SELinux-aware platforms, which dives into the SELinux-aware application suites that administrators might come in contact with Policy management, which focuses on managing, analyzing and even developing SELinux policies. By including additional content on SEPostgreSQL, libvirt, container platforms like Kubernetes, and even Xen Security Modules (which is not SELinux itself, but strongly influenced and aligned to it to the level that it even uses the SELinux userspace utilities) the book is showing how wide SELinux is being used. Even on policy development, the book now includes more support than before. While another book of mine, SELinux Cookbook, is more applicable to policy development, I did not want to keep administrators out of the loop on how to develop SELinux policies at all. Especially not since there are more tools available nowadays that support policy creation, like udica. SELinux CIL One of the changes I also introduced in the book is to include SELinux Common Intermediate Language (CIL) information and support. When we need to add in a small SELinux policy change, the book will suggest CIL based changes as well. SELinux CIL is not commonly used in large-scale policy development. Or at least, not directly. The most significant policy development out there, the SELinux Reference Policy , does not use CIL directly itself, and the level of support you find for the current development approach is very much the default way of working. So I do not ignore this more traditional approach. The reason I did include more CIL focus is because CIL has a few advantages up its sleeve that is harder to get with the traditional language. Nothing major perhaps, but enough that I feel it should be more actively promoted anyway. And this book is hopefully a nice start to it. I hope the book is a good read for administrators or even architects that would like to know more about the technology.","tags":"SELinux","url":"https://blog.siphos.be/2021/01/selinux-system-administration-3rd-edition/","loc":"https://blog.siphos.be/2021/01/selinux-system-administration-3rd-edition/"},{"title":"Abstracting infrastructure complexity","text":"IT is complex. Some even consider it to be more magic than reality. And with the ongoing evolutions and inventions, the complexity is not really going away. Sure, some IT areas are becoming easier to understand, but that is often offset with new areas being explored. Companies and organizations that have a sizeable IT footprint generally see an increase in their infrastructure, regardless of how many rationalization initiatives that are started. Personally, I find it challenging, in a fun way, to keep up with the onslaught of new technologies and services that are onboarded in the infrastructure landscape that I'm responsible for. But just understanding a technology isn't enough to deal with its position in the larger environment. Complexity is a challenging beast If someone were to attempt drawing out how the IT infrastructure of a larger IT environment looks like in reality, it would soon become very, very large and challenging to explain. Perhaps not chaotic, but definitely complicated. One of the challenges is the amount of \"something\" that is out there. That can be the amount of devices you have, the amount of servers in the network, the amount of flows going through firewalls or gateways, the amount of processes running on a server, the amount of workstations and end user devices in use, the amount of containers running in the container platform, the amount of cloud platform instances that are active... The \"something\" can even be less tangible than the previous examples such as the amount of projects that are being worked on in parallel or the amount of changes that are being prepared. However, that complexity is not one I'll deal with in this post. Another challenge is the virtualized nature of IT infrastructure, which has a huge benefit for the organization and simplifies infrastructure services for its own consumers, but does make it more, well, complicated to deal with. Virtual networks (vlans), virtual systems (hypervisors), virtual firewalls, virtual applications (with support for streaming desktop applications to the end user device without having the applications installed on that device), virtual storage environments, etc. are all wonderful technologies which allow for much more optimized resource usage, but does introduce a higher complexity of the infastructure at large. To make sense of such larger structures, we start making abstractions of what we see, structuring it in a way that we can more easily explain, assess or analyze the environment and support changes properly. These abstract views do reflect reality, but only to a certain extend. Not every question that can be asked can be answered satisfactory with the same abstract view, but when it can, it is very effective. Abstracting service complexity In my day-to-day job, I'm responsible for the infrastructure of a reasonably large environment. With \"responsible\" I don't want to imply that I'm the one and only party involved of course - responsibilities are across a range of people and roles. I am accountable for the long-term strategy on infrastructure and the high-level infrastructure architecture and its offerings, but how that plays out is a collaborative aspect. Because of this role, I do want to keep a close eye on all the services that we offer from infrastructure side of things. And hence, I am often confronted with the complexity mentioned earlier. To resolve this, I try to look at all infastructure services in an abstract way, and document it in the same way so that services are more easily explained. Figure 1 - A possible visualization of the abstraction model, here in Archimate The abstraction I apply is the following: We start with components , building blocks that are used and which refer to a single product or technology out there. A specific Java product can be considered such a component, because by itself it hardly has any value. Components are put together to create a solution . This is something that is intended to provide value to the organization at large, and is the level at which something is documented, has an organizational entity responsible for it, etc. Solutions are not yet instantiated though. An example of a solution could be a Kafka-based pub/sub solution, or an OpenLDAP-based directory solution. Solutions are used to create services . A service is something that has an SLA attached to it. In most cases, the same solution is used to create multiple services. We can think of the Kafka-based pub/sub solution that has three services in the organization: a regular non-production one, a regular production one, and a highly available production service. Services are supported through one or more clusters . These are a way for teams to organize resources in support of a service. Some services might be supported by multiple clusters, for instance spread across different data centers. An OpenLDAP-based service might be supported by a single OpenLDAP cluster with native synchronization support spread across two data centers, or by two OpenLDAP clusters with a different synchronization mechanism between the two clusters. Clusters exist out of one or more instances . These are the actual deployed technology processes that enable the cluster. In an OpenLDAP cluster, you could have two master processes ( slapd processes) running, which are the instances within the cluster. On top of the clusters, we enable containers (I call those containers, but they don't have anything to do with container technology like Docker containers). The containers are what the consumers are actually interested in. That could be an organization unit in an LDAP structure, a database within an RDBMS, a set of topics within a Kafka cluster, etc. These are the basic abstractions I apply for most of the technologies, allowing me to easily make a good view on the environment. Let's look at a few examples here. Example: Virtualization of Wintel systems In a large, virtualized environment, you generally have a specific hypervisor software being used: be it RHV (Red Hat Virtualization) based upon KVM, Microsoft HyperV, VMWare vSphere or something else - the technology used is generally well known. That's one of the components being used, but that is far from the only component. To better manage the virtualized environment the administration teams might use an orchestration engine like Ansible, Puppet or Saltstack. They might also have a component in use for automatically managing certificates and what not. All these components are needed to build a full virtualization solution. For me, as an architect, knowing which components are used is useful for things like lifecycle management (which components are EOL, which components can be easily replaced with a different one versus components that are more lock-in oriented, etc.) or inventory management (which component is deployed where, which version is used), which supports things like vulnerability management (if we can map components to their Common Platform Enumeration (CPE) then we can easily see which vulnerabilities are reported through the Common Vulnerabilities and Exposure (CVE) reports). The interaction between all these components creates a sensible solution, which is the virtualization solution. At this level, I'm mostly interested in the solution roadmap, the responsibilities and documentation associated with it, the costs, maturity of the offering within the organization, etc. It is also on the solution level that most architectural designs are made, and the best practices (and malpractices) are documented. The virtualization solution itself is then instantiated within the organization to create one or more services. These could be different services based on the environment (a lab/sandbox virtualization service with low to no SLA, a non-production one with standard SLA, a non-production one with specific disaster recovery requirements, a production one with standard SLA (and standard disaster recovery requirements), a high-performance production one, etc. These services are mostly important for other architects, project leads or other stakeholders that are going to make active use of the virtualization services - the different services (which one could document as \"service plans\") make it more obvious on what the offering is, and what differentiation is supported. Let's consider a production, standard SLA virtualization service. The system administrators of the virtualization environment might enable this service across multiple clusters. This could be for several reasons: this could be due to limits (maximum number of hosts per cluster), or because of particular resource requirements (different CPU architecture requirements - yes even with virtualization this is still a thing), or to make things manageable for the administrators in general. While knowing which cluster an application is on is, in general, not that important, it can be very important when there are problems, or when limits are being reached. As an architect, I'm definitely interested in knowing why multiple clusters are made (what is the reasoning behind it) as it gives a good view on what the administrators are generally dealing with. Within a cluster (to support the virtualization) you'll find multiple hosts. Often, a cluster is sized to be able to deal with one or two host fall-outs so that the virtual machines (which are hosted on the cluster - these are the \"containers\" that I spoke of) can be migrated to another host with only a short downtime as a consequence (if their main host crashed) or no downtime at all (if it is scheduled maintenance of the host). These hosts are the instances of the cluster. By using this abstraction, I can \"map\" the virtualization environment in a way that I have a good enough view, without proclaiming to be anything more than an informed architect, on this setup to support my own work, and to be able to advice management on major investment requirements, challenges, strategic evolutions and more. More than just documentation While the above method is used for documenting the environment in which I work (and which works well for the size of the environment I have to deal with), it can be used for simplifying management of the technologies as well. This level of abstraction can easily be used in environments that push self-servicing forward. Let's take the Open Service Broker API as an example. This is an API that defines how to expose (infrastructure) services to consumers that can easily create (provision) and destroy (deprovision) their own services. Brokers that support the API will then automatically handle the service management. This model can easily be put in to support the previous abstraction. Take the virtualization environment again. If we want to enable self-servicing on a virtualized environment, we can think of an offering where internal customers can create new virtual machines (provision) either based on a company-vetted template, or through an image (like with virtual appliances). The team that manages the virtualization environment has a number of services, which they describe in the service plans exposed by the API. An internal customer, when privisioning a virtual machine, is thus creating a \"container\" for the right service (based on their selected service plan) and on the right cluster (based upon the parameters that the internal customer passes along with the creation of its machine). We can do the same with databases: a certain database solution (say PostgreSQL) has its own offerings (exposed through service plans linked to the service), and internal customers can create their own database (\"container\") on the right cluster through this API. I personally have a few scripts that I use at home myself to quickly set up a certain technology, using the above abstraction level as the foundation. Rather than having to try and remember how to set up a multi-master OpenLDAP service, or a replicated Kafka setup, I have scripts that create this based upon this abstraction: the script parameters always use the service, cluster, instance and container terminology and underlyingly map this to the technology-specific approach. It is my intention to also promote this abstraction usage within my work environment, as I believe it allows us to more easily explain what all the infrastructure is used for, but also to more easily get new employees known to our environment. But even if that isn't reached, the abstraction is a huge help for me to assess and understand the multitude of technologies that are out there, be it our mainframe setup, the SAN offerings, the network switching setup, the databases, messaging services, cloud landing zones, firewall setups, container platforms and more.","tags":"Architecture","url":"https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/","loc":"https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/"},{"title":"Working on infra strategy","text":"After a long hiatus, I'm ready to take up blogging again on my public blog. With my day job becoming more intensive and my side-job taking the remainder of the time, I've since quit my work on the Gentoo project. I am in process of releasing a new edition of the SELinux System Administration book, so I'll probably discuss that more later. Today, I want to write about a task I had to do this year as brand new domain architect for infrastructure. Transitioning to domain architect While I have been an infrastructure architect for quite some time already, my focus then was always on specific areas within infrastructure (databases, scheduling, big data), or on general infrastructure projects or challenges (infrastructure zoning concept, region-wide disaster recovery analysis and design). As one of my ex-colleagues and mentors put it, as infrastructure architects you are allowed to piss on each other's area: you can (and perhaps should) challenge the vision and projects of others, of course in a professional way. I heeded the advice of this person, and was able to get a much better grip on all our infrastructure services, their designs and challenges. I mentioned earlier on that my day job became more intensive: it was not just the direct responsibilities that I had that became more challenging, my principle to learn and keep track of all infrastructure evolutions were a large part of it as well. This pays off, as feedback and advice within the architecture review boards is more to the point, more tied to the situation. Furthermore, as an architect, I still try to get my hands dirty on everything bouncing around. When I was focusing on big data, I learned Spark and pySpark, I revisited my Python knowledge, and used this for specific cases (like using Python to create reports rather than Excel) to make sure I get a general feel of what engineers and developers have to work with. When my focus was on databases, I tried to get acquainted with DBA tasks. When we were launching our container initiative, I set up and used Kubernetes myself (back then this was also to see if SELinux is working properly with Kubernetes and during the installation). While this does not make me anything near what our engineers and experts are doing, I feel it gives me enough knowledge to be able to talk and discuss topics with these colleagues without being that \"ivory tower\" architect, and better understand (to a certain level) what they are going through when new initiatives or solutions are thrown at them. End of 2019, the company decided that a reorganization was due, not only on department and directorate level, but also on the IT and Enterprise Architecture level. One of the areas that improved was to make sure the infrastructure in general was also covered and supported by the EA team. Part of that move, two of my infrastructure architect colleagues and myself joined the EA team. One colleague is appointed to tackle a strategic theme, another is now domain architect for workplace/workforce, and I got the task of covering the infrastructure domain. Well, it is called infrastructure, but focus on the infrastructure related to hosting of applications and services: cloud hosting, data center, network, compute, private cloud, container platform, mainframe, integration services, middleware, etc. Another large part of what I consider \"infrastructure\" is part of the workplace domain, which my colleague is pushing forward. While I was still handing over my previous workload, coaching the new colleague that got thrown in to make sure both him and the teams involved are not left with a gap, the various domain enterprise architects got a first task: draft up the strategy for the domain… and don't wait too long ;-) Tackling a domain strategy Now, I've drafted infrastructural strategies quite a few times already, although those have most focus on the technology side. The domain view goes beyond just the technological means: to be able to have a well-founded strategy, I also have to tackle the resources and human side of things, the ability of the organization to deal with (yet another?) transformation, the processes involved, etc. Unlike the more specific area focus I had in the past, where the number of direct stakeholders is limited, the infrastructure domain I now support has many more direct stakeholders involved. If I count the product managers, system architects, product owners (yes we are trying the Scaled Agile approach in our organization) and the managerial roles within the domain, I have 148 people to involve, spread across 7 Agile Release Trains with different directorate steering. The consumers of the infrastructure services (which are more part of business delivery services rather than on IT level) are even much larger than that, and are the most important ones (but also more difficult) to get in touch with. Rather than just asking what the main evolutions are in the several areas of the domains, I approached this more according to practices I read in books like \"Good Strategy, Bad Strategy\" by Richard Rumelt. I started with interviews of all the stakeholders to get to learn what their challenges and problems are. I wanted our strategy to tackle the issues at hand, not focus on technological choices. Based on these interviews, I grouped the issues and challenges to see what are the primary causes of these issues. Then I devised what action domains I need to focus on in the strategy. An action domain was an area that more clearly describes the challenges ahead: while I had close to 200 challenges observed from the interviews, I can assign the huge majority of them to one of two action domains: if we tackle these domains then we are helping the organization in most of their challenges. After validating that these action domains are indeed covering the needs of the organization, I started working on the principles how to tackle these issues. Within the principles I want to steer the evolution within the infrastructure domain, without already focusing on the tangible projects to accomplish that. The principles should map to both larger projects (which I wanted to describe in the strategy as well) as well as smaller or more continuity-related projects. I eventually settled with four principles: - one principle covering how to transform the environment, - one principle covering what we offer (and thus also what we won't be offering anymore), - one principle which extends our scope with a major area that our internal customers are demanding, and - one principle describing how we will design our services Four principles are easy enough to remember for all involved, and if they are described well, they are steering enough for the organization to take up in their solutions. But with principles alone the strategy is not tangible enough for everyone, and many choices to be made are not codified within those principles. The next step was to draw out the vision for infrastructure, based upon current knowledge and the principles above, and show the major areas of work that lays ahead, as well as give guidance on what these areas should evolve to. I settled for eight vision statements, each worked out further with high level guidance, as well as impact information: how will this impact the organization? Do we need specific knowledge or other profiles that we miss? Is this a vision that instills a cultural change (which often implies a slower adoption and the need for more support)? What are the financial consequences? What will happen if we do not pursue this vision? Within each vision, I collaborated with the various system architects and other stakeholders to draft out epics, changes that support the vision and are ready to be taken up in the Scaled Agile approach of the organization. The epics that would be due soon were fully expanded, with a lean business case (attempt) and phasing. Epics that are scheduled later (the strategy is a 5-year plan) are mainly paraphrased as expanding those right now provides little value. While the epics themselves are not fully described in the strategy (the visions give the rough approach), drafting these out is a way to verify if the vision statements are feasible and correct, and is a way to check if the organization understands and supports the vision. From the moment I got the request to the final draft of the strategy note, around 2 months have passed. The first draft was slideware and showed the intentions towards management (who wanted feedback within a few weeks after the request), after which the strategy was codified in a large document, and brought for approval on the appropriate boards. That was only the first hurdle though. Next was to communicate this strategy further… Communication and involvement are key The strategic document was almost finalized when COVID-19 struck. The company moved to working at home, and the way of working changed a lot. This also impacted how to approach the communication of the strategy and trying to get involvement of people. Rather than physically explaining the strategy, watching the body language of the people to see if they understand and support it or not, I was facing digital meetings where we did not yet have video. Furthermore, the organization was moving towards a more distributed approach with smaller teams (higher agility) with fewer means of bringing out information to larger groups. I selected a few larger meetings (such as those where all product managers and system architects are present) to present and discuss the strategy, but also started making webinars on this so that interested people could get informed about it. I even decided to have two webinars: a very short one (3 minutes) which focuses on the principles alone (and quickly summarizes the vision statements), and an average one (20-ish minutes) which covers the principles and vision statements. I also made recordings of the full explanations (e.g. those to management team), which take 1 hour, but did not move those towards a webinar (due to time pressure). Of course, I also published the strategy document itself for everyone, as well as the slides that accompany it. One of the next steps is to translate the strategy further towards the specific agile release trains, drafting up specific roadmaps, etc. This will also allow me to communicate and explain the strategy further. Right now, this is where we are at - and while I am happy with the strategy content, I do feel that the communication part received too little attention from myself, and is something I need to continue to put focus on. If a strategy is not absorbed by the organization, it fails as a strategy. And if you do not have sufficient collaboration on the strategy after it was 'finalized' (not just communication but collaboration) then the organization cannot absorb it. I also understand that the infrastructure strategy isn't the only one guiding the organization: each domain has a strategy, and while the domain architects do try to get the strategies aligned (or at least not contradictory to each other), it is still not a single, company-wide strategy. Right now, colleagues are working on consolidating the various strategies on architectural level, while the agile organization is using the strategies to formulate their specific solution visions (and for a handful of solutions I'm also directly involved). We'll see how it pans out. So, do you think this is a sensible approach I took? How did you tackle communication and collaboration of such initiatives during COVID-19 measures?","tags":"Architecture","url":"https://blog.siphos.be/2020/10/working-on-infra-strategy/","loc":"https://blog.siphos.be/2020/10/working-on-infra-strategy/"},{"title":"cvechecker 3.9 released","text":"Thanks to updates from Vignesh Jayaraman, Anton Hillebrand and Rolf Eike Beer, a new release of cvechecker is now made available. This new release (v3.9) is a bugfix release.","tags":"Free Software","url":"https://blog.siphos.be/2018/09/cvechecker-3.9-released/","loc":"https://blog.siphos.be/2018/09/cvechecker-3.9-released/"},{"title":"Automating compliance checks","text":"With the configuration baseline for a technical service being described fully (see the first , second and third post in this series), it is time to consider the validation of the settings in an automated manner. The preferred method for this is to use Open Vulnerability and Assessment Language (OVAL) , which is nowadays managed by the Center for Internet Security , abbreviated as CISecurity. Previously, OVAL was maintained and managed by Mitre under NIST supervision, and Google searches will often still point to the old sites. However, documentation is now maintained on CISecurity's github repositories . But I digress... Read-only compliance validation One of the main ideas with OVAL is to have a language (XML-based) that represents state information (what something should be) which can be verified in a read-only fashion. Even more, from an operational perspective, it is very important that compliance checks do not alter anything, but only report. Within its design, OVAL engineering has considered how to properly manage huge sets of assessment rules, and how to document this in an unambiguous manner. In the previous blog posts, ambiguity was resolved through writing style, and not much through actual, enforced definitions. OVAL enforces this. You can't write a generic or ambiguous rule in OVAL. It is very specific, but that also means that it is daunting to implement the first few times. I've written many OVAL sets, and I still struggle with it (although that's because I don't do it enough in a short time-frame, and need to reread my own documentation regularly). The capability to perform read-only validation with OVAL leads to a number of possible use cases. In the 5.10 specification a number of use cases are provided. Basically, it boils down to vulnerability discovery (is a system vulnerable or not), patch management (is the system patched accordingly or not), configuration management (are the settings according to the rules or not), inventory management (detect what is installed on the system or what the systems' assets are), malware and threat indicator (detect if a system has been compromised or particular malware is active), policy enforcement (verify if a client system adheres to particular rules before it is granted access to a network), change tracking (regularly validating the state of a system and keeping track of changes), and security information management (centralizing results of an entire organization or environment and doing standard analytics on it). In this blog post series, I'm focusing on configuration management. OVAL structure Although the OVAL standard (just like the XCCDF standard actually) entails a number of major components, I'm going to focus on the OVAL definitions. Be aware though that the results of an OVAL scan are also standardized format, as are results of XCCDF scans for instance. OVAL definitions have 4 to 5 blocks in them: - the definition itself, which describes what is being validated and how. It refers to one or more tests that are to be executed or validated for the definition result to be calculated - the test or tests, which are referred to by the definition. In each test, there is at least a reference to an object (what is being tested) and optionally to a state (what should the object look like) - the object , which is a unique representation of a resource or resources on the system (a file, a process, a mount point, a kernel parameter, etc.). Object definitions can refer to multiple resources, depending on the definition. - the state , which is a sort-of value mapping or validation that needs to be applied to an object to see if it is configured correctly - the variable , an optional definition which is what it sounds like, a variable that substitutes an abstract definition with an actual definition, allowing to write more reusable tests. Let's get an example going, but without the XML structure, so in human language. We want to define that the Kerberos definition on a Linux system should allow forwardable tickets by default. This is accomplished by ensuring that, inside the /etc/krb5.conf file (which is an INI-style configuration file), the value of the forwardable key inside the [libdefaults] section is set to true . In OVAL, the definition itself will document the above in human readable text, assign it a unique ID (like oval:com.example.oval:def:1 ) and mark it as being a definition for configuration validation ( compliance ). Then, it defines the criteria that need to be checked in order to properly validate if the rule is applicable or not. These criteria include validation if the OVAL statement is actually being run on a Linux system (as it makes no sense to run it against a Cisco router) which is Kerberos enabled, and then the criteria of the file check itself. Each criteria links to a test. The test of the file itself links to an object and a state. There are a number of ways how we can check for this specific case. One is that the object is the forwardable key in the [libdefaults] section of the /etc/krb5.conf file, and the state is the value true . In this case, the state will point to those two entries (through their unique IDs) and define that the object must exist, and all matches must have a matching state. The \"all matches\" here is not that important, because there will generally only be one such definition in the /etc/krb5.conf file. Note however that a different approach to the test can be declared as well. We could state that the object is the [libdefaults] section inside the /etc/krb5.conf file, and the state is the value true for the forwardable key. In this case, the test declares that multiple objects must exist, and (at least) one must match the state. As you can see, the OVAL language tries to map definitions to unambiguous definitions. So, how does this look like in OVAL XML? The OVAL XML structure The full example contains a few more entries than those we declare next, in order to be complete. The most important definitions though are documented below. Let's start with the definition. As stated, it will refer to tests that need to match for the definition to be valid. <definitions> <definition id=\"oval:com.example.oval:def:1\" version=\"1\" class=\"compliance\"> <metadata> <title>libdefaults.forwardable in /etc/krb5.conf must be set to true</title> <affected family=\"unix\"> <platform>Red Hat Enterprise Linux 7</platform> </affected> <description> By default, tickets obtained from the Kerberos environment must be forwardable. </description> </metadata> <criteria operator=\"AND\"> <criterion test_ref=\"oval:com.example.oval:tst:1\" comment=\"Red Hat Enterprise Linux is installed\"/> <criterion test_ref=\"oval:com.example.oval:tst:2\" comment=\"/etc/krb5.conf's libdefaults.forwardable is set to true\"/> </criteria> </definition> </definitions> The first thing to keep in mind is the (weird) identification structure. Just like with XCCDF, it is not sufficient to have your own id convention. You need to start an id with oval: followed by the reverse domain definition (here com.example.oval ), followed by the type ( def for definition) and a sequence number. Also, take a look at the criteria. Here, two tests need to be compliant (hence the AND operator). However, more complex operations can be done as well. It is even allowed to nest multiple criteria, and refer to previous definitions, like so (taken from the ssg-rhel6-oval.xml file : <criteria comment=\"package hal removed or service haldaemon is not configured to start\" operator=\"OR\"> <extend_definition comment=\"hal removed\" definition_ref=\"oval:ssg:def:211\"/> <criteria operator=\"AND\" comment=\"service haldaemon is not configured to start\"> <criterion comment=\"haldaemon runlevel 0\" test_ref=\"oval:ssg:tst:212\"/> <criterion comment=\"haldaemon runlevel 1\" test_ref=\"oval:ssg:tst:213\"/> <criterion comment=\"haldaemon runlevel 2\" test_ref=\"oval:ssg:tst:214\"/> <criterion comment=\"haldaemon runlevel 3\" test_ref=\"oval:ssg:tst:215\"/> <criterion comment=\"haldaemon runlevel 4\" test_ref=\"oval:ssg:tst:216\"/> <criterion comment=\"haldaemon runlevel 5\" test_ref=\"oval:ssg:tst:217\"/> <criterion comment=\"haldaemon runlevel 6\" test_ref=\"oval:ssg:tst:218\"/> </criteria> </criteria> Next, let's look at the tests. <tests> <unix:file_test id=\"oval:com.example.oval:tst:1\" version=\"1\" check_existence=\"all_exist\" check=\"all\" comment=\"/etc/redhat-release exists\"> <unix:object object_ref=\"oval:com.example.oval:obj:1\" /> </unix:file_test> <ind:textfilecontent54_test id=\"oval:com.example.oval:tst:2\" check=\"all\" check_existence=\"all_exist\" version=\"1\" comment=\"The value of forwardable in /etc/krb5.conf\"> <ind:object object_ref=\"oval:com.example.oval:obj:2\" /> <ind:state state_ref=\"oval:com.example.oval:ste:2\" /> </ind:textfilecontent54_test> </tests> There are two tests defined here. The first test just checks if /etc/redhat-release exists. If not, then the test will fail and the definition itself will result to false (as in, not compliant). This isn't actually a proper definition, because you want the test to not run when it is on a different platform, but for the sake of example and simplicity, let's keep it as is. The second test will check for the value of the forwardable key in /etc/krb5.conf . For it, it refers to an object and a state. The test states that all objects must exist ( check_existence=\"all_exist\" ) and that all objects must match the state ( check=\"all\" ). The object definition looks like so: <objects> <unix:file_object id=\"oval:com.example.oval:obj:1\" comment=\"The /etc/redhat-release file\" version=\"1\"> <unix:filepath>/etc/redhat-release</unix:filepath> </unix:file_object> <ind:textfilecontent54_object id=\"oval:com.example.oval:obj:2\" comment=\"The forwardable key\" version=\"1\"> <ind:filepath>/etc/krb5.conf</ind:filepath> <ind:pattern operation=\"pattern match\">&#94;\\s*forwardable\\s*=\\s*((true|false))\\w*</ind:pattern> <ind:instance datatype=\"int\" operation=\"equals\">1</ind:instance> </ind:textfilecontent54_object> </objects> The first object is a simple file reference. The second is a text file content object. More specifically, it matches the line inside /etc/krb5.conf which has forwardable = true or forwardable = false in it. An expression is made on it, so that we can refer to the subexpression as part of the test. This test looks like so: <states> <ind:textfilecontent54_state id=\"oval:com.example.oval:ste:2\" version=\"1\"> <ind:subexpression datatype=\"string\">true</ind:subexpression> </ind:textfilecontent54_state> </states> This test refers to a subexpression, and wants it to be true . Testing the checks with Open-SCAP The Open-SCAP tool is able to test OVAL statements directly. For instance, with the above definition in a file called oval.xml : ~$ oscap oval eval --results oval-results.xml oval.xml Definition oval:com.example.oval:def:1: true Evaluation done. The output of the command shows that the definition was evaluated successfully. If you want more information, open up the oval-results.xml file which contains all the details about the test. This results file is also very useful while developing OVAL as it shows the entire result of objects, tests and so forth. For instance, the /etc/redhat-release file was only checked to see if it exists, but the results file shows what other parameters can be verified with it as well: <unix-sys:file_item id=\"1233781\" status=\"exists\"> <unix-sys:filepath>/etc/redhat-release</unix-sys:filepath> <unix-sys:path>/etc</unix-sys:path> <unix-sys:filename>redhat-release</unix-sys:filename> <unix-sys:type>regular</unix-sys:type> <unix-sys:group_id datatype=\"int\">0</unix-sys:group_id> <unix-sys:user_id datatype=\"int\">0</unix-sys:user_id> <unix-sys:a_time datatype=\"int\">1515186666</unix-sys:a_time> <unix-sys:c_time datatype=\"int\">1514927465</unix-sys:c_time> <unix-sys:m_time datatype=\"int\">1498674992</unix-sys:m_time> <unix-sys:size datatype=\"int\">52</unix-sys:size> <unix-sys:suid datatype=\"boolean\">false</unix-sys:suid> <unix-sys:sgid datatype=\"boolean\">false</unix-sys:sgid> <unix-sys:sticky datatype=\"boolean\">false</unix-sys:sticky> <unix-sys:uread datatype=\"boolean\">true</unix-sys:uread> <unix-sys:uwrite datatype=\"boolean\">true</unix-sys:uwrite> <unix-sys:uexec datatype=\"boolean\">false</unix-sys:uexec> <unix-sys:gread datatype=\"boolean\">true</unix-sys:gread> <unix-sys:gwrite datatype=\"boolean\">false</unix-sys:gwrite> <unix-sys:gexec datatype=\"boolean\">false</unix-sys:gexec> <unix-sys:oread datatype=\"boolean\">true</unix-sys:oread> <unix-sys:owrite datatype=\"boolean\">false</unix-sys:owrite> <unix-sys:oexec datatype=\"boolean\">false</unix-sys:oexec> <unix-sys:has_extended_acl datatype=\"boolean\">false</unix-sys:has_extended_acl> </unix-sys:file_item> Now, this is just on OVAL level. The final step is to link it in the XCCDF file. Referring to OVAL in XCCDF The XCCDF Rule entry allows for a check element, which refers to an automated check for compliance. For instance, the above rule could be referred to like so: <Rule id=\"xccdf_com.example_rule_krb5-forwardable-true\"> <title>Enable forwardable tickets on RHEL systems</title> ... <check system=\"http://oval.mitre.org/XMLSchema/oval-definitions-5\"> <check-content-ref href=\"oval.xml\" name=\"oval:com.example.oval:def:1\" /> </check> </Rule> With this set in the Rule, Open-SCAP can validate it while checking the configuration baseline: ~$ oscap xccdf eval --oval-results --results xccdf-results.xml xccdf.xml ... Title Enable forwardable kerberos tickets in krb5.conf libdefaults Rule xccdf_com.example_rule_krb5-forwardable-tickets Ident RHEL7-01007 Result pass A huge advantage here is that, alongside the detailed results of the run, there is also better human readable output as it shows the title of the Rule being checked. The detailed capabilities of OVAL In the above example I've used two examples: a file validation (against /etc/redhat-release ) and a file content one (against /etc/krb5.conf ). However, OVAL has many more checks and support for it, and also has constraints that you need to be aware of. In the OVAL Project github account, the Language repository keeps track of the current documentation. By browsing through it, you'll notice that the OVAL capabilities are structured based on the target technology that you can check. Right now, this is AIX, Android, Apple iOS, Cisco ASA, Cisco CatOS, VMWare ESX, FreeBSD, HP-UX, Cisco iOS and iOS-XE, Juniper JunOS, Linux, MacOS, NETCONF, Cisco PIX, Microsoft SharePoint, Unix (generic), Microsoft Windows, and independent. The independent one contains tests and support for resources that are often reusable toward different platforms (as long as your OVAL and XCCDF supporting tools can run it on those platforms). A few notable supporting tests are: filehash58_test which can check for a number of common hashes (such as SHA-512 and MD5). This is useful when you want to make sure that a particular (binary or otherwise) file is available on the system. In enterprises, this could be useful for license files, or specific library files. textfilecontent54_test which can check the content of a file, with support for regular expressions. xmlfilecontent_test which is a specialized test toward XML files Keep in mind though that, as we have seen above, INI files specifically have no specialization available. It would be nice if CISecurity would develop support for common textual data formats, such as CSV (although that one is easily interpretable with the existing ones), JSON, YAML and INI. The unix one contains tests specific to Unix and Unix-like operating systems (so yes, it is also useful for Linux), and together with the linux one a wide range of configurations can be checked. This includes support for generic extended attributes ( fileextendedattribute_test ) as well as SELinux specific rules ( selinuxboolean_test and selinuxsecuritycontext_test ), network interface settings ( interface_test ), runtime processes ( process58_test ), kernel parameters ( sysctl_test ), installed software tests (such as rpminfo_test for RHEL and other RPM enabled operating systems) and more.","tags":"Security","url":"https://blog.siphos.be/2018/03/automating-compliance-checks/","loc":"https://blog.siphos.be/2018/03/automating-compliance-checks/"},{"title":"Documenting a rule","text":"In the first post I talked about why configuration documentation is important. In the second post I looked into a good structure for configuration documentation of a technological service, and ended with an XCCDF template in which this documentation can be structured. The next step is to document the rules themselves, i.e. the actual content of a configuration baseline. Fine-grained rules While from a high-level point of view, configuration items could be documented in a coarse-grained manner, a proper configuration baseline documents rules very fine-grained. Let's first consider a bad example: All application code files are root-owned, with read-write privileges for owner and group, and executable where it makes sense. While such a rule could be interpreted correctly, it also leaves room for misinterpretation and ambiguity. Furthermore, it is not explicit. What are application code files? Where are they stored? What about group ownership? The executable permission, when does that make sense? Does the rule also imply that there is no privilege for world-wide access, or does it just ignore that? A better example (or set of examples) would be: /opt/postgresql is recursively user-owned by root /opt/postgresql is recursively group-owned by root No files under /opt/postgresql are executable except when specified further All files in /opt/postgresql/bin are executable /opt/postgresql has system_u:object_r:usr_t:s0 as SELinux context /opt/postgresql/bin/postgres has system_u:object_r:postgresql_exec_t:s0 as SELinux context And even that list is still not complete, but you get the gist. The focus here is to have fine-grained rules which are explicit and not ambiguous. Of course, the above configuration rule is still a \"simple\" permission set. Configuration baselines go further than that of course. They can act on file content (\"no PAM configuration files can refer to pam_rootok.so except for runuser and su\"), run-time processes (\"The processes with /usr/sbin/sshd as command and with -D as option must run within the sshd_t SELinux domain\"), database query results, etc. This granularity is especially useful later on when you want to automate compliance checks, because the more fine-grained a description is, the easier it is to develop and maintain checks on it. But before we look into remediation, let's document the rule a bit further. Metadata on the rules Let's consider the following configuration rule: /opt/postgresql/bin/postgres has system_u:object_r:postgresql_exec_t:s0 as SELinux context In the configuration baseline, we don't just want to state that this is the rule, and be finished. We need to describe the rule in more detail, as was described in the previous post . More specifically, we definitely want to - know the rule's severity is, or how \"bad\" it would be if we detect a deviation from the rule - have an indication if the rule is security-sensitive or more oriented to manageability - a more elaborate description of the rule than just the title - an indication why this rule is in place (what does it solve, fix or simplify) - information on how to remediate if a deviation is found - know if the rule is applicable to our environment or not The severity in the Security Content Automation Protocol (SCAP) standard, which defines the XCCDF standard as well as OVAL and a few others like CVSS, uses the following possible values for severity: unknown, info, low, medium, high. To indicate if a rule is security-oriented or not, XCCDF's role attribute is best used. With the role attribute, you state if a rule is to be included in the final scoring (a weighted value given to the compliance of a system) or not. If it is, then it is security sensitive. The indication of a rule applicability in the environment might seem strange. If you document the configuration baseline, shouldn't it include only those settings you want? Well, yes and no. Personally, I like to include recommendations that we do not follow in the baseline as well. Suppose for instance that an audit comes along and says you need to enable data encryption on the database. Let's put aside that an auditor should focus mainly/solely on the risks, and let the solutions be managed by the team (but be involved in accepting solutions of course), the team might do an assessment and find that data encryption on the database level (i.e. the database files are encrypted so non-DBA users with operating system interactive rights cannot read the data) is actually not going to remediate any risk, yet introduce more complexity. In that situation, and assuming that the auditor agrees with a different control, you might want to add a rule to the configuration baseline about this. Either you document the wanted state (database files do not need to be encrypted), or you document the suggestion (database files should be encrypted) but explicitly state that you do not require or implement it, and document the reasoning for it. The rule is then augmented with references to the audit recommendation for historical reasons and to facilitate future discussions. And yes, I know the rule \"database files should be encrypted\" is still ambiguous. The actual rule should be more specific to the technology). Documenting a rule in XCCDF In XCCDF, a rule is defined through the Rule XML entity, and is placed within a Group . The Group entities are used to structure the document, while the Rule entities document specific configuration directives. The postgres related rule of above could be written as follows: <Rule id=\"xccdf_com.example_rule_pgsql-selinux-context\" role=\"full\" selected=\"1\" weight=\"5.1\" severity=\"high\" cluster-id=\"network\"> <title> /opt/postgresql/bin/postgres has system_u:object_r:postgresql_exec_t:s0 as SELinux context </title> <description> <xhtml:p> The postgres binary is the main binary of the PostgreSQL database daemon. Once started, it launches the necessary workers. To ensure that PostgreSQL runs in the proper SELinux domain (postgresql_t) its binary must be labeled with postgresql_exec_t. </xhtml:p> <xhtml:p> The current state of the label can be obtained using stat, or even more simple, the -Z option to ls: </xhtml:p> <xhtml:pre>~$ ls -Z /opt/postgresql/bin/postgres -rwxr-xr-x. root root system_u:object_r:postgresql_exec_t:s0 /opt/postgresql/bin/postgres </xhtml:pre> </description> <rationale> <xhtml:p> The domain in which a process runs defines the SELinux controls that are active on the process. Services such as PostgreSQL have an established policy set that controls what a database service can and cannot do on the system. </xhtml:p> <xhtml:p> If the PostgreSQL daemon does not run in the postgresql_t domain, then SELinux might either block regular activities of the database (service availability impact), block behavior that impacts its effectiveness (integrity issue) or allow behavior that shouldn't be allowed. The latter can have significant consequences once a vulnerability is exploited. </xhtml:p> </rationale> <fixtext> Restore the context of the file using restorecon or chcon. </fixtext> <fix strategy=\"restrict\" system=\"urn:xccdf:fix:script:sh\">restorecon /opt/postgresql/bin/postgres </fix> <ident system=\"http://example.com/configbaseline\">pgsql-01032</ident> </Rule> Although this is lots of XML, it is easy to see what each element declares. The NIST IR 7275 document is a very good resource to continuously consult in order to find the right elements and their interpretation. There is one element added that is \"specific\" to the content of this blog post series and not the XCCDF standard, namely the identification. As mentioned in an earlier post, organizations might have their own taxonomy for technical service identification, and requirements on how to number or identify rules. In the above example, the rule is identified as pgsql-01032 . There is another attribute in use above that might need more clarification: the weight of the rule. Abusing CVSS for configuration weight scoring In the above example, a weight is given to the rule scoring (weight of 5.1). This number is obtained through a CVSS calculator , which is generally used to identify the risk of a security issue or vulnerability. CVSS stands for Common Vulnerability Scoring System and is a popular way to weight security risks (which are then associated with vulnerability reports, Common Vulnerabilities and Exposures (CVE) ). Misconfigurations can also be slightly interpreted as a security risk, although it requires some mental bridges. Rather than scoring the rule, you score the risk that it mitigates, and consider the worst thing that could happen if that rule is not implemented correctly. Now, worst-case thinking is subjective, so there will always be discussion on the weight of a rule. It is therefore important to have a consensus in the team (if the configuration baseline is team-owned) if this weight is actively used. Of course, an organization might choose to ignore the weight, or use a different scoring mechanism. In the above situation, I scored what would happen if a vulnerability in PostgreSQL was successfully exploited, and SELinux couldn't mitigate the risk as the label of the file was wrong. The result of a wrong label could be that the PostgreSQL service runs in a higher privileged domain, or even in an unconfined domain (no SELinux restrictions active), so there is a heightened risk of confidentiality loss (beyond the database) and even integrity risk. However, the confidentiality risk is scored as low, and integrity even in between (base risk is low, but due to other constraints put in place integrity impact is reduced further) because PostgreSQL runs as a non-administrative user on the system, and perhaps because the organization uses dedicated systems for database hosting (so other services are not easily impacted). As mentioned, this is somewhat abusing the CVSS methodology, but is imo much more effective than trying to figure out your own scoring methodology. With CVSS, you start with scoring the risk regardless of context (CVSS Base), then adjust based on recent state or knowledge (CVSS Temporal), and finally adjust further with knowledge of the other settings or mitigating controls in place (CVSS Environmental). Personally, I prefer to only use the CVSS Base scoring for configuration baselines, because the other two are highly depending on time (which is, for documentation, challenging) and the other controls (which is more of a concern for service technical documentation). So in my preferred situation, the rule would be scored as 5.4 rather than 5.1. But that's just me. Isn't this CCE? People who use SCAP a bit more might already be thinking if I'm not reinventing the wheel here. After all, SCAP also has a standard called Common Configuration Enumeration (CCE) which seems to be exactly what I'm doing here: enumerating the configuration of a technical service. And indeed, if you look at the CCE list you'll find a number of Excel sheets (sigh) that define common configurations. For instance, for Red Hat Enterprise Linux v5, there is an enumeration identified as CCE-4361-2, which states: File permissions for /etc/pki/tls/ldap should be set correctly The CCE description then goes on stating that this is a permission setting (CCE Parameter), which can be rectified with chmod (CCE Technical Mechanism), and refers to a source for the setting. However, CCE has a number of downsides. First of all, it isn't being maintained anymore. And although XCCDF itself is also a quite old standard, it is still being looked into (a draft new version is being prepared) and is actively used as a standard. Red Hat is investing time and resources into secure configurations and compliancy aligned with SCAP, and other vendors publish SCAP-specific resources as well. CCE however would be a list, and thus requires continuous management. That RHELv5 is the most recent RHEL CCE list is a bad thing. Second, CCE's structure is for me insufficient to use in configuration baselines. XCCDF has a much more mature and elaborate set of settings for this. What CCE does is actually what I use in the above example as the organization-specific identifier. Finally, there aren't many tools that actively use CCE, unlike CVSS, XCCDF, OVAL, CVSS and other standards under the SCAP umbrella, which are all still actively used and developed upon by tools such as Open-SCAP. Profiling Before finishing this post, I want to talk about profiling. Within an XCCDF benchmark, several profiles can be defined. In the XCCDF template I defined a single profile that covers all rules, but this can be fine-tuned to the needs of the organization. In XCCDF profiles, you can select individual rules (which ones are active for a profile and which ones aren't) and even fine-tune values for rules. This is called tailoring in XCCDF. A first use case for profiles is to group different rules based on the selected setup. In case of Nginx for instance, one can consider Nginx being used as either a reverse proxy, a static website hosting or a dynamic web application hosting. In all three cases, some rules will be the same, but several rules will be different. Within XCCDF, you can document all rules, and then use profiles to group the rules related to a particular service use. XCCDF allows for profile inheritance. This means that you can define a base Profile (all the rules that need to be applied, regardless of the service use) and then extend the profiles with individual rule selections. With profiles, you can also fine-tune values. For instance, you could have a password policy in place that states that passwords on internal machines have to be at least 10 characters long, but on DMZ systems they need to be at least 15 characters long. Instead of defining two rules, the rule could refer to a particular variable (Value in XCCDF) which is then selected based on the Profile. The value for a password length is then by default 10, but the Profile for DMZ systems selects the other value (15). Now, value-based tailoring is imo already a more advanced use of XCCDF, and is best looked into when you also start using OVAL or other automated checks. The tailoring information is then passed on to the automated compliance check so that the right value is validated. Value-based tailoring also makes rules either more complex to write, or ambiguous to interpret without full profile awareness. Considering the password length requirement, the rule could become: The /etc/pam.d/password-auth file must refer to pam_passwdqc.so for the password service with a minimal password length of 10 (default) or 15 (DMZ) for the N4 password category At least the rule is specific. Another approach would be to document it as follows: The /etc/pam.d/password-auth file must refer to pam_passwdqc.so with the proper organizational password controls The documentation of the rule might document the proper controls further, but the rule is much less specific. Later checks might report that a system fails this check, referring to the title, which is insufficient for engineers or administrators to resolve. Generating the guide To close off this post, let's finish with how to generate the guide based on an XCCDF document. Personally, I use two approaches for this. The first one is to rely on Open-SCAP. With Open-SCAP, you can generate guides easily: ~$ oscap xccdf generate guide xccdf.xml > ConfigBaseline.html The second one, which I use more often, is a custom XSL style sheet, which also introduces the knowledge and interpretations of what this blog post series brings up (including the organizational identification). The end result is similar (the same content) but uses a structure/organization that is more in line with expectations. For instance, in my company, the information security officers want to have a tabular overview of all the rules in a configuration baseline. So the XSL style sheet generates such a tabular overview, and uses in-documenting linking to the more elaborate descriptions of all the rules. An older version is online for those interested. It uses JavaScript as well (in case you are security sensitive you might want to look into it) to allow collapsing rule documentation for faster online viewing. The custom XSL has an additional advantage, namely that there is no dependency on Open-SCAP to generate the guides (even though it is perfectly possible to copy the XSL and continue). I can successfully generate the guide using Microsoft's msxml utility, using xsltproc, etc depending on the platform I'm on.","tags":"Security","url":"https://blog.siphos.be/2018/01/documenting-a-rule/","loc":"https://blog.siphos.be/2018/01/documenting-a-rule/"},{"title":"Structuring a configuration baseline","text":"A good configuration baseline has a readable structure that allows all stakeholders to quickly see if the baseline is complete, as well as find a particular setting regardless of the technology. In this blog post, I'll cover a possible structure of the baseline which attempts to be sufficiently complete and technology agnostic. If you haven't read the blog post on documenting configuration changes , it might be a good idea to do so as it declares the scope of configuration baselines and why I think XCCDF is a good match for this. Chaptered documentation As mentioned previously, a configuration baseline describes the configuration of a particular technological service (rather than a business service which is an integrated set of technologies and applications). To document and maintain the configuration state of the technology, I suggest the following eight chapters (to begin with): Architecture Operating system and services Software deployment and file system Technical service settings Authentication, authorization, access control and auditing Service specific settings Cryptographic services Data and information handling Within each chapter, sections can be declared depending on how the technology works. For instance, for database technologies one can have a distinction between system-wide settings, instance-specific settings and even database-specific settings. Or, if the organization has specific standards on user definitions, a chapter on \"User settings\" can be used. The above is just a suggestion in an attempt to cover most aspects of a configuration baseline. With the sections of the chapter, rules are then defined which specify the actual configuration setting (or valid range) applicable to the technology. But the rule goes further than just a single-line configuration setting description. Each rule should have a unique identifier so that other documents can reliably link to the rules in the document. Although XCCDF has a convention for this, I feel that the XCCDF way here is more useful for technical referencing while the organization is better off with a more human addressable approach. So while a rule in XCCDF has the identifier xccdf_com.example.postgresql_rule_selinux-enforcing the human addressable identifier would be postgresql_selinux-enforcing or even postgresql-00001 . In the company that I work for, we already have a taxonomy for services and a decision to use numerical identifiers on the configuration baseline rules. Each rule should be properly described, documenting what the rule is for. In case of a ranged value, it should also document how this range can be properly applied. For instance, if the number of worker threads is based on the number of cores available in the system, document the formula. Each rule should also document the risk that it wants to mitigate (be it a security risk, or a manageability aspect of the service, or a performance related tuning parameter). This aspect of the baseline is important whenever an implementation wants an exception to the rule (not follow it) or a deviation (different value). Personally, to make sure that the baseline is manageable, I don't expect engineers to immediately fill in the risk in great detail, but rather holistically. The actual risk determination is then only done when an implementation wants an exception or deviation, and then includes a list of potential mitigating actions to take. This way, a 300+ rule document does not require all 300+ rules to have a risk determination, especially if only a dozen or so rules have exceptions or deviations in the organization. Each rule should have sources linked to it. These sources help the reader understand what the rule is based on, such as a publicly available secure configuration baseline, an audit recommendation, a specific incident, etc. If the rule is also controversial, it might benefit from links to meeting minutes. Each rule might have consequences listed as well. These are known changes or behavior aspects that follow the implementation of the rule. For instance, a rule might state that TLS mutual authentication is mandatory, and the consequence is that all interacting clients must have a properly defined certificate (so proper PKI requirements) as well as client registration in the application. Finally, and importantly as well, each rule identifies the scope at which exceptions or deviations can be granted. For smaller groups and organizations, this might not matter that much, but for larger organizations, some configuration baseline rules can be \"approved\" by a small team or by the application owner, while others need formal advise of a security officer and approval on a decision body. Finding a balanced approval hierarchy The exception management for configuration baselines should not be underestimated. It is not viable to have all settings handled by top management decision bodies, but some configuration changes might result in such a huge impact that a formal decision needs to be taken somewhere, with proper accountability assigned (yes, this is the architect in me speaking). Rather than attempting to create a match for all rules, I again like to keep the decision here in the middle, just like I do with the risk determination. The maintainer of the configuration baseline can leave the \"scope\" of a rule open, and have an intermediate decision body as the main decision body. Whenever an exception or deviation is asked, the risk determination is made and filled in, and with this documented rule now complete a waiver is asked on the decision body. Together with the waiver request, the maintainer also asks this decision body if the rule in the future also needs to be granted on that decision body or elsewhere. The scope is most likely tied to the impact of the rule towards other services. A performance specific rule that only affects the application hosted on the technology can be easily scoped as being application-only. This means that the application or service owner can decide to deviate from the baseline. A waiver for a rule that influences system behavior might need to be granted by the system administrator (or team) as well as application or service owners that use this system. Following this logic, I generally use the following scope terminology: tbd (to be determined), meaning that there is no assessment done yet application, meaning that the impact is only on a single application and thus can be taken by the application owner instance, meaning that the impact is on an instance and thus might be broader than a single application, but is otherwise contained to the technology. Waivers are granted by the responsible system administrator and application owner(s) system, meaning that the impact is on the entire system and thus goes beyond the technology. Waivers are granted by the responsible system administrator, application owner(s) and with advise from a security officer network, meaning that the impact can spread to other systems or influence behavior of other systems, but remains technical in nature. Waivers are granted by an infrastructure architecture board with advise from a security officer organization, meaning that the impact goes beyond technical influence but also impacts business processes. Waivers are granted by an architecture board with advise from a security officer and senior service owner, and might even be redirected to a higher management board. group, meaning that the impact influences multiple businesses. Waivers are granted by a specific management board Each scope also has a \"-pending\" value, so \"network-pending\". This means that the owner of the configuration baseline suggests that this is the scope on which waivers can be established, but still needs to receive formal validation. The main decision body is then a particular infrastructure architecture board, which will redirect requests to other decision bodies if the scope goes beyond what that architecture board handles. Architectural settings The first chapter in a baseline is perhaps the more controversial one, as it is not a technical setting and hard to validate. However, in my experience, tying architectural constraints in a configuration baseline is much more efficient than having a separate track for a number of reasons. For one, I strongly believe that architecture deviations are like configuration deviations. They should be documented similarly, and follow the same path as configuration baseline deviations. The scope off architectural rules are also all over the place, from application-level impact up to organization-wide. Furthermore, architectural positioning of services should not be solely an (infrastructure) architecture concern, but supported by the other stakeholders as well, and especially the responsible for the technology service. For instance, a rule could be that no databases should be positioned within an organizations DeMilitarized Zone (DMZ) , which is a network design that shields off internally positioned services from the outside world. Although this is not a configuration setting, it makes sense to place it in the configuration baseline of the database technology. There are several ways to validate automatically if this rule is followed, depending for instance the organization IP plan. Another rule could be that web applications that host browser-based applications should only be linked through a reverse proxy, or that a load balancer must be put in front of an application server, etc. This might result in additional rules in the chapter that covers access control as well (such as having a particular IP filter in place), but these rules are the consequence of the architectural positioning of the service. Operating system and services The second chapter covers settings specific to the operating system on which the technology is deployed. Such settings can be system-wide settings like Linux' sysctl parameters, services which need to be enabled or disabled when the technology is deployed, and deviations from the configuration baseline of the operating system. An example of the latter depends of course on the configuration baseline of the operating system (assuming this is a baseline for a technology deployed on top of an operating system, it could very well be a different platform). Suppose for instance that the baseline has the squashfs kernel module disabled, but the technology itself requires squashfs, then a waiver is needed. This is the level where this is documented. Another setting could be an extension of the SSH configuration (the term \"services\" in the chapter title here focuses on system services, such as OpenSSH), or the implementation of additional audit rules on OS-level (although auditing can also be covered in a different section). Software deployment and file system The third chapter focuses on the installation of the technology itself, and the file system requirements related to the technology service. Rules here look into file ownership and permissions, mount settings, and file system declarations. Some baselines might even define rules about integrity of certain files (the Open Vulnerability and Assessment Language (OVAL) supports checksum-based validations) although I think this is better tackled through a specific integrity process. Still, if such an integrity process does not exist and automated validation of baselines is implemented, then integrity validation of critical files could be in scope. Technical service settings In the fourth chapter, settings are declared regarding the service without being service-specific. A service-specific setting is one that requires functional knowledge of the service, whereas technical service settings can be interpreted without functionally understanding the technology at hand. Let's take PostgreSQL as an example. A service-specific setting would be the maximum number of non-frozen transaction IDs before a VACUUM operation is triggered (the autovacuum_freeze_max_age parameter). If you are not working with PostgreSQL much, then this makes as much sense as Prisencolinensinainciusol . It sounds like English, but that's about as far as you get. A technical service setting on PostgreSQL that is likely more understandable is the runtime account under which the database runs (you don't want it to run as root), or the TCP port on which it listens. Although both are technical in nature, they're much more understandable for others and, perhaps the most important reason of all, often more reusable in deployments across technologies. This reusability is key for larger organizations as they will have numerous technologies to support, and the technical service settings offer a good baseline for initial secure setup. They focus on the runtime account of the service, the privileges of the runtime account (be it capability-based on Linux or account rights on Windows), the interfaces on which the service is reachable, the protocol or protocols it supports, etc. Authentication, authorization, access control and auditing The next chapter focuses on the Authentication, Authorization and Accounting (AAA) services, but slightly worded differently (AAA is commonly used in networking related setups, I just borrow it and extend it). If the configuration baseline is extensive, then it might make sense to have separate sections for each of these security concepts. Some technologies have a strong focus on user management as well. In that case, it might make sense to first describe the various types of users that the technology supports (like regular users, machine users, internal service users, shared users, etc.) and then, per user type, document how these security services act on it. Service specific settings The next chapter covers settings that are very specific to the service. These are often the settings that are found in the best practices documentation, secure deployment instructions of the vendor, performance tuning parameters, etc. I tend to look first at the base configuration and administration guides for technologies, and see what the main structure is that those documents follow. Often, this can be borrowed for the configuration baseline. Next, consider performance related tuning, as that is often service specific and not related to the other chapters. Cryptographic services In this chapter, the focus is on the cryptographic services and configuration. The most well-known example here is related to any TLS configuration and tuning. Whereas the location of the private key (used for TLS services) is generally mentioned in the third chapter (or at least the secure storage of the private key), this section will focus on using this properly. It looks at selecting proper TLS version, making a decent and manageable set of ciphers to support, enabling Online Certificate Status Protocol (OCSP) on web servers, etc. But services often use cryptographic related algorithms in various other places as well. Databases can provide transparent data file encryption to ensure that offline access to the database files does not result in data leakage for instance. Or they implement column-level encryption. Application servers might support crypto related routines to the applications they host, and the configuration baseline can then identify which crypto modules are supported and which ones aren't. Services might be using cryptographic hashes which are configurable, or could be storing user passwords in a database using configurable settings. OpenLDAP for instance supports multiple hashing methods (and also supports storing in plain-text if you want this), so it makes sense to select a hashing method that is hard to brute-force (slow to compute for instance) and is salted (to make certain types of attacks more challenging). If the service makes use of stored credentials or keytabs, document how they are protected here as well. Data and information handling Information handling covers both the regular data management activities (like backup/restore, data retention, archival, etc.) as well as sensitive information handling (to comply with privacy rules). The regular data management related settings look into both the end user data handling (as far as this is infrastructurally related - this isn't meant to become a secure development guide) as well as service-internal data handling. When the technology is meant to handle data (like a database or LDAP) then certain related settings could be both in the service specific settings chapter or in this one. Personally, I tend to prefer that technology-specific and non-reusable settings are in the former, while the data and information handling chapter covers the integration and technology-agnostic data handling. If the service handles sensitive information, it is very likely that additional constraints or requirements were put in place beyond the \"traditional\" cryptographic requirements. Although such requirements are often implemented on the application level (like tagging the data properly and then, based on the tags, handle specific fine-grained access controls, archival and data retention), more and more technologies provide out-of-the-box (or at least reusable) methods that can be configured. An XCCDF template To support the above structure, I've made an XCCDF template that might be a good start for documenting the configuration baseline of a technology. It also structures the chapters a bit more with various sections, but those are definitely not mandatory to use as it strongly depends on the technology being documented, the maturity of the organization, etc.","tags":"Security","url":"https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/","loc":"https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"},{"title":"Documenting configuration changes","text":"IT teams are continuously under pressure to set up and maintain infrastructure services quickly, efficiently and securely. As an infrastructure architect, my main concerns are related to the manageability of these services and the secure setup. And within those realms, a properly documented configuration setup is in my opinion very crucial. In this blog post series, I'm going to look into using the Extensible Configuration Checklist Description Format (XCCDF) as the way to document these. This first post is an introduction to XCCDF functionally, and what I position it for. Documentation is a good thing With the ongoing struggle for time and resources, documenting configurations and architectures is often not top-of-mind. However, the lack of this information also leads to various problems: incidents due to misconfiguration, slow recovery timings due to incomprehensible setups, and not to forget: meetings. Yes, meetings, which are continuously discussing service aspects that influence one or more parameters, without any good traceability of past decisions. Some technologies allow to keep track of some metadata regarding to configurations. In configuration management tools like Puppet or Saltstack engineers define the target state of their infrastructure, and the configuration management tool enforces this state on the service. Engineers can add in historical information as comments into these systems, and use version control on the files to have traceability of the settings. However, although in-line comments are very important, even for configuration sets, it is not a full documentation approach. In larger environments, where you are regularly audited for quality and security, or where multiple roles and stakeholders need to understand the settings and configuration of services, pointing to the code is not going to cut it. Configuration items need to be documented not solely with the documentation rule itself, but with the motivation related to it, and additional fields of interest depending on how the organization deals with it. This documentation can then be referred to from the configuration management infrastructure (so engineers and technical stakeholders can trace back settings) but also vice-versa: the documentation can refer to the configuration management implementation (so other stakeholders can deduce how the settings are implemented or even enforced). With a proper configuration document at hand, especially if it is supported through the configuration management tool(s) in the organization (regardless if it is one or multiple), it is much easier to have the necessary interviews with auditors, project leaders, functional and technical analysts, architects or even remote support teams. Two-part documentation hierarchy The first thing to decide upon is at which level a team will document the settings. Is a single document possible for all infrastructure services? Most likely not. I believe that settings should be documented on the technology level (as it is specific to a particular technology) and on the 'business service' level (as it is specific to a particular implementation). On the technology level, we're talking about configuration documentation for \"PostgreSQL\", \"Apache Knox\" or \"Nginx\". At this level, the baseline is defined for a technology. The resulting document is then the configuration baseline for that component. On the business service level, we're talking about configuration documentation for a particular service that is a combination of multiple implementations. For instance, a company intranet portal service is operationally implemented through a reverse proxy (HAProxy), an intelligent load balancer (Seesaw), next-gen firewall (pfSense), web server (Nginx), application server (Node.js), database (PostgreSQL), and operating systems (Linux). And more technologies come into play when we consider software deployment, monitoring, backup/restore, software-defined network, storage provisioning, archival solutions, license management services, etc. Hence, a configuration document should be available on this service level (\"company intranet portal\") which defines the usage profile of a service (more about that later) and the specific parameters related to this service, but only when they either deviate from the configuration baseline, or take a particular value within a range defined in the configuration baseline. This document is the service technical configuration . So, as an example, on the Nginx configuration baseline, a rule might state that the maximum file size per upload is 12M (through the client_max_body_size parameter). If the service has no problem with this rule, then it does not need to be documented on the service technical configuration. However, if this needs to be adapted (say that for the company portal the maximum file size is 64M) then it is documented. Another example is a ranged setting, where the baseline identifies a set of valid values and the service technical configuration makes a particular selection. For instance, the Nginx configuration baseline might mention that there must be between 5 and 50 worker processes (through the worker_processes parameter). In the service technical configuration the particular value is then selected. From an architecture and security point of view, the first example is a deviation which must consider the risks and consequences that are applicable to the rule. These are (or should be) documented in the configuration baseline, including where this deviation can be approved (assuming the organization has a decision body for such things). The second example is not a deviation and, as such, is free to be chosen by the implementation team. The configuration baseline will generally inform the implementation teams about how to pick a proper value. Service usage profiles I've talked about a service usage profile earlier up, but didn't expand on it yet. So, what are these service usage profiles? Well, most technologies can be implemented for a number of targets and functional purposes. A database could be implemented as a dedicated service (one set of databases on a dedicated set of instances for a single business service) or a shared service (multiple databases, possibly on multiple instances for several business services). It can be tuned for online transactional purposes (OLTP) or online analytical processing (OLAP), often through data warehouse designs. A service usage profile is part of the configuration baseline, with settings specific to that particular usage. So for a database the engineering team responsible for the database technology setup might devise that the following usage profiles are applicable to their component: Dedicated OLTP Shared OLTP Dedicated DWH Shared DWH Each usage profile has a number of configuration settings (of which many, if not most, are shared across other usage profiles) and a range of valid values (fine-tuning for a service). The service technical configuration for a particular business service then selects the particular usage profile. For instance, the company intranet portal might use a Dedicated OLTP usage profile for its PostgreSQL database. How XCCDF supports this structure Until now, I've only spoken about the values related to configuration documentation, and a high-level introduction to the hierarchy on the configurations. But how does the Extensible Configuration Checklist Description position itself in this? A number of reasons why XCCDF is a valid choice for configuration documentation are given next. XCCDF allows technical writers to write the documentation in (basic) HTML while still linking the documentation to specific rules. Rather than having to use a tabular expression on all the valid configuration sets (like using a large spreadsheet table for all rules) and trying to force some documentation in it (Excel is not a text editor), XCCDF uses a hierarchical approach to structure documentation in logical sections (which it calls Groups ) and then refers to the rules applicable to that section (using the Rule identifier). XCCDF has out-of-the-box support for service profiles (through Profile declarations). Fine-tuning and selecting profiles is called tailoring in XCCDF. This also includes support for ranged values. XCCDF is meant to (but does not have to) refer to the (automated or interview-based) validation of the rules as well. Automated validation of settings means that an engine can read the XCCDF document (and the referred statements) and check if an implementation adheres to the baseline. The standard for this is called Open Vulnerability and Assessment Language (OVAL) , and a popular free software engine for this is OpenSCAP . The standard for interview-based validation is Open Checklist Interactive Language (OCIL) . I have not played around with OCIL and supporting tooling, so comments on this are always welcome. XCCDF is an XML-based format, so its \"source code\" can easily be versioned in common version control systems like Git. This allows organizations to not only track changes on the documentation, but also have an active development lifecycle management on the configuration documentation. XCCDFs schema implies a set of metadata to be defined during various declarations. It includes support for the Dublin core metadata terms for content, references to link other resources structurally, and most importantly has a wide set of supporting entities for rules (which is the level on which configuration items are documented). This includes the rationale (why is the rule defined as is), fix text (human readable), fix (machine readable), rule role (is it security-sensitive and as such must be taken up in a security assessment report or not), severity (how bad is it if this rule is not followed), and many more. This both forces the user to consider the consequences of the rule, as well as guide the writer into properly structured documentation. XCCDF also suggests a number of classes for the documentation to standardize certain information types. This includes warnings, critical text, examples, and instructions. Such semantic declarations allow for a more uniform set of documentation. However, a few constraints exist that you need to be aware of when approaching XCCDF. XCCDF is an XML-based document format, and although NIST offers the necessary XML Schema definitions, writing proper XML has always been a challenge for many people. Also, no decent GUI or WYSIWYG tool that manages XCCDF files exists in my opinion. Yes, we have the SCAP Workbench and the eSCAPe editor , but I feel that they are not as effective as they should be. As a result, the team or teams that write the baselines should either be XML-savvy, or you need to provide supporting infrastructure and services for it. However, YMMV. If the organization is not interested in compliance checks themselves (i.e. automated validation of adherence to the configuration baseline and service technical configuration) then XCCDF will entail too much overhead versus just having a template or approach (such as documenting items in a wiki). However, with some support (and perhaps automation) writing and maintaining XCCDF based configuration baselines becomes much easier. More resources In the past I've blogged about XCCDF already, but that was with a previous blog technology and the migration wasn't as successful as I originally thought. XML snippets were all removed, and I'm too lazy to go back to my backups from 2013 and individually correct blogs. A good resource on XCCDF is the NIST IR-7275 publication (PDF) which covers the XCCDF standard in much detail. The Center for Internet Security (CISecurity) maintains more than a hundred CIS Benchmarks , all available for free as PDFs, and are often based on XCCDF (available to subscribed members). In the next blog post, I'll talk about the in-document structure of a good configuration baseline.","tags":"Security","url":"https://blog.siphos.be/2018/01/documenting-configuration-changes/","loc":"https://blog.siphos.be/2018/01/documenting-configuration-changes/"},{"title":"SELinux and extended permissions","text":"One of the features present in the August release of the SELinux user space is its support for ioctl xperm rules in modular policies. In the past, this was only possible in monolithic ones (and CIL). Through this, allow rules can be extended to not only cover source (domain) and target (resource) identifiers, but also a specific number on which it applies. And ioctl's are the first (and currently only) permission on which this is implemented. Note that ioctl-level permission controls isn't a new feature by itself, but the fact that it can be used in modular policies is. What is ioctl? Many interactions on a Linux system are done through system calls. From a security perspective, most system calls can be properly categorized based on who is executing the call and what the target of the call is. For instance, the unlink() system call has the following prototype: int unlink(const char *pathname); Considering that a process (source) is executing unlink (system call) against a target (path) is sufficient for most security implementations. Either the source has the permission to unlink that file or directory, or it hasn't. SELinux maps this to the unlink permission within the file or directory classes: allow <domain> <resource> : { file dir } unlink; Now, ioctl() is somewhat different. It is a system call that allows device-specific operations which cannot be expressed by regular system calls. Devices can have multiple functions/capabilities, and with ioctl() these capabilities can be interrogated or updated. It has the following interface: int ioctl(int fd, unsigned long request, ...); The file descriptor is the target device on which an operation is launched. The second argument is the request, which is an integer whose value identifiers what kind of operation the ioctl() call is trying to execute. So unlike regular system calls, where the operation itself is the system call, ioctl() actually has a parameter that identifies this. A list of possible parameter values on a socket for instance is available in the Linux kernel source code, under include/uapi/linnux/sockios.h . SELinux allowxperm For SELinux, having the purpose of the call as part of a parameter means that a regular mapping isn't sufficient. Allowing ioctl() commands for a domain against a resource is expressed as follows: allow <domain> <resource> : <class> ioctl; This of course does not allow policy developers to differentiate between harmless or informative calls (like SIOCGIFHWADDR to obtain the hardware address associated with a network device) and impactful calls (like SIOCADDRT to add a routing table entry). To allow for a fine-grained policy approach, the SELinux developers introduced an extended allow permission, which is capable of differentiating based on an integer value. For instance, to allow a domain to get a hardware address (SIOCGIFHWADDR, which is 0x8927) from a TCP socket: allowxperm <domain> <resource> : tcp_socket ioctl 0x8927; This additional parameter can also be ranged: allowxperm <domain> <resource> : <class> ioctl 0x8910-0x8927; And of course, it can also be used to complement (i.e. allow all ioctl parameters except a certain value): allowxperm <domain> <resource> : <class> ioctl ~0x8927; Small or negligible performance hit According to a presentation given by Jeff Vander Stoep on the Linux Security Summit in 2015, the performance impact of this addition in SELinux is well under control, which helped in the introduction of this capability in the Android SELinux implementation. As a result, interested readers can find examples of allowxperm invocations in the SELinux policy in Android, such as in the app.te file: # only allow unprivileged socket ioctl commands allowxperm { appdomain -bluetooth } self:{ rawip_socket tcp_socket udp_socket } ioctl { unpriv_sock_ioctls unpriv_tty_ioctls }; And with that, we again show how fine-grained the SELinux access controls can be.","tags":"SELinux","url":"https://blog.siphos.be/2017/11/selinux-and-extended-permissions/","loc":"https://blog.siphos.be/2017/11/selinux-and-extended-permissions/"},{"title":"SELinux Userspace 2.7","text":"A few days ago, Jason \"perfinion\" Zaman stabilized the 2.7 SELinux userspace on Gentoo. This release has quite a few new features , which I'll cover in later posts, but for distribution packagers the main change is that the userspace now has many more components to package. The project has split up the policycoreutils package in separate packages so that deployments can be made more specific. Let's take a look at all the various userspace packages again, learn what their purpose is, so that you can decide if they're needed or not on a system. Also, when I cover the contents of a package, be aware that it is based on the deployment on my system, which might or might not be a complete installation (as with Gentoo, different USE flags can trigger different package deployments). libsepol - manipulating SELinux binary policies The first package, known in Gentoo as sys-libs/libsepol , is the library that enables manipulating the SELinux binary policies. This is a core library, and is the first SELinux userspace package that is installed on a system. It contains one command, chkcon , which allows users to validate if a specific security context exists within a binary policy file: ~$ chkcon policy.29 user_u:user_r:mozilla_t:s0 user_u:user_r:mozilla_t:s0 is valid The package does contain two manpages of old commands which are no longer available (or I'm blind, either way, they're not installed and not found in the SELinux userspace repository either) such as genpolusers and genpolbools. libselinux - the main SELinux handling library The libselinux library, known in Gentoo as sys-libs/libselinux , is the main SELinux library. Almost all applications that are SELinux-aware (meaning they not only know SELinux is a thing, but are actively modifying their behavior with SELinux-specific code) will link to libselinux. Because it is so core, the package also provides the necessary bindings for different scripting languages besides the standard shared objects approach, namely Python (as many SELinux related tooling is written in Python) and Ruby. Next to the bindings and libraries, libselinux also offers quite a few executables to query and manipulate SELinux settings on the system, which are shortly described on the SELinux userspace wiki but repeated here for convenience. Most of these are meant for debugging purposes, as they are simple wrappers toward the libselinux provided functions, but some of them are often used by administrations. avcstat gives statistics about the in-kernel access vector cache, such as number of lookups, hits and misses compute_create queries the kernel security server for a transition decision compute_av queries the kernel security server for an access vector decision compute_relabel queries the kernel security server for a relabel decision compute_member queries the kernel security server for a labeling decision on a polyinstantiated object getconlist uses the security\\_compute\\_user() function, and orders the resulting list based on the default\\_contexts file and per-user context files getdefaultcon is like getconlist but only returns the first context compute_user queries the kernel security server fo a set of reachable user contexts from a source context getfilecon gets the context of a file by path getpidcon gets the context of a process by PID getseuser queries the seuser file for the resulting SELinux user and contxt for a particular linux login and login context getsebool gets the current state of a SELinux boolean in the SELinux security server matchpathcon queries the active filecontext file for how a particular path should be labeled policyvers queries the kernel security server for the maximum policy version supported getenforce gets the enforcing state of the kernel access vector cache sefcontext_compile generates binary filecontext files, optimized for fast querying selabel_lookup looks up what the target default context is for various classes (supporting the X related SELinux types, database types, etc.) selabel_digest calculates the SHA1 digest of spec files, and returns a list of the specfiles used to calculate the digest. This is used by Android. selabel_partial_match determines if a direct or partial match is possible on a file path selabel_lookup_best_match obtains the best matching SELinux security context for file-based operations selinux_check_securetty_context checks whether a SELinux tty security context is defined as a securetty context selinux_check_access checks if the source context has the access permission for the specified class on the target context selinuxexeccon reports the SELinux context for an executable selinuxenabled returns if SELinux is enabled or not setfilecon sets the context of a path setenforce sets the enforcing state of the kernel access vector cache togglesebool toggles a SELinux boolean, but only runtime (so it does not persist across reboots) checkpolicy - policy compiler The checkpolicy package, known in Gentoo as sys-apps/checkpolicy , provides two main applications, checkpolicy and checkmodule . Both applications are compilers (unlike what the name implies) which build a binary SELinux policy. The main difference between these two is that one builds a policy binary, whereas the other one builds a SELinux module binary. Developers don't often call these applications themselves, but use the build scripts. For instance, the semodule_package binary would be used to combine the binary policy with additional files such as file contexts. libsemanage - facilitating use of SELinux overall The libsemanage library, known in Gentoo as sys-libs/libsemanage , contains SELinux supporting functions that are needed for any regular SELinux use. Whereas libselinux would be used everywhere, even for embedded systems, libsemanage is generally not for embedded systems but is very important for Linux systems in overall. Most SELinux management applications that administrators come in contact with will be linked with the libsemanage library. As can be expected, the semanage application as offered by the selinux-python package is one of them. The only application that is provided by libsemanage is the semanage_migrate_store , used to migrate the policy store from the /etc/selinux to the /var/lib/selinux location. This was done with the introduction of the 2.4 userspace. selinux-python - Python-based command-line management utilities The selinux-python package, known in Gentoo as sys-apps/selinux-python , is one of the split packages that originally where part of the policycoreutils package. It contains the majority of management utilities that administrators use for handling SELinux on their systems. The most known application here is semanage , but it contains quite a few others as well: sepolgen generates an initial SELinux policy module template, and is short for the sepolicy generate command audit2why translates SELinux audit messages into a description of why the access was denied. It is short for the audit2allow -w command. audit2allow generates SELinux policy allow/dontaudit rules from logs of denied operations sepolgen-ifgen generates an overview of available interfaces. This overview is used by audit2allow to guess the right interface to use when allowing or dontauditing certain operations. sepolicy is the SELinux policy inspection tool, allowing to query various aspects of a SELinux configuration (namely booleans, communication flows, interfaces, network information and transition information). It also provides the ability to generate skeleton policies (as described with sepolgen ) and manual pages. chcat changes a file's SELinux security category sepolgen-ifgen-attr-helper generates an overview of attributes and attribute mappings. This overview is used by audit2allow to guess the right attribute to use when allowing or dontauditing certain operations. semanage is a SELinux policy management tool, allowing a multitude of operations against the SELinux policy and the configuration. This includes definition import/export, login mappings, user definitions, ports and interface management, module handling, file contexts, booleans and more. semodule-utils - Developing SELinux modules The semodule-utils package, known in Gentoo as sys-apps/semodule-utils , is another split package that originally was part of the policycoreutils package. In it, SELinux policy module development utilities are provided. The package is not needed for basic operations such as loading and unloading modules though. semodule_expand expands a SELinux base module package into a kernel binary policy file semodule_deps shows the dependencies between SELinux policy packages semodule_link links SELinux policy module packages together into a single SELinux policy module semodule_unpackage extracts a SELinux module into the binary policy and its associated files (such as file context definitions) semodule_package combines a modular binary policy file with its associated files (such as file context definitions) into a module package mcstrans - Translate context info in human readable names The mcstrans package, known in Gentoo as sys-apps/mcstrans , is another split package that originally was part of the policycoreutils package. In it, the MCS translation daemon is hosted. This daemon translates the SELinux-specific context ranges, like s0-s0:c0.c1024 to a human-readable set, like SystemLow-SystemHigh . This is a purely cosmetic approach (as SELinux internally always uses the sensitivity and category numbers) but helps when dealing with a large number of separate categories. restorecond - Automatically resetting file contexts The restorecond package, known in Gentoo as sys-apps/restorecond , is another split package that originally was part of the policycoreutils package. It contains the restorecond daemon, which watches over files and directories and forces the right SELinux label on it. This daemon was originally intended to resolve a missing feature in SELinux (having more fine-grained rules for label naming) but with the named file transition support, the need for this daemon has diminished a lot. secilc - SELinux common intermediate language compiler The secilc package, known in Gentoo as sys-apps/secilc , is the CIL compiler which builds kernel binary policies based on the passed on CIL code. Although the majority of policy development still uses the more traditional SELinux language (and supporting macro's from the reference policy), developers can already use CIL code for policy generation. With secilc , a final policy file can be generated through the CIL code. selinux-dbus - SELinux DBus server The selinux-dbus package (not packaged in Gentoo at this moment) provides a SELinux DBus service which systems can use to query and interact with SELinux management utilities on the system. If installed, the org.selinux domain is used for various supported operations (such as listing SELinux modules, through org.selinux.semodule_list ). selinux-gui - Graphical SELinux settings manager The selinux-gui package (not packaged in Gentoo at this moment) provides the system-config-selinux application which offers basic SELinux management support in a graphical application. It supports boolean handling, file labeling, user mapping, SELinux user management, network port definitions and module handling. As such, it can be seen as the graphical helper utility for the semanage command. selinux-sandbox - Sandbox utility utilizing SELinux sandbox domains The selinux-sandbox package (not packaged in Gentoo at this moment) is a set of scripts to facilitate the creation of SELinux sandboxes. With these utilities, which not only use SELinux sandbox domains like sandbox_t but also Linux namespaces, end users can launch applications in a restricted environment. policycoreutils - Core SELinux management utilities The policycoreutils package, known in Gentoo as sys-apps/policycoreutils , contains basic SELinux tooling which is necessary to handle SELinux in a regular environment. Supported utilities are: newrole to switch a user session from one role to another secon to query the SELinux context of a file, program or user input genhomedircon to regenerate home directory context files, necessary when new users are defined on the system setfiles to set SELinux file security contexts on resources semodule to list, load and unload SELinux modules run_init to launch an init script in the right domain open_init_pty to run a program under a pseudo terminal with the right context set sestatus to query current policy status setsebool to set and, if wanted, persist a SELinux boolean value selinuxconfig to display the current active configuration paths restorecon to set SELinux file security contexts on resources load_policy to load the SELinux policy, generally called from initramfs systems if the init system is not SELinux-aware restorecon_xattr manages the security.restorecon_last extended attribute which is set by setfiles or restorecon Gentoo also adds in two additional scripts: * rlpkg to reset file contexts on files provided by a Gentoo package * selocal to easily handle small SELinux rule additions to the active policy There are even more Attentive readers will notice that the setools package is not discussed here. This package is not provided by the SELinux userspace project, but is an important package for SELinux policy developers as it contains the sesearch command - an often used command to query the active policy. The above list is thus a picture of the SELinux userspace utilities, which is becoming quite a big application set now that some functionality is split off from the policycoreutils package.","tags":"SELinux","url":"https://blog.siphos.be/2017/09/selinux-userspace-2.7/","loc":"https://blog.siphos.be/2017/09/selinux-userspace-2.7/"},{"title":"Authenticating with U2F","text":"In order to further secure access to my workstation, after the switch to Gentoo sources , I now enabled two-factor authentication through my Yubico U2F USB device. Well, at least for local access - remote access through SSH requires both userid/password as well as the correct SSH key, by chaining authentication methods in OpenSSH . Enabling U2F on (Gentoo) Linux is fairly easy. The various guides online which talk about the pam_u2f setup are indeed correct that it is fairly simple. For completeness sake, I've documented what I know on the Gentoo Wiki, as the pam_u2f article . The setup, basically The setup of U2F is done in a number of steps: 1. Validate that the kernel is ready for the USB device 2. Install the PAM module and supporting tools 3. Generate the necessary data elements for each user (keys and such) 4. Configure PAM to require authentication through the U2F key For the kernel, the configuration item needed is the raw HID device support. Now, in current kernels, two settings are available that both talk about raw HID device support: CONFIG_HIDRAW is the general raw HID device support, while CONFIG_USB_HIDDEV is the USB-specific raw HID device support. It is very well possible that only a single one is needed, but both where active on my kernel configuration already, and Internet sources are not clear which one is needed, so let's assume for now both are. Next, the PAM module needs to be installed. On Gentoo, this is a matter of installing the pam\\_u2f package, as the necessary dependencies will be pulled in automatically: ~# emerge pam_u2f Next, for each user, a registration has to be made. This registration is needed for the U2F components to be able to correctly authenticate the use of a U2F key for a particular user. This is done with pamu2fcfg : ~$ pamu2fcfg -u<username> > ~/.config/Yubico/u2f_keys The U2F USB key must be plugged in when the command is executed, as a succesful keypress (on the U2F device) is needed to complete the operation. Finally, enable the use of the pam\\_u2f module in PAM. On my system, this is done through the /etc/pam.d/system-local-login PAM configuration file used by all local logon services. auth required pam_u2f.so Consider the problems you might face When fiddling with PAM, it is important to keep in mind what could fail. During the setup, it is recommended to have an open administrative session on the system so that you can validate if the PAM configuration works, without locking yourself out of the system. But other issues need to be considered as well. My Yubico U2F USB key might have a high MTBF (Mean Time Between Failures) value, but once it fails, it would lock me out of my workstation (and even remote services and servers that use it). For that reason, I own a second one, safely stored, but is a valid key nonetheless for my workstation and remote systems/services. Given the low cost of a simple U2F key, it is a simple solution for this threat. Another issue that could come up is a malfunction in the PAM module itself. For me, this is handled by having remote SSH access done without this PAM module (although other PAM modules are still involved, so a generic PAM failure itself wouldn't resolve this). Of course, worst case, the system needs to be rebooted in single user mode. One issue that I faced was the SELinux policy. Some applications that provide logon services don't have the proper rights to handle U2F, and because PAM just works in the address space (and thus SELinux domain) of the application, the necessary privileges need to be added to these services. My initial investigation revealed the following necessary policy rules (refpolicy-style); udev_search_pids(...) udev_read_db(...) dev_rw_generic_usb_dev(...) The first two rules are needed because the operation to trigger the USB key uses the udev tables to find out where the key is located/attached, before it interacts with it. This interaction is then controlled through the first rule. Simple yet effective Enabling the U2F authentication on the system is very simple, and gives a higher confidence that malicious activities through regular accounts will have it somewhat more challenging to switch to a more privileged session (one control is the SELinux policy of course, but for those domains that are allowed to switch then the PAM-based authentication is another control), as even evesdropping on my password (or extracting it from memory) won't suffice to perform a successful authentication. If you want to use a different two-factor authentication, check out the use of the Google authenticator , another nice article on the Gentoo wiki. It is also possible to use Yubico keys for remote authentication, but that uses the OTP (One Time Password) functionality which isn't active on the Yubico keys that I own.","tags":"Security","url":"https://blog.siphos.be/2017/09/authenticating-with-u2f/","loc":"https://blog.siphos.be/2017/09/authenticating-with-u2f/"},{"title":"Using nVidia with SELinux","text":"Yesterday I've switched to the gentoo-sources kernel package on Gentoo Linux. And with that, I also attempted (succesfully) to use the propriatary nvidia drivers so that I can enjoy both a smoother 3D experience while playing minecraft, as well as use the CUDA support so I don't need to use cloud-based services for small exercises. The move to nvidia was quite simple, as the nvidia-drivers wiki article on the Gentoo wiki was quite easy to follow. Signing the modules One difference I found with the article (which I've promply changed) is that the signing command, necessary to sign the Linux kernel modules so that they can be loaded (as unsigned or wrongly signed modules are not allowed on the system), was different. It used to be as follows (example for a single module, it had to be repeated for each affected kernel module): ~# perl /usr/src/linux/scripts/sign-file sha512 \\ /usr/src/linux/signing_key.priv \\ /usr/src/linux/signing_key.x509 \\ /lib/modules/4.12.5-gentoo/video/nvidia-uvm.ko However, from version 4.3.3 onward (as also explained by this excellent Signed kernel module support article on the Gentoo wiki) this command no longer uses a Perl script, but is an ELF binary. Also, the location of the default signing key is moved into a certs/ subdirectory. Enabling nvidia device files When the nvidia modules are loaded, additional device files are enabled. One is the nvidia0 character device file, while the other is the nvidiactl character device file. And although I can imagine that the nvidiactl one is a control-related device file, I don't exactly know for sure. However, attempts to use 3D applications showed (through SELinux denials) that access to these device files is needed. Without that, applications just crashed, like so: org.lwjgl.LWJGLException: X Error - disp: 0x7fd164907b00 serial: 150 error: BadValue (integer parameter out of range for operation) request_code: 153 minor_code: 24 at org.lwjgl.opengl.LinuxDisplay.globalErrorHandler(LinuxDisplay.java:320) at org.lwjgl.opengl.LinuxContextImplementation.nCreate(Native Method) at org.lwjgl.opengl.LinuxContextImplementation.create(LinuxContextImplementation.java:51) at org.lwjgl.opengl.ContextGL.<init>(ContextGL.java:132) at org.lwjgl.opengl.Display.create(Display.java:850) at org.lwjgl.opengl.Display.create(Display.java:757) at org.lwjgl.opengl.Display.create(Display.java:739) at bib.at(SourceFile:635) at bib.aq(SourceFile:458) at bib.a(SourceFile:404) at net.minecraft.client.main.Main.main(SourceFile:123) Not really useful to debug for me, but the SELinux denials were a bit more obvious, showing requests for read and write to the nvidiactl character device. Thanks to matchpathcon I found out that the device files had to have the xserver_misc_device_t type (which they didn't have to begin with, as the device files were added after the automated restorecon was done on the /dev location). So, adding the following command to my local init script fixed the context setting at boot up: restorecon /dev/nvidiactl /dev/nvidia0 Also, the domains that needed to use nVidia had to receive the following addition SELinux-policy-wise: dev_rw_xserver_misc(...) Perhaps this can be made more fine-grained (as there are several other device files marked as xserver_misc_device_t ) but for now this should suffice. Optimus usage with X server The other challenge I had was that my workstation uses an integrated Intel device, and offloads calculations and rendering to nVidia. The detection by X server did not work automatically though, and it took some fiddling to get it to work. In the end, I had to add in an nvidia.conf file inside /etc/X11/xorg.conf.d with the following content: Section \"ServerLayout\" Identifier \"layout\" Screen 0 \"nvidia\" Inactive \"intel\" EndSection Section \"Device\" Identifier \"nvidia\" Driver \"nvidia\" BusID \"PCI:1:0:0\" EndSection Section \"Screen\" Identifier \"nvidia\" Device \"nvidia\" Option \"AllowEmptyInitialConfiguration\" EndSection Section \"Device\" Identifier \"intel\" Driver \"modesetting\" EndSection Section \"Screen\" Identifier \"intel\" Device \"intel\" EndSection And with a single xrandr command I re-enabled split screen support (as by default it now showed the same output on both screens): ~$ xrandr --output eDP-1-1 --left-of HDMI-1-2 I also had to set the output source to the nVidia device, by adding the following lines to my ~/.xinitrc file: xrandr --setprovideroutputsource modesetting NVIDIA-0 xrandr --auto And with that, another thing was crossed off from my TODO list. Which has become quite large after my holidays (went to Kos, Greece) as I had many books and articles on my ebook reader with me, which inspired a lot.","tags":"SELinux","url":"https://blog.siphos.be/2017/08/using-nvidia-with-selinux/","loc":"https://blog.siphos.be/2017/08/using-nvidia-with-selinux/"},{"title":"Switch to Gentoo sources","text":"You've might already read it on the Gentoo news site, the Hardened Linux kernel sources are removed from the tree due to the grsecurity change where the grsecurity Linux kernel patches are no longer provided for free. The decision was made due to supportability and maintainability reasons. That doesn't mean that users who want to stick with the grsecurity related hardening features are left alone. Agostino Sarubbo has started providing sys-kernel/grsecurity-sources for the users who want to stick with it, as it is based on minipli's unofficial patchset . I seriously hope that the patchset will continue to be maintained and, who knows, even evolve further. Personally though, I'm switching to the Gentoo sources, and stick with SELinux as one of the protection measures. And with that, I might even start using my NVidia graphics card a bit more, as that one hasn't been touched in several years (I have an Optimus-capable setup with both an Intel integrated graphics card and an NVidia one, but all attempts to use nouveau for the one game I like to play - minecraft - didn't work out that well). How secure is Gentoo sources with SELinux? It is hard to just say that one kernel tree or another is safe(r) or not. Security is not something one can get with a simple check-list. It is a matter of properly configuring services and systems, patching it when needed, limiting expoosure and what not. A huge advantage of grsecurity was that it had very insightful and advanced protection measures (many of them focusing on memory-related attacks), and prevented unwanted behavior from applications (and users) in a very fine-grained manner. With SELinux, I can still prevent some unwanted behavior, but it is important to know that SELinux and grsecurity's kernel hardening features are orthogonal to each other. It is only the grsecurity RBAC model that is somewhat in competition with SELinux. SELinux is able to define and manage behavior between types. However, within a single type, many actions are not governed at all. SELinux can manage which types (domains) are able to invoke which system calls, but once a call is allowed, SELinux doesn't do any additional controls anymore. Loosing protection controls from grsecurity, as a security activist, is not something I like. But on the other hand, I need to consider the wide SELinux using audience in Gentoo, who is most likely going to switch to the gentoo sources as well (at least the majority of them). Gentoo sources is not insecure by itself, as are many other kernel sources. A huge advantage is that the gentoo sources are well maintained, so any kernel vulnerability that gets reported and fixed will receive the proper fix in the Gentoo sources quickly as well (and if you think it can go even faster, consider becoming a Gentoo security padawan . And with SELinux enabled, some additional security controls can be implemented (the efficacy of it depends on the quality of the policy). The Kernel Self Protection Project also aims to improve the Linux kernel security, and immediately through upstreamed and accepted patches. This means that the protection measures, once in the kernel, should remain inside (awkward regressions notwithstanding). I truly hope that the KSPP moves forward. In the mean time, read up on the Kernel Self-Protection document to learn more about how to harden the Linux kernel. So that's it, just one less security control? For now, there is no immediate substitute. But that doesn't mean that there is nothing one can do to increase the secure state of a Linux desktop, workstation or even IoT device. Although remotely executable exploits do pop up and exist, many vulnerabilities in the Linux kernel are mainly exploitable through a local access pattern. That means that vulnerabilities often can only be exploited through a local invocation (or through chaining by using other vulnerabilities - often in completely different applications or services - in order to execute the local malware). Hence, hardening of the entire system is extremely important. Previously, I had an account with multiple SELinux roles assigned to it. Depending on what I wanted to do, I transitioned to the right role (either through the newrole command, or through sudo which has integrated SELinux support). With the switch to the gentoo sources, I decided to make it a bit harder for malware on my system to work: i start using separate Linux accounts depending on the purpose (which I call persona). Developing SELinux policies is now done on a separate account, managing remote systems through another account (although my servers use multi-factor authentication so there was already some additional safeguard in place there), handling my side-work with another account, playing games with another account, etc. It isn't that I don't trust SELinux for this (as each domain is well isolated and controlled). But SELinux cannot prevent vulnerabilities within applications if the action/result of a succesfully exploited vulnerability does not change the expected behavior of the application versus the other resources on the system (and even there, the fine-grained approach of policies might not even be sufficiently fine-grained, as SELinux uses labels, and many resources have the same label assigned). Suppose some malware is able to capture me giving in my password, or is trying to phish for it. By using separate accounts (with separate passphrazes of course) the impact is reduced a bit. Other things on the plate The change to different accounts was one thing I wanted to establish before switching to a new kernel tree. There are other aspects that I want to investigate in the near future as well though. First of all, I'm probably going to enable U2F authentication on my workstation as well for all interactive accounts. It has been on my list for quite some time, and quickly going through the publicly available fora doesn't reveal any major challenges to do so. Build the PAM module, update the PAM service configurations and you're done. Hopefully. ;-) Next, I'm going to play around a bit with AddressSanitizer . ASAN was incompatible with grsecurity, but now that that's out of the way, there's no reason not to investigate it further. I am not going to enable it for the kernel though (as some KSPP implemented measures are incompatible with ASAN as well), and probably not for my complete workstation yet (even though it is sufficiently powerful to handle the major performance impact). I'm going to put some more focus on Integrity Measurement Architecture support , although my main protection measure with IMA - the TPM or Trusted Platform Module - has been fried (don't ask) so I can't use it anymore. Perhaps I'm going to buy a very lightweight/small system with a TPM on it to continue development. We'll see. My current knowledge of seccomp is fairly theoretical (with a few hands-on tutorials, but that's it). It has been on my TODO list for some time to look in more depth to it. Perhaps this is the right time.","tags":"Gentoo","url":"https://blog.siphos.be/2017/08/switch-to-gentoo-sources/","loc":"https://blog.siphos.be/2017/08/switch-to-gentoo-sources/"},{"title":"Project prioritization","text":"This is a long read, skip to \"Prioritizing the projects and changes\" for the approach details... Organizations and companies generally have an IT workload (dare I say, backlog?) which needs to be properly assessed, prioritized and taken up. Sometimes, the IT team(s) get an amount of budget and HR resources to \"do their thing\", while others need to continuously ask for approval to launch a new project or instantiate a change. Sizeable organizations even require engineering and development effort on IT projects which are not readily available: specialized teams exist, but they are governance-wise assigned to projects. And as everyone thinks their project is the top-most priority one, many will be disappointed when they hear there are no resources available for their pet project. So... how should organizations prioritize such projects? Structure your workload, the SAFe approach A first exercise you want to implement is to structure the workload, ideas or projects. Some changes are small, others are large. Some are disruptive, others are evolutionary. Trying to prioritize all different types of ideas and changes in the same way is not feasible. Structuring workload is a common approach. Changes are grouped in projects, projects grouped in programs, programs grouped in strategic tracks. Lately, with the rise in Agile projects, a similar layering approach is suggested in the form of SAFe. In the Scaled Agile Framework a structure is suggested that uses, as a top-level approach, value streams. These are strategically aligned steps that an organization wants to use to build solutions that provide a continuous flow of value to a customer (which can be internal or external). For instance, for a financial service organization, a value stream could focus on 'Risk Management and Analytics'. SAFe full framework overview, picture courtesy of www.scaledagileframework.com The value streams are supported through solution trains, which implement particular solutions. This could be a final product for a customer (fitting in a particular value stream) or a set of systems which enable capabilities for a value stream. It is at this level, imo, that the benefits exercises from IT portfolio management and benefits realization management research plays its role (more about that later). For instance, a solution train could focus on an 'Advanced Analytics Platform'. Within a solution train, agile release trains provide continuous delivery for the various components or services needed within one or more solutions. Here, the necessary solutions are continuously delivered in support of the solution trains. At this level, focus is given on the culture within the organization (think DevOps), and the relatively short-lived delivery delivery periods. This is the level where I see 'projects' come into play. Finally, you have the individual teams working on deliverables supporting a particular project. SAFe is just one of the many methods for organization and development/delivery management. It is a good blueprint to look into, although I fear that larger organizations will find it challenging to dedicate resources in a manageable way. For instance, how to deal with specific expertise across solutions which you can't dedicate to a single solution at a time? What if your organization only has two telco experts to support dozens of projects? Keep that in mind, I'll come back to that later... Get non-content information about the value streams and solutions Next to the structuring of the workload, you need to obtain information about the solutions that you want to implement (keeping with the SAFe terminology). And bear in mind that seemingly dull things such as ensuring your firewalls are up to date are also deliverables within a larger ecosystem. Now, with information about the solutions, I don't mean the content-wise information, but instead focus on other areas. Way back, in 1952, Harry Markowitz introduced Modern portfolio theory as a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk (quoted from Wikipedia). This was later used in an IT portfolio approach by McFarlan in his Portfolio Approach to Information Systems article, published in September 1981. There it was already introduced that risk and return shouldn't be looked at from an individual project viewpoint, but how it contributes to the overall risk and return. A balance, if you wish. His article attempts to categorize projects based on risk profiles on various areas. Personally, I see the suggested categorization more as a way of supporting workload assessments (how many mandays of work will this be), but I digress. Since then, other publications came up which tried to document frameworks and methodologies that facilitate project portfolio prioritization and management. The focus often boils down to value or benefits realization. In The Information Paradox John Thorp comes up with a benefits realization approach, which enables organizations to better define and track benefits realization - although it again boils down on larger transformation exercises rather than the lower-level backlogs. The realm of IT portfolio management and Benefits realization management gives interesting pointers as to the lecture part of prioritizing projects. Still, although one can hardly state the resources are incorrect, a common question is how to make this tangible. Personally, I tend to view the above on the value stream level and solution train level. Here, we have a strong alignment with benefits and value for customers, and we can leverage the ideas of past research. The information needed at this level often boils down to strategic insights and business benefits, coarse-grained resource assessments, with an important focus on quality of the resources. For instance, a solution delivery might take up 500 days of work (rough estimation) but will also require significant back-end development resources. Handling value streams and solutions As we implement this on the highest level in the structure, it should be conceivable that the overview of the value streams (a dozen or so) and solutions (a handful per value stream) is manageable, and something that at an executive level is feasible to work with. These are the larger efforts for structuring and making strategic alignment. Formal methods for prioritization are generally not implemented or described. In my company, there are exercises that are aligning with SAFe, but it isn't company-wide. Still, there is a structure in place that (within IT) one could map to value streams (with some twisting ;-) and, within value streams, there are structures in place that one could map to the solution train exercises. We could assume that the enterprise knows about its resources (people, budget ...) and makes a high-level suggestion on how to distribute the resources in the mid-term (such as the next 6 months to a year). This distribution is challenged and worked out with the value stream owners. See also \"lean budgeting\" in the SAFe approach for one way of dealing with this. There is no prioritization of value streams. The enterprise has already made its decision on what it finds to be the important values and benefits and decided those in value streams. Within a value stream, the owner works together with the customers (internal or external) to position and bring out solutions. My experience here is that prioritization is generally based on timings and expectations from the customer. In case of resource contention, the most challenging decision to make here is to put a solution down (meaning, not to pursue the delivery of a solution), and such decisions are hardly taken. Prioritizing the projects and changes In the lower echelons of the project portfolio structure, we have the projects and changes. Let's say that the levels here are projects (agile release trains) and changes (team-level). Here, I tend to look at prioritization on project level, and this is the level that has a more formal approach for prioritization. Why? Because unlike the higher levels, where the prioritization is generally quality-oriented on a manageable amount of streams and solutions, we have a large quantity of projects and ideas. Hence, prioritization is more quantity-oriented in which formal methods are more efficient to handle. The method that is used in my company uses scoring criteria on a per-project level. This is not innovative per se, as past research has also revealed that project categorization and mapping is a powerful approach for handling project portfolio's. Just look for \"categorizing priority projects it portfolio\" in Google and you'll find ample resources. Kendal's Advanced Project Portfolio Management and the PMO (book) has several example project scoring criteria's. But allow me to explain our approach. It basically is like so: Each project selects three value drivers (list decided up front) For the value drivers, the projects check if they contribute to it slightly (low), moderately (medium) or fully (high) The value drivers have weights, as do the values. Sum the resulting products to get a priority score Have the priority score validated by a scoring team Let's get to the details of it. For the IT projects within the infrastructure area (which is what I'm active in), we have around 5 scoring criteria (value drivers) that are value-stream agnostic, and then 3 to 5 scoring criteria that are value-stream specific. Each scoring criteria has three potential values: low (2), medium (4) and high (9). The numbers are the weights that are given to the value. A scoring criteria also has a weight. For instance, we have a scoring criteria on efficiency (read: business case) which has a weight of 15, so a score of medium within that criteria gives a total value of 60 (4 times 15). The potential values here are based on the \"return on investment\" value, with low being a return less than 2 years, medium within a year, and high within a few months (don't hold me on the actual values, but you get the idea). The sum of all values gives a priority score. Now, hold your horses, because we're not done yet. There is a scoring rule that says a project can only be scored by at most 3 scoring criteria. Hence, project owners need to see what scoring areas their project is mostly visible in, and use those scoring criteria. This rule supports the notion that people don't bring around ideas that will fix world hunger and make a cure for cancer, but specific, well scoped ideas (the former are generally huge projects, while the latter requires much less resources). OK, so you have a score - is that your priority? No. As a project always falls within a particular value stream, we have a \"scoring team\" for each value stream which does a number of things. First, it checks if your project really belongs in the right value stream (but that's generally implied) and has a deliverable that fits the solution or target within that stream. Projects that don't give any value or aren't asked by customers are eliminated. Next, the team validates if the scoring that was used is correct: did you select the right values (low, medium or high) matching the methodology for said criteria? If not, then the score is adjusted. Finally, the team validates if the resulting score is perceived to be OK or not. Sometimes, ideas just don't map correctly on scoring criteria, and even though a project has a huge strategic importance or deliverable it might score low. In those cases, the scoring team can adjust the score manually. However, this is more of a fail-safe (due to the methodology) rather than the norm. About one in 20 projects gets its score adjusted. If too many adjustments come up, the scoring team will suggest a change in methodology to rectify the situation. With the score obtained and validated by the scoring team, the project is given a \"go\" to move to the project governance. It is the portfolio manager that then uses the scores to see when a project can start. Providing levers to management Now, these scoring criteria are not established from a random number generator. An initial suggestion was made on the scoring criteria, and their associated weights, to the higher levels within the organization (read: the people in charge of the prioritization and challenging of value streams and solutions). The same people are those that approve the weights on the scoring criteria. If management (as this is often the level at which this is decided) feels that business case is, overall, more important than risk reduction, then they will be able to put a higher value in the business case scoring than in the risk reduction. The only constraint is that the total value of all scoring criteria must be fixed. So an increase on one scoring criteria implies a reduction on at least one other scoring criteria. Also, changing the weights (or even the scoring criteria themselves) cannot be done frequently. There is some inertia in project prioritization: not the implementation (because that is a matter of following through) but the support it will get in the organization itself. Management can then use external benchmarks and other sources to gauge the level that an organization is at, and then - if needed - adjust the scoring weights to fit their needs. Resource allocation in teams Portfolio managers use the scores assigned to the projects to drive their decisions as to when (and which) projects to launch. The trivial approach is to always pick the projects with the highest scores. But that's not all. Projects can have dependencies on other projects. If these dependencies are \"hard\" and non-negotiable, then the upstream project (the one being dependent on) inherits the priority of the downstream project (the one depending on the first) if the downstream project has a higher priority. Soft dependencies however need to validate if they can (or have to) wait, or can implement workarounds if needed. Projects also have specific resource requirements. A project might have a high priority, but if it requires expertise (say DBA knowledge) which is unavailable (because those resources are already assigned to other ongoing projects) then the project will need to wait (once resources are fully allocated and the projects are started, then they need to finish - another reason why projects have a narrow scope and an established timeframe). For engineers, operators, developers and other roles, this approach allows them to see which workload is more important versus others. When their scope is always within a single value stream, then the mentioned method is sufficient. But what if a resource has two projects, each of a different value stream? As each value stream has its own scoring criteria it can use (and weight), one value stream could systematically have higher scores than others... Mixing and matching multiple value streams To allow projects to be somewhat comparable in priority values, an additional rule has been made in the scoring methodology: value streams must have a comparable amount of scoring criteria (value drivers), and the total value of all criteria must be fixed (as was already mentioned before). So if there are four scoring criteria and the total value is fixed at 20, then one value stream can have its criteria at (5,3,8,4) while another has it at (5,5,5,5). This is still not fully adequate, as one value stream could use a single criteria with the maximum amount (20,0,0,0). However, we elected not to put in an additional constraint, and have management work things out if the situation ever comes out. Luckily, even managers are just human and they tend to follow the notion of well-balanced value drivers. The result is that two projects will have priority values that are currently sufficiently comparable to allow cross-value-stream experts to be exchangeable without monopolizing these important resources to a single value stream portfolio. Current state The scoring methodology has been around for a few years already. Initially, it had fixed scoring criteria used by three value streams (out of seven, the other ones did not use the same methodology), but this year we switched to support both value stream agnostic criteria (like in the past) as well as value stream specific ones. The methodology is furthest progressed in one value stream (with focus of around 1000 projects) and is being taken up by two others (they are still looking at what their stream-specific criteria are before switching).","tags":"Architecture","url":"https://blog.siphos.be/2017/07/project-prioritization/","loc":"https://blog.siphos.be/2017/07/project-prioritization/"},{"title":"Structuring infrastructural deployments","text":"Many organizations struggle with the all-time increase in IP address allocation and the accompanying need for segmentation. In the past, governing the segments within the organization means keeping close control over the service deployments, firewall rules, etc. Lately, the idea of micro-segmentation, supported through software-defined networking solutions, seems to defy the need for a segmentation governance. However, I think that that is a very short-sighted sales proposition. Even with micro-segmentation, or even pure point-to-point / peer2peer communication flow control, you'll still be needing a high level overview of the services within your scope. In this blog post, I'll give some insights in how we are approaching this in the company I work for. In short, it starts with requirements gathering, creating labels to assign to deployments, creating groups based on one or two labels in a layered approach, and finally fixating the resulting schema and start mapping guidance documents (policies) toward the presented architecture. As always, start with the requirements From an infrastructure architect point of view, creating structure is one way of dealing with the onslaught in complexity that is prevalent within the wider organizational architecture. By creating a framework in which infrastructural services can be positioned, architects and other stakeholders (such as information security officers, process managers, service delivery owners, project and team leaders ...) can support the wide organization in its endeavor of becoming or remaining competitive. Structure can be provided through various viewpoints. As such, while creating such framework, the initial intention is not to start drawing borders or creating a complex graph. Instead, look at attributes that one would assign to an infrastructural service, and treat those as labels. Create a nice portfolio of attributes which will help guide the development of such framework. The following list gives some ideas in labels or attributes that one can use. But be creative, and use experienced people in devising the \"true\" list of attributes that fits the needs of your organization. Be sure to describe them properly and unambiguously - the list here is just an example, as are the descriptions. tenant identifies the organizational aggregation of business units which are sufficiently similar in areas such as policies (same policies in use), governance (decision bodies or approval structure), charging, etc. It could be a hierarchical aspect (such as organization) as well. location provides insight in the physical (if applicable) location of the service. This could be an actual building name, but can also be structured depending on the size of the environment. If it is structured, make sure to devise a structure up front. Consider things such as regions, countries, cities, data centers, etc. A special case location value could be the jurisdiction, if that is something that concerns the organization. service type tells you what kind of service an asset is. It can be a workstation, a server/host, server/guest, network device, virtual or physical appliance, sensor, tablet, etc. trust level provides information on how controlled and trusted the service is. Consider the differences between unmanaged (no patching, no users doing any maintenance), open (one or more admins, but no active controlled maintenance), controlled (basic maintenance and monitoring, but with still administrative access by others), managed (actively maintained, no privileged access without strict control), hardened (actively maintained, additional security measures taken) and kiosk (actively maintained, additional security measures taken and limited, well-known interfacing). compliance set identifies specific compliance-related attributes, such as the PCI-DSS compliancy level that a system has to adhere to. consumer group informs about the main consumer group, active on the service. This could be an identification of the relationship that consumer group has with the organization (anonymous, customer, provider, partner, employee, ...) or the actual name of the consumer group. architectural purpose gives insight in the purpose of the service in infrastructural terms. Is it a client system, a gateway, a mid-tier system, a processing system, a data management system, a batch server, a reporting system, etc. domain could be interpreted as to the company purpose of the system. Is it for commercial purposes (such as customer-facing software), corporate functions (company management), development, infrastructure/operations ... production status provides information about the production state of a service. Is it a production service, or a pre-production (final testing before going to production), staging (aggregation of multiple changes) or development environment? Given the final set of labels, the next step is to aggregate results to create a high-level view of the environment. Creating a layered structure Chances are high that you'll end up with several attributes, and many of these will have multiple possible values. What we don't want is to end in an N-dimensional infrastructure architecture overview. Sure, it sounds sexy to do so, but you want to show the infrastructure architecture to several stakeholders in your organization. And projecting an N-dimensional structure on a 2-dimensional slide is not only challenging - you'll possibly create a projection which leaves out important details or make it hard to interpret. Instead, we looked at a layered approach , with each layer handling one or two requirements. The top layer represents the requirement which the organization seems to see as the most defining attribute. It is the attribute where, if its value changes, most of its architecture changes (and thus the impact of a service relocation is the largest). Suppose for instance that the domain attribute is seen as the most defining one: the organization has strict rules about placing corporate services and commercial services in separate environments, or the security officers want to see the commercial services, which are well exposed to many end users, be in a separate environment from corporate services. Or perhaps the company offers commercial services for multiple tenants, and as such wants several separate \"commercial services\" environments while having a single corporate service domain. In this case, part of the infrastructure architecture overview on the top level could look like so (hypothetical example): This also shows that, next to the corporate and commercial interests of the organization, a strong development support focus is prevalent as well. This of course depends on the type of organization or company and how significant in-house development is, but in this example it is seen as a major decisive factor for service positioning. These top-level blocks (depicted as locations, for those of you using Archimate) are what we call \" zones \". These are not networks, but clearly bounded areas in which multiple services are positioned, and for which particular handling rules exist. These rules are generally written down in policies and standards - more about that later. Inside each of these zones, a substructure is made available as well, based on another attribute. For instance, let's assume that this is the architectural purpose. This could be because the company has a requirement on segregating workstations and other client-oriented zones from the application hosting related ones. Security-wise, the company might have a principle where mid-tier services (API and presentation layer exposures) are separate from processing services, and where data is located in a separate zone to ensure specific data access or more optimal infrastructure services. This zoning result could then be depicted as follows: From this viewpoint, we can also deduce that this company provides separate workstation services: corporate workstation services (most likely managed workstations with focus on application disclosure, end user computing, etc.) and development workstations (most likely controlled workstations but with more open privileged access for the developer). By making this separation explicit, the organization makes it clear that the development workstations will have a different position, and even a different access profile toward other services within the company. We're not done yet. For instance, on the mid-tier level, we could look at the consumer group of the services: This separation can be established due to security reasons (isolating services that are exposed to anonymous users from customer services or even partner services), but one can also envision this to be from a management point of view (availability requirements can differ, capacity management is more uncertain for anonymous-facing services than authenticated, etc.) Going one layer down, we use a production status attribute as the defining requirement: At this point, our company decided that the defined layers are sufficiently established and make for a good overview. We used different defining properties than the ones displayed above (again, find a good balance that fits the company or organization that you're focusing on), but found that the ones we used were mostly involved in existing policies and principles, while the other ones are not that decisive for infrastructure architectural purposes. For instance, the tenant might not be selected as a deciding attribute, because there will be larger tenants and smaller tenants (which could make the resulting zone set very convoluted) or because some commercial services are offered toward multiple tenants and the organizations' strategy would be to move toward multi-tenant services rather than multiple deployments. Now, in the zoning structure there is still another layer, which from an infrastructure architecture point is less about rules and guidelines and more about manageability from an organizational point of view. For instance, in the above example, a SAP deployment for HR purposes (which is obviously a corporate service) might have its Enterprise Portal service in the Corporate Services > Mid-tier > Group Employees > Production zone. However, another service such as an on-premise SharePoint deployment for group collaboration might be in Corporate Services > Mid-tier > Group Employees > Production zone as well. Yet both services are supported through different teams. This \"final\" layer thus enables grouping of services based on the supporting team (again, this is an example), which is organizationally aligned with the business units of the company, and potentially further isolation of services based on other attributes which are not defining for all services. For instance, the company might have a policy that services with a certain business impact assessment score must be in isolated segments with no other deployments within the same segment. What about management services Now, the above picture is missing some of the (in my opinion) most important services: infrastructure support and management services. These services do not shine in functional offerings (which many non-IT people generally look at) but are needed for non-functional requirements: manageability, cost control, security (if security can be defined as a non-functional - let's not discuss that right now). Let's first consider interfaces - gateways and other services which are positioned between zones or the \"outside world\". In the past, we would speak of a demilitarized zone (DMZ). In more recent publications, one can find this as an interface zone, or a set of Zone Interface Points (ZIPs) for accessing and interacting with the services within a zone. In many cases, several of these interface points and gateways are used in the organization to support a number of non-functional requirements. They can be used for intelligent load balancing, providing virtual patching capabilities, validating content against malware before passing it on to the actual services, etc. Depending on the top level zone, different gateways might be needed (i.e. different requirements). Interfaces for commercial services will have a strong focus on security and manageability. Those for the corporate services might be more integration-oriented, and have different data leakage requirements than those for commercial services. Also, inside such an interface zone, one can imagine a substructure to take place as well: egress interfaces (for communication that is exiting the zone), ingress interfaces (for communication that is entering the zone) and internal interfaces (used for routing between the subzones within the zone). Yet, there will also be requirements which are company-wide. Hence, one could envision a structure where there is a company-wide interface zone (with mandatory requirements regardless of the zone that they support) as well as a zone-specific interface zone (with the mandatory requirements specific to that zone). Before I show a picture of this, let's consider management services . Unlike interfaces, these services are more oriented toward the operational management of the infrastructure. Software deployment, configuration management, identity & access management services, etc. Are services one can put under management services. And like with interfaces, one can envision the need for both company-wide management services, as well as zone-specific management services. This information brings us to a final picture, one that assists the organization in providing a more manageable view on its deployment landscape. It does not show the 3rd layer (i.e. production versus non-production deployments) and only displays the second layer through specialization information, which I've quickly made a few examples for (you don't want to make such decisions in a few hours, like I did for this post). If the organization took an alternative approach for structuring (different requirements and grouping) the resulting diagram could look quite different: Flows, flows and more flows With the high-level picture ready, it is not a bad idea to look at how flows are handled in such an architecture. As the interface layer is available on both company-wide level as well as the next, flows will cross multiple zones. Consider the case of a corporate workstation connecting to a reporting server (like a Cognos or PowerBI or whatever fancy tool is used), and this reporting server is pulling data from a database system. Now, this database system is positioned in the Commercial zone, while the reporting server is in the Corporate zone. The flows could then look like so: Note for the Archimate people: I'm sorry that I'm abusing the flow relation here. I didn't want to create abstract services in the locations and then use the \"serves\" or \"used by\" relation and then explaining readers that the arrows are then inverse from what they imagine. In this picture, the corporate workstation does not connect to the reporting server directly. It goes through the internal interface layer for the corporate zone. This internal interface layer can offer services such as reverse proxies or intelligent load balancers. The idea here is that, if the organization wants, it can introduce additional controls or supporting services in this internal interface layer without impacting the system deployments themselves much. But the true flow challenge is in the next one, where a processing system connects to a data layer. Here, the processing server will first connect to the egress interface for corporate, then through the company-wide internal interface, toward the ingress interface of the commercial and then to the data layer. Now, why three different interfaces, and what would be inside it? On the corporate level, the egress interface could be focusing on privacy controls or data leakage controls. On the company-wide internal interface more functional routing capabilities could be provided, while on the commercial level the ingress could be a database activity monitoring (DAM) system such as a database firewall to provide enhanced auditing and access controls. Does that mean that all flows need to have at least three gateways? No, this is a functional picture. If the organization agrees, then one or more of these interface levels can have a simple pass-through setup. It is well possible that database connections only connect directly to a DAM service and that such flows are allowed to immediately go through other interfaces. The importance thus is not to make flows more difficult to provide, but to provide several areas where the organization can introduce controls. Making policies and standards more visible One of the effects of having a better structure of the company-wide deployments (i.e. a good zoning solution) is that one can start making policies more clear, and potentially even simple to implement with supporting tools (such as software defined network solutions). For instance, a company might want to protect its production data and establish that it cannot be used for non-production use, but that there are no restrictions for the other data environments. Another rule could be that web-based access toward the mid-tier is only allowed through an interface. These are simple statements which, if a company has a good IP plan, are easy to implement - one doesn't need zoning, although it helps. But it goes further than access controls. For instance, the company might require corporate workstations to be under heavy data leakage prevention and protection measures, while developer workstations are more open (but don't have any production data access). This not only reveals an access control, but also implies particular minimal requirements (for the Corporate > Workstation zone) and services (for the Corporate interfaces). This zoning structure does not necessarily make any statements about the location (assuming it isn't picked as one of the requirements in the beginning). One can easily extend this to include cloud-based services or services offered by third parties. Finally, it also supports making policies and standards more realistic. I often see policies that make bold statements such as \"all software deployments must be done through the company software distribution tool\", but the policies don't consider development environments (production status) or unmanaged, open or controlled deployments (trust level). When challenged, the policy owner might shrug away the comment with \"it's obvious that this policy does not apply to our sandbox environment\" or so. With a proper zoning structure, policies can establish the rules for the right set of zones, and actually pin-point which zones are affected by a statement. This is also important if a company has many, many policies. With a good zoning structure, the policies can be assigned with meta-data so that affected roles (such as project leaders, architects, solution engineers, etc.) can easily get an overview of the policies that influence a given zone. For instance, if I want to position a new management service, I am less concerned about workstation-specific policies. And if the management service is specific for the development environment (such as a new version control system) many corporate or commercially oriented policies don't apply either. Conclusion The above approach for structuring an organization is documented here in a high-level manner. It takes many assumptions or hypothetical decisions which are to be tailored toward the company itself. In my company, a different zoning structure is selected, taking into account that it is a financial service provider with entities in multiple countries, handling several thousand of systems and with an ongoing effort to include cloud providers within its infrastructure architecture. Yet the approach itself is followed in an equal fashion. We looked at requirements, created a layered structure, and finished the zoning schema. Once the schema was established, the requirements for all the zones were written out further, and a mapping of existing deployments (as-is) toward the new zoning picture is on-going. For those thinking that it is just slideware right now - it isn't. Some of the structures that come out of the zoning exercise are already prevalent in the organization, and new environments (due to mergers and acquisitions) are directed to this new situation. Still, we know we have a large exercise ahead before it is finished, but I believe that it will benefit us greatly, not only from a security point of view, but also clarity and manageability of the environment.","tags":"Architecture","url":"https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/","loc":"https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"},{"title":"Matching MD5 SSH fingerprint","text":"Today I was attempting to update a local repository, when SSH complained about a changed fingerprint, something like the following: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:p4ZGs+YjsBAw26tn2a+HPkga1dPWWAWX+NEm4Cv4I9s. Please contact your system administrator. Add correct host key in /home/user/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /home/user/.ssh/known_hosts:9 ECDSA host key for 192.168.56.101 has changed and you have requested strict checking. Host key verification failed. I checked if the host was changed recently, or the alias through which I connected switched host, or the SSH key changed. But that wasn't the case. Or at least, it wasn't the case recently, and I distinctly remember connecting to the same host two weeks ago. Now, what happened I don't know yet, but I do know I didn't want to connect until I reviewed the received SSH key fingerprint. I obtained the fingerprint from the administration (who graceously documented it on the wiki)... ... only to realize that the documented fingerprint are MD5 hashes (and in hexadecimal result) whereas the key shown by the SSH command shows it in base64 SHA256 by default. Luckily, a quick search revealed this superuser post which told me to connect to the host using the FingerprintHash md5 option: ~$ ssh -o FingerprintHash=md5 192.168.56.11 The result is SSH displaying the MD5 hashed fingerprint which I can now validate against the documented one. Once I validated that the key is the correct one, I accepted the change and continued with my endeavour. I later discovered (or, more precisely, have strong assumptions) that I had an old elliptic curve key registered in my known_hosts file, which was not used for the communication for quite some time. I recently re-enabled elliptic curve support in OpenSSH (with Gentoo's USE=\"-bindist\") which triggered the validation of the old key.","tags":"Security","url":"https://blog.siphos.be/2017/05/matching-md5-ssh-fingerprint/","loc":"https://blog.siphos.be/2017/05/matching-md5-ssh-fingerprint/"},{"title":"Switched to Lineage OS","text":"I have been a long time user of Cyanogenmod , which discontinued its services end of 2016. Due to lack of (continuous) time, I was not able to switch over toward a different ROM. Also, I wasn't sure if LineageOS would remain the best choice for me or not. I wanted to review other ROMs for my Samsung Galaxy SIII (the i9300 model) phone. Today, I made my choice and installed LineageOS. The requirements list When looking for new ROMs to use, I had a number of requirements, some must-have, others should-have or would-have (using the MoSCoW method . First of all, I want the ROM to be installable through ClockworkMod 6.4.0.something. This is a mandatory requirement, because I don't want to venture out in installing a different recovery (like TWRP ). Not that much that I'm scared from it, but it might require me to install stuff like Heimdal and update my SELinux policies on my system to allow it to run, and has the additional risk that things still fail. I tried updating the recovery ROM in the past (a year or so ago) using the mobile application approaches themselves (which require root access, that my phone had at the time) but it continuously said that it failed and that I had to revert to the more traditional way of flashing the recovery. Given that I know I need to upgrade within a day (and have other things planned today) I didn't want to loose too much time in upgrading the recovery first. Second, the ROM had to allow OTA updates. With CyanogenMod, the OTA didn't fully work on my phone (it downloaded and verified the images correctly, but couldn't install it automatically - I had to reboot in recovery manually and install the ZIP), but it worked sufficiently for me to easily update the phone on a weekly basis. I wanted to keep this luxury, and who knows, move towards an end-to-end working OTA. Furthermore, the ROM had to support Android 7.1. I want the latest Android to see how long this (nowadays aged) phone can handle things. Once the phone cannot get the latest Android anymore, I'll probably move towards a new phone. But as long as I don't have to, I'll put my money in other endeavours ;-) Finally, the ROM must be in active development. One of the reasons I want the latest Android is also because I want to keep receiving the necessary security fixes. If a ROM doesn't actively follow the security patches and code, then it might become (too) vulnerable for comfort. ROMs, ROMs everywhere (?) First, I visited the Galaxy S3 discussion on the XDA-Developers site. This often contains enough material to find ROMs which have a somewhat active development base. I was still positively surprised by the activity on this quite old phone (the i9300 was first released in May, 2012, making this phone almost 5 years old). The Vanir mod seemed to imply that TWRP was required, but past articles on Vanir showed that CWM should also work. However, from the discussion I gathered that it is based on LineageOS. Not that that's bad, but it makes LineageOS the \"preferred\" ROM first (default installed software list, larger upstream community, etc.) The Ressurrection Remix shows a very active discussion with good feedback from the developer(s). It is based on a number of other resources (including CyanogenMod), so seems to borrow and implement various other features. Although I got the slight impression that it would be a bit more filled with applications I might not want, I kept it on the short-list. SLIMROM is based on AOSP (the Android Open Source Project). It doesn't seem to support OTA though, and its release history is currently still premature. However, I will keep an eye on this one for future reference. After a while, I started looking for ROMs based on AOSP, as the majority of ROMs shown are based on LineageOS (abbreviated to LOS). Apparently, for the Samsung S3, LineageOS seems to be one of the most popular sources (and ROMs). So I put my attention to LineageOS: It supports CWM installations It offers OTA update support It closely tracks upstream It is in active development So, why not? Using LineageOS without root While deciding to use LineageOS or go through with additional ROM seeking, I stumbled upon the installation instructions that showed that the ROM can be installed without automatically enabling rooted Android access. I'm not sure if this was the case with Cyanogenmod (I've been running with a rooted Cyanogenmod for too long to remember) but it opened a possiblity for me... Personally, I don't mind having a rooted phone, as long as it is the user who decides which applications can get root access and which can't. For me, the two applications that used root access was an open source ad blocker called AdAway and the Android shell (for troubleshooting purposes, such as killing the media server if it locked my camera). But some applications seem to think that a rooted phone automatically means that the phone is open access and full of malware. It is hard to find any trustworthy, academical research on the actual secure state of rooted versus non-rooted devices. I believe that proper application vetting (don't install applications that aren't popular and long-existing, check the application vendors, etc.) and keeping your phone up-to-date is much more important than not rooting. And although these applications happily function on old, unpatched Android 4.x devices they refuse to function on my (rooted) Android 7.1 phone. So, the ability to install LineageOS without root (rooting actually requires flashing an additional package) is a nice thing as I can start with a non-rooted device first, and switch back to a rooted device if I need it later. With that, I decided to flash my phone with the latest LineageOS nightly for my phone. Switching password manager I tend to use such ROM switches (or, in case of CyanogenMod, major version upgrades) as a time to revisit the mobile application list, and reduce it to what I really used the last few months. One of the changes I did on my mobile application list is switch the password application. I used to use Remember Passwords but it hasn't seen updates for quite some time, and the backup import failed last time I migrated to a higher CyanogenMod version (possibly Android version related). Because I don't want to synchronize the passwords or see the application have any Internet oriented activity, I now use Keepass2Android Offline . This is for passwords which I don't auto-generate using SuperGenPass , my favorite password manager. I don't use the bookmarklet approach myself, but download and run it separately when generating passwords - or use a SuperGenPass mobile application . First impressions It is too soon to say if it is fully functional or not. Most standard functionality works OK (phone, SMS, camera) but it is only after a few days that specific issues can come up. Only the first boot was very slow (probably because it was optimizing the application list in the background), the second boot was well below half a minute. I didn't count it, but it's fast enough for me.","tags":"Misc","url":"https://blog.siphos.be/2017/04/switched-to-lineage-os/","loc":"https://blog.siphos.be/2017/04/switched-to-lineage-os/"},{"title":"cvechecker 3.8 released","text":"A new release is now available for the cvechecker application. This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already known, so it did not take them up to the database. As a result, systems would never check for the new CVEs. It is recommended to remove any historical files from /var/lib/cvechecker/cache like so: ~# rm /var/lib/cvechecker/cache/nvdcve-2.0-2017.* ~# rm /var/lib/cvechecker/cache/nvdcve-2.0-modified.* This will make sure that the next run of pullcves pull will re-download those files, and attempt to load the resulting CVEs back in the database. Sorry for this issue :-(","tags":"Free Software","url":"https://blog.siphos.be/2017/03/cvechecker-3.8-released/","loc":"https://blog.siphos.be/2017/03/cvechecker-3.8-released/"},{"title":"Handling certificates in Gentoo Linux","text":"I recently created a new article on the Gentoo Wiki titled Certificates which talks about how to handle certificate stores on Gentoo Linux. The write-up of the article (which might still change name later, because it does not handle everything about certificates, mostly how to handle certificate stores) was inspired by the observation that I had to adjust the certificate stores of both Chromium and Firefox separately, even though they both use NSS. Certificates? Well, when a secure communication is established from a browser to a site (or any other interaction that uses SSL/TLS, but let's stay with the browser example for now) part of the exchange is to ensure that the target site is actually the site it claims to be. Don't want someone else to trick you into giving your e-mail credentials do you? To establish this, the certificate presented by the remote site is validated (alongside other handshake steps ). A certificate contains a public key, as well as information about what the certificate can be used for, and who (or what) the certificate represents. In case of a site, the identification is (or should be) tied to the fully qualified domain name. Of course, everyone could create a certificate for accounts.google.com and try to trick you into leaving your credentials. So, part of the validation of a certificate is to verify that it is signed by a third party that you trust to only sign certificates that are trustworthy. And to validate this signature, you hence need the certificate of this third party as well. So, what about this certificate? Well, turns out, this one is also often signed by another certificate, and so on, until you reach the \"top\" of the certificate tree. This top certificate is called the \"root certificate\". And because we still have to establish that this certificate is trustworthy, we need another way to accomplish this. Enter certificate stores The root certificates of these trusted third parties (well, let us call them \"Certificate Authorities\" from now onward, because they sometimes will lose your trust ) need to be reachable by the browser. The location where they are stored in is (often) called the truststore (a naming that I came across when dealing with Java and which stuck). So, what I wanted to accomplish was to remove a particular CA certificate from the certificate store. I assumed that, because Chromium and Firefox both use NSS as the library to support their cryptographic uses, they would also both use the store location at ~/.pki/nssdb . That was wrong. Another assumption I had was that NSS also uses the /etc/pki/nssdb location as a system-wide one. Wrong again (not that NSS doesn't allow this, but it seems that it is very much up to, and often ignored by, the NSS-implementing applications). Oh, and I also assumed that there wouldn't be a hard-coded list in the application. Yup. Wrong again. How NSS tracks root CA Basically, NSS has a hard-coded root CA list inside the libnssckbi.so file. On Gentoo, this file is provided by the dev-libs/nss package. Because it is hard-coded, it seemed like there was little I could do to remove it, yet still through the user interfaces offered by Firefox and Chromium I was able to remove the trust bits from the certificate. Turns out that Firefox (inside ~/.mozilla/firefox/*.default ) and Chromium (inside ~/.pki/nssdb ) store the (modified) trust bits for those locations, so that the hardcoded list does not need to be altered if all I want to do was revoke the trust on a specific CA. And it isn't that this hard-coded list is a bad list: Mozilla has a CA Certificate Program which controls the CAs that are accepted inside this store. Still, I find it sad that the system-wide location (at /etc/pki/nssdb ) is not by default used as well (or I have something wrong on my system that makes it so). On a multi-user system, administrators who want to have some control over the certificate stores might need to either use login scripts to manipulate the user certificate stores, or adapt the user files directly currently.","tags":"Gentoo","url":"https://blog.siphos.be/2017/03/handling-certificates-in-gentoo-linux/","loc":"https://blog.siphos.be/2017/03/handling-certificates-in-gentoo-linux/"},{"title":"cvechecker 3.7 released","text":"After a long time of getting too little attention from me, I decided to make a new cvechecker release. There are few changes in it, but I am planning on making a new release soon with lots of clean-ups. What has been changed So, what has changed? With this release (now at version 3.7) two bugs have been fixed, one having a wrong URL in the CVE download and the other about the CVE sequence numbers. The first bug was an annoying one, which I should have fixed a long time ago. Well, it was fixed in the repository, but I didn't make a new release for it. When downloading the nvdcve-2.0-Modified.xml file, the pullcves command used the lowercase filename, which doesn't exist. The second bug is about parsing the CVE sequence. On January 2014 the syntax changed to allow for sequence identifiers longer than 4 digits. The cvechecker tool however did a hard validation on the length of the identifier, and cut off longer fields. That means that some CVE reports failed to parse in cvechecker, and thus cvechecker didn't \"know\" about these vulnerabilities. This has been fixed in this release, although I am not fully satisfied... What still needs to be done The codebase for cvechecker is from 2010, and is actually based on a prototype that I wrote which I decided not to rewrite into proper code. As a result, the code is not up to par. I'm going to gradually improve and clean up the code in the next few [insert timeperiod here]. I don't know if there will be feature improvements in the next few releases (not that there aren't many feature enhancements needed) but I hope that, once the code is improved, new functionality can be added more easily. But that's for another time. Right now, enjoy the new release.","tags":"Free Software","url":"https://blog.siphos.be/2017/03/cvechecker-3.7-released/","loc":"https://blog.siphos.be/2017/03/cvechecker-3.7-released/"},{"title":"I missed FOSDEM","text":"I sadly had to miss out on the FOSDEM event. The entire weekend was filled with me being apathetic, feverish and overall zombie-like. Yes, sickness can be cruel. It wasn't until today that I had the energy back to fire up my laptop. Sorry for the crew that I promised to meet at FOSDEM. I'll make it up, somehow.","tags":"Misc","url":"https://blog.siphos.be/2017/02/i-missed-fosdem/","loc":"https://blog.siphos.be/2017/02/i-missed-fosdem/"},{"title":"SELinux System Administration, 2nd Edition","text":"While still working on a few other projects, one of the time consumers of the past half year (haven't you noticed? my blog was quite silent) has come to an end: the SELinux System Administration - Second Edition book is now available. With almost double the amount of pages and a serious update of the content, the book can now be bought either through Packt Publishing itself, or the various online bookstores such as Amazon . With the holidays now approaching, I hope to be able to execute a few tasks within the Gentoo community (and of the Gentoo Foundation) and get back on track. Luckily, my absence was not jeopardizing the state of SELinux in Gentoo thanks to the efforts of Jason Zaman.","tags":"SELinux","url":"https://blog.siphos.be/2016/12/selinux-system-administration-2nd-edition/","loc":"https://blog.siphos.be/2016/12/selinux-system-administration-2nd-edition/"},{"title":"GnuPG: private key suddenly missing?","text":"After updating my workstation, I noticed that keychain reported that it could not load one of the GnuPG keys I passed it on. * keychain 2.8.1 ~ http://www.funtoo.org * Found existing ssh-agent: 2167 * Found existing gpg-agent: 2194 * Warning: can't find 0xB7BD4B0DE76AC6A4; skipping * Known ssh key: /home/swift/.ssh/id_dsa * Known ssh key: /home/swift/.ssh/id_ed25519 * Known gpg key: 0x22899E947878B0CE I did not modify my key store at all, so what happened? GnuPG upgrade to 2.1 The update I did also upgraded GnuPG to the 2.1 series. This version has quite a few updates , one of which is a change towards a new private key storage approach. I thought that it might have done a wrong conversion, or that the key which was used was of a particular method or strength that suddenly wasn't supported anymore (PGP-2 is mentioned in the article). But the key is a relatively standard RSA4096 one. Yet still, when I listed my private keys, I did not see this key. I even tried to re-import the secring.gpg file, but it only found private keys that it already saw previously. I'm blind - the key never disappeared Luckily, when I tried to sign something with the key, gpg-agent still asked me for the passphraze that I had used for a while on that key. So it isn't gone. What happened? Well, the key id is not my private key id, but the key id of one of the subkeys. Previously, gpg-agent sought and found the private key associated with the subkey, but now it no longer does. I don't know if this is a bug in the past that I accidentally used, or if this is a bug in the new version. I might investigate that a bit more, but right now I'm happy that I found it. All I had to do was use the right key id in keychain, and things worked again. Good, now I can continue debugging networking issues with an azure-hosted system...","tags":"Free Software","url":"https://blog.siphos.be/2016/10/gnupg-private-key-suddenly-missing/","loc":"https://blog.siphos.be/2016/10/gnupg-private-key-suddenly-missing/"},{"title":"We do not ship SELinux sandbox","text":"A few days ago a vulnerability was reported in the SELinux sandbox user space utility. The utility is part of the policycoreutils package. Luckily, Gentoo's sys-apps/policycoreutils package is not vulnerable - and not because we were clairvoyant about this issue, but because we don't ship this utility. What is the SELinux sandbox? The SELinux sandbox utility, aptly named sandbox , is a simple C application which executes its arguments, but only after ensuring that the task it launches is going to run in the sandbox_t domain. This domain is specifically crafted to allow applications most standard privileges needed for interacting with the user (so that the user can of course still use the application) but removes many permissions that might be abused to either obtain information from the system, or use to try and exploit vulnerabilities to gain more privileges. It also hides a number of resources on the system through namespaces. It was developed in 2009 for Fedora and Red Hat. Given the necessary SELinux policy support though, it was usable on other distributions as well, and thus became part of the SELinux user space itself. What is the vulnerability about? The SELinux sandbox utility used an execution approach that did not shield off the users' terminal access sufficiently. In the POC post we notice that characters could be sent to the terminal through the ioctl() function (which executes the ioctl system call used for input/output operations against devices) which are eventually executed when the application finishes. That's bad of course. Hence the CVE-2016-7545 registration, and of course also a possible fix has been committed upstream . Why isn't Gentoo vulnerable / shipping with SELinux sandbox? There's some history involved why Gentoo does not ship the SELinux sandbox (anymore). First of all, Gentoo already has a command that is called sandbox , installed through the sys-apps/sandbox application. So back in the days that we still shipped with the SELinux sandbox, we continuously had to patch policycoreutils to use a different name for the sandbox application (we used sesandbox then). But then we had a couple of security issues with the SELinux sandbox application. In 2011, CVE-2011-1011 came up in which the seunshare_mount function had a security issue. And in 2014, CVE-2014-3215 came up with - again - a security issue with seunshare . At that point, I had enough of this sandbox utility. First of all, it never quite worked enough on Gentoo as it is (as it also requires a policy which is not part of the upstream release) and given its wide open access approach (it was meant to contain various types of workloads, so security concessions had to be made), I decided to no longer support the SELinux sandbox in Gentoo . None of the Gentoo SELinux users ever approached me with the question to add it back. And that is why Gentoo is not vulnerable to this specific issue.","tags":"SELinux","url":"https://blog.siphos.be/2016/09/we-do-not-ship-selinux-sandbox/","loc":"https://blog.siphos.be/2016/09/we-do-not-ship-selinux-sandbox/"},{"title":"Mounting QEMU images","text":"While working on the second edition of my first book, SELinux System Administration - Second Edition I had to test out a few commands on different Linux distributions to make sure that I don't create instructions that only work on Gentoo Linux. After all, as awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a few live systems to run in Qemu/KVM. Some of these systems however use cloud-init which, while interesting to use, is not set up on my system yet. And without support for cloud-init, how can I get access to the system? Mounting qemu images on the system To resolve this, I want to mount the image on my system, and edit the /etc/shadow file so that the root account is accessible. Once that is accomplished, I can log on through the console and start setting up the system further. Images that are in the qcow2 format can be mounted through the nbd driver, but that would require some updates on my local SELinux policy that I am too lazy to do right now (I'll get to them eventually, but first need to finish the book). Still, if you are interested in using nbd, see these instructions or a related thread on the Gentoo Forums. Luckily, storage is cheap (even SSD disks), so I quickly converted the qcow2 images into raw images: ~$ qemu-img convert root.qcow2 root.raw With the image now available in raw format, I can use the loop devices to mount the image(s) on my system: ~# losetup /dev/loop0 root.raw ~# kpartx -a /dev/loop0 ~# mount /dev/mapper/loop0p1 /mnt The kpartx command will detect the partitions and ensure that those are available: the first partition becomes available at /dev/loop0p1 , the second /dev/loop0p2 and so forth. With the image now mounted, let's update the /etc/shadow file. Placing a new password hash in the shadow file A google search quickly revealed that the following command generates a shadow-compatible hash for a password: ~$ openssl passwd -1 MyMightyPassword $1$BHbMVz9i$qYHmULtXIY3dqZkyfW/oO. The challenge wasn't to find the hash though, but to edit it: ~# vim /mnt/etc/shadow vim: Permission denied The image that I downloaded used SELinux (of course), which meant that the shadow file was labeled with shadow_t which I am not allowed to access. And I didn't want to put SELinux in permissive mode just for this (sometimes I /do/ have some time left, apparently). So I remounted the image, but now with the context= mount option, like so: ~# mount -o context=\"system_u:object_r:var_t:s0: /dev/loop0p1 /mnt Now all files are labeled with var_t which I do have permissions to edit. But I also need to take care that the files that I edited get the proper label again. There are a number of ways to accomplish this. I chose to create a .autorelabel file in the root of the partition. Red Hat based distributions will pick this up and force a file system relabeling operation. Unmounting the file system After making the changes, I can now unmount the file system again: ~# umount /mnt ~# kpart -d /dev/loop0 ~# losetup -d /dev/loop0 With that done, I had root access to the image and could start testing out my own set of commands. It did trigger my interest in the cloud-init setup though...","tags":"Free Software","url":"https://blog.siphos.be/2016/09/mounting-qemu-images/","loc":"https://blog.siphos.be/2016/09/mounting-qemu-images/"},{"title":"Comparing Hadoop with mainframe","text":"At my work, I have the pleasure of being involved in a big data project that uses Hadoop as the primary platform for several services. As an architect, I try to get to know the platform's capabilities, its potential use cases, its surrounding ecosystem, etc. And although the implementation at work is not in its final form (yay agile infrastructure releases) I do start to get a grasp of where we might be going. For many analysts and architects, this Hadoop platform is a new kid on the block so I have some work explaining what it is and what it is capable of. Not for the fun of it, but to help the company make the right decisions, to support management and operations, to lift the fear of new environments. One thing I've once said is that \"Hadoop is the poor man's mainframe\", because I notice some high-level similarities between the two. Somehow, it stuck, and I was asked to elaborate. So why not bring these points into a nice blog post :) The big fat disclaimer Now, before embarking on this comparison, I would like to state that I am not saying that Hadoop offers the same services, or even quality and functionality of what can be found in mainframe environments. Considering how much time, effort and experience was already put in the mainframe platform, it would be strange if Hadoop could match the same. This post is to seek some similarities and, who knows, learn a few more tricks from one or another. Second, I am not an avid mainframe knowledgeable person. I've been involved as an IT architect in database and workload automation technical domains, which also spanned the mainframe parts of it, but most of the effort was within the distributed world. Mainframes remain somewhat opaque to me. Still, that shouldn't prevent me from making any comparisons for those areas that I do have some grasp on. And if my current understanding is just wrong, I'm sure that I'll learn from the comments that you can leave behind! With that being said, here it goes... Reliability, Availability, Serviceability Let's start with some of the promises that both platforms make - and generally are also able to deliver. Those promises are of reliability, availability and serviceability. For the mainframe platform, these quality attributes are shown as the mainframe strengths . The platform's hardware has extensive self-checking and self-recovery capabilities, the systems can recover from failed components without service interruption, and failures can be quickly determined and resolved. On the mainframes, this is done through a good balance and alignment of hardware and software, design decisions and - in my opinion - tight control over the various components and services. I notice the same promises on Hadoop. Various components are checking the state of the hardware and other components, and when something fails, it is often automatically recovered without impacting services. Instead of tight control over the components and services, Hadoop uses a service architecture and APIs with Java virtual machine abstractions. Let's consider hardware changes. For hardware failure and component substitutions , both platforms are capable of dealing with those without service disruption. Mainframe probably has a better reputation in this matter, as its components have a very high Mean Time Between Failure (MTBF), and many - if not all - of the components are set up in a redundant fashion. Lots of error detection and failure detection processes try to detect if a component is close to failure, and ensure proper transitioning of any workload towards the other components without impact. Hadoop uses redundancy on a server level. If a complete server fails, Hadoop is usually able to deal with this without impact. Either the sensor-like services disable a node before it goes haywire, or the workload and data that was running on the failed node is restarted on a different node. Hardware (component) failures on the mainframe side will not impact the services and running transactions. Component failures on Hadoop might have a noticeable impact (especially if it is OLTP-like workload), but will be quickly recovered. Failures are more likely to happen on Hadoop clusters though, as it was designed to work with many systems that have a worse MTBF design than a mainframe. The focus within Hadoop is on resiliency and fast recoverability. Depending on the service that is being used, active redundancy can be in use (so disruptions are not visible to the user). If the Hadoop workload includes anything that resembles online transactional processing, you're still better off with enterprise-grade hardware such as ECC memory to at least allow improved hardware failure detection (and perform proactive workload management). CPU failures are not that common (at least not those without any upfront Machine Check Exception - MCE), and disk/controller failures are handled through the abstraction of HDFS anyway. For system substitutions , I think both platforms can deal with this in a dynamic fashion as well: For the mainframe side (and I'm guessing here) it is possible to switch machines with no service impact if the services are running on LPARs that are joined together in a Parallel Sysplex setup (sort-of clustering through the use of the Coupling Facilities of mainframe, which is supported through high-speed data links and services for handling data sharing and IPC across LPARs). My company switched to the z13 mainframe last year, and was able to keep core services available during the migration. For Hadoop systems, the redundancy on system level is part of its design. Extending clusters, removing nodes, moving services, ... can be done with no impact. For instance, switching the active HiveServer2 instance means de-registering it in the ZooKeeper service. New client connects are then no longer served by that HiveServer2 instance, while active client connections remain until finished. There are also in-memory data grid solutions such as through the Ignite project, allowing for data sharing and IPC across nodes, as well as building up memory-based services with Arrow, allowing for efficient memory transfers. Of course, also application level code failures tend to only disrupt that application, and not the other users. Be it because of different address spaces and tight runtime control (mainframe) or the use of different containers / JVMs for the applications (Hadoop), this is a good feat to have (even though it is not something that differentiates these platforms from other platforms or operating systems). Let's talk workloads When we look at a mainframe setup, we generally look at different workload patterns as well. There are basically two main workload approaches for the mainframe: batch, and On-Line Transactional Processing (OLTP) workload. In the OLTP type, there is often an additional distinction between synchronous OLTP and asynchronous OLTP (usually message-based). Well, we have the same on Hadoop. It was once a pure batch-driven platform (and many of its components are still using batches or micro-batches in their underlying designs) but now also provides OLTP workload capabilities. Most of the OLTP workload on Hadoop is in the form of SQL-like or NoSQL database management systems with transaction manager support though. To manage these (different) workloads, and to deal with prioritization of the workload, both platforms offer the necessary services to make things both managed as well as business (or \"fit for purpose\") focused. Using the Workload Manager (WLM) on the mainframe, policies can be set on the workload classes so that an over-demand of resources (cross-LPARs) results in the \"right\" amount of allocations for the \"right\" workload. To actually manage jobs themselves, the Job Entry Subsystem (JES) to receive jobs and schedule then for processing on z/OS. For transactional workload, WLM provides the right resources to for instance the involved IMS regions. On Hadoop, workload management is done through Yet Another Resource Negotiator (YARN), which uses (logical) queues for the different workloads. Workload (Application Containers) running through these queues can be, resource-wise, controlled both on the queue level (high-level resource control) as well as process level (low-level resource control) through the use of Linux Control Groups (CGroups - when using Linux based systems course). If I would try to compare both against each other, one might say that the YARN queues are like WLMs service classes, and for batch applications, the initiators on mainframe are like the Application Containers within YARN queues. The latter can also be somewhat compared to IMS regions in case of long-running Application Containers. The comparison will not hold completely though. WLM can be tuned based on goals and will do dynamic decision making on the workloads depending on its parameters, and even do live adjustments on the resources (through the System Resources Manager - SRM). Heavy focus on workload management on mainframe environments is feasible because extending the available resources on mainframes is usually expensive (additional Million Service Units - MSU). On Hadoop, large cluster users who notice resource contention just tend to extend the cluster further. It's a different approach. Files and file access Another thing that tends to confuse some new users on Hadoop is its approach to files. But when you know some things about the mainframe, this does remain understandable. Both platforms have a sort-of master repository where data sets (mainframe) or files (Hadoop) are registered in. On the mainframe, the catalog translates data set names into the right location (or points to other catalogs that do the same) On Hadoop, the Hadoop Distributed File System (HDFS) NameNode is responsible for tracking where files (well, blocks) are located across the various systems Considering the use of the repository, both platforms thus require the allocation of files and offer the necessary APIs to work with them. But this small comparison does not end here. Depending on what you want to store (or access), the file format you use is important as well. - On mainframe, Virtual Storage Access Method (VSAM) provides both the methods (think of it as API) as well as format for a particular data organization. Inside a VSAM, multiple data entries can be stored in a structured way. Besides VSAM, there is also Partitioned Data Set/Extended (PDSE), which is more like a directory of sorts. Regular files are Physical Sequential (PS) data sets. - On Hadoop, a number of file formats are supported which optimize the use of the files across the services. One is Avro, which holds both methods and format (not unlike VSAM), another is Optimized Row Columnar (ORC). HDFS also has a number of options that can be enabled or set on certain locations (HDFS uses a folder-like structure) such as encryption, or on files themselves, such as replication factor. Although I don't say VSAM versus Avro are very similar (Hadoop focuses more on the concept of files and then the file structure, whereas mainframe focuses on the organization and allocation aspect if I'm not mistaken) they seem to be sufficiently similar to get people's attention back on the table. Services all around What makes a platform tick is its multitude of supported services. And even here can we find similarities between the two platforms. On mainframe, DBMS services can be offered my a multitude of softwares. Relational DBMS services can be provided by IBM DB2, CA Datacom/DB, NOMAD, ... while other database types are rendered by titles such as CA IDMS and ADABAS. All these titles build upon the capabilities of the underlying components and services to extend the platform's abilities. On Hadoop, several database technologies exist as well. Hive offers a SQL layer on top of Hadoop managed data (so does Drill btw), HBase is a non-relational database (mainly columnar store), Kylin provides distributed analytics, MapR-DB offers a column-store NoSQL database, etc. When we look at transaction processing, the mainframe platform shows its decades of experience with solutions such as CICS and IMS. Hadoop is still very much at its infancy here, but with projects such as Omid or commercial software solutions such as Splice Machine, transactional processing is coming here as well. Most of these are based on underlying database management systems which are extended with transactional properties. And services that offer messaging and queueing are also available on both platforms: mainframe can enjoy Tibco Rendezvous and IBM WebSphere MQ, while Hadoop is hitting the news with projects such as Kafka and Ignite. Services extend even beyond the ones that are directly user facing. For instance, both platforms can easily be orchestrated using workload automation tooling. Mainframe has a number of popular schedulers up its sleeve (such as IBM TWS, BMC Control-M or CA Workload Automation) whereas Hadoop is generally easily extended with the scheduling and workload automation software of the distributed world (which, given its market, is dominated by the same vendors, although many smaller ones exist as well). Hadoop also has its \"own\" little scheduling infrastructure called Oozie. Programming for the platforms Platforms however are more than just the sum of the services and the properties that it provides. Platforms are used to build solutions on, and that is true for both mainframe as well as Hadoop. Let's first look at scripting - using interpreted languages. On mainframe, you can use the Restructed Extended Executor (REXX) or CLIST (Command LIST). Hadoop gives you Tez and Pig, as well as Python and R (through PySpark and SparkR). If you want to directly interact with the systems, mainframe offers the Time Sharing Option/Extensions (TSO/E) and Interactive System Productivity Facility (ISPF). For Hadoop, regular shells can be used, as well as service-specific ones such as Spark shell. However, for end users, web-based services such as Ambari UI (Ambari Views) are generally better suited. If you're more fond of compiled code, mainframe supports you with COBOL, Java (okay, it's \"a bit\" interpreted, but also compiled - don't shoot me here), C/C++ and all the other popular programming languages. Hadoop builds on top of Java, but supports other languages such as Scala and allows you to run native applications as well - it's all about using the right APIs. To support development efforts, Integrated Development Environments (IDEs) are provided for both platforms as well. You can use Cobos, Micro Focus Enterprise Developer, Rational Developer for System z, Topaz Workbench and more for mainframe development. Hadoop has you covered with web-based notebook solutions such as Zeppelin and JupyterHub, as well as client-level IDEs such as Eclipse (with the Hadoop Development Tools plugins) and IntelliJ. Governing and managing the platforms Finally, there is also the aspect of managing the platforms. When working on the mainframe, management tooling such as the Hardware Management Console (HMC) and z/OS Management Facility (z/OSMF) cover operations for both hardware and system resources. On Hadoop, central management software such as Ambari, Cloudera Manager or Zettaset Orchestrator try to cover the same needs - although most of these focus more on the software side than on the hardware level. Both platforms also have a reasonable use for multiple roles: application developers, end users, system engineers, database adminstrators, operators, system administrators, production control, etc. who all need some kind of access to the platform to support their day-to-day duties. And when you talk roles, you talk authorizations. On the mainframe, the Resource Access Control Facility (RACF) provides access control and auditing facilities, and supports a multitude of services on the mainframe (such as DB2, MQ, JES, ...). Many major Hadoop services, such as HDFS, YARN, Hive and HBase support Ranger, providing a single pane for security controls on the Hadoop platform. Both platforms also offer the necessary APIs or hooks through which system developers can fine-tune the platform to fit the needs of the business, or develop new integrated solutions - including security oriented ones. Hadoop's extensive plugin-based design (not explicitly named) or mainframe's Security Access Facility (SAF) are just examples of this. Playing around Going for a mainframe or a Hadoop platform will always be a management decision. Both platforms have specific roles and need particular profiles in order to support them. They are both, in my opinion, also difficult to migrate away from once you are really using them actively (lock-in) although it is more digestible for Hadoop given its financial implications. Once you want to start meddling with it, getting access to a full platform used to be hard (the coming age of cloud services makes that this is no longer the case though), and both therefore had some potential \"small deployment\" uses. Mainframe experience could be gained through the Hercules 390 emulator, whereas most Hadoop distributions have a single-VM sandbox available for download. To do a full scale roll-out however is much harder to do by your own. You'll need to have quite some experience or even expertise on so many levels that you will soon see that you need teams (plural) to get things done. This concludes my (apparently longer than expected) write-down of this matter. If you don't agree, or are interested in some insights, be sure to comment!","tags":"Hadoop","url":"https://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/","loc":"https://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/"},{"title":"Template was specified incorrectly","text":"After reorganizing my salt configuration, I received the following error: [ERROR ] Template was specified incorrectly: False Enabling some debugging on the command gave me a slight pointer why this occurred: [DEBUG ] Could not find file from saltenv 'testing', u'salt://top.sls' [DEBUG ] No contents loaded for env: testing [DEBUG ] compile template: False [ERROR ] Template was specified incorrectly: False I was using a single top file as recommended by Salt, but apparently it was still looking for top files in the other environments. Yet, if I split the top files across the environments, I got the following warning: [WARNING ] Top file merge strategy set to 'merge' and multiple top files found. Top file merging order is undefined; for better results use 'same' option So what's all this about? When using a single top file is preferred If you want to stick with a single top file, then the first error is (or at least, in my case) caused by my environments not having a fall-back definition. My /etc/salt/master configuration file had the following file_roots setting: file_roots: base: - /srv/salt/base testing: - /srv/salt/testing The problem is that Salt expects ''a'' top file through the environment. What I had to do was to set the fallback directory to the base directory again, like so: file_roots: base: - /srv/salt/base testing: - /srv/salt/testing - /srv/salt/base With this set, the error disappeared and both salt and myself were happy again. When multiple top files are preferred If you really want to use multiple top files (which is also a use case in my configuration), then first we need to make sure that the top files of all environments correctly isolate the minion matches. If two environments would match the same minion, then this approach becomes more troublesome. On the one hand, we can just let saltstack merge the top files (default behavior) but the order of the merging is undefined (and no, you can't set it using env_order ) which might result in salt states being executed in an unexpected order. If the definitions are done to such an extend that this is not a problem, then you can just ignore the warning. See also bug 29104 about the warning itself. But better would be to have the top files of the environment(s) isolated so that each environment top file completely manages the entire environment. When that is the case, then we tell salt that only the top file of the affected environment should be used. This is done using the following setting in /etc/salt/master : top_file_merging_strategy: same If this is used, then the env_order setting is used to define in which order the environments are processed. Oh and if you're using salt-ssh , then be sure to set the environment of the minion in the roster file, as there is no running minion on the target system that informs salt about the environment to use otherwise: # In /etc/salt/roster testserver: host: testserver.example.com minion_opts: environment: testing","tags":"Free Software","url":"https://blog.siphos.be/2016/03/template-was-specified-incorrectly/","loc":"https://blog.siphos.be/2016/03/template-was-specified-incorrectly/"},{"title":"Using salt-ssh with agent forwarding","text":"Part of a system's security is to reduce the attack surface. Following this principle, I want to see if I can switch from using regular salt minions for a saltstack managed system set towards salt-ssh . This would allow to do some system management over SSH instead of ZeroMQ. I'm not confident yet that this is a solid approach to take (as performance is also important, which is greatly reduced with salt-ssh ), and the security exposure of the salt minions over ZeroMQ is also not that insecure (especially not when a local firewall ensures that only connections from the salt master are allowed). But playing doesn't hurt. Using SSH agent forwarding Anyway, I quickly got stuck with accessing minions over the SSH interface as it seemed that salt requires its own SSH keys (I don't enable password-only authentication, most of the systems use the AuthenticationMethods approach to chain both key and passwords). But first things first, the current target uses regular ssh key authentication (no chained approach, that's for later). But I don't want to assign such a powerful key to my salt master (especially not if it would later also document the passwords). I would like to use SSH agent forwarding. Luckily, salt does support that, it just forgot to document it. Basically, what you need to do is update the roster file with the priv: parameter set to agent-forwarding : myminion: host: myminion.example.com priv: agent-forwarding It will use the known_hosts file of the currently logged on user (the one executing the salt-ssh command) so make sure that the system's key is already known. ~$ salt-ssh myminion test.ping myminion: True","tags":"Free Software","url":"https://blog.siphos.be/2016/03/using-salt-ssh-with-agent-forwarding/","loc":"https://blog.siphos.be/2016/03/using-salt-ssh-with-agent-forwarding/"},{"title":"Trying out imapsync","text":"Recently, I had to migrate mail boxes for a couple of users from one mail provider to another. Both mail providers used IMAP, so I looked into IMAP related synchronization methods. I quickly found the imapsync application, also supported through Gentoo's repository. What I required The migration required that all mails, except for the spam and trash e-mails, were migrated to another mail server. The migrated mails had to retain their status flags (so unread mails had to remain unread while read mails had to remain read), and the migration had to be done in two waves: one while the primary mail server was still in use (where most of the mails where synchronized) and then, after switching the mail servers (which was done through DNS changes) re-sync to fetch the final ones. I did not get access to the credentials of all mail boxes, but together with the main administrator we enabled a sort-of shadow authentication system (a temporary OpenLDAP installation) in which the same users were enabled, but with passwords that will be used during the synchronization. The mailservers were then configured to have a secondary interface available which used this OpenLDAP rather than the primary authentication that was being used by the end users. Using imapsync Using imapsync is simple. It is a command-line application, and everything configurable is done through command arguments. The basic ones are of course the source and target definitions, as well as the authentication information for both sides. ~$ imapsync \\ --host1 src-host --user1 src-user --password1 src-pw --authmech1 LOGIN --ssl1 \\ --host2 dst-host --user2 dst-user --password2 dst-pw --authmech2 LOGIN --ssl2 The use of the --ssl1 and --ssl2 is not to enable an older or newer version of the SSL/TLS protocol. It just enables the use of SSL/TLS for the source host ( --ssl1 ) and destination host ( --ssl2 ). This would just start synchronizing messages, but we need to include the necessary directives to skip trash and spam mailboxes for instance. For this, the --exclude parameter can be used: ~$ imapsync ... --exclude \"Trash|Spam|Drafts\" It is also possible to transform some mailbox names. For instance, if the source host uses Sent as the mailbox for sent mail, while the target has Sent Items , then the following would enable migrating mails between the right folders: ~$ imapsync ... --folder \"Sent\" --regextrans2 's/Sent/Sent Items/' Conclusions and interesting resources Using the application was a breeze. I do recommend to create a test account on both sides so that you can easily see the available folders, source and target naming conventions as well as test if rerunning the application works flawlessly. In my case for instance, I had to add --skipsize so that the application does not use the mail sizes for comparing if a mail is already transferred or not, as the target mailserver showed different mail sizes for the same mails. This was luckily often documented on the various online tutorials about imapsync , such as Moving to Google Apps with imapsync on seagrief.co.uk Guide to imapsync on wiki.zimbra.com The migration took a while, but without major issues. Within a few hours, the mailboxes of all users where correctly migrated.","tags":"Free Software","url":"https://blog.siphos.be/2016/03/trying-out-imapsync/","loc":"https://blog.siphos.be/2016/03/trying-out-imapsync/"},{"title":"New cvechecker release","text":"A short while ago I got the notification that pulling new CVE information was no longer possible. The reason was that the NVD site did not support uncompressed downloads anymore. The fix for cvechecker was simple, and it also gave me a reason to push out a new release (after two years) which also includes various updates by Christopher Warner. So cvechecker 3.6 is now available for general consumption.","tags":"Free Software","url":"https://blog.siphos.be/2015/11/new-cvechecker-release/","loc":"https://blog.siphos.be/2015/11/new-cvechecker-release/"},{"title":"Switching focus at work","text":"Since 2010, I was at work responsible for the infrastructure architecture of a couple of technological domains, namely databases and scheduling/workload automation. It brought me in contact with many vendors, many technologies and most importantly, many teams within the organization. The focus domain was challenging, as I had to deal with the strategy on how the organization, which is a financial institution, will deal with databases and scheduling in the long term. This means looking at the investments related to those domains, implementation details, standards of use, features that we will or will not use, positioning of products and so forth. To do this from an architecture point of view means that I not only had to focus on the details of the technology and understand all their use, but also become a sort-of subject matter expert on those topics. Luckily, I had (well, still have) great teams of DBAs (for the databases) and batch teams (for the scheduling/workload automation) to keep things in the right direction. I helped them with a (hopefully sufficiently) clear roadmap, investment track, procurement, contract/terms and conditions for use, architectural decisions and positioning and what not. And they helped me with understanding the various components, learn about the best use of these, and of course implement the improvements that we collaboratively put on the roadmap. Times, they are changing Last week, I flipped over a page at work. Although I remain an IT architect within the same architecture team, my focus shifts entirely. Instead of a fixed domain, my focus is now more volatile. I leave behind the stability of organizationally anchored technology domains and go forward in a more tense environment. Instead of looking at just two technology domains, I need to look at all of them, and find the right balance between high flexibility demands (which might not want to use current \"standard\" offerings) which come up from a very agile context, and the almost non-negotionable requirements that are typical for financial institutions. The focus is also not primarily technology oriented anymore. I'll be part of an enterprise architecture team with direct business involvement and although my main focus will be on the technology side, it'll also involve information management, business processes and applications. The end goal is to set up a future-proof architecture in an agile, fast-moving environment (contradictio in terminis ?) which has a main focus in data analytics and information gathering/management. Yes, \"big data\", but more applied than what some of the vendors try to sell us ;-) I'm currently finishing off the high-level design and use of a Hadoop platform, and the next focus will be on a possible micro-service architecture using Docker. I've been working on this Hadoop design for a while now (but then it was together with my previous function at work) and given the evolving nature of Hadoop (and the various services that surround it) I'm confident that it will not be the last time I'm looking at it. Now let me hope I can keep things manageable ;-)","tags":"Architecture","url":"https://blog.siphos.be/2015/09/switching-focus-at-work/","loc":"https://blog.siphos.be/2015/09/switching-focus-at-work/"},{"title":"Getting su to work in init scripts","text":"While developing an init script which has to switch user, I got a couple of errors from SELinux and the system itself: ~# rc-service hadoop-namenode format Authenticating root. * Formatting HDFS ... su: Authentication service cannot retrieve authentication info (Ignored) The authentication log shows entries such as the following: Sep 14 20:20:05 localhost unix_chkpwd[5522]: could not obtain user info (hdfs) I've always had issues with getting su to work properly again. Now that I have what I think is a working set, let me document it for later (as I still need to review why they are needed): # Allow initrc_t to use unix_chkpwd to check entries # Without it gives the retrieval failure auth_domtrans_chk_passwd ( initrc_t ) # Allow initrc_t to query selinux access, otherwise avc assertion allow initrc_t self :netlink_selinux_socket { bind create read }; selinux_compute_access_vector ( initrc_t ) # Allow initrc_t to honor the pam_rootok setting allow initrc_t self :passwd { passwd rootok }; With these SELinux rules, switching the user works as expected from within an init script.","tags":"SELinux","url":"https://blog.siphos.be/2015/09/getting-su-to-work-in-init-scripts/","loc":"https://blog.siphos.be/2015/09/getting-su-to-work-in-init-scripts/"},{"title":"Custom CIL SELinux policies in Gentoo","text":"In Gentoo, we have been supporting custom policy packages for a while now. Unlike most other distributions, which focus on binary packages, Gentoo has always supported source-based packages as default (although binary packages are supported as well). A recent commit now also allows CIL files to be used. Policy ebuilds, how they work Gentoo provides its own SELinux policy, based on the reference policy , and provides per-module ebuilds (packages). For instance, the SELinux policy for the screen package is provided by the sec-policy/selinux-screen package. The package itself is pretty straight forward: # Copyright 1999-2015 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Id$ EAPI = \"5\" IUSE = \"\" MODS = \"screen\" inherit selinux-policy-2 DESCRIPTION = \"SELinux policy for screen\" if [[ $PV == 9999 * ]] ; then KEYWORDS = \"\" else KEYWORDS = \"~amd64 ~x86\" fi The real workhorse lays within a Gentoo eclass , something that can be seen as a library for ebuilds. It allows consolidation of functions and activities so that a large set of ebuilds can be simplified. The more ebuilds are standardized, the more development can be put inside an eclass instead of in the ebuilds. As a result, some ebuilds are extremely simple, and the SELinux policy ebuilds are a good example of this. The eclass for SELinux policy ebuilds is called selinux-policy-2.eclass and holds a number of functionalities. One of these (the one we focus on right now) is to support custom SELinux policy modules. Custom SELinux policy ebuilds Whenever a user has a SELinux policy that is not part of the Gentoo policy repository, then the user might want to provide these policies through packages still. This has the advantage that Portage (or whatever package manager is used) is aware of the policies on the system, and proper dependencies can be built in. To use a custom policy, the user needs to create an ebuild which informs the eclass not only about the module name (through the MODS variable) but also about the policy files themselves. These files are put in the files/ location of the ebuild, and referred to through the POLICY_FILES variable: # Copyright 1999-2015 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Id$ EAPI = \"5\" IUSE = \"\" MODS = \"oracle\" POLICY_FILES = \"oracle.te oracle.if oracle.fc\" inherit selinux-policy-2 DESCRIPTION = \"SELinux policy for screen\" if [[ $PV == 9999 * ]] ; then KEYWORDS = \"\" else KEYWORDS = \"~amd64 ~x86\" fi The eclass generally will try to build the policies, converting them into .pp files. With CIL, this is no longer needed. Instead, what we do is copy the .cil files straight into the location where we place the .pp files. From that point onwards, managing the .cil files is similar to .pp files. They are loaded with semodule -i and unloaded with semodule -r when needed. Enabling CIL in our ebuilds is a small improvement (after the heavy workload to support the 2.4 userspace) which allows Gentoo to stay ahead in the SELinux world.","tags":"Gentoo","url":"https://blog.siphos.be/2015/09/custom-cil-selinux-policies-in-gentoo/","loc":"https://blog.siphos.be/2015/09/custom-cil-selinux-policies-in-gentoo/"},{"title":"Using multiple OpenSSH daemons","text":"I administer a couple of systems which provide interactive access by end users, and for this interactive access I position OpenSSH . However, I also use this for administrative access to the system, and I tend to have harder security requirements for OpenSSH than most users do. For instance, on one system, end users with a userid + password use the sFTP server for publishing static websites. Other access is prohibited, so I really like this OpenSSH configuration to use chrooted users, internal sftp support, whereas a different OpenSSH is used for administrative access (which is only accessible by myself and some trusted parties). Running multiple instances Although I might get a similar result with a single OpenSSH instance, I prefer to have multiple instances for this. The default OpenSSH port is used for the non-administrative access whereas administrative access is on a non-default port. This has a number of advantages... First of all, the SSH configurations are simple and clean. No complex configurations, and more importantly: easy to manage through configuration management tools like SaltStack , my current favorite orchestration/automation tool. Different instances also allow for different operational support services. There is different monitoring for end-user SSH access versus administrative SSH access. Also the fail2ban configuration is different for these instances. I can also easily shut down the non-administrative service while ensuring that administrative access remains operational - something important in case of changes and maintenance. Dealing with multiple instances and SELinux Beyond enabling a non-default port for SSH (i.e. by marking it as ssh_port_t as well) there is little additional tuning necessary, but that doesn't mean that there is no additional tuning possible. For instance, we could leverage MCS' categories to only allow users (and thus the SSH daemon) access to the files assigned only that category (and not the rest) whereas the administrative SSH daemon can access all categories. On an MLS enabled system we could even use different sensitivity levels, allowing the administrative SSH to access the full scala whereas the user-facing SSH can only access the lowest sensitivity level. But as I don't use MLS myself, I won't go into detail for this. A third possibility would be to fine-tune the permissions of the SSH daemons. However, that would require different types for the daemon, which requires the daemons to be started through different scripts (so that we first transition to dedicated types) before they execute the SSHd binary (which has the sshd_exec_t type assigned). Requiring pubkey and password authentication Recent OpenSSH daemons allow chaining multiple authentication methods before access is granted. This allows the systems to force SSH key authentication first, and then - after succesful authentication - require the password to be passed on as well. Or a second step such as Google Authenticator . AuthenticationMethods publickey,password PasswordAuthentication yes I don't use the Google Authenticator, but the Yubico PAM module to require additional authentication through my U2F dongle (so ssh key, password and u2f key). Don't consider this three-factor authentication: one thing I know (password) and two things I have (U2F and ssh key). It's more that I have a couple of devices with a valid SSH key (laptop, tablet, mobile) which are of course targets for theft. The chance that both one of those devices is stolen together with the U2F dongle (which I don't keep attached to those devices of course) is somewhat less.","tags":"Free Software","url":"https://blog.siphos.be/2015/09/using-multiple-openssh-daemons/","loc":"https://blog.siphos.be/2015/09/using-multiple-openssh-daemons/"},{"title":"Maintaining packages and backporting","text":"A few days ago I committed a small update to policycoreutils , a SELinux related package that provides most of the management utilities for SELinux systems. The fix was to get two patches (which are committed upstream) into the existing release so that our users can benefit from the fixed issues without having to wait for a new release. Getting the patches To capture the patches, I used git together with the commit id: ~$ git format-patch -n -1 73b7ff41 0001-Only-invoke-RPM-on-RPM-enabled-Linux-distributions.patch ~$ git format-patch -n -1 4fbc6623 0001-Set-self.sename-to-sename-after-calling-semanage_seu.patch The two generated patch files contain all information about the commit. Thanks to the epatch support in the eutils.eclass , these patch files are immediately usable within Gentoo's ebuilds. Updating the ebuilds The SELinux userspace ebuilds in Gentoo all have live ebuilds available which are immediately usable for releases. The idea with those live ebuilds is that we can simply copy them and commit in order to make a new release. So, in case of the patch backporting, the necessary patch files are first moved into the files/ subdirectory of the package. Then, the live ebuild is updated to use the new patches: @@ -88,6 +85,8 @@ src_prepare() { epatch \"${FILESDIR}/0070-remove-symlink-attempt-fails-with-gentoo-sandbox-approach.patch\" epatch \"${FILESDIR}/0110-build-mcstrans-bug-472912.patch\" epatch \"${FILESDIR}/0120-build-failure-for-mcscolor-for-CONTEXT__CONTAINS.patch\" + epatch \"${FILESDIR}/0130-Only-invoke-RPM-on-RPM-enabled-Linux-distributions-bug-534682.patch\" + epatch \"${FILESDIR}/0140-Set-self.sename-to-sename-after-calling-semanage-bug-557370.patch\" fi # rlpkg is more useful than fixfiles The patches themselves do not apply for the live ebuilds themselves (they are ignored there) as we want the live ebuilds to be as close to the upstream project as possible. But because the ebuilds are immediately usable for releases, we add the necessary information there first. Next, the new release is created: ~$ cp policycoreutils-9999.ebuild policycoreutils-2.4-r2.ebuild Testing the changes The new release is then tested. I have a couple of scripts that I use for automated testing. So first I update these scripts to also try out the functionality that was failing before. On existing systems, these tests should fail: Running task semanage (Various semanage related operations). ... Executing step \"perm_port_on : Marking portage_t as a permissive domain \" -> ok Executing step \"perm_port_off : Removing permissive mark from portage_t \" -> ok Executing step \"selogin_modify : Modifying a SELinux login definition \" -> failed Then, on a test system where the new package has been installed, the same testset is executed (together with all other tests) to validate if the problem is fixed. Pushing out the new release Finally, with the fixes in and validated, the new release is pushed out (into ~arch first of course) and the bugs are marked as RESOLVED:TEST-REQUEST . Users can confirm that it works (which would move it to VERIFIED:TEST-REQUEST ) or we stabilize it after the regular testing period is over (which moves it to RESOLVED:FIXED or VERIFIED:FIXED ). I do still have to get used to Gentoo using git as its repository now. The workflow to use is documented though. Luckily, because I often get that the git push fails (due to changes to the tree since my last pull). So I need to run git pull --rebase=preserve followed by repoman full and then the push again sufficiently quick after each other). This simple flow is easy to get used to. Thanks to the existing foundation for package maintenance (such as epatch for patching, live ebuilds that can be immediately used for releases and the ability to just cherry pick patches towards our repository) we can serve updates with just a few minutes of work.","tags":"Gentoo","url":"https://blog.siphos.be/2015/09/maintaining-packages-and-backporting/","loc":"https://blog.siphos.be/2015/09/maintaining-packages-and-backporting/"},{"title":"Doing away with interfaces","text":"CIL is SELinux' Common Intermediate Language, which brings on a whole new set of possibilities with policy development. I hardly know CIL but am (slowly) learning. Of course, the best way to learn is to try and do lots of things with it, but real-life work and time-to-market for now forces me to stick with the M4-based refpolicy one. Still, I do try out some things here and there, and one of the things I wanted to look into was how CIL policies would deal with interfaces. Recap on interfaces With the M4 based reference policy, interfaces are M4 macros that expand into the standard SELinux rules. They are used by the reference policy to provide a way to isolate module-specific code and to have \"public\" calls. Policy modules are not allowed (by convention) to call types or domains that are not defined by the same module. If they want to interact with those modules, then they need to call the interface(s): # module \"ntp\" # domtrans: when executing an ntpd_exec_t binary, the resulting process # runs in ntpd_t interface(`ntp_domtrans',` domtrans_pattern($1, ntpd_exec_t, ntpd_t) ) # module \"hal\" ntp_domtrans(hald_t) In the above example, the purpose is to have hald_t be able to execute binaries labeled as ntpd_exec_t and have the resulting process run as the ntpd_t domain. The following would not be allowed inside the hal module: domtrans_pattern(hald_t, ntpd_exec_t, ntpd_t) This would imply that both hald_t , ntpd_exec_t and ntpd_t are defined by the same module, which is not the case. Interfaces in CIL It seems that CIL will not use interface files. Perhaps some convention surrounding it will be created - to know this, we'll have to wait until a \"cilrefpolicy\" is created. However, functionally, this is no longer necessary. Consider the myhttp_client_packet_t declaration from a previous post . In it, we wanted to allow mozilla_t to send and receive these packets. The example didn't use an interface-like construction for this, so let's see how this would be dealt with. First, the module is slightly adjusted to create a macro called myhttp_sendrecv_client_packet : ( macro myhttp_sendrecv_client_packet (( type domain )) ( typeattributeset cil_gen_require domain ) ( allow domain myhttp_client_packet_t ( packet ( send recv ))) ) Another module would then call this: ( call myhttp_sendrecv_client_packet ( mozilla_t )) That's it. When the policy modules are both loaded, then the mozilla_t domain is able to send and receive myhttp_client_packet_t labeled packets. There's more: namespaces But it doesn't end there. Whereas the reference policy had a single namespace for the interfaces, CIL is able to use namespaces. It allows to create an almost object-like approach for policy development. The above myhttp_client_packet_t definition could be written as follows: ( block myhttp ; MyHTTP client packet ( type client_packet_t ) ( roletype object_r client_packet_t ) ( typeattributeset client_packet_type ( client_packet_t )) ( typeattributeset packet_type ( client_packet_t )) ( macro sendrecv_client_packet (( type domain )) ( typeattributeset cil_gen_require domain ) ( allow domain client_packet_t ( packet ( send recv ))) ) ) The other module looks as follows: ( block mozilla ( typeattributeset cil_gen_require mozilla_t ) ( call myhttp.sendrecv_client_packet ( mozilla_t )) ) The result is similar, but not fully the same. The packet is no longer called myhttp_client_packet_t but myhttp.client_packet_t . In other words, a period ( . ) is used to separate the object name ( myhttp ) and the object/type ( client_packet_t ) as well as interface/macro ( sendrecv_client_packet ): ~$ sesearch -s mozilla_t -c packet -p send -Ad ... allow mozilla_t myhttp.client_packet_t : packet { send recv }; And it looks that namespace support goes even further than that, but I still need to learn more about it first. Still, I find this a good evolution. With CIL interfaces are no longer separate from the module definition: everything is inside the CIL file. I secretly hope that tools such as seinfo would support querying macros as well.","tags":"SELinux","url":"https://blog.siphos.be/2015/08/doing-away-with-interfaces/","loc":"https://blog.siphos.be/2015/08/doing-away-with-interfaces/"},{"title":"Slowly converting from GuideXML to HTML","text":"Gentoo has removed its support of the older GuideXML format in favor of using the Gentoo Wiki and a new content management system for the main site (or is it static pages, I don't have the faintest idea to be honest). I do still have a few GuideXML pages in my development space, which I am going to move to HTML pretty soon. In order to do so, I make use of the guidexml2wiki stylesheet I developed . But instead of migrating it to wiki syntax, I want to end with HTML. So what I do is first convert the file from GuideXML to MediaWiki with xsltproc . Next, I use pandoc to convert this to restructured text. The idea is that the main pages on my devpage are now restructured text based. I was hoping to use markdown, but the conversion from markdown to HTML is not what I hoped it was. The restructured text is then converted to HTML using rst2html.py . In the end, I use the following function (for conversion, once): # Convert GuideXML to RestructedText and to HTML gxml2html () { basefile = ${ 1 %%.xml } ; # Convert to Mediawiki syntax xsltproc ~/dev-cvs/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl $1 > ${ basefile } .mediawiki if [ -f ${ basefile } .mediawiki ] ; then # Convert to restructured text pandoc -f mediawiki -t rst -s -S -o ${ basefile } .rst ${ basefile } .mediawiki ; fi if [ -f ${ basefile } .rst ] ; then # Use your own stylesheet links (use full https URLs for this) rst2html.py --stylesheet = link-to-bootstrap.min.css,link-to-tyrian.min.css --link-stylesheet ${ basefile } .rst ${ basefile } .html fi } Is it perfect? No, but it works .","tags":"Gentoo","url":"https://blog.siphos.be/2015/08/slowly-converting-from-guidexml-to-html/","loc":"https://blog.siphos.be/2015/08/slowly-converting-from-guidexml-to-html/"},{"title":"Making the case for multi-instance support","text":"With the high attention that technologies such as Docker , Rocket and the like get (I recommend to look at Bocker by Peter Wilmott as well ;-), I still find it important that technologies are well capable of supporting a multi-instance environment. Being able to run multiple instances makes for great consolidation. The system can be optimized for the technology, access to the system limited to the admins of said technology while still providing isolation between instances. For some technologies, running on commodity hardware just doesn't cut it (not all software is written for such hardware platforms) and consolidation allows for reducing (hardware/licensing) costs. Examples of multi-instance technologies A first example that I'm pretty familiar with is multi-instance database deployments: Oracle DBs, SQL Servers, PostgreSQLs, etc. The consolidation of databases while still keeping multiple instances around (instead of consolidating into a single instance itself) is mainly for operational reasons (changes should not influence other database/schema's) or technical reasons (different requirements in parameters, locales, etc.) Other examples are web servers (for web hosting companies), which next to virtual host support (which is still part of a single instance) could benefit from multi-instance deployments for security reasons (vulnerabilities might be better contained then) as well as performance tuning. Same goes for web application servers (such as TomCat deployments). But even other technologies like mail servers can benefit from multiple instance deployments. Postfix has a nice guide on multi-instance deployments and also covers some of the use cases for it. Advantages of multi-instance setups The primary objective that most organizations have when dealing with multiple instances is the consolidation to reduce cost. Especially expensive, propriatary software which is CPU licensed gains a lot from consolidation (and don't think a CPU is a CPU, each company has its (PDF) own (PDF) core weight table to get the most money out of their customers). But beyond cost savings, using multi-instance deployments also provides for resource sharing. A high-end server can be used to host the multiple instances, with for instance SSD disks (or even flash cards), more memory, high-end CPUs, high-speed network connnectivity and more. This improves performance considerably, because most multi-instance technologies don't need all resources continuously. Another advantage, if properly designed, is that multi-instance capable software can often leverage the multi-instance deployments for fast changes. A database might be easily patched (remove vulnerabilities) by creating a second codebase deployment, patching that codebase, and then migrating the database from one instance to another. Although it often still requires downtime, it can be made considerably less, and roll-back of such changes is very easy. A last advantage that I see is security. Instances can be running as different runtime accounts, through different SELinux contexts, bound on different interfaces or chrooted into different locations. This is not an advantage compared to dedicated systems of course, but more an advantage compared to full consolidation (everything in a single instance). Don't always focus on multi-instance setups though Multiple instances isn't a silver bullet. Some technologies are generally much better when there is a single instance on a single operating system. Personally, I find that such technologies should know better. If they are really designed to be suboptimal in case of multi-instance deployments, then there is a design error. But when the advantages of multiple instances do not exist (no license cost, hardware cost is low, etc.) then organizations might focus on single-instance deployments, because multi-instance deployments might require more users to access the system (especially when it is multi-tenant) operational activities might impact other instances (for instance updating kernel parameters for one instance requires a reboot which affects other instances) the software might not be properly \"multi-instance aware\" and as such starts fighting for resources with its own sigbling instances Given that properly designed architectures are well capable of using virtualization (and in the future containerization) moving towards single-instance deployments becomes more and more interesting. What should multi-instance software consider? Software should, imo, always consider multi-instance deployments. Even when the administrator decides to stick with a single instance, all that that takes is that the software ends up with a \"single instance\" setup (it is much easier to support multiple instances and deploy a single one, than to support single instances and deploy multiple ones). The first thing software should take into account is that it might (and will) run with different runtime accounts - service accounts if you whish. That means that the software should be well aware that file locations are separate, and that these locations will have different access control settings on them (if not just a different owner). So instead of using /etc/foo as the mandatory location, consider supporting /etc/foo/instance1 , /etc/foo/instance2 if full directories are needed, or just have /etc/foo1.conf and /etc/foo2.conf . I prefer the directory approach, because it makes management much easier. It then also makes sense that the log location is /var/log/foo/instance1 , the data files are at /var/lib/foo/instance1 , etc. The second is that, if a service is network-facing (which most of them are), it must be able to either use multihomed systems easily (bind to different interfaces) or use different ports. The latter is a challenge I often come across with software - the way to configure the software to deal with multiple deployments and multiple ports is often a lengthy trial-and-error setup. What's so difficult with using a base port setting, and document how the other ports are derived from this base port. Neo4J needs 3 ports for its enterprise services (transactions, cluster management and online backup), but they all need to be explicitly configured if you want a multi-instance deployment. What if one could just set baseport = 5001 with the software automatically selecting 5002 and 5003 as other ports (or 6001 and 7001). If the software in the future needs another port, there is no need to update the configuration (assuming the administrator leaves sufficient room). Also consider the service scripts ( /etc/init.d ) or similar (depending on the init system used). Don't provide a single one which only deals with one instance. Instead, consider supporting symlinked service scripts which automatically obtain the right configuration from its name. For instance, a service script called pgsql-inst1 which is a symlink to /etc/init.d/postgresql could then look for its configuration in /var/lib/postgresql/pgsql-inst1 (or /etc/postgresql/pgsql-inst1 ). Just like supporting .d directories , I consider multi-instance support an important non-functional requirement for software.","tags":"Architecture","url":"https://blog.siphos.be/2015/08/making-the-case-for-multi-instance-support/","loc":"https://blog.siphos.be/2015/08/making-the-case-for-multi-instance-support/"},{"title":"Switching OpenSSH to ed25519 keys","text":"With Mike's news item on OpenSSH's deprecation of the DSA algorithm for the public key authentication, I started switching the few keys I still had using DSA to the suggested ED25519 algorithm. Of course, I wouldn't be a security-interested party if I did not do some additional investigation into the DSA versus Ed25519 discussion. The issue with DSA You might find DSA a bit slower than RSA: ~$ openssl speed rsa1024 rsa2048 dsa1024 dsa2048 ... sign verify sign/s verify/s rsa 1024 bits 0.000127s 0.000009s 7874.0 111147.6 rsa 2048 bits 0.000959s 0.000029s 1042.9 33956.0 sign verify sign/s verify/s dsa 1024 bits 0.000098s 0.000103s 10213.9 9702.8 dsa 2048 bits 0.000293s 0.000339s 3407.9 2947.0 As you can see, RSA verification outperforms DSA in verification, while signing with DSA is better than RSA. But for what OpenSSH is concerned, this speed difference should not be noticeable on the vast majority of OpenSSH servers. So no, it is not the speed, but the secure state of the DSS standard. The OpenSSH developers find that ssh-dss (DSA) is too weak , which is followed by various sources . Considering the impact of these keys, it is important that they follow the state-of-the-art cryptographic services. Instead, they suggest to switch to elliptic curve cryptography based algorithms, with Ed25519 and Curve25519 coming out on top. Switch to RSA or ED25519? Given that RSA is still considered very secure, one of the questions is of course if ED25519 is the right choice here or not. I don't consider myself anything in cryptography, but I do like to validate stuff through academic and (hopefully) reputable sources for information (not that I don't trust the OpenSSH and OpenSSL folks, but more from a broader interest in the subject). Ed25519 should be written fully as Ed25519-SHA-512 and is a signature algorithm. It uses elliptic curve cryptography as explained on the EdDSA wikipedia page . An often cited paper is Fast and compact elliptic-curve cryptography by Mike Hamburg, which talks about the performance improvements, but the main paper is called High-speed high-security signatures which introduces the Ed25519 implementation. Of the references I was able to (quickly) go through (not all papers are publicly reachable) none showed any concerns about the secure state of the algorithm. The (simple) process of switching Switching to Ed25519 is simple. First, generate the (new) SSH key (below just an example run): ~$ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/home/testuser/.ssh/id_ed25519): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/testuser/.ssh/id_ed25519. Your public key has been saved in /home/testuser/.ssh/id_ed25519.pub. The key fingerprint is: SHA256:RDaEw3tNAKBGMJ2S4wmN+6P3yDYIE+v90Hfzz/0r73M testuser@testserver The key's randomart image is: +--[ED25519 256]--+ |o*...o.+*. | |*o+. +o .. | |o++ o.o | |o+ ... . | | + .S | |+ o . | |o+.o . . o | |oo+o. . . o ....E| | oooo. ..o+=*| +----[SHA256]-----+ Then, make sure that the ~/.ssh/authorized_keys file contains the public key (as generated as id_ed25519.pub ). Don't remove the other keys yet until the communication is validated. For me, all I had to do was to update the file in the Salt repository and have the master push the changes to all nodes (starting with non-production first of course). Next, try to log on to the system using the Ed25519 key: ~$ ssh -i ~/.ssh/id_ed25519 testuser@testserver Make sure that your SSH agent is not running as it might still try to revert back to another key if the Ed25519 one does not work. You can validate if the connection was using Ed25519 through the auth.log file: ~$ sudo tail -f auth.log Aug 17 21:20:48 localhost sshd[13962]: Accepted publickey for root from \\ 192.168.100.1 port 43152 ssh2: ED25519 SHA256:-------redacted---------------- If this communication succeeds, then you can remove the old key from the ~/.ssh/authorized_keys files. On the client level, you might want to hide ~/.ssh/id_dsa from the SSH agent: # Obsolete - keychain ~/.ssh/id_dsa keychain ~/.ssh/id_ed25519 If a server update was forgotten, then the authentication will fail and, depending on the configuration, either fall back to the regular authentication or fail immediately. This gives a nice heads-up to you to update the server, while keeping the key handy just in case. Just refer to the old id_dsa key during the authentication and fix up the server.","tags":"Free Software","url":"https://blog.siphos.be/2015/08/switching-openssh-to-ed25519-keys/","loc":"https://blog.siphos.be/2015/08/switching-openssh-to-ed25519-keys/"},{"title":"Updates on my Pelican adventure","text":"It's been a few weeks that I switched my blog to Pelican , a static site generator build with Python. A number of adjustments have been made since, which I'll happily talk about. The full article view on index page One of the features I wanted was to have my latest blog post to be fully readable from the front page (called the index page within Pelican). Sadly, I could not find a plugin of setting that would do this, but I did find a plugin that I can use to work around this: the summary plugin. Enabling the plugin was a breeze. Extract the plugin sources in the plugin/ folder, and enable it in pelicanconf.py : PLUGINS = [ ... , 'summary' ] With this plug-in, articles can use inline comments to tell the system at which point the summary of the article stops. Usually, the summary (which is displayed on index pages) is a first paragraph (or set of paragraphs). What I do is I now manually set the summmary to the entire blog post for the latest post, and adjust later when a new post comes up. It might be some manual labour, but it fits nicely and doesn't hack around in the code too much. Commenting with Disqus I had some remarks that the Disqus integration is not as intuitive as expected. Some readers had difficulties finding out how to comment as a guest (without the need to log on through popular social media or through Disqus itself). Agreed, it is not easy to see at first sight that people need to start typing their name in the Or sign up with disqus before they can select I'd rather post as guest . As I don't have any way of controlling the format and rendered code with Disqus, I updated the theme a bit to add in two paragraphs on commenting. The first paragraph tells how to comment as guest. The second paragraph for now informs readers that non-verified comments are put in the moderation queue. Once I get a feeling of how the spam and bots act on the commenting system, I will adjust the filters and also allow guest comments to be readily accessible (no moderation queue). Give it a few more weeks to get myself settled and I'll adjust it. If the performance of the site is slowed down due to the Disqus javascripts: both Firefox (excuse me, Aurora) and Chromium have this at the initial load. Later, the scripts are properly cached and load in relatively fast (a quick test shows all pages I tried load in less than 2 seconds - WordPress was at 4). And if you're not interested in commenting, then you can even use NoScript or similar plugins to disallow any remote javascript. Still, I will continue to look at how to make commenting easier. I recently allowed unmoderated comments (unless a number of keywords are added, and comments with links are also put in the moderation queue). If someone knows of another comment-like system that I could integrate I'm happy to hear about it as well. Search My issue with Tipue Search has been fixed by reverting a change in tipue_search.py (the plugin) where the URL was assigned to the loc key instead of url . It is probably a mismatch between the plugin and the theme (the change of the key was done in May in Tipue Search itself). With this minor issue changed, the search capabilities are back on track on my blog. Enabling is was a matter of: PLUGINS = [ ... , ` tipue_search ` ] DIRECT_TEMPLATES = (( ... , 'search' )) Tags and categories WordPress supports multiple categories, but Pelican does not. So I went through the various posts that had multiple categories and decided on a single one. While doing so, I also reduced the categories to a small set: Databases Documentation Free Software Gentoo Misc Security SELinux I will try to properly tag all posts so that, if someone is interested in a very particular topic, such as PostgreSQL , he can reach those posts through the tag.","tags":"Free Software","url":"https://blog.siphos.be/2015/08/updates-on-my-pelican-adventure/","loc":"https://blog.siphos.be/2015/08/updates-on-my-pelican-adventure/"},{"title":"Finding a good compression utility","text":"I recently came across a wiki page written by Herman Brule which gives a quick benchmark on a couple of compression methods / algorithms. It gave me the idea of writing a quick script that tests out a wide number of compression utilities available in Gentoo (usually through the app-arch category), with also a number of options (in case multiple options are possible). The currently supported packages are: app-arch/bloscpack app-arch/bzip2 app-arch/freeze app-arch/gzip app-arch/lha app-arch/lrzip app-arch/lz4 app-arch/lzip app-arch/lzma app-arch/lzop app-arch/mscompress app-arch/p7zip app-arch/pigz app-arch/pixz app-arch/plzip app-arch/pxz app-arch/rar app-arch/rzip app-arch/xar app-arch/xz-utils app-arch/zopfli app-arch/zpaq The script should keep the best compression information: duration, compression ratio, compression command, as well as the compressed file itself. Finding the \"best\" compression It is not my intention to find the most optimal compression, as that would require heuristic optimizations (which has triggered my interest in seeking such software, or writing it myself) while trying out various optimization parameters. No, what I want is to find the \"best\" compression for a given file, with \"best\" being either most reduced size (which I call compression delta in my script) best reduction obtained per time unit (which I call the efficiency ) For me personally, I think I would use it for the various raw image files that I have through the photography hobby. Those image files are difficult to compress (the Nikon DS3200 I use is an entry-level camera which applies lossy compression already for its raw files) but their total size is considerable, and it would allow me to better use the storage I have available both on my laptop (which is SSD-only) as well as backup server. But next to the best compression ratio, the efficiency is also an important metric as it shows how efficient the algorithm works in a certain time aspect. If one compression method yields 80% reduction in 5 minutes, and another one yields 80,5% in 45 minutes, then I might want to prefer the first one even though that is not the best compression at all. Although the script could be used to get the most compression (without resolving to an optimization algorithm for the compression commands) for each file, this is definitely not the use case. A single run can take hours for files that are compressed in a handful of seconds. But it can show the best algorithms for a particular file type (for instance, do a few runs on a couple of raw image files and see which method is most succesful). Another use case I'm currently looking into is how much improvement I can get when multiple files (all raw image files) are first grouped in a single archive ( .tar ). Theoretically, this should improve the compression, but by how much? How the script works The script does not contain much intelligence. It iterates over a wide set of compression commands that I tested out, checks the final compressed file size, and if it is better than a previous one it keeps this compressed file (and its statistics). I tried to group some of the compressions together based on the algorithm used, but as I don't really know the details of the algorithms (it's based on manual pages and internet sites) and some of them combine multiple algorithms, it is more of a high-level selection than anything else. The script can also only run the compressions of a single application (which I use when I'm fine-tuning the parameter runs). A run shows something like the following: Original file (test.nef) size 20958430 bytes package name command duration size compr.Δ effic.: ------------ ------- -------- ---- ------- ------- app-arch/bloscpack blpk -n 4 0.1 20947097 0.00054 0.00416 app-arch/bloscpack blpk -n 8 0.1 20947097 0.00054 0.00492 app-arch/bloscpack blpk -n 16 0.1 20947097 0.00054 0.00492 app-arch/bzip2 bzip2 2.0 19285616 0.07982 0.03991 app-arch/bzip2 bzip2 -1 2.0 19881886 0.05137 0.02543 app-arch/bzip2 bzip2 -2 1.9 19673083 0.06133 0.03211 ... app-arch/p7zip 7za -tzip -mm=PPMd 5.9 19002882 0.09331 0.01592 app-arch/p7zip 7za -tzip -mm=PPMd -mmem=24 5.7 19002882 0.09331 0.01640 app-arch/p7zip 7za -tzip -mm=PPMd -mmem=25 6.4 18871933 0.09955 0.01551 app-arch/p7zip 7za -tzip -mm=PPMd -mmem=26 7.7 18771632 0.10434 0.01364 app-arch/p7zip 7za -tzip -mm=PPMd -mmem=27 9.0 18652402 0.11003 0.01224 app-arch/p7zip 7za -tzip -mm=PPMd -mmem=28 10.0 18521291 0.11628 0.01161 app-arch/p7zip 7za -t7z -m0=PPMd 5.7 18999088 0.09349 0.01634 app-arch/p7zip 7za -t7z -m0=PPMd:mem=24 5.8 18999088 0.09349 0.01617 app-arch/p7zip 7za -t7z -m0=PPMd:mem=25 6.5 18868478 0.09972 0.01534 app-arch/p7zip 7za -t7z -m0=PPMd:mem=26 7.5 18770031 0.10442 0.01387 app-arch/p7zip 7za -t7z -m0=PPMd:mem=27 8.6 18651294 0.11008 0.01282 app-arch/p7zip 7za -t7z -m0=PPMd:mem=28 10.6 18518330 0.11643 0.01100 app-arch/rar rar 0.9 20249470 0.03383 0.03980 app-arch/rar rar -m0 0.0 20958497 -0.00000 -0.00008 app-arch/rar rar -m1 0.2 20243598 0.03411 0.14829 app-arch/rar rar -m2 0.8 20252266 0.03369 0.04433 app-arch/rar rar -m3 0.8 20249470 0.03383 0.04027 app-arch/rar rar -m4 0.9 20248859 0.03386 0.03983 app-arch/rar rar -m5 0.8 20248577 0.03387 0.04181 app-arch/lrzip lrzip -z 13.1 19769417 0.05673 0.00432 app-arch/zpaq zpaq 0.2 20970029 -0.00055 -0.00252 The best compression was found with 7za -t7z -m0=PPMd:mem=28. The compression delta obtained was 0.11643 within 10.58 seconds. This file is now available as test.nef.7z. In the above example, the test file was around 20 MByte. The best compression compression command that the script found was: ~$ 7za -t7z -m0=PPMd:mem=28 a test.nef.7z test.nef The resulting file ( test.nef.7z ) is 18 MByte, a reduction of 11,64%. The compression command took almost 11 seconds to do its thing, which gave an efficiency rating of 0,011, which is definitely not a fast one. Some other algorithms don't do bad either with a better efficiency. For instance: app-arch/pbzip2 pbzip2 0.6 19287402 0.07973 0.13071 In this case, the pbzip2 command got almost 8% reduction in less than a second, which is considerably more efficient than the 11-seconds long 7za run. Want to try it out yourself? I've pushed the script to my github location. Do a quick review of the code first (to see that I did not include anything malicious) and then execute it to see how it works: ~$ sw_comprbest -h Usage: sw_comprbest --infile=<inputfile> [--family=<family>[,...]] [--command=<cmd>] sw_comprbest -i <inputfile> [-f <family>[,...]] [-c <cmd>] Supported families: blosc bwt deflate lzma ppmd zpaq. These can be provided comma-separated. Command is an additional filter - only the tests that use this base command are run. The output shows - The package (in Gentoo) that the command belongs to - The command run - The duration (in seconds) - The size (in bytes) of the resulting file - The compression delta (percentage) showing how much is reduced (higher is better) - The efficiency ratio showing how much reduction (percentage) per second (higher is better) When the command supports multithreading, we use the number of available cores on the system (as told by /proc/cpuinfo). For instance, to try it out against a PDF file: ~$ sw_comprbest -i MEA6-Sven_Vermeulen-Research_Summary.pdf Original file (MEA6-Sven_Vermeulen-Research_Summary.pdf) size 117763 bytes ... The best compression was found with zopfli --deflate. The compression delta obtained was 0.00982 within 0.19 seconds. This file is now available as MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate. So in this case, the resulting file is hardly better compressed - the PDF itself is already compressed. Let's try it against the uncompressed PDF: ~$ pdftk MEA6-Sven_Vermeulen-Research_Summary.pdf output test.pdf uncompress ~$ sw_comprbest -i test.pdf Original file (test.pdf) size 144670 bytes ... The best compression was found with lrzip -z. The compression delta obtained was 0.27739 within 0.18 seconds. This file is now available as test.pdf.lrz. This is somewhat better: ~$ ls -l MEA6-Sven_Vermeulen-Research_Summary.pdf* test.pdf* -rw-r--r--. 1 swift swift 117763 Aug 7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf -rw-r--r--. 1 swift swift 116606 Aug 7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate -rw-r--r--. 1 swift swift 144670 Aug 7 14:34 test.pdf -rw-r--r--. 1 swift swift 104540 Aug 7 14:35 test.pdf.lrz The resulting file is 11,22% reduced from the original one.","tags":"Gentoo","url":"https://blog.siphos.be/2015/08/finding-a-good-compression-utility/","loc":"https://blog.siphos.be/2015/08/finding-a-good-compression-utility/"},{"title":"Why we do confine Firefox","text":"If you're a bit following the SELinux development community you will know Dan Walsh , a Red Hat security engineer. Today he blogged about CVE-2015-4495 and SELinux, or why doesn't SELinux confine Firefox . He should've asked why the reference policy or Red Hat/Fedora policy does not confine Firefox, because SELinux is, as I've mentioned before , not the same as its policy. In effect, Gentoo's SELinux policy does confine Firefox by default. One of the principles we focus on in Gentoo Hardened is to develop desktop policies in order to reduce exposure and information leakage of user documents. We might not have the manpower to confine all desktop applications, but I do think it is worthwhile to at least attempt to do this, even though what Dan Walsh mentioned is also correct: desktops are notoriously difficult to use a mandatory access control system on. How Gentoo wants to support more confined desktop applications What Gentoo Hardened tries to do is to support the XDG Base Directory Specification for several documentation types. Downloads are marked as xdg_downloads_home_t , pictures are marked as xdg_pictures_home_t , etc. With those types defined, we grant the regular user domains full access to those types, but start removing access to user content from applications. Rules such as the following are commented out or removed from the policies: # userdom_manage_user_home_content_dirs(mozilla_t) # userdom_manage_user_home_content_files(mozilla_t) Instead, we add in a call to a template we have defined ourselves: userdom_user_content_access_template(mozilla, { mozilla_t mozilla_plugin_t }) This call makes access to user content optional through SELinux booleans. For instance, for the mozilla_t domain (which is used for Firefox), the following booleans are created: # Read generic (user_home_t) user content mozilla_read_generic_user_content -> true # Read all user content mozilla_read_all_user_content -> false # Manage generic (user_home_t) user content mozilla_manage_generic_user_content -> false # Manage all user content mozilla_manage_all_user_content -> false As you can see, the default setting is that Firefox can read user content, but only non-specific types. So ssh_home_t , which is used for the SSH related files, is not readable by Firefox with our policy by default . By changing these booleans, the policy is fine-tuned to the requirements of the administrator. On my systems, mozilla_read_generic_user_content is switched off. You might ask how we can then still support a browser if it cannot access user content to upload or download. Well, as mentioned before, we support the XDG types. The browser is allowed to manage xdg_download_home_t files and directories. For the majority of cases, this is sufficient. I also don't mind copying over files to the ~/Downloads directory just for uploading files. But I am well aware that this is not what the majority of users would want, which is why the default is as it is. There is much more work to be done sadly As said earlier, the default policy will allow reading of user files if those files are not typed specifically. Types that are protected by our policy (but not by the reference policy standard) includes SSH related files at ~/.ssh and GnuPG files at ~/.gnupg . Even other configuration files, such as for my Mutt configuration ( ~/.muttrc ) which contains a password for an IMAP server I connect to, are not reachable. However, it is still far from perfect. One of the reasons is that many desktop applications are not \"converted\" yet to our desktop policy approach. Yes, Chromium is also already converted, and policies we've added such as for Skype also do not allow direct access unless the user explicitly enabled it. But Evolution for instance isn't yet. Converting desktop policies to a more strict setup requires lots of testing, which translates to many human resources. Within Gentoo, only a few developers and contributors are working on policies, and considering that this is not a change that is already part of the (upstream) reference policy, some contributors also do not want to put lots of focus on it either. But without having done the works, it will not be easy (nor probably acceptable) to upstream this (the XDG patch has been submitted a few times already but wasn't deemed ready yet then). Having a more restrictive policy isn't the end As the blog post of Dan rightly mentioned, there are still quite some other ways of accessing information that we might want to protect. An application might not have access to user files, but can be able to communicate (for instance through DBus) with an application that does, and through that instruct it to pass on the data. Plugins might require permissions which do not match with the principles set up earlier. When we tried out Google Talk (needed for proper Google Hangouts support) we noticed that it requires many, many more privileges. Luckily, we were able to write down and develop a policy for the Google Talk plugin ( googletalk_plugin_t ) so it is still properly confined. But this is just a single plugin, and I'm sure that more plugins exist which will have similar requirements. Which leads to more policy development. But having workarounds does not make the effort we do worthless. Being able to work around a firewall through application data does not make the firewall useless, it is just one of the many security layers. The same is true with SELinux policies. I am glad that we at least try to confine desktop applications more, and that Gentoo Hardened users who use SELinux are at least somewhat more protected from the vulnerability (even with the default case) and that our investment for this is sound.","tags":"SELinux","url":"https://blog.siphos.be/2015/08/why-we-do-confine-firefox/","loc":"https://blog.siphos.be/2015/08/why-we-do-confine-firefox/"},{"title":"Can SELinux substitute DAC?","text":"A nice twitter discussion with Erling Hellenäs caught my full attention later when I was heading home: Can SELinux substitute DAC? I know it can't and doesn't in the current implementation, but why not and what would be needed? SELinux is implemented through the Linux Security Modules framework which allows for different security systems to be implemented and integrated in the Linux kernel. Through LSM, various security-sensitive operations can be secured further through additional access checks. This criteria was made to have LSM be as minimally invasive as possible. The LSM design The basic LSM design paper, called Linux Security Modules: General Security Support for the Linux Kernel as presented in 2002, is still one of the better references for learning and understanding LSM. It does show that there was a whish-list from the community where LSM hooks could override DAC checks, and that it has been partially implemented through permissive hooks (not to be mistaken with SELinux' permissive mode). However, this definitely is partially implemented because there are quite a few restrictions. One of them is that, if a request is made towards a resource and the UIDs match (see page 3, figure 2 of the paper) then the LSM hook is not consulted. When they don't match, a permissive LSM hook can be implemented. Support for permissive hooks is implemented for capabilities, a powerful DAC control that Linux supports and which is implemented through LSM as well. I have blogged about this nice feature a while ago. These restrictions are also why some other security-conscious developers, such as grsecurity's team and RSBAC do not use the LSM system. Well, it's not only through these restrictions of course - other reasons play a role in them as well. But knowing what LSM can (and cannot) do also shows what SELinux can and cannot do. The LSM design itself is already a reason why SELinux cannot substitute DAC controls. But perhaps we could disable DAC completely and thus only rely on SELinux? Disabling DAC in Linux would be an excessive workload The discretionary access controls in the Linux kernel are not easy to remove. They are often part of the code itself (just grep through the source code after -EPERM ). Some subsystems which use a common standard approach (such as VFS operations) can rely on good integrated security controls, but these too often allow the operation if DAC allows it, and will only consult the LSM hooks otherwise. VFS operations are the most known ones, but DAC controls go beyond file access. It also entails reading program memory, sending signals to applications, accessing hardware and more. But let's focus on the easier controls (as in, easier to use examples for), such as sharing files between users, restricting access to personal documents and authorizing operations in applications based on the user id (for instance, the owner can modify while other users can only read the file). We could \"work around\" the Linux DAC controls by running everything as a single user (the root user) and having all files and resources be fully accessible by this user. But the problem with that is that SELinux would not be able to take over controls either, because you will need some user-based access controls, and within SELinux this implies that a mapping is done from a user to a SELinux user. Also, access controls based on the user id would no longer work, and unless the application is made SELinux-aware it would lack any authorization system (or would need to implement it itself). With DAC Linux also provides quite some \"freedom\" which is well established in the Linux (and Unix) environment: a simple security model where the user and group membership versus the owner-privileges, group-privileges and \"rest\"-privileges are validated. Note that SELinux does not really know what a \"group\" is. It knows SELinux users, roles, types and sensitivities. So, suppose we would keep multi-user support in Linux but completely remove the DAC controls and rely solely on LSM (and SELinux). Is this something reusable? Using SELinux for DAC-alike rules Consider the use case of two users. One user wants another user to read a few of his files. With DAC controls, he can \"open up\" the necessary resources (files and directories) through extended access control lists so that the other user can access it. No need to involve administrators. With a MAC(-only) system, updates on the MAC policy usually require the security administrator to write additional policy rules to allow something. With SELinux (and without DAC) it would require the users to be somewhat isolated from each other (otherwise the users can just access everything from each other), which SELinux can do through User Based Access Control , but the target resource itself should be labeled with a type that is not managed through the UBAC control. Which means that the users will need the privilege to change labels to this type (which is possible!), assuming such a type is already made available for them. Users can't create new types themselves. UBAC is by default disabled in many distributions, because it has some nasty side-effects that need to be taken into consideration. Just recently one of these came up on the refpolicy mailinglist . But even with UBAC enabled (I have it enabled on most of my systems, but considering that I only have a couple of users to manage and am administrator on these systems to quickly \"update\" rules when necessary) it does not provide equal functionality as DAC controls. As mentioned before, SELinux does not know group membership. In order to create something group-like, we will probably need to consider roles. But in SELinux, roles are used to define what types are transitionable towards - it is not a membership approach. A type which is usable by two roles (for instance, the mozilla_t type which is allowed for staff_r and user_r ) does not care about the role. This is unlike group membership. Also, roles only focus on transitionable types (known as domains). It does not care about accessible resources (regular file types for instance). In order to allow one person to read a certain file type but not another, SELinux will need to control that one person can read this file through a particular domain while the other user can't. And given that domains are part of the SELinux policy, any situation that the policy has not thought about before will not be easily adaptable. So, we can't do it? Well, I'm pretty sure that a very extensive policy and set of rules can be made for SELinux which would make a number of DAC permissions obsolete, and that we could theoretically remove DAC from the Linux kernel. End users would require a huge training to work with this system, and it would not be reusable across other systems in different environments, because the policy will be too specific to the system (unlike the current reference policy based ones, which are quite reusable across many distributions). Furthermore, the effort to create these policies would be extremely high, whereas the DAC permissions are very simple to implement, and have been proven to be well suitable for many secured systems. So no, unless you do massive engineering, I do not believe it is possible to substitute DAC with SELinux-only controls.","tags":"SELinux","url":"https://blog.siphos.be/2015/08/can-selinux-substitute-dac/","loc":"https://blog.siphos.be/2015/08/can-selinux-substitute-dac/"},{"title":"Filtering network access per application","text":"Iptables (and the successor nftables) is a powerful packet filtering system in the Linux kernel, able to create advanced firewall capabilities. One of the features that it cannot provide is per-application filtering. Together with SELinux however, it is possible to implement this on a per domain basis. SELinux does not know applications, but it knows domains. If we ensure that each application runs in its own domain, then we can leverage the firewall capabilities with SELinux to only allow those domains access that we need. SELinux network control: packet types The basic network control we need to enable is SELinux' packet types. Most default policies will grant application domains the right set of packet types: ~# sesearch -s mozilla_t -c packet -A Found 13 semantic av rules: allow mozilla_t ipp_client_packet_t : packet { send recv } ; allow mozilla_t soundd_client_packet_t : packet { send recv } ; allow nsswitch_domain dns_client_packet_t : packet { send recv } ; allow mozilla_t speech_client_packet_t : packet { send recv } ; allow mozilla_t ftp_client_packet_t : packet { send recv } ; allow mozilla_t http_client_packet_t : packet { send recv } ; allow mozilla_t tor_client_packet_t : packet { send recv } ; allow mozilla_t squid_client_packet_t : packet { send recv } ; allow mozilla_t http_cache_client_packet_t : packet { send recv } ; DT allow mozilla_t server_packet_type : packet recv ; [ mozilla_bind_all_unreserved_ports ] DT allow mozilla_t server_packet_type : packet send ; [ mozilla_bind_all_unreserved_ports ] DT allow nsswitch_domain ldap_client_packet_t : packet recv ; [ authlogin_nsswitch_use_ldap ] DT allow nsswitch_domain ldap_client_packet_t : packet send ; [ authlogin_nsswitch_use_ldap ] As we can see, the mozilla_t domain is able to send and receive packets of type ipp_client_packet_t , soundd_client_packet_t , dns_client_packet_t , speech_client_packet_t , ftp_client_packet_t , http_client_packet_t , tor_client_packet_t , squid_client_packet_t and http_cache_client_packet_t . If the SELinux booleans mentioned at the end are enabled, additional packet types are alloed to be used as well. But even with this default policy in place, SELinux is not being consulted for filtering. To accomplish this, iptables will need to be told to label the incoming and outgoing packets. This is the SECMARK functionality that I've blogged about earlier. Enabling SECMARK filtering through iptables To enable SECMARK filtering, we use the iptables command and tell it to label SSH incoming and outgoing packets as ssh_server_packet_t : ~# iptables -t mangle -A INPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore ~# iptables -t mangle -A INPUT -p tcp --dport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0 ~# iptables -t mangle -A OUTPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore ~# iptables -t mangle -A OUTPUT -p tcp --sport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0 But be warned: the moment iptables starts with its SECMARK support, all packets will be labeled. Those that are not explicitly labeled through one of the above commands will be labeled with the unlabeled_t type, and most domains are not allowed any access to unlabeled_t . There are two things we can do to improve this situation: Define the necessary SECMARK rules for all supported ports (which is something that secmarkgen does), and/or Allow unlabeled_t for all domains. To allow the latter, we can load a SELinux rule like the following: ( allow domain unlabeled_t ( packet ( send recv ))) This will allow all domains to send and receive packets of the unlabeled_t type. Although this is something that might be security-sensitive, it might be a good idea to allow at start, together with proper auditing (you can use (auditallow ...) to audit all granted packet communication) so that the right set of packet types can be enabled. This way, administrators can iteratively improve the SECMARK rules and finally remove the unlabeled_t privilege from the domain attribute. To list the current SECMARK rules, list the firewall rules for the mangle table: ~# iptables -t mangle -nvL Only granting one application network access These two together allow for creating a firewall that only allows a single domain access to a particular target. For instance, suppose that we only want the mozilla_t domain to connect to the company proxy (10.15.10.5). We can't enable the http_client_packet_t for this connection, as all other web browsers and other HTTP-aware applications will have policy rules enabled to send and receive that packet type. Instead, we are going to create a new packet type to use. ;; Definition of myhttp_client_packet_t ( type myhttp_client_packet_t ) ( roletype object_r myhttp_client_packet_t ) ( typeattributeset client_packet_type ( myhttp_client_packet_t )) ( typeattributeset packet_type ( myhttp_client_packet_t )) ;; Grant the use to mozilla_t ( typeattributeset cil_gen_require mozilla_t ) ( allow mozilla_t myhttp_client_packet_t ( packet ( send recv ))) Putting the above in a myhttppacket.cil file and loading it allows the type to be used: ~# semodule -i myhttppacket.cil Now, the myhttp_client_packet_t type can be used in iptables rules. Also, only the mozilla_t domain is allowed to send and receive these packets, effectively creating an application-based firewall, as all we now need to do is to mark the outgoing packets towards the proxy as myhttp_client_packet_t : ~# iptables -t mangle -A OUTPUT -p tcp --dport 80 -d 10.15.10.5 -j SECMARK --selctx system_u:object_r:myhttp_client_packet_t:s0 This shows that it is possible to create such firewall rules with SELinux. It is however not an out-of-the-box solution, requiring thought and development of both firewall rules and SELinux code constructions. Still, with some advanced scripting experience this will lead to a powerful addition to a hardened system.","tags":"SELinux","url":"https://blog.siphos.be/2015/08/filtering-network-access-per-application/","loc":"https://blog.siphos.be/2015/08/filtering-network-access-per-application/"},{"title":"My application base: Obnam","text":"It is often said, yet too often forgotten: taking backups (and verifying that they work). Taking backups is not purely for companies and organizations. Individuals should also take backups to ensure that, in case of errors or calamities, the all important files are readily recoverable. For backing up files and directories, I personally use obnam , after playing around with Bacula and attic . Bacula is more meant for large distributed environments (although I also tend to use obnam for my server infrastructure) and was too complex for my taste. The choice between obnam and attic is even more personally-oriented. I found attic to be faster, but with a small supporting community. Obnam was slower, but seems to have a more active community which I find important for infrastructure that is meant to live quite long (you don't want to switch backup solutions every year). I also found it pretty easy to work with, and to restore files back, and Gentoo provides the app-backup/obnam package. I think both are decent solutions, so I had to make one choice and ended up with obnam. So, how does it work? Configuring what to backup The basic configuration file for obnam is /etc/obnam.conf . Inside this file, I tell which directories need to be backed up, as well as which subdirectories or files (through expressions) can be left alone. For instance, I don't want obnam to backup ISO files as those have been downloaded anyway. [config] repository = /srv/backup root = /root, /etc, /var/lib/portage, /srv/virt/gentoo, /home exclude = \\.img$, \\.iso$, /home/[&#94;/]*/Development/Centralized/.* exclude-caches = yes keep = 8h,14d,10w,12m,10y The root parameter tells obnam which directories (and subdirectories) to back up. With exclude a particular set of files or directories can be excluded, for instance because these contain downloaded resources (and as such do not need to be inside the backup archives). Obnam also supports the CACHEDIR.TAG specification, which I use for the various cache directories. With the use of these cache tag files I do not need to update the obnam.conf file with every new cache directory (or software build directory). The last parameter in the configuration that I want to focus on is the keep parameter. Every time obnam takes a backup, it creates what it calls a new generation . When the backup storage becomes too big, administrators can run obnam forget to drop generations. The keep parameter informs obnam which generations can be removed and which ones can be kept. In my case, I want to keep one backup per hour for the last 8 hours (I normally take one backup per day, but during some development sprees or photo manipulations I back up multiple times), one per day for the last two weeks, one per week for the last 10 weeks, one per month for the last 12 months and one per year for the last 10 years. Obnam will clean up only when obnam forget is executed. As storage is cheap, and the performance of obnam is sufficient for me, I do not need to call this very often. Backing up and restoring files My backup strategy is to backup to an external disk, and then synchronize this disk with a personal backup server somewhere else. This backup server runs no other software beyond OpenSSH (to allow secure transfer of the backups) and both the backup server disks and the external disk is LUKS encrypted. Considering that I don't have government secrets I opted not to encrypt the backup files themselves, but Obnam does support that (through GnuPG). All backup enabled systems use cron jobs which execute obnam backup to take the backup, and use rsync to synchronize the finished backup with the backup server. If I need to restore a file, I use obnam ls to see which file(s) I need to restore (add in a --generation= to list the files of a different backup generation than the last one). Then, the command to restore is: ~# obnam restore --to=/var/restore /home/swift/Images/Processing/*.NCF Or I can restore immediately to the directory again: ~# obnam restore --to=/home/swift/Images/Processing /home/swift/Images/Processing/*.NCF To support multiple clients, obnam by default identifies each client through the hostname. It is possible to use different names, but hostnames tend to be a common best practice which I don't deviate from either. Obnam is able to share blocks between clients (it is not mandatory, but supported nonetheless).","tags":"Free Software","url":"https://blog.siphos.be/2015/08/my-application-base-obnam/","loc":"https://blog.siphos.be/2015/08/my-application-base-obnam/"},{"title":"Don't confuse SELinux with its policy","text":"With the increased attention that SELinux is getting thanks to its inclusion in recent Android releases, more and more people are understanding that SELinux is not a singular security solution. Many administrators are still disabling SELinux on their servers because it does not play well with their day-to-day operations. But the Android inclusion shows that SELinux itself is not the culprit for this: it is the policy. Policy versus enforcement SELinux has conceptually segregated the enforcement from the rules/policy. There is an in-kernel enforcement (the SELinux subsystem) which is configured through an administrator-provided policy (the SELinux rules). As long as SELinux was being used on servers, chances are very high that the policy that is being used is based on the SELinux Reference Policy as this is, as far as I know, the only policy implementation for Linux systems that is widely usable. The reference policy project aims to provide a well designed, broadly usable yet still secure set of rules. And through this goal, it has to play ball with all possible use cases that the various software titles require. Given the open ecosystem of the free software world, and the Linux based ones in particular, managing such a policy is not for beginners. New policy development requires insight in the technology for which the policy is created, as well as knowledge of how the reference policy works. Compare this to the Android environment. Applications have to follow more rigid guidelines before they are accepted on Android systems. Communication between applications and services is governed through Intents and Activities which are managed by the Binder application. Interactions with the user are based on well defined interfaces. Heck, the Android OS even holds a number of permissions that applications have to subscribe to before they can use it. Such an environment is much easier to create policies for, because it allows policies to be created almost on-the-fly, with the application permissions being mapped to predefined SELinux rules. Because the freedom of implementations is limited (in order to create a manageable environment which is used by millions of devices over the world) policies can be made more strictly and yet enjoy the static nature of the environment: no continuous updates on existing policies, something that Linux distributions have to do on an almost daily basis. Aiming for a policy development ecosystem Having SELinux active on Android shows that one should not confuse SELinux with its policies. SELinux is a nice security subsystem in the Linux kernel, and can be used and tuned to cover whatever use case is given to it. The slow adoption of SELinux by Linux distributions might be attributed to its lack of policy diversification, which results in few ecosystems where additional (and perhaps innovative) policies could be developed. It is however a huge advantage that a reference policy exists, so that distributions can enjoy a working policy without having to put resources into its own policy development and maintenance. Perhaps we should try to further enhance the existing policies while support new policy ecosystems and development initiatives. The maturation of the CIL language by the SELinux userland libraries and tools might be a good catalyst for this. At one point, policies will need to be migrated to CIL (although this can happen gradually as the userland utilities can deal with CIL and other languages such as the legacy .pp files simultaneously) and there are a few developers considering a renewal of the reference policy. This would make use of the new benefits of the CIL language and implementation: some restrictions that where applicable to the legacy format no longer holds on CIL, such as rules which previously were only allowed in the base policy which can now be made part of the modules as well. But next to renewing existing policies, there is plenty of room left for innovative policy ideas and developments. The SELinux language is very versatile, and just like with programming languages we notice that only a few set of constructs are used. Some applications might even benefit from using SELinux as their decision and enforcement system (something that SEPostgreSQL has tried). The SELinux Notebook by Richard Haines is an excellent resource for developers that want to work more closely with the SELinux language constructs. Just skimming through this resource also shows how very open SELinux itself is, and that most of the users' experience with SELinux is based on a singular policy implementation. This is a prime reason why having a more open policy ecosystem makes perfect sense. If you don't like a particular car, do you ditch driving at all? No, you try out another car. Let's create other cars in the SELinux world as well.","tags":"SELinux","url":"https://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/","loc":"https://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/"},{"title":"Switching to Pelican","text":"Nothing beats a few hours of flying to get things moving on stuff. Being offline for a few hours with a good workstation helps to not be disturbed by external actions (air pockets notwithstanding). Early this year, I expressed my intentions to move to Pelican from WordPress. I wasn't actually unhappy with WordPress, but the security concerns I had were a bit too much for blog as simple as mine. Running a PHP-enabled site with a database for something that I can easily handle through a static site, well, I had to try. Today I finally moved the blog, imported all past articles as well as comments. For the commenting, I now use disqus which integrates nicely with Pelican and has a fluid feel to it. I wanted to use the Tipue Search plug-in as well for searching through the blog, but I had to put that on hold as I couldn't get the results of a search to display nicely (all I got were links to \"undefined\"). But I'll work on this. Configuring Pelican Pelican configuration is done through pelicanconf.py and publishconf.py . The former contains all definitions and settings for the site which are also useful when previewing changes. The latter contains additional (or overruled) settings related to publication. In order to keep the same links as before (to keep web crawlers happy, as well as links to the blog from other sites and even the comments themselves) I did had to update some variables, but the Internet was strong on this one and I had little problems finding the right settings: # Link structure of the site ARTICLE_URL = u '{date:%Y}/{date:%m}/ {slug} /' ARTICLE_SAVE_AS = u '{date:%Y}/{date:%m}/ {slug} /index.html' CATEGORY_URL = u 'category/ {slug} ' CATEGORY_SAVE_AS = u 'category/ {slug} /index.html' TAG_URL = u 'tag/ {slug} /' TAG_SAVE_AS = u 'tag/ {slug} /index.html' The next challenges were (and still are, I will have to check if this is working or not soon by checking the blog aggregation sites I am usually aggregated on) the RSS and Atom feeds. From the access logs of my previous blog, I believe that most of the aggregation sites are using the /feed/ , /feed/atom and /category/*/feed links. Now, I would like to move the aggregations to XML files, so that the RSS feed is available at /feed/rss.xml and the Atom feed at /feed/atom.xml , but then the existing aggregations would most likely fail because they currently don't use these URLs. To fix this, I am now trying to generate the XML files as I would like them to be, and create symbolic links afterwards from index.html to the right XML file. The RSS/ATOM settings I am currently using are as follows: CATEGORY_FEED_ATOM = 'category/ %s /feed/atom.xml' CATEGORY_FEED_RSS = 'category/ %s /feed/rss.xml' FEED_ATOM = 'feed/atom.xml' FEED_ALL_ATOM = 'feed/all.atom.xml' FEED_RSS = 'feed/rss.xml' FEED_ALL_RSS = 'feed/all.rss.xml' TAG_FEED_ATOM = 'tag/ %s /feed/atom.xml' TAG_FEED_RSS = 'tag/ %s /feed/rss.xml' TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None Hopefully, the existing aggregations still work, and I can then start asking the planets to move to the XML URL itself. Some tracking on the access logs should allow me to see how well this is going. Next steps The first thing to make sure is happening correctly is the blog aggregation and the comment system. Then, a few tweaks are still on the pipeline. One is to optimize the front page a bit. Right now, all articles are summarized, and I would like to have the last (or last few) article(s) fully expanded whereas the rest is summarized. If that isn't possible, I'll probably switch to fully expanded articles (which is a matter of setting a single variable). Next, I really want the search functionality to work again. Enabling the Tipue search worked almost flawlessly - search worked as it should, and the resulting search entries are all correct. The problem is that the URLs that the entries point to (which is what users will click on) all point to an invalid (\"undefined\") URL. Finally, I want the printer-friendly one to be without the social / links on the top right. This is theme-oriented, and I'm happily using pelican-bootstrap3 right now, so I don't expect this to be much of a hassle. But considering that my blog is mainly technology oriented for now (although I am planning on expanding that) being able to have the articles saved in PDF or printed in a nice format is an important use case for me.","tags":"Free Software","url":"https://blog.siphos.be/2015/08/switching-to-pelican/","loc":"https://blog.siphos.be/2015/08/switching-to-pelican/"},{"title":"Loading CIL modules directly","text":"In a previous post I used the secilc binary to load an additional test policy. Little did I know (and that's actually embarrassing because it was one of the things I complained about) that you can just use the CIL policy as modules directly. With this I mean that a CIL policy as mentioned in the previous post can be loaded like a prebuilt .pp module: ~# semodule -i test.cil ~# semodule -l | grep test test That's all that is to it. Loading the module resulted in the test port to be immediately declared and available: ~# semanage port -l | grep test test_port_t tcp 1440 In hindsight, it makes sense that it is this easy. After all, support for the old-style policy language is done by converting it into CIL when calling semodule so it makes sense to immediately put the module (in CIL code) ready to be taken up.","tags":"SELinux","url":"https://blog.siphos.be/2015/07/loading-cil-modules-directly/","loc":"https://blog.siphos.be/2015/07/loading-cil-modules-directly/"},{"title":"Restricting even root access to a folder","text":"In a comment Robert asked how to use SELinux to prevent even root access to a directory. The trivial solution would be not to assign an administrative role to the root account (which is definitely possible, but you want some way to gain administrative access otherwise ;-) Restricting root is one of the commonly referred features of a MAC (Mandatory Access Control) system. With a well designed user management and sudo environment, it is fairly trivial - but if you need to start from the premise that a user has direct root access, it requires some thought to implement it correctly. The main \"issue\" is not that it is difficult to implement policy-wise, but that most users will start from a pre-existing policy (such as the reference policy) and build on top of that. The use of a pre-existing policy means that some roles are already identified and privileges are already granted to users - often these higher privileged roles are assigned to the Linux root user as not to confuse users. But that does mean that restricting root access to a folder means that some additional countermeasures need to be implemented. The policy But first things first. Let's look at a simple policy for restricting access to /etc/private : policy_module(myprivate, 1.0) type etc_private_t; fs_associate(etc_private_t) This simple policy introduces a type ( etc_private_t ) which is allowed to be used for files (it associates with a file system). Do not use the files_type() interface as this would assign a set of attributes that many user roles get read access on. Now, it is not sufficient to have the type available. If we want to assign it to a type, someone or something needs to have the privileges to change the security context of a file and directory to this type. If we would just load this policy and try to do this from a privileged account, it would fail: ~# chcon -t etc_private_t /etc/private chcon: failed to change context of '/etc/private' to 'system_u:object_r:etc_private_t:s0': Permission denied With the following rule, the sysadm_t domain (which I use for system administration) is allowed to change the context to etc_private_t : allow sysadm_t etc_private_t:{dir file} relabelto; With this in place, the administrator can label resources as etc_private_t without having read access to these resources afterwards. Also, as long as there are no relabelfrom privileges assigned, the administrator cannot revert the context back to a type that he has read access to. The countermeasures But this policy is not sufficient. One way that administrators can easily access the resources is to disable SELinux controls (as in, put the system in permissive mode): ~# cat /etc/private/README cat: /etc/private/README: Permission denied ~# setenforce 0 ~# cat /etc/private/README Hello World! To prevent this, enable the secure_mode_policyload SELinux boolean: ~# setsebool secure_mode_policyload on This will prevent any policy and SELinux state manipulation... including permissive mode, but also including loading additional SELinux policies or changing booleans. Definitely experiment with this setting without persisting (i.e. do not use -P in the above command yet) to make sure it is manageable for you. Still, this isn't sufficient. Don't forget that the administrator is otherwise a full administrator - if he cannot access the /etc/private location directly, then he might be able to access it indirectly: If the resource is on a non-critical file system, he can unmount the file system and remount it with a context= mount option. This will override the file-level contexts. Bind-mounting does not seem to allow overriding the context. If the resource is on a file system that cannot be unmounted, the administrator can still reboot the system in a mode where he can access the file system regardless of SELinux controls (either through editing /etc/selinux/config or by booting with enforcing=0 , etc. The administrator can still access the block device files on which the resources are directly. Specialized tools can allow for extracting files and directories without actually (re)mounting the device. A more extensive list of methods to potentially gain access to such resources is iterated in Limiting file access with SELinux alone . This set of methods for gaining access is due to the administrative role already assigned by the existing policy. To further mitigate these risks with SELinux (although SELinux will never completely mitigate all risks) the roles assigned to the users need to be carefully revisited. If you grant people administrative access, but you don't want them to be able to reboot the system, (re)mount file systems, access block devices, etc. then create a user role that does not have these privileges at all. Creating such user roles does not require leaving behind the policy that is already active. Additional user domains can be created and granted to Linux accounts (including root). But in my experience, when you need to allow a user to log on as the \"root\" account directly, you probably need him to have true administrative privileges. Otherwise you'd work with personal accounts and a well-designed /etc/sudoers file.","tags":"SELinux","url":"https://blog.siphos.be/2015/07/restricting-even-root-access-to-a-folder/","loc":"https://blog.siphos.be/2015/07/restricting-even-root-access-to-a-folder/"},{"title":"Intermediate policies","text":"When developing SELinux policies for new software (or existing ones whose policies I don't agree with) it is often more difficult to finish the policies so that they are broadly usable. When dealing with personal policies, having them \"just work\" is often sufficient. To make the policies reusable for distributions (or for the upstream project), a number of things are necessary: Try structuring the policy using the style as suggested by refpolicy or Gentoo Add the role interfaces that are most likely to be used or required, or which are in the current draft implemented differently Refactor some of the policies to use refpolicy/Gentoo style interfaces Remove the comments from the policies (as refpolicy does not want too verbose policies) Change or update the file context definitions for default installations (rather than the custom installations I use) This often takes quite some effort. Some of these changes (such as the style updates and commenting) are even counterproductive for me personally (in the sense that I don't gain any value from doing so and would have to start maintaining two different policy files for the same policy), and necessary only for upstreaming policies. As a result, I often finish with policies that I just leave for me personally or somewhere on a public repository (like these Neo4J and Ceph policies), without any activities already scheduled to attempt to upstream those. But not contributing the policies to a broader public means that the effort is not known, and other contributors might be struggling with creating policies for their favorite (or necessary) technologies. So the majority of policies that I write I still hope to eventually push them out. But I noticed that these last few steps for upstreaming (the ones mentioned above) might only take a few hours of work, but take me over 6 months (or more) to accomplish (as I often find other stuff more interesting to do). I don't know yet how to change the process to make it more interesting to use. However, I do have a couple of wishes that might make it easier for me, and perhaps others, to contribute: Instead of reacting on contribution suggestions, work on a common repository together. Just like with a wiki, where we don't aim for a 100% correct and well designed document from the start, we should use the strength of the community to continuously improve policies (and to allow multiple people to work on the same policy). Right now, policies are a one-man publication with a number of people commenting on the suggested changes and asking the one person to refactor or update the change himself. Document the style guide properly, but don't disallow contributions if they do not adhere to the style guide completely. Instead, merge and update. On successful wikis there are even people that update styles without content updates, and their help is greatly appreciated by the community. If a naming convention is to be followed (which is the case with policies) make it clear. Too often the name of an interface is something that takes a few days of discussion. That's not productive for policy development. Find a way to truly create a \"core\" part of the policy and a modular/serviceable approach to handle additional policies. The idea of the contrib/ repository was like that, but failed to live up to its expectations: the number of people who have commit access to the contrib is almost the same as to the core, a few exceptions notwithstanding, and whenever policies are added to contrib they often require changes on the core as well. Perhaps even support overlay-type approaches to policies so that intermediate policies can be \"staged\" and tested by a larger audience before they are vetted into the upstream reference policy. Settle on how to deal with networking controls. My suggestion would be to immediately support the TCP/UDP ports as assigned by IANA (or another set of sources) so that additional policies do not need to wait for the base policy to support the ports. Or find and support a way for contributions to declare the port types themselves (we probably need to focus on CIL for this). Document \"best practices\" on policy development where certain types of policies are documented in more detail. For instance, desktop application profiles, networked daemons, user roles, etc. These best practices should not be mandatory and should in fact support a broad set of privilege isolation. With the latter, I mean that there are policies who cover an entire category of systems (init systems, web servers), a single software package or even the sub-commands and sub-daemons of that package. It would surprise me if this can't be supported better out-of-the-box (as in, through a well thought-through base policy framework and styleguide). I believe that this might create a more active community surrounding policy development.","tags":"SELinux","url":"https://blog.siphos.be/2015/07/intermediate-policies/","loc":"https://blog.siphos.be/2015/07/intermediate-policies/"},{"title":"Where does CIL play in the SELinux system?","text":"SELinux policy developers already have a number of file formats to work with. Currently, policy code is written in a set of three files: The .te file contains the SELinux policy code (type enforcement rules) The .if file contains functions which turn a set of arguments into blocks of SELinux policy code (interfaces). These functions are called by other interface files or type enforcement files The .fc file contains mappings of file path expressions towards labels (file contexts) These files are compiled into loadable modules (or a base module) which are then transformed to an active policy. But this is not a single-step approach. Transforming policy code into policy file For the Linux kernel SELinux subsystem, only a single file matters - the policy.## file (for instance policy.29 ). The suffix denotes the binary format used as higher numbers mean that additional SELinux features are supported which require different binary formats for the SELinux code in the Linux kernel. With the 2.4 userspace, the transformation of the initial files as mentioned above towards a policy file is done as follows: When a developer builds a policy module, first checkmodule is used to build a .mod intermediate file. This file contains the type enforcement rules with the expanded rules of the various interface files. Next, semodule_package is called which transforms this intermediate file, together with the file context file, into a .pp file. This .pp file is, in the 2.4 userspace, called a \"high level language\" file. There is little high-level about it, but the idea is that such high-level language files are then transformed into .cil files (CIL stands for Common Intermediate Language ). If at any moment other frameworks come around, they could create high-level languages themselves and provide a transformation engine to convert these HLL files into CIL files. For the current .pp files, this transformation is supported through the /usr/libexec/selinux/hll/pp binary which, given a .pp file, outputs CIL code. Finally, all CIL files (together) are compiled into a binary policy.29 file. All the steps coming from a .pp file towards the final binary file are handled by the semodule command. For instance, if an administrator loads an additional .pp file, its (generated) CIL code is added to the other active CIL code and together, a new policy binary file is created. Adding some CIL code The SELinux userspace development repository contains a secilc command which can compile CIL code into a binary policy file. As such, it can perform the (very) last step of the file conversions above. However, it is not integrated in the sense that, if additional code is added, the administrator can \"play\" with it as he would with SELinux policy modules. Still, that shouldn't prohibit us from playing around with it to experiment with the CIL language construct. Consider the following CIL SELinux policy code: ; Declare a test_port_t type (type test_port_t) ; Assign the type to the object_r role (roletype object_r test_port_t) ; Assign the right set of attributes to the port (typeattributeset defined_port_type test_port_t) (typeattributeset port_type test_port_t) ; Declare tcp:1440 as test_port_t (portcon tcp 1440 (system_u object_r test_port_t ((s0) (s0)))) The code declares a port type ( test_port_t ) and uses it for the TCP port 1440. In order to use this code, we have to build a policy file which includes all currently active CIL code, together with the test code: ~$ secilc -c 29 /var/lib/selinux/mcs/active/modules/400/*/cil testport.cil The result is a policy.29 (the command forces version 29 as the current Linux kernel used on this system does not support version 30) file, which can now be copied to /etc/selinux/mcs/policy . Then, after having copied the file, load the new policy file using load_policy . And lo and behold, the port type is now available: ~# semanage port -l | grep 1440 test_port_t tcp 1440 To verify that it really is available and not just parsed by the userspace, let's connect to it and hope for a nice denial message: ~$ ssh -p 1440 localhost ssh: connect to host localhost port 1440: Permission denied ~$ sudo ausearch -ts recent time->Thu Jun 11 19:35:45 2015 type=PROCTITLE msg=audit(1434044145.829:296): proctitle=737368002D700031343430006C6F63616C686F7374 type=SOCKADDR msg=audit(1434044145.829:296): saddr=0A0005A0000000000000000000000000000000000000000100000000 type=SYSCALL msg=audit(1434044145.829:296): arch=c000003e syscall=42 success=no exit=-13 a0=3 a1=6d4d1ce050 a2=1c a3=0 items=0 ppid=2005 pid=18045 auid=1001 uid=1001 gid=1001 euid=1001 suid=1001 fsuid=1001 egid=1001 sgid=1001 fsgid=1001 tty=pts0 ses=1 comm=\"ssh\" exe=\"/usr/bin/ssh\" subj=staff_u:staff_r:ssh_t:s0 key=(null) type=AVC msg=audit(1434044145.829:296): avc: denied { name_connect } for pid=18045 comm=\"ssh\" dest=1440 scontext=staff_u:staff_r:ssh_t:s0 tcontext=system_u:object_r:test_port_t:s0 tclass=tcp_socket permissive=0","tags":"SELinux","url":"https://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/","loc":"https://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/"},{"title":"Live SELinux userspace ebuilds","text":"In between courses, I pushed out live ebuilds for the SELinux userspace applications: libselinux, policycoreutils, libsemanage, libsepol, sepolgen, checkpolicy and secilc. These live ebuilds (with Gentoo version 9999) pull in the current development code of the SELinux userspace so that developers and contributors can already work with in-progress code developments as well as see how they work on a Gentoo platform. That being said, I do not recommend using the live ebuilds for anyone else except developers and contributors in development zones (definitely not on production). One of the reasons is that the ebuilds do not apply Gentoo-specific patches to the ebuilds. I would also like to remove the Gentoo-specific manipulations that we do, such as small Makefile adjustments, but let's start with just ignoring the Gentoo patches. Dropping the patches makes sure that we track upstream libraries and userspace closely, and allows developers to try and send out patches to the SELinux project to fix Gentoo related build problems. But as not all packages can be deployed successfully on a Gentoo system some patches need to be applied anyway. For this, users can drop the necessary patches inside /etc/portage/patches as all userspace ebuilds use the epatch_user method. Finally, observant users will notice that \"secilc\" is also provided. This is a new package, which is probably going to have an official release with a new userspace release. It allows for building CIL-based SELinux policy code, and was one of the drivers for me to create the live ebuilds as I'm experimenting with the CIL constructions. So expect more on that later.","tags":"Gentoo","url":"https://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/","loc":"https://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/"},{"title":"PostgreSQL with central authentication and authorization","text":"I have been running a PostgreSQL cluster for a while as the primary backend for many services. The database system is very robust, well supported by the community and very powerful. In this post, I'm going to show how I use central authentication and authorization with PostgreSQL. Centralized management is an important principle whenever deployments become very dispersed. For authentication and authorization, having a high-available LDAP is one of the more powerful components in any architecture. It isn't the only method though - it is also possible to use a distributed approach where the master data is centrally managed, but the proper data is distributed to the various systems that need it. Such a distributed approach allows for high availability without the need for a highly available central infrastructure (user ids, group membership and passwords are distributed to the servers rather than queried centrally). Here, I'm going to focus on a mixture of both methods: central authentication for password verification, and distributed authorization. PostgreSQL default uses in-database credentials By default, PostgreSQL uses in-database credentials for the authentication and authorization. When a CREATE ROLE (or CREATE USER ) command is issued with a password, it is stored in the pg_catalog.pg_authid table: postgres# select rolname, rolpassword from pg_catalog.pg_authid; rolname | rolpassword ----------------+------------------------------------- postgres_admin | dmvsl | johan | hdc_owner | hdc_reader | hdc_readwrite | hadoop | swift | sean | hdpreport | postgres | md5c127bc9fc185daf0e06e785876e38484 this cannot be moved outside): postgres# \\l db_hadoop List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+-----------+----------+------------+------------+--------------------------- db_hadoop | hdc_owner | UTF8 | en_US.utf8 | en_US.utf8 | hdc_owner=CTc/hdc_owner + | | | | | hdc_reader=c/hdc_owner + | | | | | hdc_readwrite=c/hdc_owner Furthermore, PostgreSQL has some additional access controls through its pg_hba.conf file, in which the access towards the PostgreSQL service itself can be governed based on context information (such as originating IP address, target database, etc.). For more information about the standard setups for PostgreSQL, definitely go through the official PostgreSQL documentation as it is well documented and kept up-to-date. Now, for central management, in-database settings become more difficult to handle. Using PAM for authentication The first step to move the management of authentication and authorization outside the database is to look at a way to authenticate users (password verification) outside the database. I tend not to use a distributed password approach (where a central component is responsible for changing passwords on multiple targets), instead relying on a high-available LDAP setup, but with local caching (to catch short-lived network hick-ups) and local password use for last-hope accounts (such as root and admin accounts). PostgreSQL can be configured to directly interact with an LDAP, but I like to use Linux PAM whenever I can. For my systems, it is a standard way of managing the authentication of many services, so the same goes for PostgreSQL. And with the sys-auth/pam_ldap package integrating multiple services with LDAP is a breeze. So the first step is to have PostgreSQL use PAM for authentication. This is handled through its pg_hba.conf file: # TYPE DATABASE USER ADDRESS METHOD [OPTIONS] local all all md5 host all all all pam pamservice=postgresql This will have PostgreSQL use the postgresql PAM service for authentication. The PAM configuration is thus in /etc/pam.d/postgresql . In it, we can either directly use the LDAP PAM modules, or use the SSSD modules and have SSSD work with LDAP. Yet, this isn't sufficient. We still need to tell PostgreSQL which users can be authenticated - the users need to be defined in the database (just without password credentials because that is handled externally now). This is done together with the authorization handling. Users and group membership Every service on the systems I maintain has dedicated groups in which for instance its administrators are listed. For instance, for the PostgreSQL services: # getent group gpgsqladmin gpgsqladmin:x:413843:swift,dmvsl A local batch job (ran through cron) queries this group (which I call the masterlist , as well as queries which users in PostgreSQL are assigned the postgres_admin role (which is a superuser role like postgres and is used as the intermediate role to assign to administrators of a PostgreSQL service), known as the slavelist . Delta's are then used to add the user or remove it. # Note: membersToAdd / membersToRemove / _psql are custom functions # so do not vainly search for them on your system ;-) for member in $(membersToAdd ${masterlist} ${slavelist}) ; do _psql \"CREATE USER ${member} LOGIN INHERIT;\" postgres _psql \"GRANT postgres_admin TO ${member};\" postgres done for member in $(membersToRemove ${masterlist} ${slavelist}) ; do _psql \"REVOKE postgres_admin FROM ${member};\" postgres _psql \"DROP USER ${member};\" postgres done The postgres_admin role is created whenever I create a PostgreSQL instance. Likewise, for databases, a number of roles are added as well. For instance, for the db_hadoop database, the hdc_owner , hdc_reader and hdc_readwrite roles are created with the right set of privileges. Users are then granted this role if they belong to the right group in the LDAP. For instance: # getent group gpgsqlhdc_own gpgsqlhdc_own:x:413850:hadoop,johan,christov,sean With this simple approach, granting users access to a database is a matter of adding the user to the right group (like gpgsqlhdc_ro for read-only access to the Hadoop related database(s)) and either wait for the cron-job to add it, or manually run the authorization synchronization. By standardizing on infrastructural roles (admin, auditor) and data roles (owner, rw, ro) managing multiple databases is a breeze.","tags":"Free Software","url":"https://blog.siphos.be/2015/05/postgresql-with-central-authentication-and-authorization/","loc":"https://blog.siphos.be/2015/05/postgresql-with-central-authentication-and-authorization/"},{"title":"Testing with permissive domains","text":"When testing out new technologies or new setups, not having (proper) SELinux policies can be a nuisance. Not only are the number of SELinux policies that are available through the standard repositories limited, some of these policies are not even written with the same level of confinement that an administrator might expect. Or perhaps the technology to be tested is used in a completely different manner. Without proper policies, any attempt to start such a daemon or application might or will cause permission violations. In many cases, developers or users tend to disable SELinux enforcing then so that they can continue playing with the new technology. And why not? After all, policy development is to be done after the technology is understood. But completely putting the system in permissive mode is overshooting. It is much easier to make a very simple policy to start with, and then mark the domain as a permissive domain. What happens is that the software then, after transitioning into the \"simple\" domain, is not part of the SELinux enforcements anymore whereas the rest of the system remains in SELinux enforcing mode. For instance, create a minuscule policy like so: policy_module(testdom, 1.0) type testdom_t; type testdom_exec_t; init_daemon_domain(testdom_t, testdom_exec_t) Mark the executable for the daemon as testdom_exec_t (after building and loading the minuscule policy): ~# chcon -t testdom_exec_t /opt/something/bin/daemond Finally, tell SELinux that testdom_t is to be seen as a permissive domain: ~# semanage permissive -a testdom_t When finished, don't forget to remove the permissive bit ( semanage permissive -d testdom_t ) and unload/remove the SELinux policy module. And that's it. If the daemon is now started (through a standard init script) it will run as testdom_t and everything it does will be logged, but not enforced by SELinux. That might even help in understanding the application better.","tags":"SELinux","url":"https://blog.siphos.be/2015/05/testing-with-permissive-domains/","loc":"https://blog.siphos.be/2015/05/testing-with-permissive-domains/"},{"title":"Audit buffering and rate limiting","text":"Be it because of SELinux experiments, or through general audit experiments, sometimes you'll get in touch with a message similar to the following: audit: audit_backlog=321 > audit_backlog_limit=320 audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320 audit: backlog limit exceeded The message shows up when certain audit events could not be logged through the audit subsystem. Depending on the system configuration, they might be either ignored, sent through the kernel logging infrastructure or even have the system panic. And if the messages are sent to the kernel log then they might show up, but even that log has its limitations, which can lead to output similar to the following: __ratelimit: 53 callbacks suppressed In this post, I want to give some pointers in configuring the audit subsystem as well as understand these messages... There is auditd and kauditd If you take a look at the audit processes running on the system, you'll notice that (assuming Linux auditing is used of course) two processes are running: # ps -ef | grep audit root 1483 1 0 10:11 ? 00:00:00 /sbin/auditd root 1486 2 0 10:11 ? 00:00:00 [kauditd] The /sbin/auditd daemon is the user-space audit daemon. It registers itself with the Linux kernel audit subsystem (through the audit netlink system), which responds with spawning the kauditd kernel thread/process. The fact that the process is a kernel-level one is why the kauditd is surrounded by brackets in the ps output. Once this is done, audit messages are communicated through the netlink socket to the user-space audit daemon. For the detail-oriented people amongst you, look for the kauditd_send_skb() method in the kernel/audit.c file. Now, generated audit event messages are not directly relayed to the audit daemon - they are first queued in a sort-of backlog, which is where the backlog-related messages above come from. Audit backlog queue In the kernel-level audit subsystem, a socket buffer queue is used to hold audit events. Whenever a new audit event is received, it is logged and prepared to be added to this queue. Adding to this queue can be controlled through a few parameters. The first parameter is the backlog limit. Be it through a kernel boot parameter ( audit_backlog_limit=N ) or through a message relayed by the user-space audit daemon ( auditctl -b N ), this limit will ensure that a queue cannot grow beyond a certain size (expressed in the amount of messages). If an audit event is logged which would grow the queue beyond this limit, then a failure occurs and is handled according to the system configuration (more on that later). The second parameter is the rate limit. When more audit events are logged within a second than set through this parameter (which can be controlled through a message relayed by the user-space audit system, using auditctl -r N ) then those audit events are not added to the queue. Instead, a failure occurs and is handled according to the system configuration. Only when the limits are not reached is the message added to the queue, allowing the user-space audit daemon to consume those events and log those according to the audit configuration. There are some good resources on audit configuration available on the Internet. I find this SuSe chapter worth reading, but many others exist as well. There is a useful command related to the subject of the audit backlog queue. It queries the audit subsystem for its current status: # auditctl -s AUDIT_STATUS: enabled=1 flag=1 pid=1483 rate_limit=0 backlog_limit=8192 lost=3 backlog=0 The command displays not only the audit state (enabled or not) but also the settings for rate limits (on the audit backlog) and backlog limit. It also shows how many events are currently still waiting in the backlog queue (which is zero in our case, so the audit user-space daemon has properly consumed and logged the audit events). Failure handling If an audit event cannot be logged, then this failure needs to be resolved. The Linux audit subsystem can be configured do either silently discard the message, switch to the kernel log subsystem, or panic. This can be configured through the audit user-space ( auditctl -f [0..2] ), but is usually left at the default (which is 1, being to switch to the kernel log subsystem). Before that is done, the message is displayed which reveals the cause of the failure handling: audit: audit_backlog=321 > audit_backlog_limit=320 audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320 audit: backlog limit exceeded In this case, the backlog queue was set to contain at most 320 entries (which is low for a production system) and more messages were being added (the Linux kernel in certain cases allows to have a few more entries than configured for performance and consistency reasons). The number of events already lost is displayed, as well as the current limitation settings. The message \"backlog limit exceeded\" can be \"rate limit exceeded\" if that was the limitation that was triggered. Now, if the system is not configured to silently discard it, or to panic the system, then the \"dropped\" messages are sent to the kernel log subsystem. The calls however are also governed through a configurable limitation: it uses a rate limit which can be set through sysctl : # sysctl -a | grep kernel.printk_rate kernel.printk_ratelimit = 5 kernel.printk_ratelimit_burst = 10 In the above example, this system allows one message every 5 seconds, but does allow a burst of up to 10 messages at once. When the rate limitation kicks in, then the kernel will log (at most one per second) the number of suppressed events: [40676.545099] __ratelimit: 246 callbacks suppressed Although this limit is kernel-wide, not all kernel log events are governed through it. It is the caller subsystem (in our case, the audit subsystem) which is responsible for having its events governed through this rate limitation or not. Finishing up Before waving goodbye, I would like to point out that the backlog queue is a memory queue (and not on disk, Red Hat ), just in case it wasn't obvious. Increasing the queue size can result in more kernel memory consumption. Apparently, a practical size estimate is around 9000 bytes per message. On production systems, it is advised not to make this setting too low. I personally set it to 8192. Lost audit events might result in difficulties for troubleshooting, which is the case when dealing with new or experimental SELinux policies. It would also result in missing security-important events. It is the audit subsystem, after all. So tune it properly, and enjoy the power of Linux' audit subsystem.","tags":"Free Software","url":"https://blog.siphos.be/2015/05/audit-buffering-and-rate-limiting/","loc":"https://blog.siphos.be/2015/05/audit-buffering-and-rate-limiting/"},{"title":"Use change management when you are using SELinux to its fullest","text":"If you are using SELinux on production systems (with which I mean systems that you offer services with towards customers or other parties beyond you, yourself and your ego), please consider proper change management if you don't do already. SELinux is a very sensitive security subsystem - not in the sense that it easily fails, but because it is very fine-grained and as such can easily stop applications from running when their behavior changes just a tiny bit. Sensitivity of SELinux SELinux is a wonderful security measure for Linux systems that can prevent successful exploitation of vulnerabilities or misconfigurations. Of course, it is not the sole security measure that systems should take. Proper secure configuration of services, least privilege accounts, kernel-level mitigations such as grSecurity and more are other measures that certainly need to be taken if you really find system security to be a worthy goal to attain. But I'm not going to talk about those others right now. What I am going to focus on is SELinux, and how sensitive it is to changes. An important functionality of SELinux to understand is that it segregates the security control system itself (the SELinux subsystem) from its configuration (the policy). The security control system itself is relatively small, and focuses on enforcement of the policy and logging (either because the policy asks to log something, or because something is prevented, or because an error occurred). The most difficult part of handling SELinux on a system is not enabling or interacting with it. No, it is its policy. The policy is also what makes SELinux so darn sensitive for small system changes (or behavior that is not either normal, or at least not allowed through the existing policy). Let me explain with a small situation that I recently had. Case in point: Switching an IP address A case that beautifully shows how sensitive SELinux can be is an IP address change. My systems all obtain their IP address (at least for IPv4) from a DHCP system. This is of course acceptable behavior as otherwise my systems would never be able to boot up successfully anyway. The SELinux policy that I run also allows this without any hindrance. So that was not a problem. Yet recently I had to switch an IP address for a system in production. All the services I run are set up in a dual-active mode, so I started with the change by draining the services to the second system, shutting down the service and then (after reconfiguring the DHCP system to now provide a different IP address) reload the network configuration. And then it happened - the DHCP client just stalled. As the change failed, I updated the DHCP system again to deliver the old IP address and then reloaded the network configuration on the client. Again, it failed. Dumbstruck, I looked at the AVC denials and lo and behold, I notice a dig process running in a DHCP client related domain that is trying to do UDP binds, which the policy (at that time) did not allow. But why now suddenly, after all - this system was running happily for more than a year already (and with occasional reboots for kernel updates). I won't bore you with the investigation. It boils down to the fact that the DHCP client detected a change compared to previous startups, and was configured to run a few hooks as additional steps in the IP lease setup. As these hooks were never ran previously, the policy was never challenged to face this. And since the address change occurred a revert to the previous situation didn't work either (as its previous state information was already deleted). I was able to revert the client (which is a virtual guest in KVM) to the situation right before the change (thank you savevm and loadvm functionality) so that I could work on the policy first in a non-production environment so that the next change attempt was successful. Change management The previous situation might be \"solved\" by temporarily putting the DHCP client domain in permissive mode just for the change and then back. But that is ignoring the issue, and unless you have perfect operational documentation that you always read before making system or configuration changes, I doubt that you'll remember this for the next time. The case is also a good example on the sensitivity of SELinux. It is not just when software is being upgraded. Every change (be it in configuration, behavior or operational activity) might result in a situation that is new for the loaded SELinux policy. As the default action in SELinux is to deny everything, this will result in unexpected results on the system. Sometimes very visible (no IP address obtained), sometimes hidden behind some weird behavior (hostname correctly set but not the domainname) or perhaps not even noticed until far later. Compare it to the firewall rule configurations: you might be able to easily confirm that standard flows are still passed through, but how are you certain that fallback flows or one-in-a-month connection setups are not suddenly prevented from happening. A somewhat better solution than just temporarily disabling SELinux access controls for a domain is to look into proper change management. Whenever a change has to be done, make sure that you can easily revert the change back to the previous situation (backups!) have tested the change on a non-vital (preproduction) system first These two principles are pretty vital when you are serious about using SELinux in production. I'm not talking about a system that hardly has any fine-grained policies, like where most of the system's services are running in \"unconfined\" domains (although that's still better than not running with SELinux at all), but where you are truly trying to put a least privilege policy in place for all processes and services. Being able to revert a change allows you to quickly get a service up and running again so that customers are not affected by the change (and potential issues) for long time. First fix the service, then fix the problem. If you are an engineer like me, you might rather focus on the problem (and a permanent, correct solution) first. But that's wrong - always first make sure that the customers are not affected by it. Revert and put the service back up, and then investigate so that the next change attempt will not go wrong anymore. Having a multi-master setup might give some more leeway into investigating issues (as the service itself is not disrupted) so in the case mentioned above I would probably have tried fixing the issue immediately anyway if it wasn't policy-based. But most users do not have truly multi-master service setups. Being able to test (and retest) changes in non-production also allows you to focus on automation (so that changes can be done faster and in a repeated, predictable and qualitative manner), regression testing as well as change accumulation testing. You don't have time for that? Be honest with yourself. If you support services for others (be it in a paid-for manner or because you support an organization in your free time) you'll quickly learn that service availability is one of the most qualitative aspects of what you do. No matter what mess is behind it, most users don't see all that. All they see is the service itself (and its performance / features). If a change you wanted to make made a service unavailable for hours, users will notice. And if the change wasn't communicated up front or it is the n-th time that this downtime occurs, they will start asking questions you rather not hear. Using a non-production environment is not that much of an issue if the infrastructure you work with supports bare metal restores, or snapshot/cloning (in case of VMs). After doing those a couple of times, you'll easily find that you can create a non-production environment from the production one. Or, you can go for a permanent non-production environment (although you'll need to take care that this environment is at all times representative for the production systems). And regarding qualitative changes, I really recommend to use a configuration management system. I recently switched from Puppet to Saltstack and have yet to use the latter to its fullest set (most of what I do is still scripted), but it is growing on me and I'm pretty convinced that I'll have the majority of my change management scripts removed by the end of this year towards Saltstack-based configurations. And that'll allow me to automate changes and thus provide a more qualitative service offering. With SELinux, of course.","tags":"SELinux","url":"https://blog.siphos.be/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/","loc":"https://blog.siphos.be/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/"},{"title":"Moving closer to 2.4 stabilization","text":"The SELinux userspace project has released version 2.4 in february this year, after release candidates have been tested for half a year. After its release, we at the Gentoo Hardened project have been working hard to integrate it within Gentoo. This effort has been made a bit more difficult due to the migration of the policy store from one location to another while at the same time switching to HLL- and CIL based builds. Lately, 2.4 itself has been pretty stable, and we're focusing on the proper migration from 2.3 to 2.4. The SELinux policy has been adjusted to allow the migrations to work, and a few final fixes are being tested so that we can safely transition our stable users from 2.3 to 2.4. Hopefully we'll be able to stabilize the userspace this month or beginning of next month.","tags":"Gentoo","url":"https://blog.siphos.be/2015/04/moving-closer-to-2-4-stabilization/","loc":"https://blog.siphos.be/2015/04/moving-closer-to-2-4-stabilization/"},{"title":"Trying out Pelican, part one","text":"One of the goals I've set myself to do this year (not as a new year resolution though, I *really* want to accomplish this ;-) is to move my blog from Wordpress to a statically built website. And Pelican looks to be a good solution to do so. It's based on Python, which is readily available and supported on Gentoo, and is quite readable. Also, it looks to be very active in development and support. And also: it supports taking data from an existing Wordpress installation, so that none of the posts are lost (with some rounding error that's inherit to such migrations of course). Before getting Pelican ready (which is available through Gentoo btw) I also needed to install pandoc , and that became more troublesome than expected. While installing pandoc I got hit by its massive amount of dependencies towards dev-haskell/* packages, and many of those packages really failed to install. It does some internal dependency checking and fails, informing me to run haskell-updater . Sadly, multiple re-runs of said command did not resolve the issue. In fact, it wasn't until I hit a forum post about the same issue that a first step to a working solution was found. It turns out that the ~arch versions of the haskell packages are better working. So I enabled dev-haskell/* in my package.accept_keywords file. And then started updating the packages... which also failed. Then I ran haskell-updater multiple times, but that also failed. After a while, I had to run the following set of commands (in random order) just to get everything to build fine: ~# emerge -u $(qlist -IC dev-haskell) --keep-going ~# for n in $(qlist -IC dev-haskell); do emerge -u $n; done It took quite some reruns, but it finally got through. I never thought I had this much Haskell-related packages installed on my system (89 packages here to be exact), as I never intended to do any Haskell development since I left the university. Still, I finally got pandoc to work. So, on to the migration of my Wordpress site... I thought. This is a good time to ask for stabilization requests (I'll look into it myself as well of course) but also to see if you can help out our arch testing teams to support the stabilization requests on Gentoo! We need you! I started with the official docs on importing . Looks promising, but it didn't turn out too well for me. Importing was okay, but then immediately building the site again resulted in issues about wrong arguments (file names being interpreted as an argument name or function when an underscore was used) and interpretation of code inside the posts. Then I found Jason Antman's converting wordpress posts to pelican markdown post to inform me I had to try using markdown instead of restructured text. And lo and behold - that's much better. The first builds look promising. Of all the posts that I made on Wordpress, only one gives a build failure. The next thing to investigate is theming, as well as seeing how good the migration goes (it isn't because there are no errors otherwise that the migration is successful of course) so that I know how much manual labor I have to take into consideration when I finally switch (right now, I'm still running Wordpress).","tags":"Gentoo","url":"https://blog.siphos.be/2015/03/trying-out-pelican-part-one/","loc":"https://blog.siphos.be/2015/03/trying-out-pelican-part-one/"},{"title":"CIL and attributes","text":"I keep on struggling to remember this, so let's make a blog post out of it ;-) When the SELinux policy is being built, recent userspace (2.4 and higher) will convert the policy into CIL language, and then build the binary policy. When the policy supports type attributes, these are of course also made available in the CIL code. For instance the admindomain attribute from the userdomain module: ... (typeattribute admindomain) (typeattribute userdomain) (typeattribute unpriv_userdomain) (typeattribute user_home_content_type) Interfaces provided by the module are also applied. You won't find the interface CIL code in /var/lib/selinux/mcs/active/modules though; the code at that location is already \"expanded\" and filled in. So for the sysadm_t domain we have: # Equivalent of # gen_require(` # attribute admindomain; # attribute userdomain; # ') # typeattribute sysadm_t admindomain; # typeattribute sysadm_t userdomain; (typeattributeset cil_gen_require admindomain) (typeattributeset admindomain (sysadm_t )) (typeattributeset cil_gen_require userdomain) (typeattributeset userdomain (sysadm_t )) ... However, when checking which domains use the admindomain attribute, notice the following: ~# seinfo -aadmindomain -x ERROR: Provided attribute (admindomain) is not a valid attribute name. But don't panic - this has a reason: as long as there is no SELinux rule applied towards the admindomain attribute, then the SELinux policy compiler will drop the attribute from the final policy. This can be confirmed by adding a single, cosmetic rule, like so: ## allow admindomain admindomain:process sigchld; ~# seinfo -aadmindomain -x admindomain sysadm_t So there you go. That does mean that if something previously used the attribute assignation for any decisions (like \"for each domain assigned the userdomain attribute, do something\") will need to make sure that the attribute is really used in a policy rule.","tags":"SELinux","url":"https://blog.siphos.be/2015/02/cil-and-attributes/","loc":"https://blog.siphos.be/2015/02/cil-and-attributes/"},{"title":"Have dhcpcd wait before backgrounding","text":"Many of my systems use DHCP for obtaining IP addresses. Even though they all receive a static IP address, it allows me to have them moved over (migrations), use TFTP boot, cloning (in case of quick testing), etc. But one of the things that was making my efforts somewhat more difficult was that the dhcpcd service continued (the dhcpcd daemon immediately went in the background) even though no IP address was received yet. Subsequent service scripts that required a working network connection failed to start then. The solution is to configure dhcpcd to wait for an IP address. This is done through the -w option, or the waitip instruction in the dhcpcd.conf file. With that in place, the service script now waits until an IP address is assigned.","tags":"Gentoo","url":"https://blog.siphos.be/2015/02/have-dhcpcd-wait-before-backgrounding/","loc":"https://blog.siphos.be/2015/02/have-dhcpcd-wait-before-backgrounding/"},{"title":"Old Gentoo system? Not a problem...","text":"If you have a very old Gentoo system that you want to upgrade, you might have some issues with too old software and Portage which can't just upgrade to a recent state. Although many methods exist to work around it, one that I have found to be very useful is to have access to old Portage snapshots. It often allows the administrator to upgrade the system in stages (say in 6-months blocks), perhaps not the entire world but at least the system set. Finding old snapshots might be difficult though, so at one point I decided to create a list of old snapshots , two months apart, together with the GPG signature (so people can verify that the snapshot was not tampered with by me in an attempt to create a Gentoo botnet). I haven't needed it in a while anymore, but I still try to update the list every two months, which I just did with the snapshot of January 20th this year. I hope it at least helps a few other admins out there.","tags":"Gentoo","url":"https://blog.siphos.be/2015/01/old-gentoo-system-not-a-problem/","loc":"https://blog.siphos.be/2015/01/old-gentoo-system-not-a-problem/"},{"title":"SELinux is great for enterprises (but many don't know it yet)","text":"Large companies that handle their own IT often have internal support teams for many of the technologies that they use. Most of the time, this is for reusable components like database technologies, web application servers, operating systems, middleware components (like file transfers, messaging infrastructure, ...) and more. All components that are used and deployed multiple times, and thus warrant the expenses of a dedicated engineering team. Such teams often have (or need to write) secure configuration deployment guides, so that these components are installed in the organization with as little misconfigurations as possible. A wrongly configured component is often worse than a vulnerable component, because vulnerabilities are often fixed with the software upgrades (you do patch your software, right?) whereas misconfigurations survive these updates and remain exploitable for longer periods. Also, misuse of components is harder to detect than exploiting vulnerabilities because they are often seen as regular user behavior. But next to the redeployable components, most business services are provided by a single application. Most companies don't have the budget and resources to put dedicated engineering teams on each and every application that is deployed in the organization. Even worse, many companies hire external consultants to help in the deployment of the component, and then the consultants hand over the maintenance of that software to internal teams. Some consultants don't fully bother with secure configuration deployment guides, or even feel the need to disable security constraints put forth by the organization (policies and standards) because \"it is needed\". A deployment is often seen as successful when the software functionally works, which not necessarily means that it is misconfiguration-free. As a recent example that I came across, consider an application that needs Node.js . A consultancy firm is hired to set up the infrastructure, and given full administrative rights on the operating system to make sure that this particular component is deployed fast (because the company wants to have the infrastructure in production before the end of the week). Security is initially seen as less of a concern, and the consultancy firm informs the customer (without any guarantees though) that it will be set up \"according to common best practices\". The company itself has no engineering team for Node.js nor wants to invest in the appropriate resources (such as training) for security engineers to review Node.js configurations. Yet the application that is deployed on the Node.js application server is internet-facing, so has a higher risk associated with it than a purely internal deployment. So, how to ensure that these applications cannot be exploited or, if an exploit is done, how to ensure that the risks involved with the exploit are contained? Well, this is where I believe SELinux has a great potential. And although I'm talking about SELinux here, the same goes for other similar technologies like TOMOYO Linux , grSecurity's RBAC system , RSBAC and more. SELinux can provide a container, decoupled from the application itself (but of course built for that particular application) which restricts the behavior of that application on the system to those activities that are expected. The application itself is not SELinux-aware (or does not need to be - some applications are, but those that I am focusing on here usually don't), but the SELinux access controls ensure that exploits on the application cannot reach beyond those activities/capabilities that are granted to it. Consider the Node.js deployment from before. The Node.js application server might need to connect to a MongoDB cluster, so we can configure SELinux to allow just that, but all other connections that originate from the Node.js deployment should be forbidden. Worms (if any) cannot use this deployment then to spread out. Same with access to files - the Node.js application probably only needs access to the application files and not to other system files. Instead of trying to run the application in a chroot (which requires engineering effort from those people implementing Node.js, which could be a consultancy firm that does not know or want to deploy within a chroot) SELinux is configured to disallow any file access beyond the application files. With SELinux, the application can be deployed relatively safely while ensuring that exploits (or abuse of misconfigurations) cannot spread. All that the company itself has to do is to provide resources for a SELinux engineering team (which can be just a responsibility of the Linux engineering teams, but can be specialized as well). Such a team does not need to be big, as policy development effort is usually only needed during changes (for instance when the application is updated to also send e-mails, in which case the SELinux policy can be adjusted to allow that as well), and given enough experience, the SELinux engineering team can build flexible policies that the administration teams (those that do the maintenance of the servers) can tune the policy as needed (for instance through SELinux booleans) without the need to have the SELinux team work on the policies again. Using SELinux also has a number of additional advantages which other, sometimes commercial tools (like Symantecs SPE/SCSP - really Symantec, you ask customers to disable SELinux?) severly lack. SELinux is part of a default Linux installation in many cases. RedHat Enterprise Linux ships with SELinux by default, and actively supports SELinux when customers have any problems with it. This also improves the likelihood for SELinux to be accepted, as other, third party solutions might not be supported. Ever tried getting support for a system on which both McAfee AV for Linux and Symantec SCSP are running (if you got it to work together at all)? At least McAfee gives pointers to how to update SELinux settings when they would interfere with McAfee processes. SELinux is widely known and many resources exist for users, administrators and engineers to learn more about it. The resources are freely available, and often kept up2date by a very motivated community. Unlike commercial products, whose support pages are hidden behind paywalls, customers are usually prevented from interacting with each other and tips and tricks for using the product are often not found on the Internet, SELinux information can be found almost everywhere. And if you like books, I have a couple for you to read: SELinux System Administration and SELinux Cookbook , written by yours truly. Using SELinux is widely supported by third party configuration management tools, especially in the free software world. Puppet , Chef , Ansible , SaltStack and others all support SELinux and/or have modules that integrate SELinux support in the management system. Using SELinux incurs no additional licensing costs. Now, SELinux is definitely not a holy grail. It has its limitations, so security should still be seen as a global approach where SELinux is just playing one specific role in. For instance, SELinux does not prevent application behavior that is allowed by the policy. If a user abuses a configuration and can have an application expose information that the user usually does not have access to, but the application itself does (for instance because other users on that application might) SELinux cannot do anything about it (well, not as long as the application is not made SELinux-aware). Also, vulnerabilities that exploit application internals are not controlled by SELinux access controls. It is the application behavior (\"external view\") that SELinux controls. To mitigate in-application vulnerabilities, other approaches need to be considered (such as memory protections for free software solutions, which can protect against some kinds of exploits - see grsecurity as one of the solutions that could be used). Still, I believe that SELinux can definitely provide additional protections for such \"one-time deployments\" where a company cannot invest in resources to provide engineering services on those deployments. The SELinux security controls do not require engineering on the application side, making investments in SELinux engineering very much reusable.","tags":"SELinux","url":"https://blog.siphos.be/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/","loc":"https://blog.siphos.be/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/"},{"title":"Gentoo Wiki is growing","text":"Perhaps it is because of the winter holidays, but the last weeks I've noticed a lot of updates and edits on the Gentoo wiki. The move to the Tyrian layout, whose purpose is to eventually become the unified layout for all Gentoo resources, happened first. Then, three common templates ( Code , File and Kernel ) where deprecated in favor of their \"*Box\" counterparts ( CodeBox , FileBox and KernelBox ). These provide better parameter support (which should make future updates on the templates easier to implement) as well as syntax highlighting. But the wiki also saw a number of contributions being added. I added a short article on Efibootmgr as the Gentoo handbook now also uses it for its EFI related instructions, but other users added quite a few additional articles as well. As they come along, articles are being marked by editors for translation. For me, that's a trigger. Whenever a wiki article is marked for translations, it shows up on the PageTranslation list. When I have time, I pick one of these articles and try to update it to move to a common style (the Guidelines page is the \"official\" one, and I have a Styleguide in which I elaborate a bit more on the use). Having a common style gives a better look and feel to the articles (as they are then more alike), gives a common documentation development approach (so everyone can join in and update documentation in a similar layout/structure) and - most importantly - reduces the number of edits that do little more than switch from one formatting to another. When an article has been edited, I mark it for translation, and then the real workhorse on the wiki starts. We have several active translators on the Gentoo wiki, who we cannot thank hard enough for their work (I used to start at Gentoo as a translator, I have some feeling about their work). They make the Gentoo documentation reachable for a broader audience. Thanks to the use of the translation extension (kindly offered by the Gentoo wiki admins, who have been working quite hard the last few weeks on improving the wiki infrastructure) translations are easier to handle and follow through. The advantage of a translation-marked article is that any change on the article also shows up on the list again, allowing me to look at the change and perform edits when necessary. For the end user, this is behind the scenes - an update on an article shows up immediately, which is fine. But for me (and perhaps other editors as well) this gives a nice overview of changes to articles (watchlists can only go so far) and also shows the changes in a simple yet efficient manner. Thanks to this approach, we can more actively follow up on edits and improve where necessary. Now, editing is not always just a few minutes of work. Consider the GRUB2 article on the wiki. It was marked for translation, but had some issues with its style. It was very verbose (which is not a bad thing, but suggests to split information towards multiple articles) and quite a few open discussions on its Discussions page. I started editing the article around 13.12h local time, and ended at 19.40h. Unlike with offline documentation, the entire process of the editing can be followed through the page' history ). And although I'm still not 100% satisfied with the result, it is imo easier to follow through and read. However, don't get me wrong - I do not feel that the article was wrong in any way. Although I would appreciate articles that immediately follow a style, I rather see more contributions (which we can then edit towards the new style) than that we would start penalizing contributors that don't use the style. That would work contra-productive, because it is far easier to update the style of an article than to write articles. We should try and get more contributors to document aspects of their Gentoo journey. So, please keep them coming. If you find a lack of (good) information for something, start jotting down what you know in an article. We'll gladly help you out with editing and improving the article then, but the content is something you are probably best to write down.","tags":"Documentation","url":"https://blog.siphos.be/2015/01/gentoo-wiki-is-growing/","loc":"https://blog.siphos.be/2015/01/gentoo-wiki-is-growing/"},{"title":"Added UEFI instructions to AMD64/x86 handbooks","text":"I just finished up adding some UEFI instructions to the Gentoo handbooks for AMD64 and x86 (I don't know how many systems are still using x86 instead of the AMD64 one, and if those support UEFI, but the instructions are shared and they don't collide). The entire EFI stuff can probably be improved a lot, but basically the things that were added are: boot the system using UEFI already if possible (which is needed for efibootmgr to access the EFI variables). This is not entirely mandatory (as efibootmgr is not mandatory to boot a system) but recommended. use vfat for the /boot/ location, as this now becomes the EFI System Partition. configure the Linux kernel to support EFI stub and EFI variables install the Linux kernel as the bootx64.efi file to boot the system with use efibootmgr to add boot options (if required) and create an EFI boot entry called \"Gentoo\" If you find grave errors, please do mention them (either on a talk page on the wiki, as a bug or through IRC) so it is picked up. All developers and trusted contributors on the wiki have access to the files so can edit where needed (but do take care that, if something is edited, that it is either architecture-specific or shared across all architectures - check the page when editing; if it is Handbook:Parts then it is shared, and Handbook:AMD64 is specific for the architecture). And if I'm online I'll of course act on it quickly. Oh, and no - it is not a bug that there is a (now not used) /dev/sda1 \"bios\" partition. Due to the differences with the possible installation alternatives, it is easier for us (me) to just document a common partition layout than to try and write everything out (making it just harder for new users to follow the instructions).","tags":"Documentation","url":"https://blog.siphos.be/2014/12/added-uefi-instructions-to-amd64x86-handbooks/","loc":"https://blog.siphos.be/2014/12/added-uefi-instructions-to-amd64x86-handbooks/"},{"title":"Gentoo Handbooks almost moved to wiki","text":"Content-wise, the move is done. I've done a few checks on the content to see if the structure still holds, translations are enabled on all pages, the use of partitions is sufficiently consistent for each architecture, and so on. The result can be seen on the gentoo handbook main page , from which the various architectural handbooks are linked. I sent a sort-of announcement to the gentoo-project mailinglist (which also includes the motivation of the move). If there are no objections, I will update the current handbooks to link to the wiki ones, as well as update the links on the website (and in wiki articles) to point to the wiki.","tags":"Gentoo","url":"https://blog.siphos.be/2014/12/gentoo-handbooks-almost-moved-to-wiki/","loc":"https://blog.siphos.be/2014/12/gentoo-handbooks-almost-moved-to-wiki/"},{"title":"Sometimes I forget how important communication is","text":"Free software (and documentation) developers don't always have all the time they want. Instead, they grab whatever time they have to do what they believe is the most productive - be it documentation editing, programming, updating ebuilds, SELinux policy improvements and what not. But they often don't take the time to communicate. And communication is important. For one, communication is needed to reach a larger audience than those that follow the commit history in whatever repository work is being done. Yes, there are developers that follow each commit , but development isn't just done for developers, it is also for end users. And end users deserve frequent updates and feedback. Be it through blog posts, Google+ posts, tweets or instragrams (well, I'm not sure how to communicate a software or documentation change through Instagram, but I'm sure people find lots of creative ways to do so), telling the broader world what has changed is important. Perhaps a (silent or not) user was waiting for this change. Perhaps he or she is even actually trying to fix things himself/herself but is struggling with it, and would really benefit (time-wise) from a quick fix. Without communicating about the change, (s)he does not know that no further attempts are needed, actually reducing the efficiency in overall. But communication is just one-way. Better is to get feedback as well. In that sense, communication is just one part of the feedback loop - once developers receive feedback on what they are doing (or did recently) they might even improve results faster. With feedback loops, the wisdom of the crowd (in the positive sense) can be used to improve solutions beyond what the developer originally intended. And even a simple \"cool\" and \"I like\" is good information for a developer or contributor. Still, I often forget to do it - or don't have the time to focus on communication. And that's bad. So, let me quickly state what things I forgot to communicate more broadly about: A new developer joined the Gentoo ranks: Jason Zaman. Now developers join Gentoo more often than just once in a while, but Jason is one of my \"recruits\". In a sense, he became a developer because I was tired of pulling his changes in and proxy-committing stuff. Of course, that's only half the truth; he is also a very active contributor in other areas (and was already a maintainer for a few packages through the proxy-maintainer project) and is a tremendous help in the Gentoo Hardened project. So welcome onboard Jason (or perfinion as he calls himself online). I've started with copying the Gentoo handbook to the wiki . This is still an on-going project, but was long overdue. There are many reasons why the move to the wiki is interesting. For me personally, it is to attract a larger audience to update the handbook. Although the document will be restricted for editing by developers and trusted contributors only (it does contain the installation instructions and is a primary entry point for many users) that's still a whole lot more than when just a handful (one or two actually) developers update the handbook. The SELinux userspace (2.4 release) is looking more stable; there are no specific regressions anymore (upstream is at release candidate 7) although I must admit that I have not implemented it on the majority of test systems that I maintain. Not due to fears, but mostly because I struggle a bit with available time so I can do without testing upgrades that are not needed. I do plan on moving towards 2.4 in a week or two. The reference policy has released a new version of the policy. Gentoo quickly followed through (Jason did the honors of creating the ebuilds). So, apologies for not communicating sooner, and I promise I'll try to uplift the communication frequency.","tags":"Gentoo","url":"https://blog.siphos.be/2014/12/sometimes-i-forget-how-important-communication-is/","loc":"https://blog.siphos.be/2014/12/sometimes-i-forget-how-important-communication-is/"},{"title":"No more DEPENDs for SELinux policy package dependencies","text":"I just finished updating 102 packages. The change? Removing the following from the ebuilds: DEPEND=\"selinux? ( sec-policy/selinux-${packagename} )\" In the past, we needed this construction in both DEPEND and RDEPEND. Recently however, the SELinux eclass got updated with some logic to relabel files after the policy package is deployed. As a result, the DEPEND variable no longer needs to refer to the SELinux policy package. This change also means that for those moving from a regular Gentoo installation to an SELinux installation will have much less packages to rebuild. In the past, getting USE=\"selinux\" (through the SELinux profiles) would rebuild all packages that have a DEPEND dependency to the SELinux policy package. No more - only packages that depend on the SELinux libraries (like libselinux ) or utilities rebuild. The rest will just pull in the proper policy package.","tags":"Gentoo","url":"https://blog.siphos.be/2014/11/no-more-depends-for-selinux-policy-package-dependencies/","loc":"https://blog.siphos.be/2014/11/no-more-depends-for-selinux-policy-package-dependencies/"},{"title":"Using multiple priorities with modules","text":"One of the new features of the 2.4 SELinux userspace is support for module priorities. The idea is that distributions and administrators can override a (pre)loaded SELinux policy module with another module without removing the previous module. This lower-version module will remain in the store, but will not be active until the higher-priority module is disabled or removed again. The \"old\" modules (pre-2.4) are loaded with priority 100. When policy modules with the 2.4 SELinux userspace series are loaded, they get loaded with priority 400. As a result, the following message occurs: ~# semodule -i screen.pp libsemanage.semanage_direct_install_info: Overriding screen module at lower priority 100 with module at priority 400 So unlike the previous situation, where the older module is substituted with the new one, we now have two \"screen\" modules loaded; the last one gets priority 400 and is active. To see all installed modules and priorities, use the --list-modules option: ~# semodule --list-modules=all | grep screen 100 screen pp 400 screen pp Older versions of modules can be removed by specifying the priority: ~# semodule -X 100 -r screen","tags":"SELinux","url":"https://blog.siphos.be/2014/10/using-multiple-priorities-with-modules/","loc":"https://blog.siphos.be/2014/10/using-multiple-priorities-with-modules/"},{"title":"Migrating to SELinux userspace 2.4 (small warning for users)","text":"In a few moments, SELinux users which have the \\~arch KEYWORDS set (either globally or for the SELinux utilities in particular) will notice that the SELinux userspace will upgrade to version 2.4 (release candidate 5 for now). This upgrade comes with a manual step that needs to be performed after upgrade. The information is mentioned as post-installation message of the policycoreutils package, and basically sais that you need to execute: ~# /usr/libexec/selinux/semanage_migrate_store The reason is that the SELinux utilities expect the SELinux policy module store (and the semanage related files) to be in /var/lib/selinux and no longer in /etc/selinux . Note that this does not mean that the SELinux policy itself is moved outside of that location, nor is the basic configuration file ( /etc/selinux/config ). It is what tools such as semanage manage that is moved outside that location. I tried to automate the migration as part of the packages themselves, but this would require the portage_t domain to be able to move, rebuild and load policies, which it can't (and to be honest, shouldn't). Instead of augmenting the policy or making updates to the migration script as delivered by the upstream project, we currently decided to have the migration done manually. It is a one-time migration anyway. If for some reason end users forget to do the migration, then that does not mean that the system breaks or becomes unusable. SELinux still works, SELinux aware applications still work; the only thing that will fail are updates on the SELinux configuration through tools like semanage or setsebool - the latter when you want to persist boolean changes. ~# semanage fcontext -l ValueError: SELinux policy is not managed or store cannot be accessed. ~# setsebool -P allow_ptrace on Cannot set persistent booleans without managed policy. If you get those errors or warnings, all that is left to do is to do the migration. Note in the following that there is a warning about 'else' blocks that are no longer supported: that's okay, as far as I know (and it was mentioned on the upstream mailinglist as well as not something to worry about) it does not have any impact. ~# /usr/libexec/selinux/semanage_migrate_store Migrating from /etc/selinux/mcs/modules/active to /var/lib/selinux/mcs/active Attempting to rebuild policy from /var/lib/selinux sysnetwork: Warning: 'else' blocks in optional statements are unsupported in CIL. Dropping from output. You can also add in -c so that the old policy module store is cleaned up. You can also rerun the command multiple times: ~# /usr/libexec/selinux/semanage_migrate_store -c warning: Policy type mcs has already been migrated, but modules still exist in the old store. Skipping store. Attempting to rebuild policy from /var/lib/selinux You can manually clean up the old policy module store like so: ~# rm -rf /etc/selinux/mcs/modules So... don't worry - the change is small and does not break stuff. And for those wondering about CIL I'll talk about it in one of my next posts.","tags":"Gentoo","url":"https://blog.siphos.be/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/","loc":"https://blog.siphos.be/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/"},{"title":"Lots of new challenges ahead","text":"I've been pretty busy lately, albeit behind the corners, which leads to a lower activity within the free software communities that I'm active in. Still, I'm not planning any exit, on the contrary. Lots of ideas are just waiting for some free time to engage. So what are the challenges that have been taking up my time? One of them is that I recently moved. And with moving comes a lot of work in getting the place into a good shape and getting settled. Today I finished the last job that I wanted to finish in my appartment in a short amount of time, so that's one thing off my TODO list. Another one is that I started an intensive master-after-master programme with the subject of Enterprise Architecture . This not only takes up quite some ex-cathedra time, but also additional hours of studying (and for the moment also exams). But I'm really satisfied that I can take up this course, as I've been wandering around in the world of enterprise architecture for some time now and want to grow even further in this field. But that's not all. One of my side activities has been blooming a lot, and I recently reached the 200th server that I'm administering (although I think this number will reduce to about 120 as I'm helping one organization with handing over management of their 80+ systems to their own IT staff). Together with some friends (who also have non-profit customers' IT infrastructure management as their side-business) we're now looking at consolidating our approach to system administration (and engineering). I'm also looking at investing time and resources in a start-up, depending on the business plan and required efforts. But more information on this later when things are more clear :-)","tags":"Misc","url":"https://blog.siphos.be/2014/10/lots-of-new-challenges-ahead/","loc":"https://blog.siphos.be/2014/10/lots-of-new-challenges-ahead/"},{"title":"After SELinux System Administration, now the SELinux Cookbook","text":"Almost an entire year ago (just a few days apart) I announced my first published book, called SELinux System Administration . The book covered SELinux administration commands and focuses on Linux administrators that need to interact with SELinux-enabled systems. An important part of SELinux was only covered very briefly in the book: policy development. So in the spring this year, Packt approached me and asked if I was interested in authoring a second book for them, called SELinux Cookbook . This book focuses on policy development and tuning of SELinux to fit the needs of the administrator or engineer, and as such is a logical follow-up to the previous book. Of course, given my affinity with the wonderful Gentoo Linux distribution, it is mentioned in the book (and even the reference platform) even though the book itself is checked against Red Hat Enterprise Linux and Fedora as well, ensuring that every recipe in the book works on all distributions. Luckily (or perhaps not surprisingly) the approach is quite distribution-agnostic. Today, I got word that the SELinux Cookbook is now officially published. The book uses a recipe-based approach to SELinux development and tuning, so it is quickly hands-on. It gives my view on SELinux policy development while keeping the methods and processes aligned with the upstream policy development project (the reference policy ). It's been a pleasure (but also somewhat a pain, as this is done in free time, which is scarce already) to author the book. Unlike the first book, where I struggled a bit to keep the page count to the requested amount, this book was not limited. Also, I think the various stages of the book development contributed well to the final result (something that I overlooked a bit in the first time, so I re-re-reviewed changes over and over again this time - after the first editorial reviews, then after the content reviews, then after the language reviews, then after the code reviews). You'll see me blog a bit more about the book later (as the marketing phase is now starting) but for me, this is a major milestone which allowed me to write down more of my SELinux knowledge and experience. I hope it is as good a read for you as I hope it to be.","tags":"SELinux","url":"https://blog.siphos.be/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/","loc":"https://blog.siphos.be/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/"},{"title":"Gentoo Hardened august meeting","text":"Another month has passed, so we had another online meeting to discuss the progress within Gentoo Hardened. Lead elections The yearly lead elections within Gentoo Hardened were up again. Zorry (Magnus Granberg) was re-elected as project lead so doesn't need to update his LinkedIn profile yet ;-) Toolchain blueness (Anthony G. Basile) has been working on the uclibc stages for some time. Due to the configurable nature of these setups, many /etc/portage files were provided as part of the stages, which shouldn't happen. Work is on the way to update this accordingly. For the musl setup, blueness is also rebuilding the stages to use a symbolic link to the dynamic linker ( /lib/ld-linux-arch.so ) as recommended by the musl maintainers. Kernel and grsecurity with PaX A bug has been submitted which shows that large binary files (in the bug, a chrome binary with debug information is shown to be more than 2 Gb in size) cannot be pax-mark'ed, with paxctl informing the user that the file is too big. The problem is when the PAX marks are in ELF (as the application mmaps the binary) - users of extended attributes based PaX markings do not have this problem. blueness is working on making things a bit more intelligent, and to fix this. SELinux I have been making a few changes to the SELinux setup: The live ebuilds (those with version 9999 which use the repository policy rather than snapshots of the policies) are now being used as \"master\" in case of releases: the ebuilds can just be copied to the right version to support the releases. The release script inside the repository is adjusted to reflect this as well. The SELinux eclass now supports two variables, SELINUX_GIT_REPO and SELINUX_GIT_BRANCH , which allows users to use their own repository, and developers to work in specific branches together. By setting the right value in the users' make.conf switching policy repositories or branches is now a breeze. Another change in the SELinux eclass is that, after the installation of SELinux policies, we will check the reverse dependencies of the policy package and relabel the files of these packages. This allows us to only have RDEPEND dependencies towards the SELinux policy packages (if the application itself does not otherwise link with libselinux ), making the dependency tree within the package manager more correct. We still need to update these packages to drop the DEPEND dependency, which is something we will focus on in the next few months. In order to support improved cooperation between SELinux developers in the Gentoo Hardened team - perfinion (Jason Zaman) is in the queue for becoming a new developer in our mids - a coding style for SELinux policies is being drafted up. This is of course based on the coding style of the reference policy, but with some Gentoo specific improvements and more clarifications. perfinion has been working on improving the SELinux support in OpenRC (release 0.13 and higher), making some of the additions that we had to make in the past - such as the selinux_gentoo init script - obsolete. The meeting also discussed a few bugs in more detail, but if you really want to know, just hang on and wait for the IRC logs ;-) Other usual sections (system integrity and profiles) did not have any notable topics to describe.","tags":"Gentoo","url":"https://blog.siphos.be/2014/08/gentoo-hardened-august-meeting/","loc":"https://blog.siphos.be/2014/08/gentoo-hardened-august-meeting/"},{"title":"Switching to new laptop","text":"I'm slowly but surely starting to switch to a new laptop. The old one hasn't completely died (yet) but given that I had to force its CPU frequency at the lowest Hz or the CPU would burn (and the system suddenly shut down due to heat issues), and that the connection between the battery and laptop fails (so even new battery didn't help out) so I couldn't use it as a laptop... well, let's say the new laptop is welcome ;-) Building Gentoo isn't an issue (having only a few hours per day to work on it is) and while I'm at it, I'm also experimenting with EFI (currently still without secure boot, but with EFI) and such. Considering that the Gentoo Handbook needs quite a few updates (and I'm thinking to do more than just small updates) knowing how EFI works is a Good Thing (tm). For those interested - the EFI stub kernel instructions in the article on the wiki, and also in Greg's wonderful post on booting a self-signed Linux kernel (which I will do later) work pretty well. I didn't try out the \"Adding more kernels\" section in it, as I need to be able to (sometimes) edit the boot options (which isn't easy to accomplish with EFI stub-supporting kernels afaics). So I installed Gummiboot (and created a wiki article on it). Lots of things still planned, so little time. But at least building chromium is now a bit faster - instead of 5 hours and 16 minutes, I can now enjoy the newer versions after little less than 40 minutes.","tags":"Gentoo","url":"https://blog.siphos.be/2014/08/switching-to-new-laptop/","loc":"https://blog.siphos.be/2014/08/switching-to-new-laptop/"},{"title":"Some changes under the hood","text":"In between conferences, technical writing jobs and traveling, we did a few changes under the hood for SELinux in Gentoo. First of all, new policies are bumped and also stabilized (2.20130411-r3 is now stable, 2.20130411-r5 is \\~arch). These have a few updates (mergers from upstream), and r5 also has preliminary support for tmpfiles (at least the OpenRC implementation of it), which is made part of the selinux-base-policy package. The ebuilds to support new policy releases now are relatively simple copies of the live ebuilds (which always contain the latest policies) so that bumping (either by me or other developers) is easy enough. There's also a release script in our policy repository which tags the right git commit (the point at which the release is made), creates the necessary patches, uploads them, etc. One of the changes made is to \"drop\" the BASEPOL variable. In the past, BASEPOL was a variable inside the ebuilds that pointed to the right patchset (and base policy) as we initially supported policy modules of different base releases. However, that was a mistake and we quickly moved to bumping all policies with every releaes, but kept the BASEPOL variable in it. Now, BASEPOL is \"just\" the ${PVR} value of the ebuild so no longer needs to be provided. In the future, I'll probably remove BASEPOL from the internal eclass and the selinux-base* packages as well. A more important change to the eclass is support for the SELINUX_GIT_REPO and SELINUX_GIT_BRANCH variables (for live ebuilds, i.e. those with the 9999 version). If set, then they pull from the mentioned repository (and branch) instead of the default hardened-refpolicy.git repository. This allows for developers to do some testing on a different branch easily, or for other users to use their own policy repository while still enjoying the SELinux integration support in Gentoo through the sec-policy/* packages. Finally, I wrote up a first attempt at our coding style , heavily based on the coding style from the reference policy of course (as our policy is still following this upstream project). This should allow the team to work better together and to decide on namings autonomously (instead of hours of discussing and settling for something as silly as an interface or boolean name ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2014/08/some-changes-under-the-hood/","loc":"https://blog.siphos.be/2014/08/some-changes-under-the-hood/"},{"title":"Gentoo Hardened July meeting","text":"I failed to show up myself (I fell asleep - kids are fun, but deplete your energy source quickly), but that shouldn't prevent me from making a nice write-up of the meeting. Toolchain GCC 4.9 gives some issues with kernel compilations and other components. Lately, breakage has been reported with GCC 4.9.1 compiling MySQL or with debugging symbols. So for hardened, we'll wait this one out until the bugs are fixed. For GCC 4.10, the --enable-default-pie patch has been sent upstream. If that is accepted, the SSP one will be sent as well. In uclibc land, stages are being developed for PPC. This is the final architecture that is often used in embedded worlds that needed support for it in Gentoo, and that's now being finalized. Go blueness! SELinux A libpcre upgrade broke relabeling operations on SELinux enabled systems. A fix for this has been made part of libselinux, but a little too late, so some users will be affected by the problem. It's easily worked around (removing the *.bin files in the contexts/files/ directory of the SELinux configuration) and hopefully will never occur again. The 2.3 userland has finally been stabilized (we had a few dependencies that we were waiting for - and we were a dependency ourselves for other packages as well). Finally, some thought discussion is being done (not that there's much feedback on it, but every documented step is a good step imo) on the SELinux policy within Gentoo (and the principles that we'll follow that are behind it). Kernel and grsecurity / PaX Due to some security issues, the Linux kernel sources have been stabilized more rapidly than usual, which left little time for broad validation and regression testing. Updates and fixes have been applied since and new stabilizations occurred. Hopefully we're now at the right, stable set again. The C-based install-xattr application (which is performance-wise a big improvement over the Python-based one) is working well in \"lab environments\" (some developers are using it exclusively). It is included in the Portage repository &#94;(if\\ I\\ understand\\ the\\ chat\\ excerpts\\ correctly)&#94; but as such not available for broader usage yet. An update against elfix is made as well as there was a dependency mismatch when building with USE=-ptpax . This will be corrected in elfix-0.9. Finally, blueness is also working on a GLEP (Gentoo Linux Enhancement Proposal) to export VDB information (especially NEEDED.ELF.2 ) as this is important for ELF/library graph information (as used by revdep-pax, migrate-pax, etc.). Although Portage already does this, this is not part of the PMS and as such other package managers might not do this (such as Paludis). Profiles Updates on the profiles has been made to properly include multilib related variables and other metadata. For some profiles, this went as easy as expected (nice stacking), but other profiles have inheritance troubles making it much harder to include the necessary information. Although some talks have arised on the gentoo-dev mailinglist about refactoring how Gentoo handles profiles, there hasn't been done much more than just talking :-( But I'm sure we haven't heard the last of this yet. Documentation Blueness has added information on EMULTRAMP in the kernel configuration, especially noting to the user that it is needed for Python support in Gentoo Hardened. It is also in the PaX Quickstart document, although this document is becoming a very large one and users might overlook it.","tags":"Gentoo","url":"https://blog.siphos.be/2014/08/gentoo-hardened-july-meeting/","loc":"https://blog.siphos.be/2014/08/gentoo-hardened-july-meeting/"},{"title":"Segmentation fault when emerging packages after libpcre upgrade?","text":"SELinux users might be facing failures when emerge is merging a package to the file system, with an error that looks like so: >>> Setting SELinux security labels /usr/lib64/portage/bin/misc-functions.sh: line 1112: 23719 Segmentation fault /usr/sbin/setfiles \"${file_contexts_path}\" -r \"${D}\" \"${D}\" * ERROR: dev-libs/libpcre-8.35::gentoo failed: * Failed to set SELinux security labels. This has been reported as bug 516608 and, after some investigation, the cause is found. First the quick workaround: ~# cd /etc/selinux/strict/contexts/files ~# rm *.bin And do the same for the other SELinux policy stores on the system (targeted, mcs, mls, ...). Now, what is happening... Inside the mentioned directory, binary files exist such as file_contexts.bin . These files contain the compiled regular expressions of the non-binary files (like file_contexts ). By using the precompiled versions, regular expression matching by the SELinux utilities is a lot faster. Not that it is massively slow otherwise, but it is a nice speed improvement nonetheless. However, when pcre updates occur, then the basic structures that pcre uses internally might change. For instance, a number might switch from a signed integer to an unsigned integer. As pcre is meant to be used within the same application run, most applications do not have any issues with such changes. However, the SELinux utilities effectively serialize these structures and later read them back in. If the new pcre uses a changed structure, then the read-in structures are incompatible and even corrupt. Hence the segmentation faults. To resolve this, Stephen Smalley created a patch that includes PCRE version checking. This patch is now included in sys-libs/libselinux version 2.3-r1. The package also recompiles the existing *.bin files so that the older binary files are no longer on the system. But there is a significant chance that this update will not trickle down to the users in time, so the workaround might be needed. I considered updating the pcre ebuilds as well with this workaround, but considering that libselinux is most likely to be stabilized faster than any libpcre bump I let it go. At least we have a solution for future upgrades; sorry for the noise. Edit: libselinux-2.2.2-r5 also has the fix included.","tags":"SELinux","url":"https://blog.siphos.be/2014/07/segmentation-fault-when-emerging-packages-after-libpcre-upgrade/","loc":"https://blog.siphos.be/2014/07/segmentation-fault-when-emerging-packages-after-libpcre-upgrade/"},{"title":"Multilib in Gentoo","text":"One of the areas in Gentoo that is seeing lots of active development is its ongoing effort to have proper multilib support throughout the tree. In the past, this support was provided through special emulation packages, but those have the (serious) downside that they are often outdated, sometimes even having security issues. But this active development is not because we all just started looking in the same direction. No, it's thanks to a few developers that have put their shoulders under this effort, directing the development workload where needed and pressing other developers to help in this endeavor. And pushing is more than just creating bugreports and telling developers to do something. It is also about communicating , giving feedback and patiently helping developers when they have questions. I can only hope that other activities within Gentoo and its potential broad impact work on this as well. Kudos to all involved, as well as all developers that have undoubtedly put numerous hours of development effort in the hope to make their ebuilds multilib-capable (I know I had to put lots of effort in it, but I find it is worthwhile and a big learning opportunity).","tags":"Gentoo","url":"https://blog.siphos.be/2014/07/multilib-in-gentoo/","loc":"https://blog.siphos.be/2014/07/multilib-in-gentoo/"},{"title":"D-Bus and SELinux","text":"After a post about D-Bus comes the inevitable related post about SELinux with D-Bus. Some users might not know that D-Bus is an SELinux-aware application. That means it has SELinux-specific code in it, which has the D-Bus behavior based on the SELinux policy (and might not necessarily honor the \"permissive\" flag). This code is used as an additional authentication control within D-Bus. Inside the SELinux policy, a dbus permission class is supported, even though the Linux kernel doesn't do anything with this class. The class is purely for D-Bus, and it is D-Bus that checks the permission (although work is being made to implement D-Bus in kernel (kdbus) ). The class supports two permission checks: acquire_svc which tells the domain(s) allowed to \"own\" a service (which might, thanks to the SELinux support, be different from the domain itself) send_msg which tells which domain(s) can send messages to a service domain Inside the D-Bus security configuration (the busconfig XML file, remember) a service configuration might tell D-Bus that the service itself is labeled differently from the process that owned the service. The default is that the service inherits the label from the domain, so when dnsmasq_t registers a service on the system bus, then this service also inherits the dnsmasq_t label. The necessary permission checks for the sysadm_t user domain to send messages to the dnsmasq service, and the dnsmasq service itself to register it as a service: allow dnsmasq_t self:dbus { acquire_svc send_msg }; allow sysadm_t dnsmasq_t:dbus send_msg; allow dnsmasq_t sysadm_t:dbus send_msg; For the sysadm_t domain, the two rules are needed as we usually not only want to send a message to a D-Bus service, but also receive a reply (which is also handled through a send_msg permission but in the inverse direction). However, with the following XML snippet inside its service configuration file, owning a certain resource is checked against a different label: <busconfig> <selinux> <associate own=\"uk.org.thekelleys.dnsmasq\" context=\"system_u:object_r:dnsmasq_dbus_t:s0\" /> </selinux> </busconfig> With this, the rules would become as follows: allow dnsmasq_t dnsmasq_dbus_t:dbus acquire_svc; allow dnsmasq_t self:dbus send_msg; allow sysadm_t dnsmasq_t:dbus send_msg; allow dnsmasq_t sysadm_t:dbus send_msg; Note that only the access for acquiring a service based on a name (i.e. owning a service) is checked based on the different label. Sending and receiving messages is still handled by the domains of the processes (actually the labels of the connections, but these are always the process domains). I am not aware of any policy implementation that uses a different label for owning services, and the implementation is more suited to \"force\" D-Bus to only allow services with a correct label. This ensures that other domains that might have enough privileges to interact with D-Bus and own a service cannot own these particular services. After all, other services don't usually have the privileges (policy-wise) to acquire_svc a service with a different label than their own label.","tags":"SELinux","url":"https://blog.siphos.be/2014/06/d-bus-and-selinux/","loc":"https://blog.siphos.be/2014/06/d-bus-and-selinux/"},{"title":"D-Bus, quick recap","text":"I've never fully investigated the what and how of D-Bus. I know it is some sort of IPC, but higher level than the POSIX IPC methods. After some reading, I think I start to understand how it works and how administrators can work with it. So a quick write-down is in place so I don't forget in the future. There is one system bus and, for each X session of a user, also a session bus. A bus is governed by a dbus-daemon process. A bus itself has objects on it, which are represented through path-like constructs (like /org/freedesktop/ConsoleKit ). These objects are provided by a service (application). Applications \"own\" such services, and identify these through a namespace-like value (such as org.freedesktop.ConsoleKit ). Applications can send signals to the bus, or messages through methods exposed by the service. If methods are invoked (i.e. messages send) then the application must specify the interface (such as org.freedesktop.ConsoleKit.Manager.Stop ). Administrators can monitor the bus through dbus-monitor , or send messages through dbus-send . For instance, the following command invokes the org.freedesktop.ConsoleKit.Manager.Stop method provided by the object at /org/freedesktop/ConsoleKit owned by the service/application at org.freedesktop.ConsoleKit : ~$ dbus-send --system --print-reply --dest=org.freedesktop.ConsoleKit /org/freedesktop/ConsoleKit/Manager org.freedesktop.ConsoleKit.Manager.Stop What I found most interesting however was to query the busses. You can do this with dbus-send although it is much easier to use tools such as d-feet or qdbus . To list current services on the system bus: ~# qdbus --system :1.1 org.freedesktop.ConsoleKit :1.10 :1.2 :1.3 org.freedesktop.PolicyKit1 :1.36 fi.epitest.hostap.WPASupplicant fi.w1.wpa_supplicant1 :1.4 :1.42 :1.5 :1.6 :1.7 org.freedesktop.UPower :1.8 :1.9 org.freedesktop.DBus The numbers are generated by D-Bus itself, the namespace-like strings are taken by the objects. To see what is provided by a particular service: ~# qdbus --system org.freedesktop.PolicyKit1 / /org /org/freedesktop /org/freedesktop/PolicyKit1 /org/freedesktop/PolicyKit1/Authority The methods made available through one of these: ~# qdbus --system org.freedesktop.PolicyKit1 /org/freedesktop/PolicyKit1/Authority method QDBusVariant org.freedesktop.DBus.Properties.Get(QString interface_name, QString property_name) method QVariantMap org.freedesktop.DBus.Properties.GetAll(QString interface_name) ... property read uint org.freedesktop.PolicyKit1.Authority.BackendFeatures property read QString org.freedesktop.PolicyKit1.Authority.BackendName property read QString org.freedesktop.PolicyKit1.Authority.BackendVersion method void org.freedesktop.PolicyKit1.Authority.AuthenticationAgentResponse(QString cookie, QDBusRawType::(sa{sv} identity) method void org.freedesktop.PolicyKit1.Authority.CancelCheckAuthorization(QString cancellation_id) signal void org.freedesktop.PolicyKit1.Authority.Changed() ... Access to methods and interfaces is governed through XML files in /etc/dbus-1/system.d (or session.d depending on the bus). Let's look at /etc/dbus-1/system.d/dnsmasq.conf as an example: <busconfig> <policy user=\"root\"> <allow own=\"uk.org.thekelleys.dnsmasq\"/> <allow send_destination=\"uk.org.thekelleys.dnsmasq\"/> </policy> <policy user=\"dnsmasq\"> <allow own=\"uk.org.thekelleys.dnsmasq\"/> <allow send_destination=\"uk.org.thekelleys.dnsmasq\"/> </policy> <policy context=\"default\"> <deny own=\"uk.org.thekelleys.dnsmasq\"/> <deny send_destination=\"uk.org.thekelleys.dnsmasq\"/> </policy> </busconfig> The configuration mentions that only the root Linux user can 'assign' a service/application to the uk.org.thekelleys.dnsmasq name, and root can send messages to this same service/application name. The default is that no-one can own and send to this service/application name. As a result, only the Linux root user can interact with this object. D-Bus also supports starting of services when a method is invoked (instead of running this service immediately). This is configured through *.service files inside /usr/share/dbus-1/system-services/ .","tags":"Free Software","url":"https://blog.siphos.be/2014/06/d-bus-quick-recap/","loc":"https://blog.siphos.be/2014/06/d-bus-quick-recap/"},{"title":"Chroots for SELinux enabled applications","text":"Today I had to prepare a chroot jail (thank you grsecurity for the neat additional chroot protection features) for a SELinux-enabled application. As a result, \"just\" making a chroot was insufficient: the application needed access to /sys/fs/selinux . Of course, granting access to /sys is not something I like to see for a chroot jail. Luckily, all other accesses are not needed, so I was able to create a static /sys/fs/selinux directory structure in the chroot, and then just mount the SELinux file system on that: ~# mount -t selinuxfs none /var/chroot/sys/fs/selinux In hindsight, I probably could just have created a /selinux location as that location, although deprecated, is still checked by the SELinux libraries. Anyway, there was a second requirement: access to /etc/selinux . Luckily it was purely for read operations, so I was first contemplating of copying the data and doing a chmod -R a-w /var/chroot/etc/selinux , but then considered a bind-mount: ~# mount -o bind,ro /etc/selinux /var/chroot/etc/selinux Alas, bad luck - the read-only flag is ignored during the mount, and the bind-mount is still read-write. A simple article on lwn.net informed me about the solution: I need to do a remount afterwards to enable the read-only state: ~# mount -o remount,ro /var/chroot/etc/selinux Great! And because my brain isn't what it used to be, I just make a quick blog for future reference ;-)","tags":"SELinux","url":"https://blog.siphos.be/2014/06/chroots-for-selinux-enabled-applications/","loc":"https://blog.siphos.be/2014/06/chroots-for-selinux-enabled-applications/"},{"title":"Gentoo Hardened, June 2014","text":"Friday the Gentoo Hardened project had its monthly online meeting to talk about the progress within the various tools, responsibilities and subprojects. On the toolchain part, Zorry mentioned that GCC 4.9 and 4.8.3 will have SSP enabled by default. The hardened profiles will still have a different SSP setting than the default (so yes, there will still be differences between the two) but this will help in securing the Gentoo default installations. Zorry is also working on upstreaming the PIE patches for GCC 4.10. Next to the regular toolchain, blueness also mentioned his intentions to launch a Hardened musl subproject which will focus on the musl C library (rather than glibc or uclibc) and hardening. On the kernel side, two recent kernel vulnerabilities in the vanilla kernel Linux (pty race and privilege escalation through futex code) painted the discussions on IRC recently. Some versions of the hardened kernels are still available in the tree, but the more recent (non-vulnerable) kernels have proven not to be as stable as we'd hoped. The pty race vulnerability is possibly not applicable to hardened kernels thanks to grSecurity, due to its protection to access the kernel symbols. The latest kernels should not be used with KSTACKOVERFLOW on production systems though; there are some issues reported with virtio network interface support (on the guests) and ZFS. Also, on the Pax support, the install-xattr saga continues. The new wrapper that blueness worked in dismissed some code to keep the PWD so the $S directory knowledge was \"lost\". This is now fixed. All that is left is to have the wrapper included and stabilized. On SELinux side, it was the usual set of progress. Policy stabilization and user land application and library stabilization. The latter is waiting a bit because of the multilib support that's now being integrated in the ebuilds as well (and thus has a larger set of dependencies to go through) but no show-stoppers there. Also, the SELinux documentation portal on the wiki was briefly mentioned. Also, the policycoreutils vulnerability has been worked around so it is no longer applicable to us. On the hardened profiles , we had a nice discussion on enabling capabilities support (and move towards capabilities instead of setuid binaries), which klondike will try to tackle during the summer holidays. As I didn't take notes during the meeting, this post might miss a few (and I forgot to enable logging as well) but as Zorry sends out the meeting logs anyway later, you can read up there ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2014/06/gentoo-hardened-june-2014/","loc":"https://blog.siphos.be/2014/06/gentoo-hardened-june-2014/"},{"title":"Visualizing constraints","text":"SELinux constraints are an interesting way to implement specific, well, constraints on what SELinux allows. Most SELinux rules that users come in contact with are purely type oriented: allow something to do something against something. In fact, most of the SELinux rules applied on a system are such allow rules. The restriction of such allow rules is that they only take into consideration the type of the contexts that participate. This is the type enforcement part of the SELinux mandatory access control system. Constraints on the other hand work on the user, role and type part of a context. Consider this piece of constraint code: constrain file all_file_perms ( u1 == u2 or u1 == system_u or u2 == system_u or t1 != ubac_constrained_type or t2 != ubac_constrained_type ); This particular constraint definition tells the SELinux subsystem that, when an operation against a file class is performed (any operation, as all_file_perms is used, but individual, specific permissions can be listed as well), this is denied if none of the following conditions are met: The SELinux user of the subject and object are the same The SELinux user of the subject or object is system_u The SELinux type of the subject does not have the ubac_constrained_type attribute set The SELinux type of the object does not have the ubac_constrained_type attribute set If none of the conditions are met, then the action is denied, regardless of the allow rules set otherwise. If at least one condition is met, then the allow rules (and other SELinux rules) decide if an action can be taken or not. Constraints are currently difficult to query though. There is seinfo --constrain which gives all constraints, using the Reverse Polish Notation - not something easily readable by users: ~$ seinfo --constrain constrain { sem } { create destroy getattr setattr read write associate unix_read unix_write } ( u1 u2 == u1 system_u == || u2 system_u == || t1 { screen_var_run_t gnome_xdg_config_home_t admin_crontab_t links_input_xevent_t gpg_pinentry_tmp_t virt_content_t print_spool_t crontab_tmp_t httpd_user_htaccess_t ssh_keysign_t remote_input_xevent_t gnome_home_t mozilla_tmpfs_t staff_gkeyringd_t consolekit_input_xevent_t user_mail_tmp_t chromium_xdg_config_t mozilla_input_xevent_t chromium_tmp_t httpd_user_script_exec_t gnome_keyring_tmp_t links_tmpfs_t skype_tmp_t user_gkeyringd_t svirt_home_t sysadm_su_t virt_home_t skype_home_t wireshark_tmp_t xscreensaver_xproperty_t consolekit_xproperty_t user_home_dir_t gpg_pinentry_xproperty_t mplayer_home_t mozilla_plugin_input_xevent_t mozilla_plugin_tmp_t mozilla_xproperty_t xdm_input_xevent_t chromium_input_xevent_t java_tmpfs_t googletalk_plugin_xproperty_t sysadm_t gorg_t gpg_t java_t links_t staff_dbusd_t httpd_user_ra_content_t httpd_user_rw_content_t googletalk_plugin_tmp_t gpg_agent_tmp_t ssh_agent_tmp_t sysadm_ssh_agent_t user_fonts_cache_t user_tmp_t googletalk_plugin_input_xevent_t user_dbusd_t xserver_tmpfs_t iceauth_home_t qemu_input_xevent_t xauth_home_t mutt_home_t sysadm_dbusd_t remote_xproperty_t gnome_xdg_config_t screen_home_t chromium_xproperty_t chromium_tmpfs_t wireshark_tmpfs_t xdg_videos_home_t pulseaudio_input_xevent_t krb5_home_t pulseaudio_xproperty_t xscreensaver_input_xevent_t gpg_pinentry_input_xevent_t httpd_user_script_t gnome_xdg_cache_home_t mozilla_plugin_tmpfs_t user_home_t user_sudo_t ssh_input_xevent_t ssh_tmpfs_t xdg_music_home_t gconf_tmp_t flash_home_t java_home_t skype_tmpfs_t xdg_pictures_home_t xdg_data_home_t gnome_keyring_home_t wireshark_home_t chromium_renderer_xproperty_t gpg_pinentry_t mozilla_t session_dbusd_tmp_t staff_sudo_t xdg_config_home_t user_su_t pan_input_xevent_t user_devpts_t mysqld_home_t pan_tmpfs_t root_input_xevent_t links_home_t sysadm_screen_t pulseaudio_tmpfs_t sysadm_gkeyringd_t mail_home_rw_t gconf_home_t mozilla_plugin_xproperty_t mutt_tmp_t httpd_user_content_t mozilla_xdg_cache_t mozilla_home_t alsa_home_t pulseaudio_t mencoder_t admin_crontab_tmp_t xdg_documents_home_t user_tty_device_t java_tmp_t gnome_xdg_data_home_t wireshark_t mozilla_plugin_home_t googletalk_plugin_tmpfs_t user_cron_spool_t mplayer_input_xevent_t skype_input_xevent_t xxe_home_t mozilla_tmp_t gconfd_t lpr_t mutt_t pan_t ssh_t staff_t user_t xauth_t skype_xproperty_t mozilla_plugin_config_t links_xproperty_t mplayer_xproperty_t xdg_runtime_home_t cert_home_t mplayer_tmpfs_t user_fonts_t user_tmpfs_t mutt_conf_t gpg_secret_t gpg_helper_t staff_ssh_agent_t pulseaudio_tmp_t xscreensaver_t googletalk_plugin_xdg_config_t staff_screen_t user_fonts_config_t ssh_home_t staff_su_t screen_tmp_t mozilla_plugin_t user_input_xevent_t xserver_tmp_t wireshark_xproperty_t user_mail_t pulseaudio_home_t xdg_cache_home_t user_ssh_agent_t xdg_downloads_home_t chromium_renderer_input_xevent_t cronjob_t crontab_t pan_home_t session_dbusd_home_t gpg_agent_t xauth_tmp_t xscreensaver_tmpfs_t iceauth_t mplayer_t chromium_xdg_cache_t lpr_tmp_t gpg_pinentry_tmpfs_t pan_xproperty_t ssh_xproperty_t xdm_xproperty_t java_xproperty_t sysadm_sudo_t qemu_xproperty_t root_xproperty_t user_xproperty_t mail_home_t xserver_t java_input_xevent_t user_screen_t wireshark_input_xevent_t } != || t2 { screen_var_run_t gnome_xdg_config_home_t admin_crontab_t links_input_xevent_t gpg_pinentry_tmp_t virt_content_t print_spool_t crontab_tmp_t httpd_user_htaccess_t ssh_keysign_t remote_input_xevent_t gnome_home_t mozilla_tmpfs_t staff_gkeyringd_t consolekit_input_xevent_t user_mail_tmp_t chromium_xdg_config_t mozilla_input_xevent_t chromium_tmp_t httpd_user_script_exec_t gnome_keyring_tmp_t links_tmpfs_t skype_tmp_t user_gkeyringd_t svirt_home_t sysadm_su_t virt_home_t skype_home_t wireshark_tmp_t xscreensaver_xproperty_t consolekit_xproperty_t user_home_dir_t gpg_pinentry_xproperty_t mplayer_home_t mozilla_plugin_input_xevent_t mozilla_plugin_tmp_t mozilla_xproperty_t xdm_input_xevent_t chromium_input_xevent_t java_tmpfs_t googletalk_plugin_xproperty_t sysadm_t gorg_t gpg_t java_t links_t staff_dbusd_t httpd_user_ra_content_t httpd_user_rw_content_t googletalk_plugin_tmp_t gpg_agent_tmp_t ssh_agent_tmp_t sysadm_ssh_agent_t user_fonts_cache_t user_tmp_t googletalk_plugin_input_xevent_t user_dbusd_t xserver_tmpfs_t iceauth_home_t qemu_input_xevent_t xauth_home_t mutt_home_t sysadm_dbusd_t remote_xproperty_t gnome_xdg_config_t screen_home_t chromium_xproperty_t chromium_tmpfs_t wireshark_tmpfs_t xdg_videos_home_t pulseaudio_input_xevent_t krb5_home_t pulseaudio_xproperty_t xscreensaver_input_xevent_t gpg_pinentry_input_xevent_t httpd_user_script_t gnome_xdg_cache_home_t mozilla_plugin_tmpfs_t user_home_t user_sudo_t ssh_input_xevent_t ssh_tmpfs_t xdg_music_home_t gconf_tmp_t flash_home_t java_home_t skype_tmpfs_t xdg_pictures_home_t xdg_data_home_t gnome_keyring_home_t wireshark_home_t chromium_renderer_xproperty_t gpg_pinentry_t mozilla_t session_dbusd_tmp_t staff_sudo_t xdg_config_home_t user_su_t pan_input_xevent_t user_devpts_t mysqld_home_t pan_tmpfs_t root_input_xevent_t links_home_t sysadm_screen_t pulseaudio_tmpfs_t sysadm_gkeyringd_t mail_home_rw_t gconf_home_t mozilla_plugin_xproperty_t mutt_tmp_t httpd_user_content_t mozilla_xdg_cache_t mozilla_home_t alsa_home_t pulseaudio_t mencoder_t admin_crontab_tmp_t xdg_documents_home_t user_tty_device_t java_tmp_t gnome_xdg_data_home_t wireshark_t mozilla_plugin_home_t googletalk_plugin_tmpfs_t user_cron_spool_t mplayer_input_xevent_t skype_input_xevent_t xxe_home_t mozilla_tmp_t gconfd_t lpr_t mutt_t pan_t ssh_t staff_t user_t xauth_t skype_xproperty_t mozilla_plugin_config_t links_xproperty_t mplayer_xproperty_t xdg_runtime_home_t cert_home_t mplayer_tmpfs_t user_fonts_t user_tmpfs_t mutt_conf_t gpg_secret_t gpg_helper_t staff_ssh_agent_t pulseaudio_tmp_t xscreensaver_t googletalk_plugin_xdg_config_t staff_screen_t user_fonts_config_t ssh_home_t staff_su_t screen_tmp_t mozilla_plugin_t user_input_xevent_t xserver_tmp_t wireshark_xproperty_t user_mail_t pulseaudio_home_t xdg_cache_home_t user_ssh_agent_t xdg_downloads_home_t chromium_renderer_input_xevent_t cronjob_t crontab_t pan_home_t session_dbusd_home_t gpg_agent_t xauth_tmp_t xscreensaver_tmpfs_t iceauth_t mplayer_t chromium_xdg_cache_t lpr_tmp_t gpg_pinentry_tmpfs_t pan_xproperty_t ssh_xproperty_t xdm_xproperty_t java_xproperty_t sysadm_sudo_t qemu_xproperty_t root_xproperty_t user_xproperty_t mail_home_t xserver_t java_input_xevent_t user_screen_t wireshark_input_xevent_t } != || t1 == || ); There RPN notation however isn't the only reason why constraints are difficult to read. The other reason is that seinfo does not know (anymore) about the attributes used to generate the constraints. As a result, we get a huge list of all possible types that match a common attribute - but we don't know which anymore. Not everyone can read the source files in which the constraints are defined, so I hacked together a script that generates GraphViz dot file based on the seinfo --constrain output for a given class and permission and, optionally, limiting the huge list of types to a set that the user (err, that is me ;-) is interested in. For instance, to generate a graph of the constraints related to file reads, limited to the user_t and staff_t types if huge lists would otherwise be shown: ~$ seshowconstraint file read \"user_t staff_t\" > constraint-file.dot ~$ dot -Tsvg -O constraint-file.dot This generates the following graph: !SELinux constraint flow If you're interested in the (ugly) script that does this, you can find it on my github location. There are some patches laying around to support naming constraints and taking the name up in the policy, so that denials based on constraints can at least give feedback to the user which constraint is holding an access back (rather than just a denial that the user doesn't know why). Hopefully such patches can be made available in the kernel and user space utilities soon.","tags":"SELinux","url":"https://blog.siphos.be/2014/05/visualizing-constraints/","loc":"https://blog.siphos.be/2014/05/visualizing-constraints/"},{"title":"Revamped our SELinux documentation","text":"In the move to the Gentoo wiki , I have updated and revamped most of our SELinux documentation. The end result can be seen through the main SELinux page . Most of the content is below this page (as subpages). We start with a new introduction to SELinux article which goes over a large set of SELinux' features and concepts. Next, we cover the various concepts within SELinux. This is mostly SELinux features but explained more in-depth. Then we go on to the user guides. We start of course with the installation of SELinux on Gentoo and then cover the remainder of administrative topics within SELinux (user management, handling AVC denials, label management, booleans, etc.) The above is most likely sufficient for the majority of SELinux users. A few more expert-specific documents are provided as well (some of them still work in progress, but I didn't want to wait to get some feedback) and there is also a section specific for (Gentoo) developers. Give it a review and tell me what you think.","tags":"Gentoo","url":"https://blog.siphos.be/2014/05/revamped-our-selinux-documentation/","loc":"https://blog.siphos.be/2014/05/revamped-our-selinux-documentation/"},{"title":"Dropping sesandbox support","text":"A vulnerability in seunshare , part of policycoreutils , came to light recently (through bug 509896 ). The issue is within libcap-ng actually, but the specific situation in which the vulnerability can be exploited is only available in seunshare . Now, seunshare is not built by default on Gentoo. You need to define USE=\"sesandbox\" , which I implemented as an optional build because I see no need for the seunshare command and the SELinux sandbox (sesandbox) support. Upstream (Fedora/RedHat) calls it sandbox , which Gentoo translates to sesandbox as it collides with the Gentoo sandbox support otherwise. But I digress. The build of the SELinux sandbox support is optional, mostly because we do not have a direct reason to support it. There are no Gentoo users that I'm aware of that use it. It is used to start an application in a chroot-like environment, based on Linux namespaces and a specific SELinux policy called sandbox_t . The idea isn't that bad, but I rather focus on proper application confinement and full system enforcement support (rather than specific services). The SELinux sandbox makes a bit more sense when the system supports unconfined domains (and users are in the unconfined_t domain), but Gentoo focuses on strict policy support. Anyway, this isn't the first vulnerability in seunshare . In 2011, another privilege escalation vulnerability was found in the application (see bug 374897 ). But having a vulnerability in the application (or its interaction with libcap-ng ) doesn't mean an exploitable vulnerability. Most users will not even have seunshare , and those that do have it will not be able to call it if you are running with SELinux in strict or have USE=\"-unconfined\" set for the other policies. If USE=\"unconfined\" is set and you run mcs , targeted or mls (which isn't default either, the default is strict ) then if your users are still mapped to the regular user domains ( user_t , staff_t or even sysadm_t ) then seunshare doesn't work as the SELinux policy prevents its behavior before the vulnerability is triggered. Assuming you do have a targeted policy with users mapped to unconfined_t and you have built policycoreutils with USE=\"sesandbox\" or you run in SELinux in permissive mode, then please tell me if you can trigger the exploit. On my systems, seunshare fails with the message that it can't drop its privileges and thus exits (instead of executing the exploit code as it suggested by the reports). Since I mentioned that most user don't use SELinux sandbox, and because I can't even get it to work (regardless of the vulnerability), I decided to drop support for it from the builds. That also allows me to more quickly introduce the new userspace utilities as I don't need to refactor the code to switch from sandbox to sesandbox anymore. So, policycoreutils-2.2.5-r4 and policycoreutils-2.3_rc1-r1 are now available which do not build seunshare anymore. And now I can focus on providing the full 2.3 userspace that has been announced today.","tags":"Gentoo","url":"https://blog.siphos.be/2014/05/dropping-sesandbox-support/","loc":"https://blog.siphos.be/2014/05/dropping-sesandbox-support/"},{"title":"Stepping through the build process with ebuild","text":"Today I had to verify a patch that I pushed upstream but which was slightly modified. As I don't use the tool myself (it was a user-reported issue) I decided to quickly drum up a live ebuild for the application and install it (as the patch was in the upstream repository but not in a release yet). The patch is for fcron 's SELinux support, so the file I created is fcron-9999.ebuild . Sadly, the build failed at the documentation generation (something about \"No targets to create en/HTML/index.html\"). That's unfortunate, because that means I'm not going to ask to push the live ebuild to the Portage tree itself (yet). But as my primary focus is to validate the patch (and not create a live ebuild) I want to ignore this error and go on. I don't need the fcron documentation right now, so how about I just continue? To do so, I start using the ebuild command. As the failure occurred in the build phase ( compile ) and at the end (documentation was the last step), I tell Portage that it should assume the build has completed: ~# touch /var/portage/portage/sys-process/fcron-9999/.compiled Then I tell Portage to install the (built) files into the images/ directory: ~# ebuild /home/swift/dev/gentoo.overlay/sys-process/fcron/fcron-9999.ebuild install The installation phase fails again (with the same error as during the build, which is logical as the Makefile can't install files that haven't been properly build yet.) As documentation is the last step, I tell Portage to assume the installation phase has completed as well, continuing with the merging of the files to the life file system: ~# touch /var/portage/portage/sys-process/fcron-9999/.installed ~# ebuild /home/swift/dev/gentoo.overlay/sys-process/fcron/fcron-9999.ebuild qmerge Et voila, fcron-9999 is now installed on the system, ready to validate the patch I had to check.","tags":"Gentoo","url":"https://blog.siphos.be/2014/04/stepping-through-the-build-process-with-ebuild/","loc":"https://blog.siphos.be/2014/04/stepping-through-the-build-process-with-ebuild/"},{"title":"If things are weird, check for policy.29","text":"Today we analyzed a weird issue one of our SELinux users had with their system. He had a denial when calling audit2allow , informing us that sysadm_t had no rights to read the SELinux policy. This is a known issue that has been resolved in our current SELinux policy repository but which needs to be pushed to the tree (which is my job, sorry about that). The problem however is when he added the policy - it didn't work. Even worse, sesearch told us that the policy has been modified correctly - but it still doesn't work. Check your policy with sestatus and seinfo and they're all saying things are working well. And yet ... things don't. Apparently, all policy changes are ignored. The reason? There was a policy.29 file in /etc/selinux/mcs/policy which was always loaded, even though the user already edited /etc/selinux/semanage.conf to have policy-version set to 28. It is already a problem that we need to tell users to edit semanage.conf to a fixed version (because binary version 29 is not supported by most Linux kernels as it has been very recently introduced) but having load_policy (which is called by semodule when a policy needs to be loaded) loading a stale policy.29 file is just... disappointing. Anyway - if you see weird behavior, check both the semanage.conf file (and set policy-version = 28 ) as well as the contents of your /etc/selinux/*/policy directory. If you see any policy.* that isn't version 28, delete them.","tags":"SELinux","url":"https://blog.siphos.be/2014/04/if-things-are-weird-check-for-policy-29/","loc":"https://blog.siphos.be/2014/04/if-things-are-weird-check-for-policy-29/"},{"title":"What is that net-pf-## thingie?","text":"When checking audit logs, you might come across applications that request loading of a net-pf-## module, with ## being an integer. Having requests for net-pf-10 is a more known cause (enable IPv6) but what about net-pf-34 ? The answer can be found in /usr/src/linux/include/linux/socket.h : #define AF_ATMPVC 8 /* ATM PVCs */ #define AF_X25 9 /* Reserved for X.25 project */ #define AF_INET6 10 /* IP version 6 */ #define AF_ROSE 11 /* Amateur Radio X.25 PLP */ #define AF_DECnet 12 /* Reserved for DECnet project */ ... #define AF_BLUETOOTH 31 /* Bluetooth sockets */ #define AF_IUCV 32 /* IUCV sockets */ #define AF_RXRPC 33 /* RxRPC sockets */ #define AF_ISDN 34 /* mISDN sockets */ #define AF_PHONET 35 /* Phonet sockets */ #define AF_IEEE802154 36 /* IEEE802154 sockets */ So next time you get such a weird module load request, check socket.h for more information.","tags":"Free Software","url":"https://blog.siphos.be/2014/04/what-is-that-net-pf-thingie/","loc":"https://blog.siphos.be/2014/04/what-is-that-net-pf-thingie/"},{"title":"Proof of concept for USE enabled policies","text":"tl;dr: Some ( -9999 ) policy ebuilds now have USE support for building in (or leaving out) SELinux policy statements. One of the \"problems\" I have been facing since I took on the maintenance of SELinux policies within Gentoo Hardened is the (seeming) inability to make a \"least privilege\" policy that suits the flexibility that Gentoo offers. As a quick recap: SELinux policies describe the \"acceptable behavior\" of an application (well, domain to be exact), often known as the \"normalized behavior\" in the security world. When an application (which runs within a SELinux domain) wants to perform some action which is not part of the policy, then this action is denied. Some applications can have very broad acceptable behavior. A web server for instance might need to connect to a database, but that is not the case if the web server only serves static information, or dynamic information that doesn't need a database. To support this, SELinux has booleans through which optional policy statements can be enabled or disabled. So far so good. Let's look at a second example: ALSA. When ALSA enabled applications want to access the sound devices, they use IPC resources to \"collaborate\" around the sound subsystem (semaphores and shared memory to be exact). Semaphores inherit the type of the domain that first created the semaphore (so if mplayer creates it, then the semaphore has the mplayer_t context) whereas shared memory usually gets the tmpfs-related type ( mplayer_tmpfs_t ). When a second application wants to access the sound device as well, it needs access to the semaphore and shared memory. Assuming this second application is the browser, then mozilla_t needs access to semaphores by mplayer_t . And the same for chromium_t . Or java_t applications that are ALSA-enabled. And alsa_t . And all other applications that are ALSA enabled. In Gentoo, ALSA support can be made optional through USE=\"alsa\" . If a user decides not to use ALSA, then it doesn't make sense to allow all those domains access to each others' semaphores and shared memory. And although SELinux booleans can help, this would mean that for each application domain, something like the following policy would need to be, optionally, allowed: # For the mplayer_t domain: optional_policy(` tunable_policy(`use_alsa',` mozilla_rw_semaphores(mplayer_t) mozilla_rw_shm(mplayer_t) mozilla_tmpfs_rw_files(mplayer_t) ') ') optional_policy(` tunable_policy(`use_alsa',` chromium_rw_semaphores(mplayer_t) chromium_rw_shm(mplayer_t) chromium_tmpfs_rw_files(mplayer_t) ') ') And this for all domains that are ALSA-enabled. Every time a new application is added that knows ALSA, the same code needs to be added to all policies. And this only uses a single SELinux boolean (whereas Gentoo supports USE=\"alsa\" on per-package level), although we can create separate booleans for each domain if we want to. Not that that will make it more manageable. One way of dealing with this would be to use attributes. Say we have a policy like so: attribute alsadomain; attribute alsatmpfsfile; allow alsadomain alsadomain:sem rw_sem_perms; allow alsadomain alsadomain:shm rw_shm_perms; allow alsadomain alsatmpfsfile:file rw_file_perms; By assigning the attribute to the proper domains whenever ALSA support is needed, we can toggle this more easily: # In alsa.if interface(`alsa_domain',` gen_require(` attribute alsadomain; attribute alsatmpfsfile; ') typeattribute $1 alsadomain; typeattribute $2 alsatmpfsfile; ') # In mplayer.te optional_policy(` tunable_policy(`use_alsa',` alsa_domain(mplayer_t, mplayer_tmpfs_t) ') ') That would solve the problem of needlessly adding more calls in a policy for every ALSA application. And hey, we can probably live with either a global boolean ( use_alsa ) or per-domain one ( mplayer_use_alsa ) and toggle this according to our needs. Sadly, the above is not possible: one cannot define typeattribute assignments inside a tunable_policy code: attributes are part of the non-conditional part of a SELinux policy. The solution would be to create build-time conditionals (rather than run-time): ifdef(`use_alsa',` optional_policy(` alsa_domain(mplayer_t, mplayer_tmpfs_t) ') ') This does mean that use_alsa has to be known when the policy is built. For Gentoo, that's not that bad, as policies are part of separate packages, like sec-policy/selinux-mplayer . So what I now added was USE-enabled build-time decisions that trigger this code. The selinux-mplayer package has IUSE=\"alsa\" which will enable, if set, the use_alsa build-time conditional. As a result, we now support a better, fine-grained privilege setting inside the SELinux policy which is triggered through the proper USE flags. Is this a perfect solution? No, but it is manageable and known to Gentoo users. It isn't perfect, because it listens to the USE flag setting for the selinux-mplayer package (and of course globally set USE flags) but doesn't \"detect\" that the firefox application (for which the policy is meant) is or isn't built with USE=\"alsa\" . So users/administrators will need to keep this in mind when using package-local USE flag definitions. Also, this will make it a bit more troublesome for myself to manage the SELinux policy for Gentoo (as upstream will not use this setup, and as such patches from upstream might need a few manual corrections before they apply to our tree). However, I gladly take that up if it means my system will have somewhat better confinement.","tags":"Gentoo","url":"https://blog.siphos.be/2014/03/proof-of-concept-for-use-enabled-policies/","loc":"https://blog.siphos.be/2014/03/proof-of-concept-for-use-enabled-policies/"},{"title":"Decoding the hex-coded path information in AVC denials","text":"When investigating AVC denials, some denials show a path that isn't human readable, like so: type=AVC msg=audit(1396189189.734:1913): avc: denied { execute } for pid=17955 comm=\"emerge\" path=2F7661722F666669737A69596157202864656C6574656429 dev=\"dm-3\" ino=1838 scontext=staff_u:sysadm_r:portage_t tcontext=staff_u:object_r:var_t tclass=file To know what this file is (or actually was, because such encoded paths mean that the file ~~wasn't accessible anymore at the time that the denial was shown~~ contains a space), you need to hex-decode the value. For instance, with python: ~$ python -c \"import base64; print(base64.b16decode(\\\"2F7661722F666669737A69596157202864656C6574656429\\\"));\"; b'/var/ffisziYaW (deleted)' In the above example, /var/ffisziYaW was the path of the file (note that, as it starts with ffi, it is caused by libffi which I've blogged about before). The reason that the file was deleted at the time the denial was generated is because what libffi does is create a file, get the file descriptor and unlink the file (so it is deleted and only the (open) file handle allows for accessing it) before it wants to execute it. As a result, the execution (which is denied) triggers a denial for the file whose path is no longer valid (as it is now appended with \" (deleted) \"). Edit 1: Thanks to IooNag who pointed me to the truth that it is due to a space in the file name, not because it was deleted. Having the file deleted makes the patch be appended with \" (deleted) \" which contains a space.","tags":"SELinux","url":"https://blog.siphos.be/2014/03/decoding-the-hex-coded-path-information-in-avc-denials/","loc":"https://blog.siphos.be/2014/03/decoding-the-hex-coded-path-information-in-avc-denials/"},{"title":"Managing Inter-Process Communication (IPC)","text":"As a Linux administrator, you'll eventually need to concern you about Inter-Process Communication (IPC) . The IPC primitives that most POSIX operating systems provide are semaphores, shared memory and message queues. On Linux, the first utility that helps you with those primitives is ipcs . Let's start with semaphores first. Semaphores in general are integer variables that have a positive value, and are accessible by multiple processes (users/tasks/whatever). The idea behind a semaphore is that it is used to streamline access to a shared resource. For instance, a device' control channel might be used by multiple applications, but only one application at a time is allowed to put something on the channel. Through semaphores, applications check the semaphore value. If it is zero, they wait. If it is higher, they attempt decrement the semaphore. If it fails (because another application in the mean time has decremented the semaphore) then the application waits, otherwise it continues as it has successfully decremented the semaphore. In effect, it acts as a sort-of lock towards a common resource. An example you can come across is with ALSA. Some of the ALSA plugins (such as dmix) use IPC semaphores to allow multiple ALSA applications to connect to and use the sound subsystem. When an ALSA-enabled application is using the sound system, you'll see that a semaphore is active: ~$ ipcs -s ------ Semaphore Arrays -------- key semid owner perms nsems 0x0056a4d5 32768 swift 660 1 More information about a particular semaphore can be obtained using ipcs -s -i SEMID where SEMID is the value in the semid column: ~$ ipcs -s -i 32768 Semaphore Array semid=32768 uid=1001 gid=18 cuid=1001 cgid=100 mode=0660, access_perms=0660 nsems = 1 otime = Sun Mar 30 12:33:46 2014 ctime = Sun Mar 30 12:33:38 2014 semnum value ncount zcount pid 0 0 0 0 32061 As with all IPC resources, we have information about the owner of the semaphore ( uid and gid ), the creator of the semaphore ( cuid and cgid ) as well as its access mask, similar to the file access mask on Linux systems ( mode and access_perms ). Specific to the IPC semaphore, you can also notice the nsems = 1 . Unlike the general semaphores, IPC semaphores are actually a wrapper around one or more \"real\" semaphores. The nsems variable shows how many \"real\" semaphores are handled by the IPC semaphore. Another very popular IPC resource is shared memory . This is memory that is accessible by multiple applications, and provides a very versatile approach to sharing information and collaboration between processes. Usually, a semaphore is also used to govern writes and reads to the shared memory, so that one process that wants to update a part of the shared memory takes a semaphore (a sort-of lock), makes the updates, and then increments the semaphore again. You can see the currently defined shared memory using ipcs -m : ~$ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 655370 swift 600 393216 2 dest Again, more information can be obtained through -i SHMID . An interesting value to look at as well is the creator PID (just in case the process still runs, or through the audit logs) and the last PID used to operate on the shared memory (which also might no longer exist, but is still an important value to investigate). ~$ ipcs -m -p ------ Shared Memory Creator/Last-op PIDs -------- shmid owner cpid lpid 655370 swift 6147 6017 ~$ ps -ef | grep -E '(6147|6017)' root 6017 6016 0 09:49 tty1 00:01:30 /usr/bin/X -nolisten tcp :0 -auth /home/swift/.serverauth.6000 swift 6147 1 2 09:50 tty1 00:05:10 firefox In this case, the shared memory is most likely used to handle the UI of firefox towards the X server. A last IPC resource are message queues, through which processes can put messages on a queue and remove messages (by reading them) from the queue. I don't have an example at hand for the moment, but just like semaphores and shared memory, queues can be looked at through ipcs -q with more information being available through ipcs -q -i MSQID . Now what if you need to operate these? For this, you can use ipcrm to remove an IPC resource whereas ipcmk can be used to create one (although the latter is not that often used for administrative purposes, whereas ipcrm can help you troubleshoot and fix issues without having to reboot a system). Of course, removing IPC resources from the system should only be done when there is a bug in the application(s) that use it (for instance, a process decreased a semaphore and then crashed - in that case, remove the semaphore and start one of the application(s) that also operates on the semaphore as they usually recreate it and continue happily). Now before finishing this post, I do need to tell you about the difference between an IPC resource key and its identifier. The key is like a path or URL, and is a value used by the applications to find and obtain existing IPC resources (something like, \"give me the list of semaphores that I can access with key 12345\"). The identifier is a unique ID generated by the Linux kernel at the moment that the IPC resource is created. Unlike the key, which can be used for multiple IPC resources, the identifier is unique. This is why the identifier is used in the ipcs -i command rather than the key. Also, that means that if applications would properly document their IPC usage then we would easily know what an IPC resource is used for.","tags":"Free Software","url":"https://blog.siphos.be/2014/03/managing-inter-process-communication-ipc/","loc":"https://blog.siphos.be/2014/03/managing-inter-process-communication-ipc/"},{"title":"Querying SELinux policy for boolean information","text":"Within an SELinux policy, certain access vectors (permissions) can be conditionally granted based on the value of a SELinux boolean . To find the list of SELinux booleans that are available on your system, you can use the getsebool -a method, or semanage boolean -l . The latter also displays the description of the boolean: ~# semanage boolean -l | grep user_ping user_ping (on , on) Control users use of ping and traceroute You can easily query the SELinux policy to see what this boolean triggers: ~# sesearch -b user_ping -A -C Found 22 semantic av rules: ET allow ping_t staff_t : process sigchld ; [ user_ping ] ET allow ping_t staff_t : fd use ; [ user_ping ] ET allow ping_t staff_t : fifo_file { ioctl read write getattr lock append open } ; [ user_ping ] ET allow ping_t user_t : process sigchld ; [ user_ping ] ET allow ping_t user_t : fd use ; [ user_ping ] ... However, often you want to know if a particular access is allowed and, if it is conditionally allowed, which boolean enables it. In the case of user ping, we want to know if (and when) a user domain ( user_t ) is allowed to transition to the ping domain ( ping_t ): ~# sesearch -s user_t -t ping_t -c process -p transition -ACTS Found 1 semantic av rules: ET allow user_t ping_t : process transition ; [ user_ping ] So there you go - it is allowed if the user_ping SELinux boolean is enabled.","tags":"SELinux","url":"https://blog.siphos.be/2014/03/querying-selinux-policy-for-boolean-information/","loc":"https://blog.siphos.be/2014/03/querying-selinux-policy-for-boolean-information/"},{"title":"Online hardened meeting of March","text":"I'm back from the depths of the unknown, so time to pick up my usual write-up of the online Gentoo Hardened meeting. Toolchain GCC 4.9 is being worked on, and might be released by end of April (based on the amount of open bugs). You can find the changes online . Speaking of GCC, pipacs asked if it is possible in the upcoming 4.8.2 ebuilds to disable the SSP protection for development purposes (such as when you're developing GCC plugins that do similar protection measures like SSP, but you don't want those to collide with each other). Recent discussion on Gentoo development mailinglist had a consensus that the SSP protection measures ( -fstack-protector ) can be enabled by default, but of course if people are developing new GCC plugins which might interfere with SSP, disabling it is needed. One can use -fno-stack-protector for this, or build stuff with -D__KERNEL__ (as for kernel builds the default SSP handling is disabled anyway, allowing for kernel-specific implementations). Other than those, there is no direct method to make SSP generally unavailable. Blueness is also working on musc-libc on Gentoo, which would give a strong incentive for hardened embedded devices. For desktops, well, don't hold your breath just yet. Kernel grSec/PaX It looks like kernel 3.13 will be Ubuntu's LTS kernel choice, which also makes it the kernel version that grSecurity will put the long term support for in. And with Linux 3.14 almost out, the grsec patches for it are ready as well. Of the previous LTS kernels, 3.2 will probably finish seeing grsec support somewhere this year. The C wrapper (called install-xattr ) used to preserve xattr information during Portage builds has not been integrated in Portage yet, but the development should be finished. During the chat session, we also discussed the gold linker and how it might be used by more and more packages (so not only by users that explicitly ask for it). udev version 210 onwards is one example, but some others exist. But other than its existence there's not much to say right here. SELinux The 20140311 release of the reference policy is now in the Portage tree. Also, prometheanfire caught a vulnerability ( CVE-2014-1874 ) in SELinux which has been fixed in the latest kernels. System Integrity I made a few updates to the gentoo hardening guide in XCCDF/OVAL format. Nothing major, and I still need to add in a lot of other best practices (as well as automate the tests through OVAL), but I do intend to update the files (at least the gentoo one and ssh as OpenSSH 6 is now readily available) regularly in the next few weeks. Profiles A few minor changes have been made to hardened/uclibc to support multilib, but other than that nothing has been done (nor needed to be done) to our profiles. That's it for this months hardened meeting write-up. See you next time!","tags":"Gentoo","url":"https://blog.siphos.be/2014/03/online-hardened-meeting-of-march/","loc":"https://blog.siphos.be/2014/03/online-hardened-meeting-of-march/"},{"title":"Fixing the busybox build failure","text":"Since a few months I have a build failure every time I try to generate an initial ram file system (as my current primary workstation uses a separate /usr and LVM for everything except /boot ): * busybox: >> Compiling... * ERROR: Failed to compile the \"all\" target... * * -- Grepping log... -- * * - busybox-1.7.4-signal-hack.patch * busybox: >> Configuring... *COMMAND: make -j2 CC=\"gcc\" LD=\"ld\" AS=\"as\" * HOSTCC scripts/basic/fixdep *make: execvp: /var/tmp/genkernel/18562.2920.28766.17301/busybox-1.20.2/scripts/gen_build_files.sh: Permission denied *make: *** [gen_build_files] Error 127 *make: *** Waiting for unfinished jobs.... */bin/sh: scripts/basic/fixdep: Permission denied *make[1]: *** [scripts/basic/fixdep] Error 1 *make: *** [scripts_basic] Error 2 I know it isn't SELinux that is causing this, as I have no denial messages and even putting SELinux in permissive mode doesn't help. Today I found the time to look at it with more fresh eyes, and noticed that it wants to execute a file ( gen_build_files.sh ) situated in /var/tmp somewhere. That file system however is mounted with noexec (amongst other settings) so executing anything from within that file system is not allowed. The solution? Update /etc/genkernel.conf and have TMPDIR point to a location where executing is allowed. Of course, this being a SELinux system, the new location will need to be labeled as tmp_t as well, but that's just a simple thing to do. ~# semanage fcontext -a -t tmp_t /var/build/genkernel(/.*)? ~# restorecon -R /var/build/genkernel The new location is not world-writable (only for root as only root builds initial ram file systems here) so not having noexec here is ok.","tags":"Gentoo","url":"https://blog.siphos.be/2014/03/fixing-the-busybox-build-failure/","loc":"https://blog.siphos.be/2014/03/fixing-the-busybox-build-failure/"},{"title":"Talk about SELinux on GSE Linux/Security","text":"On today's GSE Linux / GSE Security meeting (in cooperation with IMUG ) I gave a small (30 minutes) presentation about what SELinux is. The slides are online and cover two aspects of SELinux: some of its design principles, and then a set of features provided by SELinux. The talk is directed towards less technical folks - still IT of course, but not immediately involved in daily operations - so no commands and example/output. SELinux came across the board a few times during the entire day. In the talks about Open Source Security and Security Guidelines for z/VM and Linux on System z SELinux came (of course) up as the technology of choice for providing in-operating system mandatory access control (on the zEnterprise' Z/VM level - the hypervisor - this is handled through RACF Mandatory Access Control) and the Security Enablement on Virtual Machines had SELinux in the front line for the sVirt security protection measures (which focuses on the segregation through MLS categories). And during the talk about A customer story about logging and audit , well, you can guess which technology is also one of the many sources of logging. Right. SELinux ;-) Anyway, if your company is interested in such GSE events, make sure to follow the gsebelux.com site for updates. It's a great way for networking as well as sharing experiences.","tags":"Security","url":"https://blog.siphos.be/2014/03/talk-about-selinux-on-gse-linuxsecurity/","loc":"https://blog.siphos.be/2014/03/talk-about-selinux-on-gse-linuxsecurity/"},{"title":"Create your own SELinux Gentoo profile","text":"Or any other profile for that matter ;-) A month or so ago we got the question how to enable SELinux on a Gentoo profile that doesn't have a <some profilename>/selinux equivalent. Because we don't create SELinux profiles for all possible profiles out there, having a way to do this yourself is good to know. Sadly, the most efficient way to deal with this isn't supported by Portage: creating a parent file pointing to /usr/portage/profiles/features/selinux in /etc/portage/profile , as is done for all SELinux enabled profiles. The /etc/portage/profile location (where users can do local changes to the profile settings) does not support a parent file in there. Luckily, enabling SELinux is a matter of merging the files in /usr/portage/profiles/features/selinux into /etc/portage/profile . If you don't have any files in there, you can blindly copy over the files from features/selinux . Edit: aballier on #gentoo-dev mentioned that you can create a /etc/portage/make.profile directory (instead of having it be a symlink managed by eselect profile ) which does support parent files. In that case, just create one with two entries: one path to the profile you want, and one path to the features/selinux location.","tags":"Gentoo","url":"https://blog.siphos.be/2014/03/create-your-own-selinux-gentoo-profile/","loc":"https://blog.siphos.be/2014/03/create-your-own-selinux-gentoo-profile/"},{"title":"Closing week? No, starting week...","text":"I've been away for a while, and this week will (hopefully) be the last week of all the effort that is causing this. And that means I'll get back to blogging, documentation development, SELinux integration, SELinux policy development and more. To be honest, I'm eagerly awaiting this moment of getting back on my feet. That doesn't mean it's been silent the last few months. A few days ago, the Reference Policy made a new release of the policy. For Gentoo, that won't be a major update as we already follow the reference policy (git repository) quite closely with our policy packages. I'll hopefully merge the last few changes and bump the version on the Gentoo policy packages as well soon. The Gentoo wiki has seen quite a few updates. I tried to approve most of the translation-ready changes a few days ago. Those that I didn't do I should get feedback on (through the Talk/Discussion pages) in the next week. And then on to the huge backlog of issues on bugzie...","tags":"Gentoo","url":"https://blog.siphos.be/2014/03/closing-week-no-starting-week/","loc":"https://blog.siphos.be/2014/03/closing-week-no-starting-week/"},{"title":"Switching context depending on user code-wise","text":"I blogged about how SELinux decides what the context should be for a particular Linux user; how it checks the default context(s) and tells the SELinux-aware application on what the new context should be. Let's look into the C code that does so, and how an application should behave depending on the enforcing/permissive mode... I use the following, extremely simple C that fork()'s and executes id -Z : #include <stdio.h> #include <fcntl.h> #include <unistd.h> #include <sys/types.h> #include <sys/wait.h> #include <stdarg.h> #define DEBUG 7 #define INFO 6 #define NOTICE 5 #define WARN 4 #define ERR 3 #define CRIT 2 #define ALERT 1 #define EMERG 0 #ifndef LOGLEVEL #define LOGLEVEL 4 #endif /* out - Simple output */ void out(int level, char * msg, ...) { if (level <= LOGLEVEL) { va_list ap; printf(\"%d - \", level); va_start(ap, msg); vprintf(msg, ap); va_end(ap); }; }; int main(int argc, char * argv[]) { int rc = 0; pid_t child; child = fork(); if (child < 0) { out(WARN, \"fork() failed\\n\", NULL); }; if (child == 0) { int pidrc; pidrc = execl(\"/usr/bin/id\", \"id\", \"-Z\", NULL); if (pidrc != 0) { out(WARN, \"Command failed with return code %d\\n\", pidrc); }; return(0); } else { int status; out(DEBUG, \"Child is %d\\n\", child); wait(&status); out(DEBUG, \"Child exited with %d\\n\", status); }; return 0; }; The code is ran as follows: ~$ test myusername staff_u:staff_r:staff_t As you can see, it shows the output of the id -Z command. Let's enhance this code with some SELinux specific functions. The purpose of the application now is to ask SELinux what the context should be that the command should run in, and switch to that context for the id -Z invocation. We will include the necessary SELinux code with #ifdef SELINUX , allowing the application to be build without SELinux code if wanted. First, add in the proper #include directives. #ifdef SELINUX #include <selinux/selinux.h> #include <selinux/flask.h> #include <selinux/av_permissions.h> #include <selinux/get_context_list.h> #endif Next, we create a function called selinux_prepare_fork() which takes one input variable: the Linux user name for which we are going to transition (and thus run id -Z for). This function can always be called, even if SELinux is not built in. If that happens, we return 0 immediately. /* selinux_prepare_fork - Initialize context switching * * Returns * - 0 if everything is OK, * - +1 if the code should continue, even if SELinux wouldn't allow * (for instance due to permissive mode) * - -1 if the code should not continue */ int selinux_prepare_fork(char * name) { #ifndef SELINUX return 0; #else // ... this is where the remainder goes #endif }; We include this call in the application above, and take into account the return codes passed on. As can be seen from the comment, if the returncode is 0 (zero) then everything can go on as expected. A positive return code means that there are some issues, but the application should continue with its logic as SELinux is either in permissive, or the domain in which the application runs is permissive - in either case, the code will succeed. A returncode of -1 means that the code will most likely fail and thus the application should log an error and exit or break. pid_t child; rc = selinux_prepare_fork(argv[1]); if (rc < 0) { out(WARN, \"The necessary context change will fail.\\n\"); // Continuing here would mean that the newly started process // runs in the wrong context (current context) which might // be either too privileged, or not privileged enough. return -1; } else if (rc > 0) { out(WARN, \"The necessary context change will fail, but permissive mode is active.\\n\"); }; child = fork(); Now all we need to do is fill in the logic in selinux_prepare_fork . Let's start with the variable declarations (boring stuff): #ifndef SELINUX return 0; #else security_context_t newcon = 0; security_context_t curcon = 0; struct av_decision avd; int rc; int permissive = 0; int dom_permissive = 0; char * sename = 0; char * selevel = 0; With that out of the way, let's take our first step: we want to see if SELinux is enabled or not. Applications that are SELinux-aware should always check if SELinux itself is enabled and, if not, just continue with the (application) logic. /* * See if SELinux is enabled. * If not, then we can immediately tell the code * that everything is OK. */ rc = is_selinux_enabled(); if (rc == 0) { out(DEBUG, \"SELinux is not enabled.\\n\"); return 0; } else if (rc == -1) { out(WARN, \"Could not check SELinux state (is_selinux_enabled() failed)\\n\"); return 1; }; out(DEBUG, \"SELinux is enabled.\\n\"); As you can see, we use is_selinux_enabled here to do just that. If it returns 0, then it is not enabled. A returncode of 1 means it is enabled, and -1 means something wicked happened. I recommend that applications who are SELinux-aware enable info on these matters in debugging output. Nothing is more annoying than having to debug permission issues that might be SELinux-related, but are not enforced through SELinux (and as such do not show up in any logs). Next, see if SELinux is in permissive mode and register this (as we need this later for deciding to continue or not). /* * See if SELinux is in enforcing mode * or permissive mode */ rc = security_getenforce(); if (rc == 0) { permissive = 1; } else if (rc == 1) { permissive = 0; } else { out(WARN, \"Could not check SELinux mode (security_getenforce() failed)\\n\"); } out(DEBUG, \"SELinux mode is %s\\n\", permissive ? \"permissive\" : \"enforcing\"); The security_getenforce method will check the current SELinux mode (enforcing or permissive). If SELinux is in permissive mode, then the application logic should always go through - even if that means contexts will go wrong and such. The end user marked the system in permissive mode, meaning he does not want to have SELinux (or SELinux-aware applications) to block things purely due to SELinux decisions, but log when things are going wrong (for instance for policy development). Now, let's look up what the current context is (the context that the process is running in). This will be used later for logging by the SELinux-aware application in debugging mode. Often, applications that fail run too short to find out if their context is correct or not, and having it logged definitely helps. This step is not mandatory per se (as you will see from the code later). /* * Get the current SELinux context of the process. * Always interesting to log this for end users * trying to debug a possible issue. */ rc = getcon(&curcon); if (rc) { out(WARN, \"Could not get current SELinux context (getcon() failed)\\n\"); if (permissive) return +1; else return -1; }; out(DEBUG, \"Currently in SELinux context \\\"%s\\\"\\n\", (char *) curcon); The getcon() method places the current context in the curcon variable. Note that from this point onwards, we should always freecon() the context before exiting the selinux_prepare_fork() method. A second important note is that, if we have a failure, we now check the permissive state and return a positive error (SELinux is in permissive mode, so log but continue) or negative error (SELinux is in enforcing mode). The negative error is needed so that the code itself does not go run the fork() as it will fail anyway (or, it might succeed, but run in the parent context which is not what the application should do). Next, we try to find out what the SELinux user is for the given Linux account name. /* * Get the SELinux user given the Linux user * name passed on to this function. */ rc = getseuserbyname(name, &sename, &selevel); if (rc) { out(WARN, \"Could not find SELinux user for Linux user \\\"%s\\\" (getseuserbyname() failed)\\n\", name); freecon(curcon); if (permissive) return +1; else return -1; }; out(DEBUG, \"SELinux user for Linux user \\\"%s\\\" is \\\"%s\\\"\\n\", name, sename); The getseuserbyname() method returns the SELinux name for the given Linux user. It also returns the MLS level (but we're not going to use that in the remainder of the code). Again, if it fails, we check the permissive state to see how to bail out. Now get the context to which we should transition when calling id -Z : /* * Find out what the context is that this process should transition * to. */ rc = get_default_context(sename, NULL, &newcon); if (rc) { out(WARN, \"Could not deduce default context for SELinux user \\\"%s\\\" given our current context (\\\"%s\\\")\\n\", sename, (char *) curcon); freecon(curcon); if (permissive) return +1; else return -1; }; out(DEBUG, \"SELinux context to transition to is \\\"%s\\\"\\n\", (char *) newcon); The get_default_context() will do what I blogged about earlier. It'll check what the contexts are in the user-specific context files or the default_contexts file, given the current context. You might notice I don't pass on this context - the NULL second argument means \"use the current context\". This is why the getcon() method earlier is not strictly needed. But again, for logging (and thus debugging) this is very much recommended. From this point onward, we also need to freecon() the newcon variable before exiting the function. Now let's see if we are allowed to transition. We will query the SELinux policy and see if a transition from the current context to the new context is allowed (class process , privilege transition ). I know, to truly see if a transition is allowed, more steps should be checked, but let's stick with this one permission. /* * Now let's look if we are allowed to transition to the new context. * We currently only check the transition access for the process class. However, * transitioning is a bit more complex (execute rights on target context, * entrypoint of that context for the new domain, no constraints like target * domain not being a valid one, MLS constraints, etc.). */ rc = security_compute_av_flags(curcon, newcon, SECCLASS_PROCESS, PROCESS__TRANSITION, &avd); if (rc) { out(WARN, \"Could not deduce rights for transitioning \\\"%s\\\" -> \\\"%s\\\" (security_compute_av_flags() failed)\\n\", (char *) curcon, (char *) newcon); freecon(curcon); freecon(newcon); if (permissive) return +1; else return -1; }; In the above code, I didn't yet check the result. This is done in two steps. In the first step, I want to know if the current context is a permissive domain. Since a few years, SELinux supports permissive domains, so that a single domain is permissive even though the rest of the system is in enforcing mode. Currently, we only know if the system is in permissive mode or not. /* Validate the response * * We are interested in two things: * - Is the transition allowed, but also * - Is the permissive flag set * * If the permissive flag is set, then we * know the current domain is permissive * (even if the rest of the system is in * enforcing mode). */ if (avd.flags & SELINUX_AVD_FLAGS_PERMISSIVE) { out(DEBUG, \"The SELINUX_AVD_FLAGS_PERMISSIVE flag is set, so domain is permissive.\\n\"); dom_permissive = 1; }; We check the flags provided to us by the SELinux subsystem and check if SELINUX_AVD_FLAGS_PERMISSIVE is set. If it is, then the current domain is permissive, and we register this (in the dom_permissive variable). From this point onwards, permissive=1 or dom_permissive=1 is enough to tell the real application logic to continue (even if things would fail SELinux-wise) - the actions are executed by a permissive domain (or system) and thus should continue. if (!(avd.allowed & PROCESS__TRANSITION)) { // The transition is denied if (permissive) { out(DEBUG, \"Transition is not allowed by SELinux, but permissive mode is enabled. Continuing.\\n\"); }; if (dom_permissive) { out(DEBUG, \"Transition is not allowed by SELinux, but domain is in permissive mode. Continuing.\\n\"); }; if ((permissive == 0) && (dom_permissive == 0)) { out(WARN, \"The domain transition is not allowed and we are not in permissive mode.\\n\"); freecon(curcon); freecon(newcon); return -1; }; }; In the second step, we checked if the requested operation (transition) is allowed or not. If denied, we log it, but do not break out of the function if either permissive (SELinux permissive mode) or dom_permissive (domain is permissive) is set. Finally, we set the (new) context, telling the SELinux subsystem that the next exec() done by the application should also switch the domain of the process to the new context (i.e. a domain transition): /* * Set the context for the fork (process execution). */ rc = setexeccon(newcon); if (rc) { out(WARN, \"Could not set execution context (setexeccon() failed)\\n\"); freecon(curcon); freecon(newcon); if ((permissive) || (dom_permissive)) return +1; else return -1; }; freecon(newcon); freecon(curcon); return 0; #endif That's it - we free'd all our variables and can now have the application continue (taking into account the return code of this function). As mentioned before, a positive return code (0 or higher) means the logic should continue; a strictly negative return code means that the application should gracefully fail.","tags":"SELinux","url":"https://blog.siphos.be/2014/01/switching-context-depending-on-user-code-wise/","loc":"https://blog.siphos.be/2014/01/switching-context-depending-on-user-code-wise/"},{"title":"Can Gentoo play a role in a RHEL-only environment?","text":"Sounds like a stupid question, as the answer is already in the title. If a company has only RedHat Enterprise Linux as allowed / supported Linux platform (be it for a support model requirement, ISV certification, management tooling support or what not) how could or would Gentoo still play a role in it. But the answer is, surprisingly, that Gentoo can still be made available in the company architecture. One of the possible approaches is a virtual appliance role. Virtual appliances are entire operating systems, provided through VM images (be it VMDK or in an OVF package), which offer a well defined service to its consumers. More and more products are being presented as virtual appliances. Why not - in the past, they would be in sealed hardware appliances (but still running some form of Linux on it) but nowadays the hypervisor and other infrastructure is strong and powerful enough to handle even the most intensive tasks in a virtual guest. Gentoo is extremely powerful as a meta-distribution. You can tweak, update, enhance and tune a Gentoo Linux installation to fulfill whatever requirement you have. And in the end, you can easily create a virtual image from it, and have it run as a virtual appliance in the company. An example could be to offer a web-based password management suite. A Gentoo Linux deployment could be created, fully hardened of course, with a MAC such as SELinux active. On it, a properly secured web server with the password management suite, with underlying database (of course only listening on localhost - don't want to expose the database to the wider network). Through a simple menu, the various administrative services needed to integrate the \"appliance\" in a larger environment can be configured: downloading an SSL certificate request (and uploading the signed one), (encrypted) backup/restore routines, SNMP configuration and more. If properly designed, all configuration data could be easily exported and imported (or provided through a secundary mount) so that updates on the appliance are as simple as booting a new image and uploading/mounting the configuration data. Building such a virtual appliance can be simplified with Gentoo Prefix , multi-tenancy on the web application level through the webapp-config tool while all necessary software is readily available in the Portage tree. All you need is some imagination...","tags":"Gentoo","url":"https://blog.siphos.be/2014/01/can-gentoo-play-a-role-in-a-rhel-only-environment/","loc":"https://blog.siphos.be/2014/01/can-gentoo-play-a-role-in-a-rhel-only-environment/"},{"title":"Upgrading old Gentoo installations","text":"Today I got \"pinged\" on bug #463240 about the difficulty of upgrading a Gentoo Linux deployment after a long time of inactivity on the system. We already have an Upgrading Gentoo article on the Gentoo wiki that describes in great detail how upgrades can be accomplished. But one of the methods I personally suggest most is to do small, incremental upgrades. Say you have a system from early 2009. Not too long ago, but also not that recent anymore. If you would upgrade that system using the regular approach, your system would probably be using a non-existing profile (the /etc/make.profile symlink would point to a non-existing location), and if you switch the profile to an existing one, you might have to deal with problems like the profile requiring certain features (or EAPI version) that the software currently available on your system doesn't support. This problem is mentioned in the upgrade guide through the following: Make sure your Portage is updated before performing any profile changes. However, it does not tell how to update Portage. In my opinion the best way forward is to install an older Portage tree snapshot (somewhat more recent than your own deployment) and upgrade at least portage, perhaps also the packages belonging to the system set. So for a system that has not been updated since January 2009, you might want to try the portage tree snapshot of July 2009, then January 2010, then July 2010, etc. until you have a recent deployment again. All that is left for you to do is to find such a snapshot (as the Portage tree snapshots from the mirrors are cleaned out after a few months). I try to keep a set of Portage tree snapshots available with a 2-month period which should be sufficient for most users to gradually upgrade their systems. Considering I've used this method a few times already I'm going to add them to the upgrading instructions as well.","tags":"Gentoo","url":"https://blog.siphos.be/2013/12/upgrading-old-gentoo-installations/","loc":"https://blog.siphos.be/2013/12/upgrading-old-gentoo-installations/"},{"title":"Giving weights to compliance rules","text":"Now that we wrote up a few OVAL statements and used those instead of SCE driven checks (where possible), let's finish up and go back to the XCCDF document and see how we can put weights in place. The CVE (Common Vulnerability Exposure) standard allows for vulnerabilities to be given weights through a scoring mechanism called CVSS (Common Vulnerability Scoring System) . The method for giving weights to such vulnerabilities is based on several factors, which you can play with through an online CVSS calculator . Giving weights on a vulnerability based on these metrics is not that difficult, but what about compliance misconfigurations? There is a suggested standard for this, CCSS (Common Configuration Scoring System) which is based on the CVSS scoring and CMSS scoring. Especially the base scoring is tailored to the CVSS scoring, so let's look at an example from the Gentoo Security Benchmark (still in draft): (XML content lost during blog conversion) The base scoring of a misconfiguration focuses on the following items: Access Vector (AV) How can the misconfiguration be \"reached\" or exploited - Local (on the system), Adjacent Network or Network Access Complexity (AC) How complex would it be to exploit the misconfiguration - High, Medium or Low Authentication (Au) Does the attacker need to be authenticated in order to exploit the misconfiguration - None, Single (one account) or Multiple (several accounts or multi-factor authenticated) Confidentiality (C) Does a successful exploit have impact on the confidentiality of the system or data (None, Partial or Complete) Integrity (I) Does a successful exploit have impact on the integrity of the system or data (None, Partial or Complete) Availability (A) Does a successful exploit have impact on the availability of the system or data (None, Partial or Complete) In order to exploit that /tmp is not on a separate file system, we can think about dumping lots of information in /tmp , flooding the root file system. This is simple to accomplish locally and requires a single authentication (you need to be authenticated on the system). Once performed, this only impacts availability. The CCSS (and thus CVSS) base vector looks like so: AV:L/AC:L/Au:S/C:N/I:N/A:C and gives a base score of 4.6, which is reflected in the XCCDF in the weight=\"4.6\" attribute. The severity I give in the XCCDF is \"gut feeling\". Basically, I use the following description: high constitutes a grave or critical problem. A rule with this severity MUST be tackled as it detected a misconfiguration that is easily exploitable and could lead to full system compromise. medium reflects a fairly serious problem. A rule with this severity SHOULD be tackled as it detected a misconfiguration that is easily exploitable. low reflects a non-serious problem. A rule with this severity has detected a misconfiguration but its influence on the overall system security is minor (if other compliance rules are followed). info reflects an informational rule. Failure to comply with this rule does not mean failure to comply with the document itself. Of course, you can put your own weights and severities in your XCCDF documents. Important however is to make sure it is properly documented - other people who read the document must be aware of the consequences of the rules if they are not compliant. By introducing weights and severities, administrators of systems that are not compliant (or of a large set of systems) can prioritize which misconfigurations or vulnerabilities they will handle first. And it reduces the amount of discussions as well, because without these, your administrators will start debating what to tackle first, each with their own vision and opinion. Which is great, but not when time is ticking. Having a predefined priority list makes it clear how to react now. That's it for this post series. I hope you enjoyed it and learned from it. Of course, this wont be the last post related to SCAP so stay tuned for more ;-) This post is the final one in a series on SCAP content: Documenting security best practices – XCCDF introduction An XCCDF skeleton for PostgreSQL Documenting a bit more than just descriptions Running a bit with the XCCDF document Remediation through SCAP What is OVAL? Doing a content check with OVAL","tags":"Security","url":"https://blog.siphos.be/2013/12/giving-weights-to-compliance-rules/","loc":"https://blog.siphos.be/2013/12/giving-weights-to-compliance-rules/"},{"title":"Doing a content check with OVAL","text":"Let's create an OVAL check to see if /etc/inittab 's single user definitions only refer to /sbin/sulogin or /sbin/rc single . First, the skeleton: (XML content lost during blog conversion) The first thing we notice is that there are several namespaces defined within OVAL. These namespaces refer to the platforms on which the tests can be executed. OVAL has independent definitions, unix-global definitions or linux-specific definitions. You can find the overview of all supported schemas and definitions online - definitely something to bookmark if you plan on developing your own OVAL checks. So let's create the definition: (XML content lost during blog conversion) There is lots of information to be found in this simple snippet. First of all, notice the class=\"compliance\" part. OVAL definitions can be given a class that informs the OVAL interpreter what kind of test it is. Supported classes are: compliance Does the system adhere to a predefined wanted state inventory Is the given software or hardware available/installed on the system patch Is the selected patch installed on the system vulnerability Is the system vulnerable towards this particular exposure (CVE) miscellaneous Everything that doesn't fit the above Next, we see metadata that tells the OVAL interpreter that the definition applies to Unix family systems, and more specifically a Gentoo Linux system. However, this is not a CPE entry ( cpe:/o:gentoo:linux ). The idea is that the OVAL Interpreter should interpret the information as it wants without focusing on CPE details - I think (I might be mistaken though) because the SCAP standard does not want to introduce loops - a CPE that refers to an OVAL to validate, which in turn refers to the same CPE. Also, a reference is included in the OVAL. Remember that we also had references in the XCCDF document? Well, the same is true for OVAL statements - you can add in references that help administrators get more information about a definition. In this case, it refers to a CCE (Common Configuration Enumeration) entry. You can find all official CCE entries online as well . This particular one, CCE-4241-6, sais: CCE-4241-6 Platform: rhel5 Date: (C)2011-10-07 (M)2013-11-28 The requirement for a password to boot into single-user mode should be configured correctly. Parameter: enabled/disabled Technical Mechanism: via /etc/inittab By requiring sulogin or rc single in inittab , Gentoo Linux will ask for the root password before granting a shell, thereby complying with the requirement to have a password before providing a shell in single-user mode. Finally, the definition refers to a single test, which we will now look into: (XML content lost during blog conversion) This particular test is part of the independent definitions. Checking the content of a file is something all platforms support. Within this independent definition set, a large set of tests are supported, including file hash checking (does the checksum of a file still match), environment variable test (verifying the existence and content of an environment variable), LDAP tests and also text file content tests. In the test, there are two important attributes to closely look into: check and check_existence . The check_existence attribute tells the OVAL interpreter how to deal with the object definition. In our case, the object will refer to the lines in the /etc/inittab file that match a certain pattern. With check_existence=\"at_least_one_exists\" the OVAL interpreter knows it has to have at least one line that matches the pattern before it can continue. If no line matches, then the test fails. Other values for check_existence are \"all_exist\" (every object described must exist), any_exist (doesn't matter if zero, one or more exists), none_exist (no object described must exist) and \"only_one_exists\" (one, and only one match for the described objects must exist). The check attributes tells the OVAL interpreter how to match the object (if there is one) with the state. In our example, check=\"all\" tells the OVAL interpreter that all lines that match the object definition must also match the state definition. Other values for check are \"at least one\", \"none satisfy\" and \"only one\". These should be self-explanatory. Notice that there are no underscores involved here (unlike with the check_existence attribute). See the common schema for more general OVAL attribute information. The test refers to the following object: (XML content lost during blog conversion) The object represents lines in the /etc/inittab file that match the expression &#94;[\\S]+:S:[\\S]+:.* . The OVAL definition uses perl-style regular expressions , so this means that the lines must start with a non-whitespace string, followed by a colon (:), followed by the letter \"S\", followed by a colon, followed by non-whitespace string, followed by colon and then a remainder string. Also, the object evaluates if at least one such line is found. The state, also referred to by the test, looks like so: (XML content lost during blog conversion) Here again we see a regular expression; this time, the expression sais that the line must start with \"su\" and that the fourth field equals /sbin/rc single or /sbin/sulogin . In our example, if there is at least one \"single user\" line that does not match this expression, then the OVAL statement will return a failure and the system is non-compliant. Now you could be wondering if this is the best approach. We can create an object that refers to all single-user lines in /etc/inittab that do not comply with the expression just in the object definition. The expression would be more complex by itself, but wouldn't need a state anymore. True, but the advantage here is that the object itself matches all single user lines, and can be reused later in other tests. Also, if we later evaluate the OVAL statements, we will get an overview of all lines that match the object (and then evaluate these lines against the state) - similar to the script output we got with SCE tests. We can create other OVALs for all other tests. To refer to these OVAL tests in an XCCDF document, take a look at the following example: (XML content lost during blog conversion) Instead of referring to the SCE engine (with system=\"http://open-scap.org/page/SCE\" ) we refer to the OVAL with system=\"http://oval.mitre.org/XMLSchema/oval-definitions-5\" , point the XCCDF interpreter where the OVAL statements are stored in href=\"gentoo-oval.xml\" and what definition we want to test ( oval:org.gentoo.dev.swift:def:22 ). The XCCDF interpreter will then pass this information on to the OVAL interpreter (in case of openscap, this is the same tool, but it doesn't have to be) so it can evaluate the right OVAL statement on the system. In the next post, I'll use the Gentoo Security Benchmark as a guide to explain how to further structure and document things in XCCDF/OVAL. This post is part of a series on SCAP content: Documenting security best practices - XCCDF introduction An XCCDF skeleton for PostgreSQL Documenting a bit more than just descriptions Running a bit with the XCCDF document Remediation through SCAP What is OVAL?","tags":"Security","url":"https://blog.siphos.be/2013/12/doing-a-content-check-with-oval/","loc":"https://blog.siphos.be/2013/12/doing-a-content-check-with-oval/"},{"title":"What is OVAL?","text":"Time to discuss OVAL (Open Vulnerability Assessment Language) . In all the previous posts I focused the checking of rules (does the system comply with the given rule) on scripts, through the Script Check Engine supported by openscap. The advantage of SCE is that most people can quickly provide automated checks to run in script format. But SCE has a few downsides. You cannot guarantee that the scripts will do no harm on the system. A badly written script might manipulate system settings, get a huge amount of resources, leave stale result files on the system, flood file systems and more. If you get scripts from other parties, you'll need to review them thoroughly before running them against all your systems. Especially when you run the compliance validation tool (openscap in our example) as root. SCE support is only available for openscap (and perhaps one or two others) as it is not an international standard. If you use any of the SCAP validated tools then you will not be able to benefit from the SCE scripts. And that would make the XCCDF document back to a purely documenting best practice. Every rule requires separate scripts, even though many of the rules will be very similar and thus reuse a lot of the scripts. OVAL on the other hand provides those advantages. An OVAL file is an XML file that contains the tests to run, in an (I must say) somewhat complex manner. Really, OVAL is not simple, but it does contain advantages that SCE doesn't. It is a standard, part of the SCAP standards. OVAL files are reusable across multiple tools, allowing you to focus once on the rules rather than having to rewrite the rules for every time you change the tool. OVAL can be platform-agnostic. Of course, not all tests are platform-agnostic (validating registry keys is a Windows-only check) but many are. All rules can be mentioned in a single file (or spread across multiple files if that makes management easier), but more importantly rules will also reuse definitions from other rules. If you have three rules that pertain to a file (say /etc/rc.conf ) then the definition of that file is shared across all rules. The OVAL standard is designed to be non-intrusive. All declarations you do in an OVAL file are pure read-only statements. This gives more confidence to have OVAL statements from third parties ran across your organization. Of course, reviewing them never hurts, but you already know that they will not modify any setting. Like SCE, OVAL checks are individual checks that are executed and returned. They too return a success or failure (or error) and can deliver more detailed information as part of their result (like the SCE results output we looked at before) so allow administrators to investigate further why a rule failed (without needing to log on to the system and look for themselves). A basic structure of OVAL is a definition that describes what the rule is for. The definition refers to one or more tests that are evaluated on a system. These tests refer to an object that needs to be checked, and optionally a state to which the object should (or shouldn't) match. Consider the test we made with SCE to see if a platform is a Gentoo Linux system: 1 2 3 4 5 #!/bin/sh # If /etc/gentoo-release exists then the system is a Gentoo Linux system. test -f /etc/gentoo-release && exit ${ XCCDF_RESULT_PASS } ; exit ${ XCCDF_RESULT_FAIL } ; In OVAL, this would be structured as follows (pseudo-OVAL): definition The system is a Gentoo Linux system test The object that represents /etc/gentoo-release must exist object The /etc/gentoo-release file The resulting OVAL file is quite complex for a simple rule. I see the OVAL complexity as part of a normalization (similar to database normalization) process to allow higher reuse. If we later want to check the content of the gentoo-release file, we will reuse the definition (object with id oval:org.gentoo.dev.swift:obj:1 ) rather than making a second object for it, and use that definition to create new tests. The structure of OVAL is the same everywhere. First define the definitions, then the tests, then the objects and then, optionally, the states. A very important aspect is to have the identifiers ( id=\"...\" ) correct. The structure of OVAL identifiers is standardized as well: (XML content lost during blog conversion) namespace Like the namespace used in XCCDF documents, this is the reverse notation of a domainname. In the example above, this is org.gentoo.dev.swift. type The type of the entry in OVAL. This can be def (definition), tst (test), obj (object), ste (state) or var (variable). id The identifier of this particular entry. This identifier has to be a positive integer. By standardizing the identifiers, you can create repositories within your organization, and have other teams reuse your OVAL components when needed. As the identifier remains the same (even when you update the OVAL object itself to be more precise) those tests keep validating correctly. For instance, say that Gentoo Linux would be changed in the future not to provide a gentoo-release file anymore, but gentoo-linux-release file instead (not that it is planning that, it is just hypothetical), then you can update the test (with description \"Gentoo Linux is installed\") to check if either of the two files exist: (XML content lost due to blog conversion) If we save all Gentoo releated OVAL statements in a file called gentoo-oval.xml then we can update the gentoo-cpe.xml file (which we discussed in the past) to the following: (XML content lost during blog conversion) With this change, openscap (or any other XCCDF interpreter) will use the OVAL definition to see if a platform is Gentoo Linux or not, and does not need to execute the gentoo-platform.sh script anymore, which is now fully deprecated and superceded by the OVAL statement. In the next posts, I'll write up one of the other tests we had (which checks the content of a file - one of the most used tests I think) in OVAL, and have the XCCDF document updated to only use OVAL statements. This post is part of a series on SCAP content: Documenting security best practices - XCCDF introduction An XCCDF skeleton for PostgreSQL Documenting a bit more than just descriptions Running a bit with the XCCDF document Remediation through SCAP","tags":"Security","url":"https://blog.siphos.be/2013/12/what-is-oval/","loc":"https://blog.siphos.be/2013/12/what-is-oval/"},{"title":"December hardened meeting","text":"Yesterday evening (UTC, that is) the members of the Gentoo Hardened project filled the #gentoo-hardened IRC channel again - it was time for another online follow-up meeting. Toolchain A few patches on the toolchain need to be created to mark SSP as default, but this is just a minor workload. And on the ASAN (Address Sanitizer) debacle; well... still the same. Doesn't work with PaX. I think there is a standstill on this. Kernel, grsecurity/PaX It is not clear yet what the next LTS (Long Term Stable) kernel will be that the grSecurity team will support. This depends on the Ubuntu LTS support, and as it is not clear which kernel that distribution will use for LTS, the grSecurity team can also not say what kernel it will be. (So please stop asking ;-) grsecurity 3.0 is out, with the following commit message: commit 4f48151d49f2697c3e2e108a50513a8d61fb150d Author: Brad Spengler Date: Sun Nov 24 17:47:14 2013 -0500 Version bumped to 3.0 (we'd been on 2.9.1 for way too long and numerous features have been added since then) Introduce new atomic RBAC reload method, developed as part of sponsorship by EIG This is accompanied by an updated 3.0 gradm which will use the new reload method when -R is passed to gradm. The old method will still be available via gradm -r (which is what a 2.9.1 gradm will continue to use). The new RBAC reload method is atomic in the sense that at no point in the reload process will the system not be covered by a coherent full policy. In contrast to previous reload behavior, it also preserves inherited subjects and special roles. The old RBAC reload method has also been made atomic. Both methods have been updated to perform role_allowed_ip checks only against the IP tagged to the task at the time its role was first applied or changed. This resolves long-standing usability problems with the use of role_allowed_ip and matches the policies created by learning. The new version requires the use of >=gradm-3.0 . Some hardened-sources packages already contain the 3.0 patchset (currently in testing). In a few days, a final hardened-sources with a 2.9.1 patchset will be stabilized; after that, only 3.0 patchset sources will be used. Another open issue (for a while) is the install.py wrapper used to properly pax-mark binaries during package building (instead of post-merge changes). Although it works functionally well, it has serious performance regressions when hundreds of files need to be merged and marked. For each file, the Python interpreter is launched again, making this a very time-consuming effort. Blueness is currently working on a C-based wrapper which should load much faster. SELinux The live repository of the Gentoo Hardened policies is well up to date with the latest evolutions of the reference policy. If you want to use these, use the -9999 ebuilds for the SELinux policy packages. This can be accomplished with the following line in package.accept_keywords : sec-policy/* ** Recently, revision 5 of the SELinux policies has been pushed to the tree (currently in testing). This one also contains a few required changes to the policy for the new userspace utilities which are also now available in the tree. An important update on the new userspace utilities is that they contain many of the patches we needed in Gentoo and of course the necessary updates, patches and improvements all-round. Once the SELinux policies are stabilized, the userspace utilities will be too. After a few successful runs with SELinux on ARM devices, we will most likely be tagging our SELinux packages (policies and userspace utilities) as ~arm . Documentation will need to be updated on this as well, not only to cater for the additional keyword, but also because of one (perhaps a few) changes needed, like fixing the SELinux binary policy (as most ARM systems come with a lower kernel version). SELinux support on ZFS also seems to work well, with the last patches in (thanks to prometheanfire). As a final aspect, the SELinux eclass in Gentoo Linux now also supports fetching the latest policy sources from git through HTTP (instead of git/ssh). Integrity Not much to discuss here; IMA/EVM and kernel signing all work well. In the next few days/weeks, I will be working on a Gentoo Security Benchmark as a sort-of follow-up (improvement) of the current Gentoo Security Handbook , now using SCAP methods. Profiles There has been discussion about supporting a hardened desktop profile in Gentoo again. This does come with the nasty surprise that these profiles don't play well together, so a solution needs to be brought in place. This could be a \"hardened desktop\" profile separate from the gentoo desktop one (and as such manually synchronized), or an improved approach on profile stacking. One idea was to support stacking with a maximum depth, allowing to use changes made by a profile without inheriting the changes that that profile inherited to a certain extend. Another idea is to use a more dependency-based syntax (similar to OpenRC dependency system for init scripts), which not only allows for proper stacking and the necessary flexibility, but also improves readability: before { hardened/linux/amd64 } after { default/linux/amd64 } depends { targets/desktop/gnome } The next few months will be interesting to see how this will evolve ;-) Documentation All our documents are on the wiki , so if you notice gaps or see possibilities for improvement - by all means, do them. All in all a good session. Thanks for the good work guys!","tags":"Gentoo","url":"https://blog.siphos.be/2013/12/december-hardened-meeting/","loc":"https://blog.siphos.be/2013/12/december-hardened-meeting/"},{"title":"Remediation through SCAP","text":"I promised in my previous post to give some information about remediation. Remediation is the process where you fix a system to become compliant again after finding out there is a violation on the system. The easiest form of remediation of course is to just notify the administrator and give him the instructions to fix the problem - and in the majority of cases, this is exactly what you will do, considering that automatically fixing things might create more breakage if you are not careful. But suppose that you know, for a few rules, what the remediation really should be, and you want to automate it. Well, in that case, you can document the remediation (the commands or scripts) in the XCCDF document. As you might have noticed in the previous example, this is handled through a <fix> entity. In the fix, we mention how the fix should be executed ( urn:xccdf:fix:system:commands is to tell the XCCDF interpreter that the remediation are commands executed on the command line (or verbatim within a script). The platform attribute allows us to differentiate based on the platform (or even version of the platform). The other attributes, such as complexity , disruption and reboot is metadata that helps in deciding which auto-remediation you want to execute. With openscap, the remediation can be triggered online during the evaluation, by adding --remediate ~$ oscap xccdf eval --remediate --profile ${PROFILE} --cpe gentoo-cpe.xml --results results-test-xccdf.xml test-xccdf.xml Title There should be no /dev/ROOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-root Result pass Title There should be no /dev/BOOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot Result pass Title rc_sys should be defined in /etc/rc.conf Rule xccdf_org.gentoo.dev.swift_rule_installation-rc_sys Result fail --- Starting remediation --- Title rc_sys should be defined in /etc/rc.conf Rule xccdf_org.gentoo.dev.swift_rule_installation-rc_sys Result fixed And indeed, the file has been changed and now complies with the rules again. We can also generate the remediation scripts offline: ~$ oscap xccdf eval --profile ${PROFILE} --results results-test-xccdf.xml --cpe gentoo-cpe.xml test-xccdf.xml ~$ oscap xccdf generate fix --output remediate.sh results-test-xccdf.xml The resulting remediate.sh script then contains the steps to remediate the failures reported in the results-test-xccdf.xml file. In general however, auto-remediation is not that recommended. The amount of effort you put in creating remediation scripts that are safe to execute is huge. If you do this for a single system, it is much easier to just remediate manually. If you need to do it for a large set of systems, it makes more sense to use a configuration management solution like ansible or puppet . So, now that we have experience with documenting our best practices and running validation, I'll talk about OVAL in the next post. This post is part of a series on SCAP content: Documenting security best practices - XCCDF introduction An XCCDF skeleton for PostgreSQL Documenting a bit more than just descriptions Running a bit with the XCCDF document","tags":"Security","url":"https://blog.siphos.be/2013/12/remediation-through-scap/","loc":"https://blog.siphos.be/2013/12/remediation-through-scap/"},{"title":"GPT or MBR in the Gentoo Handbook","text":"I just committed a set of changes against the Gentoo Handbook (x86 and amd64) with the intent to have better instructions on GPT (GUID Partition Table) layout versus MBR (Master Boot Record) or MSDOS-style layout. The part on \"Preparing the Disks\" saw the most changes. It starts with explaining the differences between the two layouts with advantages and disadvantages. It then helps the user decide what layout is best for him (or her). Second, the example (and let me stress that one out again, because many people have reported bugs on it: it is an example ) partition layout now includes a BIOS boot partition in the beginning, 2 MB in size. This is to support GRUB2 on GPT, but doesn't hurt for GRUB2 with the MSDOS-style layout either. That means that the partition numbers now move up one (the example /boot is now at sda2, the swap at sda3 and root on sda4). The partitioning instructions now also include the proper alignment instructions (using MB alignment), and use parted as the default partitioning method. The changes are submitted to CVS so will show up on the Gentoo site in an hour or so (documentation on the sites is synchronized hourly if I'm not mistaken). Please do give it a good read and report bugs on bugs.gentoo.org . You might also want to ping me on IRC if it is urgent, although no guarantees that I'm behind my computer at any point.","tags":"Gentoo","url":"https://blog.siphos.be/2013/12/gpt-or-mbr-in-the-gentoo-handbook/","loc":"https://blog.siphos.be/2013/12/gpt-or-mbr-in-the-gentoo-handbook/"},{"title":"Running a bit with the XCCDF document","text":"In my previous post I introduced automated checking of rules through SCE (Script Check Engine) . Let's focus a bit more now on running with an XCCDF document: how to automatically check the system, read the results and find more information of those results. To provide a usable example, you can download the following files: test-xccdf.xml , a sample XCCDF document that documents and tests a few settings (save it with the .xml extension, I publish it as txt to make downloading and viewing easier). gentoo-cpe.xml which defines when a system is a Gentoo system (I'll cover this in a minute). gentoo-platform.sh which is the script that tests if a system is a Gentoo system. gentoo-fstab-noroot.sh which tests that /dev/ROOT is not set in /etc/fstab gentoo-fstab-noboot.sh which tests that /dev/BOOT is not set in /etc/fstab gentoo-rc.conf-rc_sys.sh which tests that the rc_sys variable is declared in /etc/rc.conf . Extract it to a directory of your choice, and let's get started. With openscap (available as app-forensics/openscap in Gentoo), we can generate a guide of the XCCDF document as follows: ~$ oscap xccdf generate guide test-xccdf.xml > guide-test-xccdf.html The result is an HTML guide that reflects the content of the XCCDF document. By default, it contains all text and rules, but shows no information about the profiles (if any). We can add in the --profile ... tag to include an overview of the checks that are selected if that profile is selected. That would give a result similar to this one . Using the --format docbook arguments, the output can also be DocBook instead of HTML. The advantage of DocBook is that it can generate a multitude of other formats, including PDF , although I had to do some manual cleanups in the output to have it render a PDF here using FOP (there are other methods to create PDFs too) such as removing the <preface> part. Let's try evaluating the XCCDF document on the system: ~$ oscap xccdf eval test-xccdf.xml Nothing happened. That is because there are no rules that are by default selected (all rules in the document have selected=\"false\" ) and we have not passed on a profile. I don't know if there is a way to automatically make a particular profile default, so let's try it with the xccdf_org.gentoo.dev.swift_profile_default (which I always use as the default profile name for all my XCCDF documents): ~$ export PROFILE=\"xccdf_org.gentoo.dev.swift_profile_default\" ~$ oscap xccdf eval --profile ${PROFILE} test-xccdf.xml Title There should be no /dev/ROOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-root Result notapplicable Title There should be no /dev/BOOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot Result notapplicable Title rc_sys should be defined in /etc/rc.conf Rule xccdf_org.gentoo.dev.swift_rule_installation-rc_sys Result notapplicable At least we have output now, but still no results. In fact, all rules have notapplicable as a result. What gives? The reason is that the XCCDF interpreter ( oscap ) does not know about the Gentoo Linux platform, whereas the XCCDF document explicitly mentions that it is applicable to a Gentoo Linux system. What we need to do is to provide the XCCDF interpreter with a test that helps it evaluate if a system is a Gentoo Linux system or not. In other words, a test that the XCCDF interpreter will run if the cpe:/o:gentoo:linux platform is mentioned. We do this with a CPE dictionary element which is saved as gentoo-cpe.xml . In the dictionary, the coupling between cpe:/o:gentoo:linux and a scripted check called gentoo-platform.sh is made. Let's now give this info to oscap: ~$ oscap xccdf eval --profile ${PROFILE} --cpe gentoo-cpe.xml test-xccdf.xml Title There should be no /dev/ROOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-root Result pass Title There should be no /dev/BOOT in /etc/fstab Rule xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot Result pass Title rc_sys should be defined in /etc/rc.conf Rule xccdf_org.gentoo.dev.swift_rule_installation-rc_sys Result pass OpenSCAP Error: Document is empty [./gentoo-platform.sh:1] [elements.c:207] No definition with ID: (null) in definition model. [oval_probe.c:338] No definition with ID: (null) in result model. [oval_agent.c:184] No definition with ID: (null) in definition model. [oval_probe.c:338] No definition with ID: (null) in result model. [oval_agent.c:184] No definition with ID: (null) in definition model. [oval_probe.c:338] No definition with ID: (null) in result model. [oval_agent.c:184] Great; we now see that openscap ran the tests and gave feedback. It also gave a few errors. These can be ignored now - it is openscap that tries to interpret the shell scripts as OVAL scripts (I'll talk about OVAL in a later post). After changing my system to be non-compliant, I see that openscap detects that as well: Title rc_sys should be defined in /etc/rc.conf Rule xccdf_org.gentoo.dev.swift_rule_installation-rc_sys Result fail Now, by itself the rule might give us enough clues as to what is wrong, but sometimes you might want to get the output of the scripts as well. You can enable this through the --check-engine-results option. This will leave the generated output of the scripts available as XML files. In it, we see the output (through <sceres:stdout> ) of the grep command we did in the script. Finally, by adding in a --report report-test-xccdf.html to the argument list, the results of the XCCDF evaluation is also saved as HTML . The oscap command has many more options, which I will not discuss in more detail now, but are important to know (for instance, you can save the XCCDF results in XML format for future processing). ~$ oscap xccdf eval -h oscap -> xccdf -> eval Perform evaluation driven by XCCDF file and use OVAL as checking engine Usage: oscap [options] xccdf eval [options] INPUT_FILE [oval-definitions-files] INPUT_FILE - XCCDF file or a source data stream file Options: --profile - The name of Profile to be evaluated. --tailoring-file - Use given XCCDF Tailoring file. --tailoring-id - Use given DS component as XCCDF Tailoring file. --cpe - Use given CPE dictionary or language (autodetected) for applicability checks. --oval-results - Save OVAL results as well. --sce-results - Save SCE results as well. (DEPRECATED! use --check-engine-results) --check-engine-results - Save results from check engines loaded from plugins as well. --export-variables - Export OVAL external variables provided by XCCDF. --results - Write XCCDF Results into file. --results-arf - Write ARF (result data stream) into file. --report - Write HTML report into file. --skip-valid - Skip validation. --fetch-remote-resources - Download remote content referenced by XCCDF. --progress - Switch to sparse output suitable for progress reporting. Format is \"$rule_id:$result\\n\". --datastream-id - ID of the datastream in the collection to use. (only applicable for source datastreams) --xccdf-id - ID of component-ref with XCCDF in the datastream that should be evaluated. (only applicable for source datastreams) --benchmark-id - ID of XCCDF Benchmark in some component in the datastream that should be evaluated. (only applicable for source datastreams) (only applicable when datastream-id AND xccdf-id are not specified) --remediate - Automatically execute XCCDF fix elements for failed rules. Use of this option is always at your own risk. In my next post, I'll talk a bit more about remediation. This post is part of a series on SCAP content: Documenting security best practices - XCCDF introduction An XCCDF skeleton for PostgreSQL Documenting a bit more than just descriptions","tags":"Security","url":"https://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/","loc":"https://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"},{"title":"Updated Linux Sea, now with viewport thingie","text":"I just pushed out an update to Linux Sea (an online resource to introduce you to Linux, using Gentoo Linux as an example), including its PDF and ePub versions. The changes are pretty small (see its ChangeLog ). Together with the update, it now also includes a <meta name=\"viewport\"...> so that the document renders better on a mobile device. Not perfect, but at least it's not small text anymore where you need to pinch-zoom and then scroll left/right all over the place. Same with my blog btw.","tags":"Documentation","url":"https://blog.siphos.be/2013/12/updated-linux-sea-now-with-viewport-thingie/","loc":"https://blog.siphos.be/2013/12/updated-linux-sea-now-with-viewport-thingie/"},{"title":"XCCDF - Documenting a bit more than just descriptions","text":"In my previous post I made a skeleton XCCDF document. By now, we can create a well documented \"baseline\" (best practice) for our subject (say PostgreSQL). But for now I only talked about <description> whereas XCCDF allows many other tags as well. You can add metadata information for a particular Group . It is recommended to use the dublin core notation: (XML content lost during blog conversion)` If you use metadata information however, it should not be used instead of XCCDF elements. Another set of elements that can be used are warning elements: (XML content lost during blog conversion) The <rationale> element can be used to explain in more detail why a rule is important. (XML content lost during blog conversion) Some elements we saw before also apply on the specific <Group> elements, such as <status> or <version> . The combination of these elements should allow for a pretty good explanation of the secure setup we want to achieve. But documentation is one thing - how about checking something automatically? Enter the XCCDF Rule element. Rules are particular tests, checks if you wish, that you want to have executed. To start off, let's look at a Rule element that, as automated approach, calls a script. To accomplish this, we use the SCE (Script Check Engine) method. This is not part of the SCAP standard by itself (SCAP uses OVAL for automated checks - I'll discuss OVAL later) but XCCDF allows for other check systems to be used. And SCE is supported by openscap . (XML content lost during blog conversion) First of all, we have the Rule element itself, with the specially crafted id attribute as seen before. There are three attributes used in the example: selected=\"false\" tells the XCCDF interpreter that the Rule should not be automatically selected. In other words, only if a Profile refers to the rule will be rule be triggered (and the check executed). severity=\"low\" is a matter of documenting how severe a non-compliant rule is. weight=\"0.0\" gives a weight on the Rule . In this case, the weight is 0, meaning that the rule might be recommended but by itself does not introduce a security vulnerability or mismatch. Of course, you are free to use whatever value suits you best. We also notice a fixtext element. When the rule failed (the system is not compliant to the rule) then the fixtext should assist administrators in securing the system again. In other words, fixtext are the human-readable instructions on remediating the system. Finally, we have the check element. This element tells the XCCDF interpreter that an automated validation is defined. The type of automated validation is provided by the system attribute, which in this case refers to the SCE system. The check-content-ref element refers to the script to be executed. Let's look at the contents of the script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #!/bin/sh # Get CHOST value echo \"Getting CHOST variable content through portageq.\" ; my_chost = $( portageq envvar CHOST ) ; if [ -z \" ${ my_chost } \" ] ; then echo \"-- The portageq command failed. Falling back to glibc build info.\" ; my_chost = $( cat /var/db/pkg/sys-libs/glibc-*/CHOST | tail -1 ) ; fi echo \"-- Got CHOST= ${ my_chost } \" ; # Get current GCC version echo \"Getting current GCC version through /etc/env.d/gcc/config-*\" ; current_gcc = $( grep CURRENT /etc/env.d/gcc/config-* | sed -e \"s:CURRENT= ${ my_chost } -::g\" | sed -e \"s:\\([0-9\\.-r]*\\){ $ ,-.* $ }:\\1:g\" ) ; echo \"-- Got version= ${ current_gcc } \" ; # Get type echo \"Getting compiler type (profile/spec) through its CURRENT= value.\" ; current_type = $( grep CURRENT /etc/env.d/gcc/config-* | sed -e \"s:CURRENT= ${ my_chost } - ${ current_gcc } ::g\" | sed -e 's:&#94;-::g' ) ; echo \"-- Got type= ${ current_type } \" ; echo \"Checking USE flags of gcc- ${ current_gcc } for hardened USE flag.\" ; grep -q hardened /var/db/pkg/sys-devel/gcc- ${ current_gcc } /USE ; current_hardened_use = $? ; if [ ${ current_hardened_use } -ne 0 ] ; then echo \"!! GCC ${ current_gcc } is not build with USE=hardened!\" ; echo \"!! Please enable a hardened profile.\" ; exit ${ XCCDF_RESULT_FAIL } ; else echo \"-- GCC ${ current_gcc } is build with USE=hardened.\" ; if [ -z \" ${ current_type } \" ] ; then echo \"-- The default type is used which is a hardened type.\" ; exit ${ XCCDF_RESULT_PASS } ; else echo \"!! A non-default type is used: ${ current_type } \" ; echo \"!! This means not all hardened toolchain measures are enabled.\" ; exit ${ XCCDF_RESULT_FAIL } ; fi fi As you can see, the script can give output when needed, but the most important part is that it has to return a particular return value. This return value is provided through environment variables ( XCCDF_RESULT_* ). All we need to do now is to include this Rule in the Profile . (XML content lost during blog conversion) We can now evaluate the XCCDF file on the system if we refer to the right profile. By selecting the profile, the XCCDF interpreter will also automatically check the rules referred to by the profile (and the rules that do not have selected=\"false\" set too). # oscap xccdf eval --profile ... gentoo-xccdf.xml Title Test if the hardened toolchain is used Rule xccdf_org.gentoo.dev.swift_rule_installation-toolchain-hardened Result pass Title Test if sulogin is used for single-user boot (/etc/inittab) Rule xccdf_org.gentoo.dev.swift_rule_inittab-sulogin Result fail Now, if you want to check this on several systems, you would need to distribute not only the XCCDF file, but also all files referred to by the XCCDF document. As this is counterproductive, SCAP supports Data Streams . A data stream is a single file that includes the content of all files. With openscap, data streams can be made as follows: # oscap ds sds-compose postgresql-xccdf.xml postgresql-ds.xml So now we have a document explaining the secure setup of a component, and included automated checks to validate system compliance with the document using scripts. In the next post, I'll go on with OVAL. This post is part of a series on SCAP content: Documenting security best practices - XCCDF introduction An XCCDF skeleton for PostgreSQL","tags":"Security","url":"https://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/","loc":"https://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"},{"title":"An XCCDF skeleton for PostgreSQL","text":"In a previous post I wrote about the documentation structure I have in mind for a PostgreSQL security best practice. Considering what XCCDF can give us, the idea is to have the following structure: Hardening PostgreSQL +- Basic setup +- Instance level configuration | +- Pre-startup configuration | `- PostgreSQL internal configuration +- Database recommendations `- User definitions For the profiles, I had: infrastructure instance user +- administrator +- end user `- functional account Let's bring this into an XCCDF document. The XCCDF (Extensible Configuration Checklist Description Format) format is an XML structure in which we can document whatever we want - but it is primarily used for configuration checklists and best practices. The documenting aspect of a security best practice in XCCDF is done through XHTML basic tags (do not use fancy things - limit yourself to p , pre , em , strong , ... tags), so some knowledge on XHTML (next to XML in general) is quite important while developing XCCDF guides. At least, if you don't use special editors for that. We start with the basics: (XML content lost during blog conversion) Two things I want to focus on here: the xmlns:h and id attributes. The xmlns:h attribute is an XML requirement, telling whatever XML parser is used later that tags that use the h: namespace is for XHTML tags. So later in the document, we'll use <h:p>...</h:p> for XHTML paragraphs. The id attribute is XCCDF specific, and since XCCDF 1.2 also requires to be in this particular syntax: </p> xccdf_<namespace>_benchmark_<name> <p> The `<namespace>` is recommended to be an inverted domain name structure. I also added my nickname so there are no collisions with namespaces provided by other developers in Gentoo. So *SwifT's dev.gentoo.org* becomes *org.gentoo.dev.swift*. This id structure will be used in other tags as well. Instead of *_benchmark it would be *_rule (for Rule ids), *_group (for Group ids), etc. You get the idea. Now we add in some metadata in the document (with Benchmark as parent): (XML content lost during blog conversion) So what is all this? The <status> tag helps in tracking the state of the document. The <platform> tag is to tell the XCCDF interpreter when the document is applicable. It references a CPE (Common Platform Enumeration) entity, in this case for PostgreSQL. Later, we will see that an automated test is assigned to the detection of this CPE. If the test succeeds, then PostgreSQL is installed and the XCCDF interpreter can continue testing and evaluating the system for PostgreSQL best practices. If not, then PostgreSQL is not installed and the XCCDF does not apply to the system. There is a huge advantage to this: you can check all your systems for compliance with the PostgreSQL best practices (this XCCDF document) and on the systems that PostgreSQL is not installed, it will simply state that the document is not applicable (without actually trying to validate all the rules in the document). So there is no direct need to only check systems you know have PostgreSQL on (and thus potentially ignore systems that have PostgreSQL but that you don't know of - usually those systems are much less secure as well ;-). The <version> tag versions the document. The <model> tags tell the XCCDF interpreter which scoring system should be used. Scoring will give points to rules, and the XCCDF interpreter will sum the scores of all rules to give a final score to the \"compliance\" state of the system. But scoring can be done on several levels. The default one uses the hierarchy of the document (nested Group s and Rule s) to give a final number whereas the flat one does not care about the structure. Finally, the flat-unweighted one does not take into account the scores given by the author - all rules get the value of 1. Now we define the Profile s to use. I will give the example for two: user and administrator , you can fill in the other ones ;-) (XML content lost during blog conversion) Finally, the Group s (still with Benchmark as their parent, but below the Profile s) which define the documentation structure of the guide: (XML content lost during blog conversion) With all this defined, our basic skeleton for the PostgreSQL best practice document is ready. To create proper content, we can use the XHTML code inside the <description> tags, like so: (XML content lost during blog conversion) As said in the previous post though, just documenting various aspects is not enough. It is recommended to add references. In XCCDF, this is done through the <reference> tag, which is within a Group and usually below the <description> information: (XML content lost during blog conversion) With this alone, it is already possible to write up an XCCDF guide describing how to securely setup (in our case) PostgreSQL while keeping track of the resources that helped define the secure setup. Tools like openscap can generate HTML or even Docbook (which in turn can be converted to manual pages, PDF, Word, RTF, ...) from this information: # oscap xccdf generate guide --format docbook --output guide.docbook postgresql-xccdf.xml In the next post, I'll talk about the other documenting entities within XCCDF (besides <description> and their meaning) and start with enhancing the document with automated checks.","tags":"Security","url":"https://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/","loc":"https://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"},{"title":"Documenting security best practices - XCCDF introduction","text":"When I have some free time, I try to work on a Gentoo Security Benchmark which not only documents security best practices (loosely based on the Gentoo Security Handbook which hasn't seen much updates in the last few years) but also uses the SCAP protocols. This set of protocols allows security administrators to automate and document many of their tasks, and a security best practices guide is almost a must-have in any organization. So I decided to do a few write-ups about these SCAP protocols and how I hope to be using them more in the future. In this post, I'm going to focus on a very simple matter: documenting. SCAP goes much, much beyond documenting, but I'll discuss those various features in subsequent posts. The end goal of the series is to have a best practice document for PostgreSQL. To document the secure state of a component, it is important to first have an idea about what you are going to document. Some people might want to document best practices across many technologies so that there is a coherent, single document explaining the security best practices for the entire organization. In my opinion, that is not manageable in the long term. We tried that with the Gentoo Security Handbook, but you quickly start wrestling with the order of chapters, style concerns and what not. Also, some technologies will be much more discussed in depth than others, making the book look \"unfinished\". Personally, I rather focus on a specific technology. For instance: Hardening OpenSSH (very much work in progress - the rules are generated automatically for now and will be rewritten in the near future). It talks about a single component (OpenSSH) allowing the freedom for the author to focus on what matters. By providing security best practices on these component levels, you'll create a set of security best practices that can often be reused. This is what the Center for Internet Security is doing with its benchmarks: popular technologies are described in detail on how to configure them to be more secure. Once you know what technology you want to describe, we need to consider how this technology is used. Some technologies are very flexible in their setup, and might have different security setups depending on their use. For instance, an OpenLDAP server can be used internally as a public address book, or disclosed on the Internet in a multi-replicated setup with authentication data in it. The security best practices for these deployments will vary. The XCCDF (Extensible Configuration Checklist Description Format) standard allows authors to write a single guide, while taking into account the different deployment approaches through the use of Profile settings. In XCCDF, Profile s allow for selectively enabling or disabling document fragments (called Group s) and checks (called Rule s - I will post about checks later) or even change values (like the minimum password length) depending on the profile. A document can then describe settings with different profiles depending on the use and deployment of the technology. Profiles can also inherit from each other, so you can have a base (default) security conscious setup, and enhance it through other profiles. Next to the \"how\", we also need to consider the structure we want for such a best practice: We will have rules in place for the deployment of PostgreSQL itself. These rules range from making sure a stable, patched version is used, to the proper rights on the software files, partitioning and file system rules and operating system level requirements (such as particular kernel parameters). We will also have rules for each instance. We could plan on running multiple PostgreSQL instances next to each other, so these rules are distinct from the deployment rules. These rules include settings on instance level, process ownership (in case of running PostgreSQL as different service user), etc. We might even have rules for databases and users (roles) in the database. It might make sense to split the best practices in separate documents, such as one for PostgreSQL infrastructure (which is database-agnostic) and one for PostgreSQL databases (and users). I would start with one document for the technology if I was responsible for the entire definition, but if this responsibility is not with one person (or team), it makes sense to use different documents. Also, as we will see later, XCCDF documents can be \"played\" against a target. If the target is different (for infrastructure, the target usually is the host on which PostgreSQL is installed, whereas for the database settings the target is probably the PostgreSQL instance itself) I would definitely have the definitions through separate profiles, but that does not mean the document needs to be split either. Finally, documenting a secure best practice also involves keeping track of the references. It is not only about documenting something and why you think this is the best approach, but also about referring readers to more information and other resources that collaborate your story. These can be generic control objectives (such as those provided by the open security architecture ) or specific best practices of the vendor itself or third parties. At the end, for a PostgreSQL security guide, we would probably start with: Hardening PostgreSQL +- Basic setup +- Instance level configuration | +- Pre-startup configuration | `- PostgreSQL internal configuration +- Database recommendations `- User definitions Profile-wise, I probably would need an infrastructure profile, an instance profile, a user profile and a database profile. I might even have profiles for the different roles ( functional account , administrator and end user profiles which inherit from the user profile) as they will have different rules assigned to them. In my next post, we'll create a skeleton XCCDF document and already talk about some of the XCCDF features that we can benefit from immediately.","tags":"Security","url":"https://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/","loc":"https://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"},{"title":"Gentoo SELinux policy release script","text":"A few months ago, I wrote a small script that aids in the creation of new SELinux policy packages. The script is on the repository itself, in the gentoo/ subdirectory, and is called release-prepare.sh . The reason for the script is that there are a number of steps to perform, one of which (tagging the release) I forgot to do too often. So today I made a new release of the policy packages (2.20130424-r4) with the script, and decided to blog about it as other developers in the hardened team might one day be asked to make a release when I'm not available. When the script is called, it spits out the usual help information. $ sh release-prepare.sh -h Usage: release-prepare.sh Example: release-prepare.sh 2.20130424-r2 2.20130424-r3 The script will copy the ebuilds of the towards the and update the string occurrences of that version (mostly for the BASEPOL variable). The following environment variables must be declared correctly for the script to function properly: - GENTOOX86 should point to the gentoo-x86 checkout E.g. export GENTOOX86=\"/home/user/dev/gentoo-x86\" - HARDENEDREFPOL should point to the hardened-refpolicy.git checkout E.g. export HARDENEDREFPOL=\"/home/user/dev/hardened-refpolicy\" - REFPOLRELEASE should point to the current latest /release/ of the reference policy (so NOT to a checkout), extracted somewhere on the file system. E.g. export REFPOLRELEASE=\"/home/user/local/refpolicy-20130424\" So first, we need to export three environment variables needed by the script: GENTOOX86 points to the CVS checkout of the Portage tree. It is used to create new ebuilds. HARDENEDREFPOL is the git checkout of the policy repository. This one is used to read the changes from to generate a patch. REFPOLRELEASE is an extracted refpolicy-20130424.tar.gz (the upstream release of the reference policy). This extracted location is needed to generate the patch (the difference between our repository and the upstream release). After setting the variables, the script does its magic: $ sh release-prepare.sh 2.20130424-r3 2.20130424-r4 Creating patch 0001-full-patch-against-stable-release.patch... done Creating patch bundle for 2.20130424-r4... done Copying patch bundle into /usr/portage/distfiles and dev.g.o... done Removing old patchbundle references in Manifest (in case of rebuild)... done Creating new ebuilds based on old version... done Marking ebuilds as ~arch... done Creating tag 2.20130424-r4 in our repository... done The release has now been prepared. Please go do the following to finish up: - In /home/swift/dev/gentoo-x86/sec-policy go \"cvs add\" all the new ebuilds - In /home/swift/dev/gentoo-x86/sec-policy run \"repoman manifest\" and \"repoman full\" Then, before finally committing - do a run yourself, ensuring that the right version is deployed of course: - \"emerge -1 sec-policy/selinux-aide sec-policy/selinux-alsa ... Only then do a 'repoman commit -m 'Release of 2.20130424-r4''. The script performs the following steps: It creates the patch with the difference between the main refpolicy release and our repository. Our repository closely follows the upstream release, but still contains quite a few changes that have not been upstreamed yet (due to history loss of the changes, or the changes are very gentoo-specific, or the changes still need to be improved). In the past, we maintained all the patches separately, but this meant that the deployment of the policy ebuilds took too long (around 100 patches being applied takes a while, and took more than 80% of the total deployment time on a regular server system). By using a single patch file, the deployment time is reduced drastically. It then compresses this patch file and stores it in /usr/portage/distfiles (so that later repoman manifest can take the file into account) as well as on my dev.gentoo.org location (where it is referenced). If other developers create a release, they will need to change this location (and the pointer in the ebuilds). Previous file references in the Manifest files are removed, so that repoman does not think the digest can be skipped. New ebuilds are created, copied from the previous version. In these ebuilds, the KEYWORDS variable is updated to only contain ~arch keywords. A release tag is created in the git repository. Then the script tells the user to add the new files to the repository, run repoman manifest and repoman full to verify the quality of the ebuilds and generate the necessary digests. Then, and also after testing, the created ebuilds can be committed to the repository. The last few steps have explicitly not been automated so the developer has the chance to update the ebuilds (in case more than just the policy contents has changed between releases) or do dry runs without affecting the gentoo-x86 repository.","tags":"Gentoo","url":"https://blog.siphos.be/2013/12/gentoo-selinux-policy-release-script/","loc":"https://blog.siphos.be/2013/12/gentoo-selinux-policy-release-script/"},{"title":"November online hardened meeting","text":"Later than usual, as I wasn't able to make the meeting myself (thus had to wait for the meeting logs in order to draft up this summary), so here it is. The next meeting is scheduled for next week, btw ;-) Toolchain The 4.8.2 ebuild for GCC is available in the tree, currently still masked. Kernel and grSecurity The grSecurity project has made the patches for the 3.12 kernel available; a hardened 3.12 kernel is available in the hardened-development overlay. SELinux Matthew is working on ZFSonLinux and SELinux support. Profiles Matthew and Steev have been working on SELinux and ARM support. It seems to work, but kernel versions matter greatly. We might want to open up ARM keywords. That's about it. As blueness wasn't there as well, the topics were discussed quite fast. The full logs can be found on the gentoo-hardened mailinglist .","tags":"Gentoo","url":"https://blog.siphos.be/2013/12/november-online-hardened-meeting/","loc":"https://blog.siphos.be/2013/12/november-online-hardened-meeting/"},{"title":"Majority of GDP documents moved to Gentoo wiki","text":"The majority of the English gentoo documents that resided in www.gentoo.org/doc/en have now been moved to the Gentoo Wiki . All those documents have been made available in the main namespace, meaning that non-developers can continue to contribute on those articles and guides, fully in the spirit of a wiki and community. We are now working on a method for displaying an overview of documents that have been moved, or have seen enough development on the wiki that they are deemed of proper quality and stable (not too many changes left). Currently, such documents are (requested to be) marked for translation, allowing me or other developers to review if the guides are indeed ok to move forward. If so, they are marked as translatable (allowing the various translation members on the wiki to move forward with translating the guides) and will be made part of the overview as well. One of the few documents that has not been moved yet is the Gentoo Handbook as it is of a different format than the guides, and includes dynamic documentation generation features which I have yet to fully investigate (wiki-wise). I currently focused mainly on the non-handbook guides, so the handbook itself will get more attention later (after the documentation overview is completed). Big thanks to Alex Legler for assisting me and providing the necessary features and functionality in the Gentoo wiki to make this all possible. Edit: added link to overview page.","tags":"Documentation","url":"https://blog.siphos.be/2013/12/majority-of-gdp-documents-moved-to-gentoo-wiki/","loc":"https://blog.siphos.be/2013/12/majority-of-gdp-documents-moved-to-gentoo-wiki/"},{"title":"New SELinux userspace release","text":"Between now and an hour, Gentoo users using the \\~arch branch will notice that new versions of the SELinux userspace applications are now available. Released on October 30th, they contain many bug fixes sent previously as well as a couple of interesting developments and enhancements (more work on sepolicy, for instance). My tests revealed only a single issue that I still need to solve (another issue, bug 490436 , is hopefully properly worked around), which is bug 490442 , where audit2allow does not want to generate refpolicy-style SELinux policy modules. Other than that, most infrastructural tests were successful. Two SELinux policy updates were needed: manage_lnk_files_pattern(semanage_t, semanage_store_t, semanage_store_t) selinux_read_policy(sysadm_t) The first one is sent upstream as I think it is an obvious one (new userspace now using symbolic links). The other one I'm not that sure of, but for now it works. I made the above policy changes locally; if approved I'll commit them to our tree asap. So, go play with it and report whatever you can on bugzilla (SELinux component).","tags":"Gentoo","url":"https://blog.siphos.be/2013/11/new-selinux-userspace-release-2/","loc":"https://blog.siphos.be/2013/11/new-selinux-userspace-release-2/"},{"title":"The mix of libffi with other changes","text":"I once again came across libffi. Not only does the libffi approach fight with SELinux alone, it also triggers the TPE (Trusted Path Execution) protections in grSecurity. And when I tried to reinstall Portage, Portage seemed to create some sort of runtime environment in a temporary directory as well, and SELinux wasn't up to allowing that either. Let's first talk about a quick workaround for the libffi-with-TPE issue. Because libffi wants to create executable files in a world-writable directory and then execute that file (try finding the potential security issue here) TPE is prohibiting the execution. The easiest workaround is to add the portage Linux user, as well as the Linux accounts that you use to run emerge with (even just things like emerge --info ) in the wheel group. This group is exempt from TPE protections (unless you configured a different group in your kernel for this: ~# zgrep CONFIG_GRKERNSEC_TPE_TRUSTED_GID /proc/config.gz CONFIG_GRKERNSEC_TPE_TRUSTED_GID=10 Next we also need to allow the portage_t domain to execute files labeled with portage_tmpfs_t . You can do this by creating your own SELinux policy module with the following content (or use selocal): can_exec(portage_t, portage_tmpfs_t) This works around the libffi issue for now. A better solution still has to be implemented (as discussed in the previous post). With regards to the portage installation failing, you'll notice this quickly when you get an error like so: ~# emerge -1 portage Calculating dependencies ... done! Traceback (most recent call last): File \"/usr/bin/emerge\", line 50, in <module> retval = emerge_main() File \"/usr/lib64/portage/pym/_emerge/main.py\", line 1031, in emerge_main return run_action(emerge_config) File \"/usr/lib64/portage/pym/_emerge/actions.py\", line 4062, in run_action emerge_config.args, spinner) File \"/usr/lib64/portage/pym/_emerge/actions.py\", line 453, in action_build retval = mergetask.merge() File \"/usr/lib64/portage/pym/_emerge/Scheduler.py\", line 946, in merge rval = self._handle_self_update() File \"/usr/lib64/portage/pym/_emerge/Scheduler.py\", line 316, in _handle_self_update _prepare_self_update(self.settings) File \"/usr/lib64/portage/pym/portage/package/ebuild/doebuild.py\", line 2326, in _prepare_self_update shutil.copytree(orig_bin_path, portage._bin_path, symlinks=True) File \"/usr/lib64/portage/pym/portage/__init__.py\", line 259, in __call__ rval = self._func(*wrapped_args, **wrapped_kwargs) File \"/usr/lib64/python3.3/shutil.py\", line 343, in copytree raise Error(errors) shutil.Error: [(b'/usr/lib64/portage/bin/ebuild-helpers/prepalldocs', b'/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepalldocs', \"[Errno 13] Permission denied: '/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepalldocs'\"), (b'/usr/lib64/portage/bin/ebuild-helpers/prepinfo', b'/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepinfo', \"[Errno 13] Permission denied: '/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepinfo'\"), (b'/usr/lib64/portage/bin/ebuild-helpers/newlib.so', b'/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/newlib.so', \"[Errno 13] Permission denied: '/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/newlib.so'\"), [...] And the errors go on and on and on. I've been able to get it working again by allowing the following SELinux permissions: allow portage_t portage_tmp_t:dir relabel_dir_perms; allow portage_t portage_tmp_t:lnk_file relabel_lnk_file_perms; allow portage_t bin_t:dir relabel_dir_perms; allow portage_t bin_t:file relabel_file_perms; allow portage_t bin_t:lnk_file relabel_lnk_file_perms; allow portage_t portage_exec_t:file relabel_file_perms; allow portage_t portage_fetch_exec_t:file relabel_file_perms; allow portage_t lib_t:dir relabel_dir_perms; allow portage_t lib_t:file relabel_file_perms; You can somewhat shorten this by combining types (but this doesn't work with selocal for now): allow portage_t { portage_tmp_t bin_t lib_t}:dir relabel_dir_perms; allow portage_t { portage_tmp_t bin_t }:lnk_file relabel_lnk_file_perms; allow portage_t { bin_t portage_exec_t portage_fetch_exec_t lib_t}:file relabel_file_perms; At the end of the emerge process (when the new portage is installed) you might want to reset the labels of all files provided by the portage package: ~# rlpkg portage These changes have not been passed into the policy yet as I first need to find out why exactly they are needed, and as you can see from the gentoo devaway page, time is not on my side to do this. I'll try to free up some time in the next few days to handle this as well as the SELinux userspace release but no promises here. Edit: I found why - it is the _prepare_self_update in the doebuild.py script. It creates temporary copies of the Portage binaries and Portage python libraries, which is why we need to support relabel operations on the files. Support for this is now in the policy repository.","tags":"Security","url":"https://blog.siphos.be/2013/11/the-mix-of-libffi-with-other-changes/","loc":"https://blog.siphos.be/2013/11/the-mix-of-libffi-with-other-changes/"},{"title":"Gentoo Hardened meeting 201310","text":"We gathered online again to talk about the progress, changes and other stuff related to the Gentoo Hardened project. New Developer We welcomed Zero_Chaos as a new addition to our team. Big welcome, with the usual IRC kick in between, ensued. Toolchain GCC 4.8.x is unmasked and ready for testing. The ASAN problem however is not resolved and it doesn't look like upstream wants to provide the fix for this. As a result, if you want to use ASAN, you will need to disable UDEREF in the kernel. A difficult problem here is that, if a user forgets to disable UDEREF, then building with ASAN will fail. If he disabled UDEREF and uses ASAN, then booting into a UDEREF enabled kernel will fail. If he starts building with ASAN on UDEREF kernels, things will break. Its ugly, and unless the ASAN code is changed to support other technologies using the address space layout information, it will remain mutually exclusive. For now, we'll properly document the situation. GCC 4.9 will end its stage1 development phase on November 21st. The uclibc stages are currently built with GCC-4.7.3-r1, except for the MIPS architectures which use GCC 4.8.1-r1. The stages are built once every two months, give or take. Kernel grSec/PaX Standard maintenance, such as keeping up with upstream changes, has taken place. The XATTR problems with the install binary/phase has not been resolved yet due to time constraints. First focus will be on writing the C-based wrapper to see if this significantly speeds things up. USE=pax_kernel There is ambiguity of the meaning or use of the pax_kernel USE flag. It seems to be used for multiple, exclusive things, like limiting builds on PaX kernels or limiting run-time behavior on PaX kernels. To build Gentoo images, developers need to set pax_kernel on some packages and disable it on others in order for the image to build properly. <@blueness> USE=pax_kernel in the case of userland should apply patches etc, that fix the code so that it won't trip pax protection eg mprotect <@blueness> in the case of kernel modules, it can't mean that obviously! One of the mentioned problems is that some ebuilds only enable pax markings when pax_kernel is set. However, that shouldn't be done conditionally. PaX markings can be done even without a PaX kernel. Zero_Chaos (new developers always have the necessary energy) will try to update ebuilds where applicable. SELinux and System Integrity Nothing worth mentioning here. Profile Many users were using hardened profiles on the desktop. In the past, we had the desktop/hardened profiles removed (referring users to the regular hardened profiles and asking them to update their system with the USE flags (and other settings) they want for their desktop. Apparently, this is giving some problems for some users, so the idea is to reinstate the desktop/hardened profiles where the hardened settings overrule the desktop settings. However, we should take care of tackling the issues we had in the past (which is the reason why we removed the profiles in the first place). It is recommended that we discuss this outside the IRC meeting to make sure we don't reintroduce the issues again while using a flexible approach. That's it. A big thanks to the developers, contributors and thousands of users!","tags":"Gentoo","url":"https://blog.siphos.be/2013/10/gentoo-hardened-meeting-201310/","loc":"https://blog.siphos.be/2013/10/gentoo-hardened-meeting-201310/"},{"title":"In-browser encryption for online password management","text":"Lately I've been trying to find a good free software project that uses PHP or cgi-bin (one of the requirements for this particular organization) that allows its users to store passwords centrally, but uses encryption on the browser level before the passwords are sent to the central server. I've found one - Clipperz - but was not able to get it to build and install. With the continuous revelations regarding hacked sites and servers (and even potential snooping into server data by governments) the requirement isn't that weird: by using strong encryption (I currently still assume that AES-256 is safe for use) on the browser level, no unencrypted sensitive data (such as usernames and passwords) would be sent to the server, let alone stored (in plain text) on the server database. I did a small test to see how difficult it would be to implement this in a simple PHP password management tool called online passwords . The PHP-based application does not even use a database, relying on flat-files instead. By design, the tool encrypts the data before storing on the file system, but I wanted to go a bit further, implementing the in-browser encryption. The Javascript AES is provided by movable-type.co.uk and for the hashing algorithm I found pajhome's implementation often cited. The first thing I did was substitute the password information needed to log on to the site (and which is also used as encryption key for the back-end side encryption) with a hashed version of the password. For the application, this hardly matters - it is still the encryption key it will use on the backend, although most likely a bit stronger than most passwords would be. Next, I keep the real password in a local session storage (which is supported by most modern browsers nowadays) so that the user only has to enter it once (when logging on to the site) and it is kept in memory then, never leaving the browser. This is needed in order to decrypt the data as we get it without having to ask the user for the password over and over again. Of course, I don't want to keep this password in a Cookie (or pass it on through the URL) because that would void the idea of keeping the password (reasonably) secure. To accomplish this, I hide the password field of the PHP application itself, and create a second input field (outside the <form> </form> to make sure its value is never POSTed to the site) in which the user enters his password. Upon submit of the data, the following javascript code will create the hash of the password (and user name) to use as the \"site password\" for the application, and put that in the (hidden) input field. It then also stores the site password in the local session storage in the browser. The code is triggered through the onSubmit handler of the form. function storeAppPassword() { var sitepw = document.getElementById('password'); var siteus = document.getElementById('login'); var userpw = document.getElementById('userpassword'); sessionStorage.setItem('userpassword', userpw.value); sitepw.value = hex_sha1(siteus.value + userpw.value); } Now I need to make sure that the fields that need to be encrypted (the various user ids and passwords that are stored on the site) are encrypted before they are sent to the server, and decrypted after having received them by the browser. For instance, if the fields are within a form, the following javascript function could be triggered on the onSubmit handler again: function encryptFields() { var useridFld = document.getElementById('userid'); var passwordFld = document.getElementById('password'); var notesFld = document.getElementById('notes'); var pw = sessionStorage.getItem('userpassword'); useridFld.value = Aes.Ctr.encrypt(useridFld.value, pw, 256); passwordFld.value = Aes.Ctr.encrypt(passwordFld.value, pw, 256); notesFld.value = Aes.Ctr.encrypt(notesFld.value, pw, 256); } Similarly, to decrypt the fields (inside the same form), that part of the code would become: useridFld.value = Aes.Ctr.decrypt(useridFld.value, pw, 256); Decryption of the fields can be called by a simple javascript call at the end of the page. If the data is within regular fields (non-form related), such as a table, you'll need to find the right DOM element and call the decryption function there. With those few changes, I was able to get it up and running quickly. I don't think I'll use the PHP application itself in production though, as it doesn't look like it sanitizes the field data in the PHP code and it starts to show performance issues when called with only a few hundred accounts, each having a few dozen passwords. But that hardly matters for this post where I want to point out that it isn't that hard to put some higher security on such sites. The big downside right now is that, if the user forgets his password, he wont have access to all his data (similar to the Clipperz one). And unlike Clipperz, the approach above does not allow for password changes yet (although it doesn't look that hard to implement some logic decrypting and re-encrypting the data with a different password if that comes about). An approach to resolve that would be to encrypt all data with a static key, and then encrypt that key with the password, storing the encrypted key on the server. A password change only requires a decrypt/encrypt of the key while all values remain encrypted with the static key. Moral of the story: application managers of web password storage sites: please add in-browser encryption for those of us that want to make *really* sure that no sensitive data is sent over unencrypted (I don't count SSL/TLS as that \"ends\" at the remote side while this one is full end-to-end encryption).","tags":"Security","url":"https://blog.siphos.be/2013/10/in-browser-encryption-for-online-password-management/","loc":"https://blog.siphos.be/2013/10/in-browser-encryption-for-online-password-management/"},{"title":"A bug please...","text":"I know contacting me (or other developers) through IRC is often fast, but having a bug report on our bugzilla is very important to me and other developers. Allow me to explain a bit why. First of all, IRC is ephemeral . If we are not immediately on IRC noticing it, we might not notice it at all. Even with highlights on IRC or in a separate /QUERY window we might still miss it, because the IRC client might disconnect (for instance because the server on which we run our chat client reboots for a kernel update, or because of some weird issue where we decide quit/restart is better). That doesn't make IRC the wrong method - it can be perfect to ask for some attention on a bug, and if you are on IRC while we are tackling it, it is very easy to ask for some feedback. A second aspect is about the completeness of the report . On a bugzilla report, you can (well, should) add in the necessary information that developers need to troubleshoot it. Gentoo supports many setups, so the output of emerge --info and other output that might be requested (like build logs) are very important. On IRC, a \"report\" might seem as simple as \"package foo has a circular dependency\", but without knowing with which package the circular dependency is or with which versions, it might be difficult to deduce. Dependencies can even be USE-driven, so without knowing what the USE flags are, the circular dependency issue might not be that obvious. Having a complete bugreport (with the necessary attachments) makes for a much easier resolution development lifecycle. Third, and in my opinion extremely important , is that bug reports are searchable . Other users might have the same problem you are facing. By having a bugreport they can chime in (if the problem has not been resolved yet), providing faster feedback (for instance, other users give feedback on additional questions you ask while you are sleeping and when you are back up the bug is resolved). Or, if the bug has been resolved (but the change is still in ~arch ) they will find the solution more easily. And often bugreports also document the workaround (even if just as a way for the developer to confirm that the issue is what it is) allowing other users to switch gears faster with the problem. Also, the entire communication chain between developer and reporter(s) also gives a lot of interesting information. Some developers give a lengthy explanation as to why the issue occurred (which is always useful to know) or explain in which circumstances it would or wouldn't be visible. Most developers also link the bug in the commits and ChangeLog entries so that people who read through the changes can quickly find more information about the change. Bugreports also allow for tracking by people who are not on IRC (or mailinglists) but that do want to help out. For instance, bugs assigned to the SELinux alias on Gentoo's bugzilla are followed up by a handful of non-developers who often give feedback much faster than I or another developer can. If this would be reported on IRC only, you would miss the opportunity to work with these excellent people (and thus get a faster resolution). Another advantage of bugreports is that dependencies between bugs can be placed. A bug might only be resolved if another bug is resolved as well - this dependency information is made part of bug reports to give a clear view on the situation. Bug reports on a bugzilla (or other bug tracking software) have other advantages as well, but the above ones are my top reasons. So while I don't mind if I can quickly fix things I notice on IRC (such as a missing dependency), if you want to make sure I catch it, please use bugzilla.","tags":"Gentoo","url":"https://blog.siphos.be/2013/09/a-bug-please/","loc":"https://blog.siphos.be/2013/09/a-bug-please/"},{"title":"It has finally arrived: SELinux System Administration","text":"Almost everyone has it - either physical or in their heads: a list of things you want to do or achieve before you... well, stop existing. Mine still has numerous things on it (I should get on it, I know) but one of the items on that list has recently been removed: write and have a book published. And the result is a book called SELinux System Administration . Somewhere in the second quarter of this year, Packt Publishing contacted me to see if I am interested in authoring a book about SELinux, focusing on the usage of SELinux (you know - handling booleans, dealing with file contexts, etc.) in a short technical book (the aim was 100 pages). Considering that I'm almost always busy with documentation and editing (for instance, I joined Gentoo as documentation translator and editor beginning of 2003 if I remember correctly) and am busy keeping SELinux support within Gentoo on a good level, I of course said yes to the request. Now, 100 pages is not a lot for a topic as complex and diverse as SELinux so was really challenging, but I do think I managed to get everything in it while keeping it practical. The book first starts with the fundamentals of SELinux - concepts you really need to grasp before diving into SELinux. Then, it goes on about switching SELinux state (disabling, permissive, granular permissive, etc.), logging, managing SELinux users and roles, handling process domains, etc. Just take a look at the table of contents and you'll see what I mean ;-) Inside the book, examples are given based on Fedora (and thus also RedHat Enterprise Linux) and Gentoo Hardened while ensuring that there are few distribution specific sections in it, making it usable for Linux administrators of systems with a different Linux distribution installed to it. Take a look at the sample chapter and, if you like it, put it on your wish list and let everyone know ;-)","tags":"SELinux","url":"https://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/","loc":"https://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/"},{"title":"Aaaand we're back - hardened monthly meeting","text":"It almost feels like we had our monthly online meeting just a week ago. Below a small write-up of the highlights. If you want to know the gory details, just wait a few hours/days until the IRC logs are sent out ;-) Now remember, the project does more than what the highlights tell you - there is lots of maintenance done \"under the hood\", allowing everyone to keep using the various technologies supported through our project. Toolchain As per a discussion on the Gentoo development mailinglist, GCC 4.8 will most likely have SSP enabled as default. Gentoo Hardened will still enable full SSP ( -fstack-protector-all ) whereas Gentoo by default will probably work with -fstack-protector ). We will also still provide GCC specifications that disable stack protection completely (the hardened-nossp specs) for when developers or users need it. Kernel and grsec/PaX Blueness informed us about one issue with XATTR_PAX implementation, being the install.py wrapper that we need in order to set the right attributes as early as possible. The problem is that it is very slow (as it is invoked over and over again, so the overhead of it being an interpreted script becomes huge). A solution for this still has to be defined. Some ideas are to use a compiled version, but other possible solutions such as hooking into Portage directly or using lists have been suggested as well. SELinux Nothing big - standard maintenance stuff, such as pushing our own patches upstream for others to benefit (and hopefully have the master projects include the patches so we don't need to maintain them ourselves). Also, revision 3 of the 2.20130424 policies are now in the tree (\\~arch'ed for now). System Integrity Within Gentoo, we have a couple of SCAP scanners/software available, such as open-scap and ovaldi. Recently, openscap-9999 is made available (allowing users to directly use the latest openscap release) which is used to validate and evaluate security benchmarks I am developing for Gentoo and related services. Next to the SCAP-related developments, a guide has been put on the Gentoo wiki about using signed kernel modules. Profiles The hardened profiles have been updated to use EAPI-5 so we can benefit from its features, such as improved multilib support. Documentation As I mentioned before, I'm working a bit on a Gentoo Security Benchmark that can be used with SCAP software. The sources are in the hardened-docs repository for now. Also, most/all hardened documentation is moved from the (developer-editable only) Project: namespace to the general one, allowing users and contributors to help us with the documents as well.","tags":"Gentoo","url":"https://blog.siphos.be/2013/09/aaaand-were-back-hardened-monthly-meeting/","loc":"https://blog.siphos.be/2013/09/aaaand-were-back-hardened-monthly-meeting/"},{"title":"Underestimated or underused: Portage (e)logging","text":"Within 30 minutes of each other, two people on the #gentoo channel asked if Portage kept logs of the messages displayed during the build and installation of a package. Of course, the answer is a sounding \"yes\" - and depending on your needs, you can even save more of the logging. If you haven't read the Gentoo handbook yet, do so - it contains a section on Logging Features that explains how and when logging is enabled. Let's start with the default elog support. By default, Gentoo's Portage will log messages that ebuild developers have put in the packages through the use of the elog , ewarn and eerror functions. These methods are used by the developers to notify the administrator about something useful or important (usually elog ), a potential issue that might occur ( ewarn ) or even a problem that was found ( eerror ). Let's look at a snippet of the app-admin/eselect-postgresql-1.0.10 package: pkg_postinst() { ewarn \"If you are updating from app-admin/eselect-postgresql-0.4 or older, run:\" ewarn ewarn \" eselect postgresql update\" ewarn ewarn \"To get your system in a proper state.\" elog \"You should set your desired PostgreSQL slot:\" elog \" eselect postgresql list\" elog \" eselect postgresql set \" } Here, the developer warns the administrator that, if an upgrade of the package from version 0.4 or older occurred, an additional action needs to be taken. For others, he informs the administrators how to mark the proper active PostgreSQL slot (using the elog methods). Portage will save this information by default in /var/log/portage/elog as separate files for each deployment: ~$ ls /var/log/portage/elog/*eselect-postgres* /var/log/portage/elog/app-admin:eselect-postgresql-1.0.10:20130818-184756.log /var/log/portage/elog/app-admin:eselect-postgresql-1.2.0:20130915-131631.log /var/log/portage/elog/app-admin:eselect-postgresql-1.2.0:20130915-200851.log If FEATURES=\"split-elog\" is set in your make.conf file, then the elog files will be stored in separate category subdirectories. Below is the content of such a single file: ~$ cat /var/log/portage/elog/app-admin\\:eselect-postgresql-1.0.10\\:20130818-184756.log INFO: setup Package: app-admin/eselect-postgresql-1.0.10 Repository: gentoo Maintainer: titanofold@gentoo.org postgresql USE: abi_x86_64 amd64 elibc_glibc kernel_linux multilib selinux userland_GNU FEATURES: preserve-libs sandbox selinux sesandbox WARN: postinst If you are updating from app-admin/eselect-postgresql-0.4 or older, run: eselect postgresql update To get your system in a proper state. LOG: postinst You should set your desired PostgreSQL slot: eselect postgresql list eselect postgresql set <slot> In the file, package information is provided, and each set of logging paragraphs is accompanied with information where in the build process the logging came up. For instance, WARN: postinst sais that the following text is through the ewarn method in the post install phase of the package installation process. By default Portage logs this, but you can ask it to mail the logs as well. For more information about that, check the handbook link earlier and look for the PORTAGE_ELOG_SYSTEM and PORTAGE_ELOG_MAIL* variables. Or look at man make.conf which also contains all the information needed. Now, this only pertains to the specific logging that ebuild maintainers added in the packages. But what if you want to keep track of all the build logs? In that case, you need to define PORT_LOGDIR in your make.conf file. This variable has to point to a location where Portage (which usually runs as the portage Linux user) has write access to and where it is allowed to store the build logs: ~$ grep PORT_LOGDIR /etc/portage/make.conf PORT_LOGDIR=\"/var/log/portage\" ~$ ls -ldZ /var/log/portage/ drwxrwsr-x. 3 portage portage system_u:object_r:portage_log_t 679936 Sep 24 13:12 /var/log/portage/ With the variable set, build logs will be provided for each emerge (and unmerge) operation of the package. These logs contain everything shown on the terminal during a build process. Of course, this directory will grow considerably in size so it is wise to properly handle old log files. You can use logrotate for this, or just a single cronjob that cleans up log files older than say 6 months. But Portage also provides this functionality. If you set FEATURES=\"clean-logs\" in your make.conf file, then all log files older than 7 days will be removed from the system. You can fine-tune this by setting PORT_LOGDIR_CLEAN to the command you want executed. Its default value can be found in the /usr/share/portage/config/make.globals file. If you set PORT_LOGDIR , be aware that the elog files (described at the beginning) will be stored in ${PORT_LOGDIR}/elog . Similar to the elogging, build logging can also be done in category subdirectories. If FEATURES=\"split-log\" is set, the build logs will be stored in ${PORT_LOGDIR}/build/<category> instead of ${PORT_LOGDIR} directly. Hopefully this post brings some users closer to this nice feature of Portage.","tags":"Gentoo","url":"https://blog.siphos.be/2013/09/underestimated-or-underused-portage-elogging/","loc":"https://blog.siphos.be/2013/09/underestimated-or-underused-portage-elogging/"},{"title":"Creating a poor man central SCAP system","text":"A few weeks ago, I was asked to give some explanation about how SCAP content can be used in companies to improve their infrastructure knowledge. The focus back then was to look at benchmarks (secure states) and violations, but other functionality should not be ignored. I'm not going to talk about how SCAP can be used in various cases - that'll be for later - but one of the remarks came how SCAP can be used in larger environments. The question was not all that weird, as I explained the functionality through Open-SCAP which currently uses a local scanning approach, and for larger environments you want to have remote scanning capabilities. There are many commercial products available that provide remote scanning (so you centrally manage the SCAP content and it is \"played\" against remote systems), but as a free software advocate I want to see how this can be achieved in free software. There is spacewalk , the upstream project of the RedHat Network Sattelite project, but that looked a bit too complex for \"just\" handling SCAP content on remote systems. That, and I'm wondering if it would be that usable for non-RedHat systems. So I decided to make a quick prototype of how I would approach handling SCAP content in a larger environment. I pushed the result to a github project, called poor man central SCAP . Or, in abbreviated form, pmcs . The idea is simple: have a central configuration repository that defines the SCAP content to be played on the remote systems, and have the remote systems pull this information, evaluating the SCAP content and sending it to a central reporting repository (which of course can be the same target where the central configuration is stored). After a few hours of coding and writing documentation, I have a workable solution with some nice features. pmcs supports a local-configuration-less approach on the target systems. The only thing you need to do is schedule the pmcs agent (currently only available as a shell script, but I'll be working on a perl or python equivalent soon so that I can support other platforms than Unix) with one URL - which is where the configuration is stored. No need for local configuration files. pmcs uses local SCAP scanning software (that needs to be available on the target platforms) but has no strict requirements to that software other than that it has to be triggered command-line. This allows us to leverage the existing knowledge and features available in tools like open-scap or ovaldi or jOVAL . By using a somewhat hierarchical configuration structure and keywording support, administrators can fine-tune which SCAP content is evaluated on which systems pmcs also supports a \"daemonized\" approach where administrators can ask for the immediate evaluation of some SCAP content. This allows admins to quickly obtain system information using OVAL/XCCDF. For more information, consult the README or DESIGN document. I'm working on a use case document as well. The tool hardly contains much coding (thanks to the available KISS tools on many Unix/Linux systems) and is GPL-3.","tags":"Free Software","url":"https://blog.siphos.be/2013/09/creating-a-poor-man-central-scap-system/","loc":"https://blog.siphos.be/2013/09/creating-a-poor-man-central-scap-system/"},{"title":"Switching gpg key to 0x2EDD52403B68AF47","text":"I recently switched my GnuPG key. The previous key - which is still in place for now (no revocation send out yet) - was 0x5DFAB3ECCDBA2FDB and was a 1024 bit DSA key. The new one, 0x2EDD52403B68AF47, is a 4096 bit RSA key. It also has the following preferences: gpg> showpref [ultimate] (1). Sven Vermeulen <sven.vermeulen@siphos.be> [ultimate] (2) Sven Vermeulen <swift@gentoo.org> Cipher: AES256, AES192, AES, CAST5, 3DES Digest: SHA512, SHA384, SHA256, SHA224, SHA1 Compression: BZIP2, ZLIB, ZIP, Uncompressed Features: MDC, Keyserver no-modify The new key's fingerprint is as follows: gpg> fpr pub 4096R/0x2EDD52403B68AF47 2013-09-16 Sven Vermeulen Primary key fingerprint: 7264 9F85 E8F1 6F6A 4B68 1102 2EDD 5240 3B68 AF47 A signed key transition statement can be found on my Gentoo development page; the document is signed with both of my keys (the old one and new one).","tags":"Security","url":"https://blog.siphos.be/2013/09/switching-gpg-key-to-0x2edd52403b68af47/","loc":"https://blog.siphos.be/2013/09/switching-gpg-key-to-0x2edd52403b68af47/"},{"title":"cvechecker 3.3 released","text":"I just uploaded a new release of cvechecker to the project files. The release is a (long overdue) bugfix release, but includes two small enhancements: support standard input for the binary list (so you can pipe the output of one command to cvechecker) and the introduction of the CVECHECKER_CONFFILE variable to refer to another location for the configuration file. Big thanks to Anne Mulhern for the various patches submitted!","tags":"Security","url":"https://blog.siphos.be/2013/09/cvechecker-3-3-released/","loc":"https://blog.siphos.be/2013/09/cvechecker-3-3-released/"},{"title":"Gentoo Hardened progress report","text":"Today, we had our monthly online meeting to discuss the progress amongst the various Gentoo Hardened projects. As usual, here is a small write-up. Lead election As every year, we also reviewed the current project leads. No surprises here, everybody is happy with the current leads so they are re-elected for another term. We did have two candidates for the lead position, but even the other candidate vote for Zorry - so we had a unanimous vote ;-) Toolchain GCC version 4.8.1 will be unmasked pretty soon, and the hardenedno* specs on it will work. However, there is still no progress on the asan (Address Sanitizer) support together with UDEREF. As mentioned in a previous post, UDEREF \"reduces\" the address space a bit which doesn't play well with asan. Still, it isn't inevitable, since PowerPC also has a reduced address space and so does Windows. So perhaps we can use the same model for UDEREF enabled kernels? We'll send the suggestion and the already-existing fixes upstream and hope for the best. In GCC 4.8.1, the -fstack-check option might be enabled by default, but the question is for which architectures and platforms. We know a few packages, such as ffmpeg and libav have problems with it. In those cases, the ebuild will be modified to use -fno-stack-check (if hardened). We opt to enable it for all as we don't really expect much (if any) breakage that can't be dealt with swiftly. Support for hardened uClibc is still going steadily. Blueness is heating his room a bit with it, seeing that mips32r2 takes about 2 weeks to build hardened and vanilla stages - he is using an Ubiquity router station for this. Kernel and Grsecurity/PaX Due to some boot freezes, as explained in bugs 482010 and 481790 , we don't have a stable 3.10.x kernel yet. However, most of the issues should be resolved and we're waiting for confirmation, so we can be looking at a stable 3.10.x kernel soon. The 3.10 kernel will probably not be a long-term support kernel for PaX and Grsecurity - such LTS kernel will be picked next year, most likely the same kernel version that Ubuntu LTS settles on. SELinux A small update on policycoreutils has been made that updates rlpkg and selocal . Other than that, our policies are in nice shape. A new revision will be pushed to the tree soon. Integrity On the Integrity side, recent kernels support custom IMA policies (again) so our documentation is accurate again. Next to IMA/EVM, I'll be working on documentation for cryptographically signed kernel module support soon as well as SCAP-based security baselines for Gentoo. Profiles Blueness added a MUSL-based Gentoo profile ( hardened/linux/musl ). Musl is an even more slim libc and its profile is extremely experimental for now. The profile structure is still a bit off though, a reorganization will be suggested soon so that the profile inheritance is clear and predictable, starting off with a non-hardened one ( default/linux/{uclibc,musl} ) and then a hardened specific one that inherits from the default. Documentation The Gentoo Hardened project now has its main project page on the Gentoo Wiki , and all (most) documentation is moved to there as well for the Gentoo Hardened subprojects. I also explained to the folks that I have authored a book on SELinux System Administration (for Packt Publishing), which was why I was less active the last few months. However, that is now done so I'm back on track. More information about the book follows later on my blog ;-) Media And as usual, klondike has been tweeting the entire meeting through our @GentooHardened twitter account ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2013/08/gentoo-hardened-progress-report/","loc":"https://blog.siphos.be/2013/08/gentoo-hardened-progress-report/"},{"title":"Umounting IPv6 NFS(v4) mounts","text":"I had issues umounting my NFSv4 shares on an IPv6-only network. When trying to umount the share, it said that it couldn't find the mount in /proc/mounts : ~# umount /mnt/nfs/portage /mnt/nfs/portage was not found in /proc/mounts The solution: copy /proc/mounts to /etc/mtab , and the umount works correctly again.","tags":"Misc","url":"https://blog.siphos.be/2013/08/umounting-ipv6-nfsv4-mounts/","loc":"https://blog.siphos.be/2013/08/umounting-ipv6-nfsv4-mounts/"},{"title":"Why our policies don't like emerge --config","text":"One of the features that Portage provides is to have post-processing done on request of the administrator for certain packages. For instance, for the dev-db/postgresql-server package we can call its pkg_config() phase to create the PostgreSQL instance and configure it so that the configuration of the database is stored in /etc/postgresql-9.2 rather than together with the data files. When you run Gentoo with SELinux however, you might already have noticed that this doesn't work. The reason is that, whenever an administrator calls emerge , the process (and by default its child processes) will run in a confined domain called portage_t . The domain is still quite privileged, but not as privileged as the administrator domain sysadm_t . It holds the rights to build software and install files, directories and other things on the file system. But it does not support switching users for instance, which is what the PostgreSQL pkg_config() does: it wants to run a certain command as the postgres user, which is prohibited by SELinux. I'm not sure yet how to tackle this properly. One thing is that I might update Portage to run in the user domain by default, and transition dynamically towards the proper domains according to the task(s) it is executing. We already do this for building software (where we transition to a portage_sandbox_t confined domain), perhaps it can be expanded to support transitioning to portage_t when it isn't running the pkg_config() phase. But that means injecting (more) SELinux-specific code in Portage, something I'd rather not do (introduces additional complexity and maintenance). Another possibility would be to have administrators explicitly state that no transition should occur, like so: ~# runcon -t sysadm_t emerge --config =dev-db/postgresql-server-9.2.4 With a minor addition to the policy, this gave me a good hope... until I noticed that emerge underlyingly calls ebuild and ebuild.sh , both labeled as portage_exec_t , so these calls transition to portage_t again. I'm going to look further into this - there are quite a few options still open.","tags":"Gentoo","url":"https://blog.siphos.be/2013/08/why-our-policies-dont-like-emerge-config/","loc":"https://blog.siphos.be/2013/08/why-our-policies-dont-like-emerge-config/"},{"title":"Network routing based on SELinux?","text":"Today we had a question on #selinux if it was possible to route traffic of a specific process using SELinux. The answer to this is \"no\", although it has to be explained a bit in more detail. SELinux does not route traffic. SELinux is a local mandatory access control system; its purpose is to allow or deny certain actions, not route traffic. However, Linux' NetFilter does support security markings (SECMARK). I've blogged about it in the past, and there are good tutorials elsewhere on the Internet, such as Dan Walsh' Using SELinux and iptables together . Linux support for security marking does allow for routing decisions - but it does not \"use\" SELinux in this regard. To mark traffic with a certain label, the administrator has to put in the rules himself using iptables or ip6tables commands. And if you have to do that, then you are already working with the routing commands so why not just route immediately? Of course, the advantage of labeling the traffic is that you can then use SELinux to allow or deny processes to send or receive those packets - but SELinux is not involved with routing. Another possibility is to use labeled networking, such as Labeled IPSec or NetLabel/CIPSO support. With labeled networking, all hosts that participate in the network need to support the labeled networking technology. If that is the case, then SELinux policy can be used to deny traffic from one host or another - but again, not to route traffic. You can use SELinux to deny one process to send out data to one set of hosts and allow it to send data to another, but that is not routing. The advantage of Labeled IPSec is that contexts are retained across the network - decisions that SELinux has to take on one system can be made based on the context of the process on the other system. So no, SELinux cannot be used to route traffic, but it plays very nicely with various networking controls to make proper decisions on its access control enforcement.","tags":"SELinux","url":"https://blog.siphos.be/2013/08/network-routing-based-on-selinux/","loc":"https://blog.siphos.be/2013/08/network-routing-based-on-selinux/"},{"title":"Using CUSTOM_BUILDOPT in refpolicy for USE flag-alike functionality?","text":"As you are probably aware, Gentoo uses the reference policy as its base for SELinux policies. Yes, we do customize it and not everything is already pushed upstream (for instance, our approach to use xdg_*_home_t customizable types to further restrict user application access has been sent up for comments a few times but we still need to iron it out further) but all in all, we're pretty close to the upstream releases. This is also visible when there are changes upstream as we very easily integrate them back in our repository. But there are still a few things that I want to implement further, and one of these things is perhaps too specific for Gentoo but would benefit us (security-wise) in great detail: enabling domain privileges based on USE flags. Allow me to quickly use an example to make this more tangible. Consider the MPlayer application. As a media application, it of course offers support for ALSA and PulseAudio (amongst other things). In the SELinux policy, support for (and thus privileges related to) ALSA and PulseAudio is handled through optional_policy statements: optional_policy(` pulseaudio_tmpfs_content(mplayer_tmpfs_t) ') This means that the rules defined in pulseaudio_tmpfs_content are executed if the dependencies match: interface(`pulseaudio_tmpfs_content',` gen_require(` attribute pulseaudio_tmpfsfile; ') typeattribute $1 pulseaudio_tmpfsfile; ') If the pulseaudio_tmpfsfile attribute exists, then the mplayer_tmpfs_t type gets the pulseaudio_tmpfsfile attribute assigned to it. This is flexible, because if the server/workstation does not use PulseAudio, then in Gentoo, no pulseaudio SELinux module will be loaded and thus the attribute will not exist. However, Gentoo tries to be a bit more flexible in this - it is very well possible to have PulseAudio installed, but disable PulseAudio support in MPlayer (build mplayer with USE=\"-pulseaudio\"). The current definitions in the policy do not support this flexibility: if the pulseaudio module is loaded, then the privileges become active. One way SELinux supports a more flexible approach is to use conditionals in the policy. One could create booleans that administrators can toggle to enable / disable SELinux rules. For instance, in the mplayer policy: tunable_policy(`allow_mplayer_execstack',` allow mencoder_t self:process { execmem execstack }; ') If an administrator toggles the allow_mplayer_execstack boolean to \"on\", then the mentioned allow rule becomes active. Sadly, this approach is not fully usable for USE driven decisions. Not all rules can be enclosed in tunable_policy statements, and assigning attributes to a type is one of them (cfr our pulseaudio example). A recent discussion on the reference policy mailinglist gave me two ideas to investigate: See if we can support CIL (a new language for SELinux policy definitions) where such an approach would be easier Use build-time decisions In this post, I want to go through the build-time decisions idea. The reference policy supports build-time options using ifdef constructs. These look at parameters provided by the build system (M4/Makefile based) to see if rules need to be activated or not. For type attribute declarations, this is a valid approach. So one idea would be to transform USE flags, if they are set, into use_${USEFLAG} , and make decisions based on this in the policy code: ifdef(`use_pulseaudio',` optional_policy(` pulseaudio_tmpfs_content(mplayer_tmpfs_t) ') ') We can add in the USE flags, if set, through the CUSTOM_BUILDOPT parameter that the reference policy provides. So introducing this is not that difficult. The only thing I'm currently a bit weary about is the impact on the policy files themselves (which is why I haven't done this already) and the fact that USE flags on the \"real\" package are not know to policy packages. In other words, if a user explicitly marks USE=\"-pulseaudio\" on mplayer, but has USE=\"pulseaudio\" set as general value, then the selinux-mplayer package will still have pulseaudio enabled. Still, I do like the idea. It would make it more consistent with what Gentoo aims to do: if you do not want a certain support/feature in the code, then why would the policy still have to allow it? With the proper documentation towards administrators, I think this would be a good approach.","tags":"Gentoo","url":"https://blog.siphos.be/2013/08/using-custom_buildopt-in-refpolicy-for-use-flag-alike-functionality/","loc":"https://blog.siphos.be/2013/08/using-custom_buildopt-in-refpolicy-for-use-flag-alike-functionality/"},{"title":"Today was a productive day","text":"Fixed 14 bugs today, with a few more pending (those for packages only get marked as FIXED if it is moved to the stable state). One of the changes is the GRUB2 support in the Gentoo Handbook (yes, finally, sorry about that). That opens up the road for the stabilization of GRUB2. I also added a dozen fixes to the Gentoo SELinux policy repository and sent a few others upstream that have been lingering in our policy for quite some time. But the highlight for me was that I got to play with my Wacom Bamboo pen and touch which, I must say, integrated easily with my system. I only had to build the wacom.ko kernel module and install the xf86-input-wacom package, but that's all - no reboot needed. Even GIMP immediately detected the new device and I can start drawing and fixing pictures more easily (I work on pictures often and a mouse just isn't that obvious). One of the huge benefits is that the pressure you put on the drawing pad is related (and configurable) to a drawing function in GIMP, something you don't have with mice. For instance, if you need to Dodge or Burn on layers, you don't need to continuously switch between the amount - just adjust the pressure.","tags":"Misc","url":"https://blog.siphos.be/2013/08/today-was-a-productive-day/","loc":"https://blog.siphos.be/2013/08/today-was-a-productive-day/"},{"title":"Some things sound more scary than they are","text":"A few days ago I finally got to the next thing on my Want to do this year list: put a new android ( Cyanogenmod ) on my tablet, which was still running the stock Android - but hasn't seen any updates in more than a year. Considering the (in)security of Android this was long overdue for me. But the fear of getting an unbootable tablet (\"bricked\" as it is often called) was keeping me from doing so. So when I finally got the nerves, I first had to run around screaming for hours because the first step in the instructions didn't work. The next day I read that it might have to do with the cable - and indeed, tried with a different cable and the instructions just went along just fine. So today I'm happily running with a more up-to-date Android again on my tablet. Because my systems run Gentoo Hardened with SELinux, I did had to do some small magic tricks to get the Clockworkmod recovery on the tablet: the wheelie binary (yes, I couldn't find the sources - if they are even available) that I had to run required me to disable size overflow detection in the kernel (a PaX countermeasure), allowed executable memory (both through paxctl-ng as well as in SELinux using the allow_execmem boolean) and had to temporarily add in the dev_rw_generic_usb_dev right (refpolicy macro) to my user. Also adb had to be pax-marked, although I now know I don't need adb at all - I can just download the latest Android ZIP file from the phone itself and refer to it from the recovery manager. All in all nothing to worry about - everything worked like a charm. Edit: (so I remember next time), if the system is stuck in CMR (recovery), reboot with VolDown+Pwr, but don't select recovery. After 5 seconds, it will ask if you want a cold boot. Select it, and things work again ;-)","tags":"SELinux","url":"https://blog.siphos.be/2013/08/some-things-sound-more-scary-than-they-are/","loc":"https://blog.siphos.be/2013/08/some-things-sound-more-scary-than-they-are/"},{"title":"And now, 31 days later...","text":"... the Gentoo Hardened team had its monthly online meeting again ;-) On the agenda were the usual suspects, such as the toolchain . In this category, Zorry mentioned that he has a fix for GCC 4.8.1 for the hardenedno* and vanilla gcc-config options which will be added to the tree after some more testing. The problem is that with GCC 4.8, certain settings need to be set sooner than before (in the code path), which is what the fix focuses on. The ASAN issue is still unresolved, but otherwise GCC 4.8 is working fine. On SELinux , the policycoreutils package has been bumped to include support for mcstrans , a translation daemon that visualizes humanly readable strings instead of the standard sensitivity/category sets in case of MCS/MLS policies. Regarding documentation, the wiki team (most notably a3li) is working hard to support project pages on the Gentoo Wiki . Once we can, we will be moving our project page with all related documentation to the wiki, allowing for easier documentation development and a more modern look. To support this, an XML-to-wiki stylesheet is available that translates ProjectXML and GuideXML to the wiki language. During the meeting, we also mentioned the stabilization policy (or at least, no-longer-stabilization) of the vanilla sources (plain kernel.org Linux kernel sources). This doesn't immediately effect the hardened project, but is important to know nonetheless, especially for users of hardened technologies that are in the main kernel already (like SELinux or IMA/EVM) as they have to be aware to either use the latest (regardless of the keywords in use) or switch to gentoo-sources or (preferably) hardened-sources. For uclibc support, the stages will be provided every 2 months rather than every month as this is a resource-intensive process that isn't fully automated yet (except for amd64 and x86 which are automated). Finally, on PaX and grSecurity support, the XATTR patch for tmpfs is now in the kernel, and the problem about loosing PaX markings during installation is fixed as Portage (2.1.12.9 and higher) now preserves the flags during installation (a wrapper on install is used that preserves usr.pax.flags ).","tags":"Gentoo","url":"https://blog.siphos.be/2013/08/and-now-31-days-later/","loc":"https://blog.siphos.be/2013/08/and-now-31-days-later/"},{"title":"Putting OVAL at work","text":"When we look at the SCAP security standards , you might get the feeling of \"How does this work\". The underlying interfaces, like OVAL and XCCDF, might seem a bit daunting to implement. This is correct, but you need to remember that the standards are protocols, agreements that can be made across products so that several products, each with their own expertise, can work together easily. It is a matter of interoperability between components. Let's look at the following diagram to see how OVAL and XCCDF can be used. I'm not saying this is the only way forward, but it is a possible approach. (Diagram lost during blog conversion) On the local side (and local here doesn't mean a single server, but rather an organization or company) a list of checks is maintained. These checks are OVAL checks, which can be downloaded from reputable sites like NVD or are given to you by vendors (some vendors provide OVAL as part of vulnerability reports). Do not expect this list to be hundreds of checks - start small, the local database of checks will grow anyhow. The advantage is that the downloaded checks (OVALs) already have a unique identifier (the OVAL ID). For instance, the check \"Disable Java in Firefox\" for Windows is oval:org.mitre.oval:def:12609 . If additional Windows operating systems are added, this ID remains the same (it is updated) because the check (and purpose) remains the same. Locally, the OVAL checks are ran against targets by an OVAL interpreter. Usually, you will have multiple interpreters in the organization, some of them focused on desktops, some on servers, some perhaps on network equipment, etc. By itself that doesn't matter, as long as they interpret the OVAL checks. The list of targets to check against are usually managed in a configuration management database. Targets can be of various granularity. The \"Disable Java in Firefox\" will be against an operating system (where the check then sees if the installed Firefox indeed has the setting disabled), but a check that validates the permissions (rights) of a user will be against this user account. The results of the OVAL checks are stored in a database that maps the result against the target. By itself this result database does not contain much more logic than \"This rule is OK against this target and that rule isn't\" (well, there is some granularity, but not much more) and the time stamp when this was done. Next comes the XCCDF. XCCDF defines the state that you want the system to be in. It is a benchmark, a document describing how the system / target should be configured. XCCDF documents usually contain the whole shebang of configuration settings, and then differentiate them based on profiles. For instance, a web server attached to the Internet might have a different profile than a web server used internally or for development purposes. The XCCDF document refers to OVAL checks, and thus uses the results from the OVAL result database to see if a target is fully aligned with the requirements or not. The XCCDF results themselves are also stored, often together with exceptions (if any) that are approved (for instance, you want to keep track of the workstations where Java is enabled in Firefox and only report for those systems where it is enabled by the user without approval). Based on these results, reports can be generated on the state of your park. Not all checks are already available as OVAL checks. Of course you can write them yourself, but there are also other possibilities. Next to OVAL, there are (less standardized) methods for doing checks which integrate with XCCDF as well. The idea you'll need to focus on then is the same as with OVAL: what is your source, how do you store it, you need interpreters that can \"play\" it, and on the reporting side you'll need to store the results so you can combine them later in your reporting.","tags":"Security","url":"https://blog.siphos.be/2013/08/putting-oval-at-work/","loc":"https://blog.siphos.be/2013/08/putting-oval-at-work/"},{"title":"Moving Gentoo docs to the wiki","text":"Slowly but surely Gentoo documentation guides are being moved to the Gentoo Wiki . Thanks to the translation support provided by the infrastructure, all \"reasons\" not to go forward with this have been resolved. At first, I'm focusing on documentation with open bugs that have not been picked up (usually due to (human) resource limits), but other documents will follow. Examples of already moved documents are the Xorg configuration guide , the GCC optimization guide , UTF-8 and System testing with UML . Many more have been moved as well. The migrations are assisted by a conversion script that translates GuideXML into wiki style content, although manual corrections remain needed. For instance, all <pre caption=\"...\"> stuff is changed into {{Code|...}} even though the Wiki has several templates for code, such as {{Kernel|...}} for kernel configurations, {{RootCmd|...}} for commands ran as a privileged user and {{Cmd|...}} for unprivileged user commands. I updated the documentation list on the main Gentoo site to reflect the movement of documents as well, as this list will be slowly shrinking.","tags":"Documentation","url":"https://blog.siphos.be/2013/07/moving-gentoo-docs-to-the-wiki/","loc":"https://blog.siphos.be/2013/07/moving-gentoo-docs-to-the-wiki/"},{"title":"Rebuilding SELinux contexts with sefcontext_compile","text":"A recent update of libpcre caused the binary precompiled regular expression files of SELinux to become outdated (and even blatantly wrong). The details are in bug 471718 but that doesn't help the users that are already facing the problem, nor have we found a good place to put the fix in. Anyway, if you are facing issues with SELinux labeling (having files being labeled as portage_tmp_t instead of the proper label), check with matchpathcon if the label is correct. If matchpathcon sais that the label should be <<none>> then you need to rebuild the SELinux context files: # cd /etc/selinux/strict/contexts/files # for n in *.bin; do sefcontext_compile ${n%%.bin}; done The sefcontext_compile command will rebuild the SELinux context files. When that has been done, matchpathcon should show the right context again, and Portage will relabel files correctly. Until then, you will need to relabel the packages that have been built since (and including) the libpcre build. If someone has a good suggestion where to put these rebuilds in, please do drop a note in the bug. Although the proper one might be libpcre itself, I'd rather not put too much SELinux logic in the ebuild unless it is pretty safeguarded... In any case, it has also been documented in the Gentoo SELinux FAQ on the Gentoo wiki.","tags":"SELinux","url":"https://blog.siphos.be/2013/07/rebuilding-selinux-contexts-with-sefcontext_compile/","loc":"https://blog.siphos.be/2013/07/rebuilding-selinux-contexts-with-sefcontext_compile/"},{"title":"Adding mcstrans to Gentoo","text":"If you use SELinux, you might be using an MLS-enabled policy. These are policies that support sensitivity labels on resources and domains. In Gentoo, these are supported in the mcs and mls policy stores. Now sensitivity ranges are fun to work with, but the moment you have several sensitivity levels, or you have several dozen categories (sets or tags that can be used in conjunction with pure sensitivity levels) these can become a burden to maintain. The SELinux developers have had the same issue, so they wrote a tool called mcstransd , a translation daemon that reads the sensitivity labels from the SELinux context (such as s0-s0:c0.c1023 or s0:c12 ) and displays a more human readable string for this (such as SystemLow-SystemHigh or SalesTeam ). The daemon is not a super intelligent one - we just configure it by creating a mapping file in /etc/selinux/mcs called setrans.conf which contains the mappings: ## setrans.conf ## s0-s0:c0.c1023=SystemLow-SystemHigh s0:c12=SalesTeam The SELinux libraries (libselinux and libsemanage) use a socket to communicate with the daemon to see if \"translated\" values need to be displayed. If not (because the daemon is missing) the libraries keep the SELinux syntax displayed. If it is, then the translated labels are displayed. Support for categories and sensitivity labels is handled through the chcat tool, so you can list the current categories (and their translated values) as well as assign them to files (or even logins). Although we supported categories for a while already, a recent update on the policycoreutils package now includes the mcstrans daemon as well. Documentation is available, currently in the pending changes section of the SELinux handbook (as this is not in the stable package yet) and it will be moved to the main document when the package has stabilized.","tags":"Gentoo","url":"https://blog.siphos.be/2013/07/adding-mcstrans-to-gentoo/","loc":"https://blog.siphos.be/2013/07/adding-mcstrans-to-gentoo/"},{"title":"Hardening is our business... new monthly report ;-)","text":"We're back with another report on the Gentoo Hardened project. Please excuse my brevity, as you've noticed I'm not that active (yet) due to work on an external project - I'll be back mid-July though. I promise. On the Toolchain side, GCC 4.8.1 is in the tree and has the GCC plugin header fix. Also, for IA64 and ARM, the necessary PIE patches are available as well to make this work on those architectures too. For uclibc, blueness is continuing the necessary support for everything so far. He has also added mips64 n32 uclibc because new router boards use this. In his time, blueness is also playing with a uclibc-powered desktop and another C library called musl . On the Kernel , grSecurity and PaX side, we are having troubles with the 3.8+ kernels and UEFI machines when the machines have ltitle memory available (for instance when it is limited with mem= ). The PaX extended attribute support is still on-going, mainly because we need to have support for the user.pax attributes in common tools like install , which is heavily used in Gentoo's ebuilds. The merge phase, where the data is moved from the image location to the root, has been supporting xattr moves for a while thanks to zmedico and arfrever, but other installation phases still needed to be fixed or worked around. We tried with a common patch on this, but there was little interest in this approach, so we settled with a wrapper around install inside of Portage. This will be soon released and we again have full end-to-end xattr pax flag support. On the SELinux support, the latest userland and policy releases have been stabilized in the Gentoo tree. On the Profiles , blueness added a musl subprofile for testing of the musl C library.","tags":"Gentoo","url":"https://blog.siphos.be/2013/06/hardening-is-our-business-new-monthly-report/","loc":"https://blog.siphos.be/2013/06/hardening-is-our-business-new-monthly-report/"},{"title":"My application base: graphviz","text":"Visualization of data is often needed in order to understand what the data means. When data needs to be visualized automatically, I often use the graphviz tools. Not that they are extremely pretty, but it works very well and is made to be automated. Let me give a few examples of when visualization helps... In SELinux, there is the notion of domain transitions: security contexts that can transition to another security context (and thus change the permissions that the application/process has). Knowing where domains can transition to (and how) as well as how domains can be transitioned to (so input/output, if you may) is an important aspect to validate the security of a system. The information can be obtained from tools such as sesearch , but even on a small system you easily find hundreds of transitions that can occur. Visualizing the transitions in a graph (using dot or neato ) shows how a starting point can move (or cannot move - equally important to know ;-) to another domain. So a simple sesearch with a few awk statements in the middle and a dot at the end produces a nice graph in PNG format to analyze further. A second visualization is about dependencies. Be it package dependencies or library dependencies, or even architectural dependencies (in IT architecturing, abstraction of assets and such also provides a dependency-like structure), with the Graphviz tools the generation of dependency graphs can be done automatically. At work, I sometimes use a simple home-brew web-based API to generate the data (similar to Ashitani's Ajax/Graphviz ) since the workstations don't allow installation of your own software - and they're windows. Another purpose I use graphviz for is to quickly visualize processes during the design. Of course, this can be done using Visio or Draw.io easily as well, but these have the disadvantage that you already require some idea on how the process will evolve. With the dot language, I can just start writing processes in a simple way, combining steps into clusters (or in scheduling terms: streams or applications ;-) and let Graphviz visualize it for me. When the process is almost finished, I can either copy the result in Draw.io to generate a nicer drawing or use the Graphviz result (especially when the purpose was just rapid prototyping). And sometimes it is just fun to generate graphs based on data. For instance, I can take the IRC logs of #gentoo or #gentoo-hardened to generate graphs showing interactions between people (who speaks to who and how frequently) or to find out the strength of topics (get the keywords and generate communication graphs based on those keywords).","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-graphviz/","loc":"https://blog.siphos.be/2013/06/my-application-base-graphviz/"},{"title":"My application base: LibreOffice","text":"Of course, working with a Linux desktop eventually requires you to work with an office suite. Although I have used alternatives like AbiWord and Calligra in the past, and although I do think that Google Docs might eventually become powerful enough to use instead, I'm currently using LibreOffice . The use of LibreOffice for Linux users is well known: it has decent Microsoft Office support (although I hardly ever need it; most users don't mind exporting the files in an open document format and publishers often support OpenOffice/LibreOffice formats themselves) and its features are becoming more and more powerful, such as the CMIS support (for online collaboration through content management systems). It also has a huge community, sharing templates and other documents that make life with LibreOffice even much prettier. Don't forget to check out its extensive documentation . The aspects of LibreOffice I use the most are of course its writer (word processor) and calc (spreadsheet application). The writer-part is for when I do technical writing, whereas the spreadsheet application is for generating simple management sheets for startups and households that want to keep track of things (such as budgets, creating invoices, data for mail-merge, etc.). At my work, Excel is one of the most used \"end user computing\" tools, so I happen to get acquainted with quite a few spreadsheet tips and tricks that are beneficial for small companies or organizations ;-) Also, Calc has support for macro-like enhancements, which makes it a good start for fast application development (until the requests of the user/client has been stabilized, after which I usually suggest a real application development ;-) I generally don't use its presentation part much though - if I get a powerpoint, I first see if Google Docs doesn't show it sufficiently well. If not, then I try it out in LibreOffice. But usually, if someone sends me a presentation, I tend to ask for a PDF version.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-libreoffice/","loc":"https://blog.siphos.be/2013/06/my-application-base-libreoffice/"},{"title":"My application base: firefox","text":"Browsers are becoming application disclosure frameworks rather than the visualization tools they were in the past. More and more services, like the Draw.io one I discussed not that long ago, are using browsers are their client side while retaining the full capabilities of end clients (such as drag and drop, file management, editing capabilities and more). The browser I use consistently is Firefox . I do think I will move to Chromium (or at least use it more actively) sooner or later, but firefox at this point in time covers all my needs. It isn't just the browser itself though, but also the wide support in add-ons that I am relying upon. This did make me push out SELinux policies to restrict the actions that firefox can do, because it has become almost an entire operating system by itself (like ChromeOS versus Chrome/Chromium). With a few tunable settings (SELinux booleans) I can enable/disable access to system devices (such as webcams), often vulnerable plugins (flash, java), access to sensitive user information (I don't allow firefox access to regular user files, only to the downloaded content) and more. One of the add-ons that is keeping me with Firefox for now is NoScript . Being a security-conscious guy, being able to limit the exposure of my surfing habits to advertisement companies (and others) is very important to me. The NoScript add-on does this perfectly. The add-on is very extensible (although I don't use that - just the temporary/permanent allow) and easy to work with: on a site where you notice some functionality isn't working, right-click and seek the proper domain to allow methods from. Try-out a few of them temporarily until you find the \"sweet spot\" and then allow those for future reference. Another extension I use often (not often enough) is the spelling checker capabilities. On multi-line fields, this gives me enough feedback about what I am typing and if it doesn't use a mixture of American English and British English. But with a simple javascript bookmarklet, I can even enable spell check on a rendered page (simple javascript that sets the designMode variable and the contentEditable variable to true), which is perfect for the Gorg integration while developing Gentoo documentation. The abilities of a browser are endless: I have extensions that offer ePub reading capabilities, full web development capabilities (to edit/verify CSS and HTML changes), HTTPS Everywhere (to enforce SSL when the site supports it), SQLite manager, Tamper Data (to track and manipulate HTTP headers) and more. With the GoogleTalk plugins, doing video chats and such is all done through the browser. This entire eco-system of plugins and extensions make the browser a big but powerful interface, but also an important resource to properly manage: keep it up-to-date, backup your settings (including auto-stored passwords if you enable that), verify its integrity and ensure it runs in its confined SELinux domain.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-firefox/","loc":"https://blog.siphos.be/2013/06/my-application-base-firefox/"},{"title":"My application base: bash and kiss tools","text":"Okay, this just had to be here. I'm an automation guy - partially because of my job in which I'm responsible for the long-term strategy behind batch, scheduling and workload automation, but also because I believe proper automation makes life just that much easier. And for personal work, why not automate the majority of stuff as well? For most of the automation I use, I use bash scripts (or POSIX sh scripts that I try out with the dash shell if I need to export the scripts to non-bash users). The Bourne-Again SHell (or bash ) is the default shell on Gentoo Linux systems, and is a powerful shell in features as well. There are numerous resources available on bash scripting, such as the Advanced Bash-Scripting Guide or the commandlinefu.com (not purely bash), and specific features of Bash have several posts and articles all over the web. Shell scripts are easy to write, but their power comes from the various tools that a Linux system contains (including the often forgotten GNU-provided ones, of which bash is one of them). My system is filled with scripts, some small, some large, all with a specific function that I imagined I would need to use again later. I prefix almost all my scripts with sw (first letters of SwifT) or mgl (in case the scripts have the potential to be used by others) so I can easily find them (if they are within my ${PATH} of course, not all of them are): just type the first letters followed by two tabs and bash shows me the list of scripts I have: $ sw\\t\\t swbackup swdocbook2html swsandboxfirefox swletter swpics swstartvm swstripcomment swvmconsole swgenpdf swcheckmistakes swdoctransaccuracy swhardened-secmerge swmailman2mbox swmassrename swmassstable swmovepics swbumpmod swsecontribmerge swseduplicate swfindbracket swmergeoverlay swshowtree swsetvid swfileprocmon swlocalize swgendigest swgenmkbase swgenfinoverview swmatchcve $ mgl\\t\\t mglshow mglverify mglgxml2docbook mglautogenif mgltellabout mgltellhowto mgltellwhynot mglgenmodoverview mglgenoval mglgensetup mglcertcli mglcleannode mglwaitforfile With the proper basic template, I can keep the scripts sane and well documented. None of the scripts execute something without arguments, and \"-h\" and \"--help\" are always mapped to the help information. Those that (re)move files often have a \"-p\" (or \"--pretend\") flag that instead of executing the logic, echo's it to the screen. A simple example is the swpics script. It mounts the SD card, moves the images to a first location ( Pictures/local/raw ), unmounts the SD card, renames the pictures based on the metadata information, finds duplicates based on two checksums (in case I forgot to wipe the SD card afterwards - I don't wipe it from the script) and removes the duplicates, converts the raws into JPEGs and moves these to a minidlna-served location so I can review the images from DLNA-compliant devices when I want and then starts the Geeqie application. When the Geeqie application has finished, it searches for the removed raws and removes those from the minidlna-served location as well. It's simple, nothing fancy, and saves me a few minutes of work every time. The kiss tools are not really a toolset that is called kiss, but rather a set of commands that are simple in their use. Examples are exiv2 (to manage JPEG EXIF information, including renaming them based on the EXIF timestamp), inotifywait (passive waiting for file modification/writes), sipcalc (calculating IP addresses and subnetwork ranges), socat (network socket \"cat\" tool), screen (or tmux, to implement virtual sessions), git (okay, not that KISS, but perfect for what it does - versioning stuff) and more. Because these applications just do what they are supposed to, without too many bells and whistles, it makes it easy to \"glue\" them together to get an automated flow. Automation saves you from performing repetitive steps manually, so is a real time-saver. And bash is a perfect scripting language for it.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-bash-and-kiss-tools/","loc":"https://blog.siphos.be/2013/06/my-application-base-bash-and-kiss-tools/"},{"title":"My application base: geekie","text":"In the past, when I had to manage my images (pictures) I used GQview (which started back in 2008 ). But the application doesn't get many updates, and if an application does not get many updates, it either means it is no longer maintained or that it does its job perfectly. Sadly, for GQview, it is the unmaintained reason (even though the application seems to work pretty well for most tasks). Enter Geeqie, a fork of GQview to keep evolution on the application up to speed. The Geeqie image viewer is a simple viewer that allows to easily manipulate images (like rotation). I launch it the moment I insert my camera's SD card into my laptop for image processing. It quickly shows the thumbnails of all images and I start processing them to see which ones are eligible for manipulations later on (or are just perfect - not that that occurs frequently) and which can be deleted immediately. You can also quickly set Exif information (to annotate the image further) and view some basic aspects of the picture (such as histogram information). Two features however are what is keeping me with this image viewer: finding duplicates, and side-by-side comparison. With the duplicate feature, geekie can compare images by name, size, date, dimensions, checksum, path and - most interestingly, similarity. If you start working on images, you often create intermediate snapshots or tryouts. Or, when you start taking pictures, you take several ones in a short time-frame. With the \"find duplicate\" feature, you can search through the images to find all images that had the same base (or are taking quickly after each other) and see them all simultaneously. That allows you to remove those you don't need anymore and keep the good ones. I also use this feature often when people come with their external hard drive filled with images - none of them having any exif information anymore and not in any way structured - and ask to see if there are any duplicates on it. A simple checksum might reveal the obvious ones, but the similarity search of geeqie goes much, much further. The side-by-side comparison creates a split view of the application, in which each pane has another image. This feature I use when I have two pictures that are taken closely after another (so very, very similar in nature) and I need to see which one is better. With the side-by-side comparison, I can look at artifacts in the image or the consequences of the different aperture, ISO and shutter speed. And the moment I start working on images, Gimp and Darktable are just a single click away.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-geekie/","loc":"https://blog.siphos.be/2013/06/my-application-base-geekie/"},{"title":"My application base: freemind","text":"Anyone who is even remotely busy with innovation will know what mindmaps are. They are a means to visualize information, ideas or tasks in whatever structure you like. By using graphical annotations the information is easier to look through, even when the mindmap becomes very large. In the commercial world, mindmapping software such as XMind and Mindmanager are often used. But these companies should really start looking into Freemind. The Freemind software is a java-based mind map software, running perfectly on Windows, Linux or other platforms. Installation is a breeze (if you are allowed to on your work, you can just launch it from a USB drive if you want, so no installation hassles whatsoever) and its interface is very intuitive. For all the whistles and bells that the commercial ones provide, I just want to create my mindmaps and export them into a format that others can easily use and view. At my real-time job, we (have to) use XMind. If someone shares a mindmap (\"their mind\" map as I often see it - I seem to have a different mind than most others in how I structure things, except for one colleague who imo does not structure things at all) they just share the XMind file and hope that the recipients can read it. Although XMind can export mindmaps just fine, I do like the freemind approach where a simple java applet can show the entire mindmap as interactively as you would navigate through the application itself. This makes it perfect for discussing ideas because you can close and open branches easily. The export/import capabilities of freemind are also interesting. Before being forced to use XMind, we were using Mindmanager and I could just easily import the mindmaps into freemind. The file format that freemind uses is an XML-based one, so translating those onto other formats is not that difficult if you know some XSLT. I personally use freemind when I embark on a new project, to structure the approach, centralize all information, keep track of problems (and their solutions), etc. The only thing I am missing is a nice interface for mobile devices though.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/my-application-base-freemind/","loc":"https://blog.siphos.be/2013/06/my-application-base-freemind/"},{"title":"My application base: draw.io","text":"The next few weeks (months even) will be challenging my free time as I'm working on (too many) projects simultaneously (sadly, only a few of those are free software related, most are house renovations). But that shouldn't stop me from starting a new set of posts, being my application base . In this series, I'll cover a few applications (or websites) that I either use often or that I should use more. In either case, the application does its job very well so why not give some input on it? The first on the agenda is the Draw.io website. With Draw.io, you get a web-browser based drawing application for diagrams, flowcharts, UML, BPMN etc. I came across this application while looking for an alternative to Dia , which by itself was supposed to be an alternative to Microsoft Visio (err, no). Don't get me wrong, Dia is nice, but it lacks evolution and just doesn't feel easy. Draw.io on the other hand is evolving constantly, and it is also active on Google Plus where you can follow up on all recent developments and thoughts (I hope I get the G+ link correctly, it's not that I don't like numbers, just not in URLs). I started using Draw.io while documenting free software IT architectures (such as implementations of BIND, PostgreSQL, etc.) for which I needed some diagrams. Although Draw.io is an online application (and its underlying engine is not completely free software) you can easily work with it from different locations. It integrates with Google Drive to store the diagrams on if you want - and if you don't, you can always save the diagrams in their native XML format on your system and open them later again. The interface is very easy to use, and I recently found out that it now also supports mobile devices, which is perfect for tablets (the mobile device support is recent afaik and still undergoing updates). The site also works well in various browsers (tried IExplorer 10 at work, Firefox and Google Chrome and they all seem to work nicely) - eat that stupid commercial vendors that force me into using IExplorer 8 or Firefox 10 - you know who you are! A site/service to keep a close eye on. The service itself is free (and doesn't seem too limited due to it), but Draw.io also has commercial support if you want through Google Apps and Confluence integration. I don't have much experience with those yet but that might change in the near future (projects, projects).","tags":"Documentation","url":"https://blog.siphos.be/2013/06/my-application-base-draw-io/","loc":"https://blog.siphos.be/2013/06/my-application-base-draw-io/"},{"title":"Using extended attributes for custom information","text":"One of the things I have been meaning to implement on my system is a way to properly \"remove\" old files from the system. Currently, I do this through frequently listing all files, going through them and deleting those I feel I no longer need (in any case, I can retrieve them back from the backup within 60 days). But this isn't always easy since it requires me to reopen the files and consider what I want to do with them... again. Most of the time, when files are created, you generally know how long they are needed on the system. For instance, an attachment you download from an e-mail to view usually has a very short lifespan (you can always re-retrieve it from the e-mail as long as the e-mail itself isn't removed). Same with output you captured from a shell command, a strace logfile, etc. So I'm wondering if I can't create a simple method for keeping track of expiration dates on files, similar to the expiration dates supported for z/OS data sets. And to implement this, I am considering to use extended attributes. The idea is simple: when working with a file, I want to be able to immediately set an expiration date to it: $ strace -o strace.log ... $ expdate +7d strace.log This would set an extended attribute named user.expiration with the value being the number of seconds since epoch (which you can obtain through date +%s if you want) on which the file can be expired (and thus deleted from the system). A system cronjob can then regularly scan the system for files with the extended attribute set and, if the expiration date is beyond the current date, the file can be removed from the system (perhaps first into a specific area where it lingers for an additional while just in case). It is just an example of course. The idea is that the extended attributes keep information about the file close to the file itself. I'm probably going to have an additional layer on top if it, checking SELinux contexts and automatically identifying expiration dates based on their last modification time. Setting the expiration dates manually after creating the files is prone to be forgotten after a while. And perhaps introduce the flexibility of setting an user.expire_after attribute is well, telling that the file can be removed if it hasn't been touched (modification time) in at least XX number of days.","tags":"Free Software","url":"https://blog.siphos.be/2013/06/using-extended-attributes-for-custom-information/","loc":"https://blog.siphos.be/2013/06/using-extended-attributes-for-custom-information/"},{"title":"Hacking java bytecode with dhex","text":"I found myself in a weird situation: a long long time ago, I wrote a java application that I didn't touch nor ran for a few years. Today, I found it on a backup and wanted to run it again (its a graphical application for generating HTML pages). However, it failed in a particular feature. Not with an exception or stack trace, just functionally. Now, I have the source code at hand, so I look into the code and find the logical error. Below is a snippet of it: if (myHandler != null) { int i = startValue + maxRange; for (int j = endValue; j > i; j--) { ... (do some logic) } } It doesn't matter what the code is supposed to do, but from what I can remember, I shouldn't be adding maxRange to the i variable (yet - as I do that later in the code). But instead of setting up the java development environment, emerging the IDE etc. I decided to just edit the class file directly using dhex (a wonderful utility I recently discovered) because doing things the hard way is sometimes fun as well. So I ran javap -c MyClass to get some java bytecode information from the method, which gives me: 8: ifnull 116 11: iload_2 12: iload_3 13: iadd 14: istore 5 16: iload_2 17: istore 6 19: iload 6 21: iload 5 23: if_icmpge 106 I know lines 11 and 12 is about pushing the 2nd and 3rd arguments of the function (which are startValue and maxRange ) to the stack to add them (line 13). To remove the third argument, I can change this opcode from 1d (iload_3) to 03 (iconst_0). This way, zero is added and the code itself just continues as needed. And for some reason, that seems to be the only mistake I made then because the application now works flawlessly. Hacking is fun.","tags":"Misc","url":"https://blog.siphos.be/2013/06/hacking-java-bytecode-with-dhex/","loc":"https://blog.siphos.be/2013/06/hacking-java-bytecode-with-dhex/"},{"title":"A SELinux policy for incron: finishing up","text":"After 9 posts, it's time to wrap things up. You can review the final results online ( incron.te , incron.if and incron.fc ) and adapt to your own needs if you want. But we should also review what we have accomplished so far... We built the start of an entire policy for a daemon (the inotify cron daemon) for two main types: the daemon itself, and its management application incrontab . We defined new types and contexts, we used attributes, declared a boolean and worked with interfaces. That's a lot to digest, and yet it is only a part of the various capabilities that SELinux offers. The policy isn't complete though. We defined a type called incron_initrc_exec_t but don't really use it further. In practice, we would need to define an additional interface (probably named incron_admin ) that allows users and roles to manage incron without needing to grant this user/role sysadm_r privileges. I leave that up to you as an exercise for now, but I'll post more about admin interfaces and how to work with them on a system in the near future. We also made a few assumptions and decisions while building the policy that might not be how you yourself would want to build the policy. SELinux is a MAC system, but the policy language is very flexible. You can use an entirely different approach in policies if you want. For instance, incron supports launching the incrond as a command-line, foreground process. This could help users run incrond under their privileges for their own files - we did not consider this case in our design. Although most policies try to capture all use cases of an application, there will be cases when a policy developer did either not consider the use case or found that it infringed with his own principles on policy development (and allowed activities on a system). In Gentoo Hardened, I try to write down the principles and policies that we follow in a Gentoo Hardened SELinux Development Policy document. As decisions need to be taken, such a document might help find common consensus on how to approach SELinux policy development further, and I seriously recommend that you consider writing up a similar document yourself, especially if you are going to develop policies for a larger organization. One of the deficiencies of the current policy is that it worked with the unmodified incron version. If we would patch incron so that it could change context on executing the incrontab files of a user, then we can start making use of the default context approach (and perhaps even enhance with PAM services). In that case, user incrontabs could be launched entirely from the users' context (like user_u:user_r:user_t ) instead of the system_u:system_r:incrond_t or transitioned system_u:system_r:whatever_t contexts. Having user provided commands executed in the system context is a security risk, so in our policy we would not grant the incron_role to untrusted users - probably only to sysadm_t and even then he probably would be better with using the /etc/incron.d anyway. The downside of patching code however is that this is only viable if upstream wants to support this - otherwise we would need to maintain the patches ourselves for a long time, creating delays in releases (upstream released a new version and we still need to reapply and refactor patches) and removing precious (human) resources from other, Gentoo Hardened/SELinux specific tasks (like bugfixing and documentation writing ;-) Still, the policy returned a fairly good view on how policies can be developed. And as I said, there are still other things that weren't discussed, such as: Build-time decisions, which can change policies based on build options of the policy. In the reference policy, this is most often used for distribution-specific choices: if Gentoo would use one approach and Redhat another, then the differences would be separated through ifdef(`distro_gentoo',`...') and ifdef(`distro_redhat',`...') calls. Some calls might only be needed if another policy is loaded. I think all calls made currently are part of base modules, so can be expected to be available at all times. But if we would need something like icecast_signal(incrond_t) , then we would need to put that call inside a optional_policy(`...') statement. Otherwise, our policy would fail to load because the icecast SELinux policy isn't loaded. We could even introduce specific statements like dontaudit or neverallow to fine-tune the policy. Note though that neverallow is a compile-time statement: it is not a way to negate allow rules: if there is one allow that would violate the neverallow , then that module just refuses to build. Furthermore, if you want to create policies to be pushed upstream to the reference policy project, you will need to look into the StyleGuide and InterfaceNaming documents as those define the order that rules should be placed and the name syntax for interfaces. I have been contributing a lot to the reference policy and I still miss a few of these, so for me they are not that obvious. But using a common style is important as it allows for simple patching, code comparison and even allows us to easily read through complex policies. If you don't want to contribute it, but still use it on your Gentoo system, you can use a simple ebuild to install the files. Create an ebuild (for instance selinux-incron ), put the three files in the files/ subdirectory, and use the following ebuild code: # Copyright 1999-2013 Gentoo Foundation # Distributed under the terms of the GNU General Public License v2 # $Header$ EAPI=\"4\" IUSE=\"\" MODS=\"incron\" BASEPOL=\"2.20130424-r1\" POLICY_FILES=\"incron.te incron.fc incron.if\" inherit selinux-policy-2 DESCRIPTION=\"SELinux policy for incron, the inotify cron daemon\" KEYWORDS=\"~amd64 ~x86\" When installed, the interface files will be published as well and can then be used by other modules (something we couldn't do in the past few posts) or by the selocal tool.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-finishing-up/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-finishing-up/"},{"title":"A SELinux policy for incron: using booleans","text":"After using a default set of directories to watch, and allowing admins to mark other types as such as well, let's consider another approach for making the policy more flexible: booleans. The idea now is that a boolean called incron_notify_non_security_files enables incrond to be notified on changes on all possible non-security related files (the latter is merely an approach, you can define other sets as well if you want, including all possible files). Booleans in SELinux policy can be generated in the incron.te file as follows: ## <desc> ## <p> ## Determine whether incron can watch all non-security ## file types ## </p> ## </desc> gen_tunable(incron_notify_non_security_files, false) With this boolean in place, the policy can be enhanced with code like the following: tunable_policy(`incron_notify_non_security_files',` files_read_non_security_files(incrond_t) files_read_all_dirs_except(incrond_t) ') This code tells SELinux that, if the incron_notify_non_security_files boolean is set (which by default is not the case), then incrond_t is able to read non security files. Let's try to watch for changes in the AIDE log directory: # tail audit.log type=AVC msg=audit(1368777675.597:28611): avc: denied { search } for pid=11704 comm=\"incrond\" name=\"log\" dev=\"dm-4\" ino=13 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_log_t tclass=dir type=AVC msg=audit(1368777675.597:28612): avc: denied { search } for pid=11704 comm=\"incrond\" name=\"log\" dev=\"dm-4\" ino=13 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_log_t tclass=dir # tail cron.log May 17 10:01:15 test incrond[11704]: access denied on /var/log/aide - events will be discarded silently # getsebool incron_notify_non_security_files incron_notify_non_security_files --> off Let's enable the boolean and try again: # setsebool incron_notify_non_security_files on Reloading the incrontab tables now works, and the notifications work as well. As you can see, once a policy is somewhat working, policy developers are considering the various \"use cases\" of an application, trying to write down policies that can be used by the majority of users, without granting too many rights automatically.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-using-booleans/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-using-booleans/"},{"title":"A SELinux policy for incron: marking types eligible for watching","text":"In the previous post we made incrond able to watch public_content_t and public_content_rw_t types. However, this is not scalable, so we might want to be able to update the policy more dynamically with additional types. To accomplish this, we will make types eligible for watching through an attribute. So how does this work? First, we create an attribute called incron_notify_type (we can choose the name we want of course) and grant incrond_t the proper rights on all types that have been assigned the incron_notify_type attribute. Then, we create an interface that other modules (or admins) can use to mark specific types eligible for watching, called incron_notify_file . This interface will assign the incron_notify_type attribute to the provided type. First, the attribute and its associated privileges: attribute incron_notify_type; ... allow incrond_t incron_notify_type:dir list_dir_perms; allow incrond_t incron_notify_type:file read_file_perms; allow incrond_t incron_notify_type:lnk_file read_lnk_file_perms; That's it. For now, this won't do much as there are no types associated with the incron_notify_type attribute, so let's change that by introducing the interface: ######################################## ## <summary> ## Make the specified type a file or directory ## that incrond can watch on. ## </summary> ## <param name=\"file_type\"> ## <summary> ## Type of the file to be allowed to watch ## </summary> ## </param> # interface(`incron_notify_file',` gen_require(` attribute incron_notify_type; ') typeattribute $1 incron_notify_type; ') That's it! If you want incrond to watch user content, one can now do something like: incron_notify_file(home_root_t) incron_notify_file(user_home_dir_t) incron_notify_file(user_home_t) Moreover, we can now also easily check what additional types have been marked as such: $ seinfo -aincron_notify_type -x incron_notify_type user_home_dir_t user_home_t home_root_t This attribute approach is commonly used for such setups and is becoming more and more a \"standard\" approach. In the next post, we'll cover a boolean-triggered approach where incrond will be eligible for watching all (non-security) content.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/"},{"title":"A SELinux policy for incron: default set","text":"I finished the last post a bit with a cliffhanger as incrond is still not working properly, and we got a few denials that needed to be resolved; here they are again for your convenience: type=AVC msg=audit(1368734110.912:28353): avc: denied { getattr } for pid=9716 comm=\"incrond\" path=\"/home/user/test2\" dev=\"dm-0\" ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir type=AVC msg=audit(1368734110.913:28354): avc: denied { read } for pid=9716 comm=\"incrond\" name=\"test2\" dev=\"dm-0\" ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir The permission we are looking for here is userdom_list_user_home_content , but this is only for when we want to watch a user home directory. What if we want to watch a server upload directory? Or a cache directory? We might need to have incrond have the proper accesses on all directories. But then again, all does sound a bit... much, doesn't it? So let's split it up in three waves: The incrond_t domain will support a minimal set of types that it can watch, based on common approaches I will introduce an interface that allows other modules to mark specific types as being \"watch-worthy\" A boolean will be set to allow incrond_t to watch a very large set of types (just in case the admin trusts it sufficiently) Let's first consider a decent minimal set. Within most SELinux policies, two types are often used for public access (or for uploading of data). These types are public_content_t and public_content_rw_t , and is used for instance for FTP definitions (upload folders), HTTP servers and such. So we introduce the proper rights to watch that data. There is an interface available called miscfiles_read_public_files but let's first see if that interface isn't too broad (after all, watching might not be the same as reading). # This is only to temporarily check if the rights of the interface are too broad or not # You can set this using \"selocal\" or in a module (in which case you'll need to 'require' # the two types) allow incrond_t public_content_t:dir { read getattr }; After editing the incrontab to watch a directory labeled with public_content_t , we now get the following: # tail cron.log May 17 08:46:12 test incrond[9716]: (user) CMD (/usr/local/bin/test) May 17 08:46:12 test incrond[11281]: cannot exec process: Operation not permitted May 17 08:46:12 test incrond[9716]: cannot send SIGCHLD token to notification pipe # tail audit.log type=AVC msg=audit(1368773172.313:28386): avc: denied { setgid } for pid=11281 comm=\"incrond\" capability=6 scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=capability type=AVC msg=audit(1368773172.314:28387): avc: denied { read } for pid=9716 comm=\"incrond\" path=\"pipe:[14027]\" dev=\"pipefs\" ino=14027 scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=fifo_file type=AVC msg=audit(1368773172.315:28388): avc: denied { write } for pid=9716 comm=\"incrond\" path=\"pipe:[14027]\" dev=\"pipefs\" ino=14027 scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=fifo_file As the incrontab is a user incrontab, we can expect incrond_t to require setuid and setgid privileges. Also, the fifo_file access is after forking (notice the difference in PID values) and most likely to communicate to the master process. So let's allow those: allow incrond_t self:capability { setuid setgid }; allow incrond_t self:fifo_file { read write }; With that set, we get the following upon triggering a file write: # tail cron.log May 17 08:52:46 test incrond[9716]: (user) CMD (/usr/local/bin/test) May 17 08:52:46 test incrond[11338]: cannot exec process: Permission denied # tail audit.log type=AVC msg=audit(1368773566.606:28394): avc: denied { read } for pid=11338 comm=\"incrond\" name=\"ngroups_max\" dev=\"proc\" ino=5711 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:sysctl_kernel_t tclass=file type=AVC msg=audit(1368773566.607:28395): avc: denied { search } for pid=11338 comm=\"incrond\" name=\"bin\" dev=\"dm-3\" ino=1048578 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:bin_t tclass=dir The ngroups_max pseudo-file (in /proc/sys/kernel ) returns the maximum number of supplementary group IDs per process, and is consulted through the initgroups() method provided by a system library, so it might make sense to allow it. For now though, I will not enable it (as reading sysctl_kernel_t exposes a lot of other system information) but I might be forced to do so later if things don't work out well. The search privilege on bin_t is needed to find the script that I have prepared ( /usr/local/bin/test ) to be executed, so I add in a corecmd_search_bin and retry. # tail cron.log May 17 09:02:55 test incrond[9716]: (user) CMD (/usr/local/bin/test) May 17 09:02:55 test incrond[11427]: cannot exec process: Permission denied # tail audit.log type=AVC msg=audit(1368774175.646:28441): avc: denied { read } for pid=11427 comm=\"incrond\" name=\"sh\" dev=\"dm-2\" ino=131454 scontext=system_u:system_r:incrond_t tcontext=root:object_r:bin_t tclass=lnk_file Still not there yet apparently. The incrond forked process wants to execute the script, but to do so it has to follow a symbolic link labeled bin_t . This is because the script points to #!/bin/sh which is a symlink to the system shell. We need to follow this link before the execution can occur; only after execution will the transition from incrond_t to system_cronjob_t be done. corecmd_read_bin_symlinks(incrond_t) With that set in the policy, the watch works, incrond properly launches the command and the command properly transitions into system_cronjob_t as we defined earlier (I check this by echo'ing the output of id -Z into a temporary file). So we are left with the (temporary) rights we granted on public_content_t . Consider the rules we had versus the rules applied with miscfiles_read_public_files : allow incrond_t public_content_t:dir { read getattr }; # miscfiles_read_public_files allow $1 { public_content_t public_content_rw_t }:dir list_dir_perms; read_files_pattern($1, { public_content_t public_content_rw_t }, { public_content_t public_content_rw_t }) read_lnk_files_pattern($1, { public_content_t public_content_rw_t }, { public_content_t public_content_rw_t }) The rights here seem to bemore than what we need. Playing around a bit with the directories reveals that incrond requires a bit more. For instance, when you create additional directories (subdirectories) and want to match multiple ones: # tail cron.log May 17 09:16:08 test incrond[11704]: access denied on /var/www/test/* - events will be discarded silently May 17 09:16:08 test incrond[11704]: cannot create watch for user user: (13) Permission denied # tail audit.log type=AVC msg=audit(1368774968.416:28504): avc: denied { search } for pid=11704 comm=\"incrond\" name=\"test\" dev=\"dm-4\" ino=1488 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=dir type=AVC msg=audit(1368774968.416:28505): avc: denied { search } for pid=11704 comm=\"incrond\" name=\"test\" dev=\"dm-4\" ino=1488 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=dir Similarly if you want to watch on a particular file: type=AVC msg=audit(1368775274.655:28552): avc: denied { getattr } for pid=11704 comm=\"incrond\" path=\"/var/www/test/testfile\" dev=\"dm-4\" ino=1709 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=file type=AVC msg=audit(1368775274.655:28553): avc: denied { read } for pid=11704 comm=\"incrond\" name=\"testfile\" dev=\"dm-4\" ino=1709 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=file So it looks like miscfiles_read_public_files isn't that bad after all. All we are left with is the access to ngroups_max . We can ignore the calls and make sure they don't show up in standard auditing using kernel_dontaudit_read_kernel_sysctls or we can allow it with kernel_read_kernel_sysctls . I'm going to take the former approach for my system, but your own idea might be different. I tested all this with user incrontabs (as those are the \"most\" advanced) but one can easily test with system incrontabs as well (placing one in /etc/incron.d ). Just be aware that incrond will take the first match and will not seek other matches. So if a system incrontab watches /var/www and another line (or user incrontab) watches /var/www/localhost/upload it is very well possible that only the /var/www watch is triggered. So right now, our incrond_t policy looks like so: ########################################### # # incrond policy # allow incrond_t self:capability { setgid setuid }; allow incrond_t incron_spool_t:dir list_dir_perms; allow incrond_t incron_spool_t:file read_file_perms; allow incrond_t self:fifo_file { read write }; allow incrond_t incrond_var_run_t:file manage_file_perms; files_pid_filetrans(incrond_t, incrond_var_run_t, file) kernel_dontaudit_read_kernel_sysctls(incrond_t) corecmd_read_bin_symlinks(incrond_t) corecmd_search_bin(incrond_t) files_search_spool(incrond_t) logging_send_syslog_msg(incrond_t) auth_use_nsswitch(incrond_t) miscfiles_read_localization(incrond_t) miscfiles_read_public_files(incrond_t) Next on the agenda is another interface to make other types \"watch-worthy\".","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-default-set/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-default-set/"},{"title":"A SELinux policy for incron: the incrond daemon","text":"With incrontab_t (hopefully) complete, let's look at the incrond_t domain. As this domain will also be used to execute the user (and system) commands provided through the incrontabs, we need to consider how we are going to deal with this wide range of possible permissions that it might take. One would be to make incrond_t quite powerful, and extend its privileges as we go further. But in my opinion, that's not a good way to deal with it. Another would be to support a small set of permissions, and introduce an interface that other modules can use to create a transition when incrond_t executes a script properly labeled for a transition. For instance, a domain foo_t might have an executable type foo_exec_t . Most modules support an interface similar to foo_domtrans (and foo_role if roles are applicable as well), but that assumes that the incron policy is modified every time a new target module is made available (since we then need to add the proper *_domtrans rules to the incron policy. Instead, we might want to make this something that the foo SELinux module can decide. It is that approach that we are going to take here. To do so, we will create a new interface called incron_entry , taken a bit from the cron_system_entry interface already in place for the regular cron domain (the following comes in incron.if ): ## <summary> ## Make the specified program domain ## accessible from the incrond job. ## </summary> ## <param name=\"domain\"> ## <summary> ## The type of the process to transition to ## </summary> ## </param> ## <param name=\"entrypoint\"> ## <summary> ## The type of the file used as an entrypoint to this domain ## </summary> ## </param> # interface(`incron_entry',` gen_require(` type incrond_t; ') domtrans_pattern(incrond_t, $2, $1) ') With this in place, the foo SELinux module can call incron_entry(foo_t, foo_exec_t) so that, the moment incrond_t executes a file with label foo_exec_t , the resulting process will run in foo_t . I am going to test (and I stress that it is only for testing ) this by assigning incron_entry(system_cronjob_t, shell_exec_t) , making every shell script being called run in system_cronjob_t domain (for instance in the localuser.te file that already assigned incron_role to the user_t domain. With that in place, it's time to start our iterations again. # run_init rc-service incrond start * start-stop-daemon: failed to start '/usr/sbin/incrond' [ !! ] * ERROR: incrond failed to start # tail audit.log type=AVC msg=audit(1368732494.275:28319): avc: denied { read } for pid=9282 comm=\"incrond\" name=\"localtime\" dev=\"dm-2\" ino=393663 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:locale_t tclass=file type=AVC msg=audit(1368732494.275:28320): avc: denied { create } for pid=9282 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368732494.276:28321): avc: denied { read } for pid=9282 comm=\"incrond\" name=\"incron.d\" dev=\"dm-2\" ino=394140 scontext=system_u:system_r:incrond_t tcontext=root:object_r:incron_spool_t tclass=dir type=AVC msg=audit(1368732494.276:28322): avc: denied { create } for pid=9282 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368732494.276:28323): avc: denied { create } for pid=9282 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket Ignoring the unix_dgram_socket for now, we need to allow incrond_t to read locale information, and to read the files in the /var/spool/incron location (this goes in incron.te again): ########################################### # # incrond policy # read_files_pattern(incrond_t, incron_spool_t, incron_spool_t) files_search_spool(incrond_t) miscfiles_read_localization(incrond_t) The next run fails again, with the following denials: type=AVC msg=audit(1368732806.757:28328): avc: denied { create } for pid=9419 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368732806.757:28329): avc: denied { read } for pid=9419 comm=\"incrond\" name=\"incron.d\" dev=\"dm-2\" ino=394140 scontext=system_u:system_r:incrond_t tcontext=root:object_r:incron_spool_t tclass=dir type=AVC msg=audit(1368732806.757:28330): avc: denied { create } for pid=9419 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368732806.757:28331): avc: denied { create } for pid=9419 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket So although incrond_t has search rights on the incron_spool_t directories (through the read_files_pattern ), we need to grant it list_dir_perms as well (which contains the read permission). As list_dir_perms contains search anyhow, we can just update the line with: allow incrond_t incron_spool_t:dir list_dir_perms; allow incrond_t incron_spool_t:file read_file_perms; Now the startup seems to work, but we still get denials: # run_init rc-service incrond start * Starting incrond... [ ok ] # ps -eZ | grep incrond # tail /var/log/cron.log (nothing) # tail audit.log type=AVC msg=audit(1368733443.799:28340): avc: denied { create } for pid=9551 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368733443.802:28341): avc: denied { write } for pid=9552 comm=\"incrond\" name=\"/\" dev=\"tmpfs\" ino=1970 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_run_t tclass=dir type=AVC msg=audit(1368733443.806:28342): avc: denied { create } for pid=9552 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket type=AVC msg=audit(1368733443.806:28343): avc: denied { create } for pid=9552 comm=\"incrond\" scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket Those unix_dgram_sockets are here again. But seeing that cron.log is empty, and logging_send_syslog_msg is one of the interfaces that would enable it, we might want to do just that so that we get more information about why incrond doesn't properly start. Also, it tries to write into var_run_t labeled directories, probably for its PID file, so add in a proper file transition as well as manage rights: type incrond_var_run_t; files_pid_file(incrond_var_run_t) ... allow incrond_t incrond_var_run_t:file manage_file_perms; files_pid_filetrans(incrond_t, incrond_var_run_t, file) ... logging_send_syslog_msg(incrond_t) With that in place: # run_init rc-service incrond start * Starting incrond... [ ok ] # ps -eZ | grep incron system_u:system_r:incrond_t 9648 ? 00:00:00 incrond # tail /var/log/cron.log May 16 21:51:34 test incrond[9647]: starting service (version 0.5.10, built on May 16 2013 12:11:29) May 16 21:51:34 test incrond[9648]: loading system tables May 16 21:51:34 test incrond[9648]: loading user tables May 16 21:51:34 test incrond[9648]: table for invalid user user found (ignored) May 16 21:51:34 test incrond[9648]: ready to process filesystem events # tail audit.log type=AVC msg=audit(1368733894.641:28347): avc: denied { read } for pid=9648 comm=\"incrond\" name=\"nsswitch.conf\" dev=\"dm-2\" ino=393768 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:etc_t tclass=file type=AVC msg=audit(1368733894.645:28349): avc: denied { read } for pid=9648 comm=\"incrond\" name=\"passwd\" dev=\"dm-2\" ino=394223 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:etc_t tclass=file It looks like we're getting there. Similar as with incrontab we allow auth_use_nsswitch as well, and then get: # tail cron.log May 16 21:55:10 test incrond[9715]: starting service (version 0.5.10, built on May 16 2013 12:11:29) May 16 21:55:10 test incrond[9716]: loading system tables May 16 21:55:10 test incrond[9716]: loading user tables May 16 21:55:10 test incrond[9716]: loading table for user user May 16 21:55:10 test incrond[9716]: access denied on /home/user/test2 - events will be discarded silently May 16 21:55:10 test incrond[9716]: cannot create watch for user user: (13) Permission denied May 16 21:55:10 test incrond[9716]: ready to process filesystem events # tail audit.log type=AVC msg=audit(1368734110.912:28353): avc: denied { getattr } for pid=9716 comm=\"incrond\" path=\"/home/user/test2\" dev=\"dm-0\" ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir type=AVC msg=audit(1368734110.913:28354): avc: denied { read } for pid=9716 comm=\"incrond\" name=\"test2\" dev=\"dm-0\" ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir What happens is that incrond read the (user) crontab, found that it had to \"watch\" /home/user/test2 but fails because SELinux doesn't allow it to do so. We could just allow that, but we might do it a bit better by looking into what we want it to do in a flexible manner... next time ;-)","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/"},{"title":"A SELinux policy for incron: new types and transitions","text":"So I've shown the iterative approach used to develop policies. Again, please be aware that this is my way of developing policies, other policy developers might have a different approach. We were working on the incrontab command, so let's continue with trying to create a new user incrontab: $ incrontab -e cannot create temporary file: Permission denied # tail audit.log type=AVC msg=audit(1368709633.285:28211): avc: denied { setgid } for pid=8159 comm=\"incrontab\" capability=6 scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability type=AVC msg=audit(1368709633.285:28212): avc: denied { setuid } for pid=8159 comm=\"incrontab\" capability=7 scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability type=AVC msg=audit(1368709633.287:28213): avc: denied { search } for pid=8159 comm=\"incrontab\" name=\"/\" dev=\"tmpfs\" ino=3927 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:tmp_t tclass=dir The requests for the setuid and setgid capabilities are needed for the application to safely handle the user incrontabs. Note that SELinux does not \"remove\" the setuid bit on the binary itself, but does govern the related capabilities. Since this is required, we will add these capabilities to the policy. We also notice that incrontab searched in the /tmp location. allow incrontab_t self:capability { setuid setgid }; ... files_search_tmp(incrontab_t) In the next round of iteration, we notice the same error message with the following denial: type=AVC msg=audit(1368728433.521:28215): avc: denied { write } for pid=8913 comm=\"incrontab\" name=\"/\" dev=\"tmpfs\" ino=3927 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:tmp_t tclass=dir It is safe to assume here that the process wants to create a temporary file (if it is a directory, we will find out later and can adapt). But when temporary files are created, we better make those files a specific type, like incrontab_tmp_t . So we define that on top of the policy: type incrontab_tmp_t; files_tmp_file(incrontab_tmp_t) Also, we need to allow the incrontab_t domain write privileges into the tmp_t labeled directory, but with an automatic file transition towards incrontab_tmp_t for every file written. This is done through the files_tmp_filetrans method: files_tmp_filetrans(incrontab_t, incrontab_tmp_t, file) What this sais is that, if a domain incrontab_t wants to create a file inside tmp_t , then this file is automatically labeled incrontab_tmp_t . With SELinux, you can make this more precise: if you know what the file name would be, then you can add that as a fourth argument. However, this does not seem necessary now since we definitely want all files created in tmp_t to become incrontab_tmp_t . All that rests us is to allow incrontab to actually manage those files: allow incrontab_t incrontab_tmp_t:file manage_file_perms; With those in place, let's look at the outcome: $ incrontab -e editor finished with error: No such file or directory # tail audit.log type=AVC msg=audit(1368729268.465:28217): avc: denied { search } for pid=8981 comm=\"incrontab\" name=\"bin\" dev=\"dm-3\" ino=524289 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:bin_t tclass=dir Considering that here, incrontab is going to launch the users $EDITOR application to allow him (or her) to create an incrontab, we need to allow incrontab_t not only search privileges inside bin_t directories, but also execute rights: corecmd_exec_bin(incrontab_t) . We choose here to execute the editor inside the existing domain ( incrontab_t ) instead of creating a different domain for the editor for the following reasons: If we would create a separate domain for the editor, the editor would eventually need to have major permissions, depending on when it is used. Editors can be used to modify the sudoers files, passwd files, the /etc/selinux/config file, etc. Instead, it makes much more sense to just be able to launch the editor in the current domain (which is much more confined to its specific purpose) The additional privileges needed to launch the editor are usually very slim, or even nonexistent. It generally only makes sense if, by executing it, the existing domain would need many more privileges, because then a new (confined) domain keeps the privileges for the current domain low. Let's see if things work now: $ incrontab -e (Editor opened, so I added in an incrontab line. Upon closing:) cannot move temporary table: Permission denied # tail audit.log type=AVC msg=audit(1368729825.673:28237): avc: denied { dac_read_search } for pid=9030 comm=\"incrontab\" capability=2 scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability type=AVC msg=audit(1368729825.673:28237): avc: denied { dac_override } for pid=9030 comm=\"incrontab\" capability=1 scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability From a quick look through ps , I notice that the application runs as the user (luckily, otherwise I could use the editor to escape and get a root shell) after which it tries to do something. Of course, it makes sense that it wants to move this newly created incrontab file somewhere in /var/spool/incron so we grant it the permission to dac_read_search (which is lower than dac_override as explained before ): allow incrontab_t self:capability { dac_read_search setuid setgid }; On to the next failure: $ incrontab -e cannot move temporary table: Permission denied # tail audit.log type=AVC msg=audit(1368730155.706:28296): avc: denied { write } for pid=9088 comm=\"incrontab\" name=\"incron\" dev=\"dm-4\" ino=19725 scontext=user_u:user_r:incrontab_t tcontext=root:object_r:incron_spool_t tclass=dir Now the application wants to write this file there. Now remember we already have search_dir_perms permissions into incron_spool_t ? We need to expand those with read/write permissions into the directory, and manage permissions on files (manage because users should be able to create, modify and delete their files). These two permissions are combined in the manage_files_pattern interface, and makes the search one obsolete: manage_files_pattern(incrontab_t, incron_spool_t, incron_spool_t) $ incrontab -e ... table updated Finally! And looking at the other options in incrontab , it seems that the policy for incrontab_t is finally complete, and looks like so: ########################################### # # incrontab policy # allow incrontab_t self:capability { setuid setgid dac_read_search }; manage_files_pattern(incrontab_t, incron_spool_t, incron_spool_t) allow incrontab_t incrontab_tmp_t:file manage_file_perms; files_tmp_filetrans(incrontab_t, incrontab_tmp_t, file) corecmd_exec_bin(incrontab_t) domain_use_interactive_fds(incrontab_t) files_search_spool(incrontab_t) files_search_tmp(incrontab_t) auth_use_nsswitch(incrontab_t) userdom_use_user_terminals(incrontab_t) Next on the agenda: the incrond_t domain.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-new-types-and-transitions/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-new-types-and-transitions/"},{"title":"A SELinux policy for incron: basic set for incrontab","text":"Now that our regular user is allowed to execute incrontab , let's fire it up and look at the denials to build up the policy. $ incrontab --help That doesn't show much does it? Well, if you look into the audit.log (or avc.log ) file, you'll notice a lot of denials. If you are developing a policy, it is wise to clear the entire log and reproduce the \"situation\" so you get a proper idea of the scope. # cd /var/log/audit # > audit.log # tail -f audit.log | grep AVC Now let's run incrontab --help again and look at the denials: type=AVC msg=audit(1368707274.429:28180): avc: denied { read write } for pid=7742 comm=\"incrontab\" path=\"/dev/tty2\" dev=\"devtmpfs\" ino=1042 scontext=user_u:user_r:incrontab_t tcontext=user_u:object_r:user_tty_device_t tclass=chr_file type=AVC msg=audit(1368707274.429:28180): avc: denied { use } for pid=7742 comm=\"incrontab\" path=\"/dev/tty2\" dev=\"devtmpfs\" ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd type=AVC msg=audit(1368707274.429:28180): avc: denied { use } for pid=7742 comm=\"incrontab\" path=\"/dev/tty2\" dev=\"devtmpfs\" ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd type=AVC msg=audit(1368707274.429:28180): avc: denied { use } for pid=7742 comm=\"incrontab\" path=\"/dev/tty2\" dev=\"devtmpfs\" ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd You can start piping this information into audit2allow to generate policy statements, but I personally prefer not to use audit2allow for building new policies. For one, it is not intelligent enough to deduce if a denial should be fixed by allowing it, or by relabeling or even by creating a new type. Instead, it always grants it. Second, it does not know if a denial is cosmetic (and thus can be ignored) or not. This latter is also why I don't run domains in permissive mode to see the majority of denials first and to build from those: you might see denials that are actually never triggered when running in enforcing mode. So let's look at the access to /dev/tty2 . Given that this is a user application where we expect output to the screen, we want to grant it the proper access. With sefindif as documented before, we can look for the proper interfaces we need. I look for user_tty_device_t with rw (commonly used for read-write): $ sefindif user_tty_device_t.*rw system/userdomain.if: template(`userdom_base_user_template',` system/userdomain.if: allow $1_t user_tty_device_t:chr_file { setattr rw_chr_file_perms }; system/userdomain.if: interface(`userdom_use_user_ttys',` system/userdomain.if: allow $1 user_tty_device_t:chr_file rw_term_perms; system/userdomain.if: interface(`userdom_use_user_terminals',` system/userdomain.if: allow $1 user_tty_device_t:chr_file rw_term_perms; system/userdomain.if: interface(`userdom_dontaudit_use_user_terminals',` system/userdomain.if: dontaudit $1 user_tty_device_t:chr_file rw_term_perms; system/userdomain.if: interface(`userdom_dontaudit_use_user_ttys',` system/userdomain.if: dontaudit $1 user_tty_device_t:chr_file rw_file_perms; Two of these look interesting: userdom_use_user_ttys and userdom_use_user_terminals . Looking at the API documentation (or the rules defined therein using seshowif ) reveals that userdom_use_user_terminals is needed if you also want the application to work when invoked through a devpts terminal, which is probably also something our user(s) want to do, so we'll add that. The second one - using the file descriptor that has the getty_t context - is related to this, but not granted through the userdom_use_user_ttys . We could grant getty_use_fds but my experience tells me that domain_use_interactive_fds is more likely to be needed: the application inherits and uses a file descriptor currently owned by getty_t but it could be from any of the other domains that has such file descriptors. For instance, if you grant the incron_role to sysadm_r , then a user that switched roles through newrole will see denials for using a file descriptor owned by newrole_t . Experience is an important aspect in developing policies. If you would go through with getty_use_fds it would work as well, and you'll probably hit the above mentioned experience later when you try the application through a few different paths (such as within a screen session or so). When you think that the target context (in this case getty_t ) could be a placeholder (so other types are likely to be needed as well), make sure you check which attributes are assigned to the type: # seinfo -tgetty_t -x getty_t privfd mcssetcats mlsfileread mlsfilewrite application_domain_type domain Of the above ones, privfd is the important one: $ sefindif privfd.*use kernel/domain.if: interface(`domain_use_interactive_fds',` kernel/domain.if: allow $1 privfd:fd use; kernel/domain.if: interface(`domain_dontaudit_use_interactive_fds',` kernel/domain.if: dontaudit $1 privfd:fd use; So let's update incron.te accordingly: ... type incron_spool_t; files_type(incron_spool_t) ########################################### # # incrontab policy # userdom_use_user_terminals(incrontab_t) domain_use_interactive_fds(incrontab_t) Rebuild the policy and load it in memory. If we now run incrontab we get the online help as we expected. Let's now look at the currently installed incrontabs (there shouldn't be any of course): $ incrontab -l cannot determine current user In the denials, we notice: type=AVC msg=audit(1368708632.060:28192): avc: denied { create } for pid=7968 comm=\"incrontab\" scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=unix_stream_socket type=AVC msg=audit(1368708632.060:28194): avc: denied { read } for pid=7968 comm=\"incrontab\" name=\"nsswitch.conf\" dev=\"dm-2\" ino=393768 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:etc_t tclass=file type=AVC msg=audit(1368708632.062:28196): avc: denied { read } for pid=7968 comm=\"incrontab\" name=\"passwd\" dev=\"dm-2\" ino=394223 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:etc_t tclass=file Let's first focus on nsswitch.conf and passwd . Although both require read access to etc_t files, it might be wrong to just add in files_read_etc (which is what audit2allow is probably going to suggest). For nsswitch, there is a special interface available: auth_use_nsswitch . It is very, very likely that you'll need this one, especially if you want to share the policy with others who might not have all of the system databases in local files (as etc_t files). ... domain_use_interactive_fds(incrontab_t) auth_use_nsswitch(incrontab_t) Let's retry: $ incrontab -l cannot read table for 'user': Permission denied # tail audit.log type=AVC msg=audit(1368708893.260:28199): avc: denied { search } for pid=7997 comm=\"incrontab\" name=\"spool\" dev=\"dm-4\" ino=20 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:var_spool_t tclass=dir So we need to grant search privileges on var_spool_t . This is offered through files_search_spool . Add it to the policy, rebuild and retry. $ incrontab -l cannot read table for 'user': Permission denied # tail audit.log type=AVC msg=audit(1368709146.426:28201): avc: denied { search } for pid=8046 comm=\"incrontab\" name=\"incron\" dev=\"dm-4\" ino=19725 scontext=user_u:user_r:incrontab_t tcontext=root:object_r:incron_spool_t tclass=dir For this one, no interface exists yet. We might be able to create one for ourselves, but as long as other domains don't need it, we can just add it locally in our policy: allow incrontab_t incron_spool_t:dir search_dir_perms; Adding raw allow rules in a policy is, according to the refpolicy styleguide , only allowed if the policy module defines both the source and the destination type of the rule. If you look into other policies you might also find that you can use the search_dirs_patter call. However, that one only makes sense if you need to do this on top of another directory - just look at the definition of search_dirs_pattern . So with this permission set, let's retry. $ incrontab -l no table for user Great, we have successfully updated the policy until the commands worked. In the next post, we'll enhance it even further while creating new incrontabs.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/"},{"title":"A SELinux policy for incron: our first interface","text":"The next step after having a basic skeleton is to get incrontab running. We know however that everything invoked from the main daemon will be running with the rights of the daemon context (unless we would patch the source code, but that is beyond the scope of this set of posts). As a result, we probably do not want everyone to be able to launch commands through this application. What we want to do is to limit who can invoke incrontab and, as such, limit who can decide what is invoked through incrond . First of all, we define a role attribute called incrontab_roles . Every role that gets this attribute assigned will be able to transition to the incrontab_t domain. We can accomplish this by editing the incron.te file: policy_module(incron, 0.2) # Declare the incrontab_roles attribute attribute_role incrontab_roles; ... type incrontab_t; type incrontab_exec_t; application_domain(incrontab_t, incrontab_exec_t) # Allow incrontab_t for all incrontab_roles role incrontab_roles types incrontab_t; Next, we need something where we can allow user domains to call incrontab. This will be done through an interface. Let's look at incron.if with one such interface in it: the incron_role interface. ## inotify-based cron-like daemon ######################################### ## <summary> ## Role access for incrontab ## </summary> ## <param name=\"role\"> ## <summary> ## Role allowed access. ## </summary> ## </param> ## <param name=\"domain\"> ## <summary> ## User domain for the role. ## </summary> ## </param> # interface(`incron_role',` gen_require(` attribute_role incrontab_roles; type incrontab_exec_t, incrontab_t; ') roleattribute $1 incrontab_roles; domtrans_pattern($2, incrontab_exec_t, incrontab_t) ps_process_pattern($2, incrontab_t) allow $2 incrontab_t:process signal; ') The comments in the file are somewhat special: if the comments start with two hashes ( ## ) then it is taken into account while building the policy documentation in /usr/share/doc/selinux-base-* . The interface itself, incron_role , grants a user role and domain the necessary privileges to transition to the incrontab_t domain as well as read process information (as used through ps , hence the name of the pattern being ps_process_pattern ) and send a standard signal to it. Most of the time, you can use signal_perms here but from looking at the application we see that the application is setuid root, so we don't want to grant too many privileges by default if they are not needed. With this interface file created, we can rebuild the module and load it. # make -f /usr/share/selinux/strict/include/Makefile incron.pp # semodule -i incron.pp But how to assign this interface to users? Well, what we want to do is something like the following: incron_role(user_r, user_t) When interfaces are part of the policy provided by the distribution, the definitions of it are stored in the proper location and you can easily add it. For instance, in Gentoo, if you want to allow the user_r role and user_t domain the cron_role access (and assuming it doesn't have so already), then you can call selocal as follows: # selocal -a \"cron_role(user_r, user_t)\" -c \"Granting user_t cron access\" -Lb However, because the interface is currently not known yet, we need to create a second small policy that does this. Create a file (called localuser.te or so) with the following content: policy_module(localuser, 0.1) gen_require(` type user_t; role user_r; ') incron_role(user_r, user_t) Now build the policies and load them. We'll now just build and load all the policies in the current directory (which will be the incron and localuser ones): # make -f /usr/share/selinux/strict/include/Makefile # semodule -i *.pp You can now verify that the user is allowed to transition to the incrontab_t domain: # seinfo -ruser_r -x | grep incron incrontab_t # sesearch -s user_t -t incrontab_exec_t -AdCTS Found 1 semantic av rules: allow user_t incrontab_exec_t : file { read getattr execute open } ; Found 1 semantic te rules: type_transition user_t incrontab_exec_t : process incrontab_t; Great, let's get to our first failure to resolve... in the next post ;-)","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-our-first-interface/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-our-first-interface/"},{"title":"A SELinux policy for incron: the basic skeleton","text":"So, in the previous post I talked about incron and why I think moving it into the existing cron policy would not be a good idea. It works, somewhat, but is probably not that future-proof. So we're going to create our own policy for it. In SELinux, policies are generally written through 3 files: a type enforcement file that contains the SELinux rules applicable to the domain(s) related to the application (in our example, incron ) a file context file that tells the SELinux utilities how the files and directories offered by the application should be labeled an interface definition file that allows other SELinux policy modules to gain rights offered through the (to be written) incron policy We now need to create a skeleton for the policy. This skeleton will define the types related to the application. Such types can be the domains for the processes (the context of the incrond and perhaps also incrontab applications), the contexts for the directories (if any) and files, etc. So let's take a look at the content of the incron package. On Gentoo, we can use qlist incron for this. In the output of qlist , I added comments to show you how contexts can be (easily) deduced. # Application binary for managing user crontabs. We want to give this a specific # context because we want the application (which will manage the incrontabs in # /var/spool/incron) in a specific domain /usr/bin/incrontab ## incrontab_exec_t # General application information files, do not need specific attention # (the default context is fine) /usr/share/doc/incron-0.5.10/README.bz2 /usr/share/doc/incron-0.5.10/TODO.bz2 /usr/share/doc/incron-0.5.10/incron.conf.example.bz2 /usr/share/doc/incron-0.5.10/CHANGELOG.bz2 /usr/share/man/man8/incrond.8.bz2 /usr/share/man/man5/incron.conf.5.bz2 /usr/share/man/man5/incrontab.5.bz2 /usr/share/man/man1/incrontab.1.bz2 # Binary for the incrond daemon. This definitely needs its own context, since # it will be launched from an init script and we do not want it to run in the # initrc_t domain. /usr/sbin/incrond ## incrond_exec_t # This is the init script for the incrond daemon. If we want to allow # some users the rights to administer incrond without needing to grant # those users the sysadm_r role, we need to give this file a different # context as well. /etc/init.d/incrond ## incrond_initrc_exec_t With this information at hand, and the behavior of the application we know from the previous post, can lead to the following incron.fc file, which defines the file contexts for the application. /etc/incron.d(/.*)? gen_context(system_u:object_r:incron_spool_t,s0) /etc/rc\\.d/init\\.d/incrond -- gen_context(system_u:object_r:incrond_initrc_exec_t,s0) /usr/bin/incrontab -- gen_context(system_u:object_r:incrontab_exec_t,s0) /usr/sbin/incrond -- gen_context(system_u:object_r:incrond_exec_t,s0) /var/spool/incron(/.*)? gen_context(system_u:object_r:incron_spool_t,s0) The syntax of this file closely follows the syntax that semanage fcontext takes - at least for the regular expressions in the beginning. The last column is specifically for policy development to generate a context based on the policies' requirements: an MCS/MLS enabled policy will get the trailing sensitivity with it, but when MCS/MLS is disabled then it is dropped. The middle column is to specify if the label should only be set on regular files ( -- ), directories ( -d ), sockets ( -s ), symlinks ( -l ), etc. If it is omitted, it matches whatever class the path matches. The second file needed for the skeleton is the incron.te file, which would look like so. I added in inline comments here to explain why certain lines are prepared, but generally this is omitted when the policy is upstreamed . policy_module(incron, 0.1) # The above line declares that this file is a SELinux policy file. Its name # is incron, so the file should saved as incron.te # First, we declare the incrond_t domain, used for the \"incrond\" process. # Because it is launched from an init script, we tell the policy that # incrond_exec_t (the context of incrond), when launched from init, should # transition to incrond_t. # # Basically, the syntax here is: # type # type # type incrond_t; type incrond_exec_t; init_daemon_domain(incrond_t, incrond_exec_t) # Next we declare that the incrond_initrc_exec_t is an init script context # so that init can execute it (remember, SELinux is a mandatory access control # system, so if we do not tell that init can execute it, it won't). type incrond_initrc_exec_t; init_script_file(incrond_initrc_exec_t) # We also create the incrontab_t domain (for the \"incrontab\" application), which # is triggered through the incrontab_exec_t labeled file. This again follows a bit # the syntax as we used above, but now the interface call is \"application_domain\". type incrontab_t; type incrontab_exec_t; application_domain(incrontab_t, incrontab_exec_t) # Finally we declare the spool type as well (incron_spool_t) and tell SELinux that # it will be used for regular files. type incron_spool_t; files_type(incron_spool_t) Knowing which interface calls, like init_daemon_domain and application_domain , we should use is not obvious at first. Most of this can be gathered from existing policies. Other frequently occurring interfaces to be used immediately at the skeleton side are (examples for a foo_t domain): logging_log_file(foo_log_t) to inform SELinux that the context is used for logging purposes. This allows generic log-related daemons to do \"their thing\" with the file. files_tmp_file(foo_tmp_t) to identify the context as being used for temporary files files_tmpfs_file(foo_tmpfs_t) for tmpfs files (which could be shared memory) files_pid_file(foo_var_run_t) for PID files (and other run metadata files) files_config_file(foo_conf_t) for configuration files (often within /etc ) files_lock_file(foo_lock_t) for lock files (often within /run/lock ) We might be using these later as we progress with the policy (for instance, the PID file is a very high candidate for needing to be included). However, with the information currently at hand, we have our first policy module ready for building. Save the type enforcement rules in incron.te and the file contexts in incron.fc and you can then build the SELinux policy: # make -f /usr/share/selinux/strict/include/Makefile incron.pp # semodule -i incron.pp On Gentoo, you can then relabel the files and directories offered through the package using rlpkg : # rlpkg incron Next is to start looking at the incrontab application.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/"},{"title":"A SELinux policy for incron: what does it do?","text":"In this series of posts, we'll go through the creation of a SELinux policy for incron , a simple inotify based cron-like application. I will talk about the various steps that I would take in the creation of this policy, and give feedback when certain decisions are taken and why. At the end of the series, we'll have a hopefully well working policy. The first step in developing a policy is to know what the application does and how/where it works. This allows us to check if its behavior matches an existing policy (and as such might be best just added to this policy) or if a new policy needs to be written. So, what does incron do? From the documentation, we know that incron is a cron-like application that, unlike cron, works with file system notification events instead of time-related events. Other than that, it uses a similar way of working: A daemon called incrond is the run-time application that reads in the incrontab files and creates the proper inotify watches. When a watch is triggered, it will execute the matching rule. The daemon looks at two definitions (incrontabs): one system-wide (in /etc/incron.d ) and one for users (in /var/spool/incron ). The user tabfiles are managed through incrontab (the command) Logging is done through syslog User commands are executed with the users' privileges (so the application calls setuid() and setgid() ) With this, one can create a script to be executed when a file is uploaded (or deleted) to/from a file server, or when a process coredump occurred, or whatever automation you want to trigger when some file system event occurred. Events are plenty and can be found in /usr/include/sys/inotify.h . So, with this information, it is safe to assume that we might be able to push incron in the existing cron policy. After all, it defines the contexts for all these and probably doesn't need any additional tweaking. And this seems to work at first, but a few tests reveal that the behavior is not that optimal. # chcon -t crond_exec_t /usr/sbin/incrond # chcon -t crontab_exec_t /usr/bin/incrontab # chcon -R -t system_cron_spool_t /etc/incron.d # chcon -t cron_log_t /var/log/cron.log # chcon -R -t cron_spool_t /var/spool/incron System tables work somewhat, but all commands are executed in the crond_t domain, not in a system_cronjob_t or related domain. User tables fail when dealing with files in the users directories, since these too run in crond_t and thus have no read access to the user home directories. The problems we notice come from the fact that the application is very simple in its code: it is not SELinux-aware (so it doesn't change the runtime context) as most cron daemons are, and when it changes the user id it does not call PAM, so we cannot trigger pam_selinux.so to handle context changes either. As a result, the entire daemon keeps running in crond_t . This is one reason why a separate domain could be interesting: we might want to extend the rights of the daemon domain a bit, but don't want to extend these rights to the other cron daemons (who also run in crond_t ). Another reason is that the cron policy has a few booleans that would not affect the behavior at all, making it less obvious for users to troubleshoot. As a result, we'll go for the separate policy instead - which will be for the next post.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-what-does-it-do/","loc":"https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-what-does-it-do/"},{"title":"Why oh why does a process run in unlabeled_t?","text":"If you notice that a process is running in the unlabeled_t domain, the first question to ask is how it got there. Well, one way is to have a process running in a known domain, like screen_t , after which the SELinux policy module that provides this domain is removed from the system (or updated and the update does not contain the screen_t definition anymore): test ~ # ps -eZ | grep screen root:sysadm_r:sysadm_screen_t 5047 ? 00:00:00 screen test ~ # semodule -r screen test ~ # ps -eZ | grep screen system_u:object_r:unlabeled_t 5047 ? 00:00:00 screen In permissive mode, this will be visible easily; in enforcing mode, the domains you are running in might not be allowed to do anything with unlabeled_t files, directories and processes, so ps might not show it even though it still exists: test audit # ps -eZ | grep 5047 test audit # ls -dZ /proc/5047 ls: cannot access /proc/5047: Permission denied test audit # tail audit.log | grep unlabeled type=AVC msg=audit(1368698097.494:27806): avc: denied { getattr } for pid=4137 comm=\"bash\" path=\"/proc/5047\" dev=\"proc\" ino=6677 scontext=root:sysadm_r:sysadm_t tcontext=system_u:object_r:unlabeled_t tclass=dir Notice that, if you reload the module, the process becomes visible again. That is because the process context itself ( screen_t ) is retained, but because the policy doesn't know it anymore, it shows it as unlabeled_t . Basically, the moment the policy doesn't know how a label would be (should be), it uses unlabeled_t . The SELinux policy then defines how this unlabeled_t domain is handled. Processes getting into unlabeled_t is not that common though as there is no supported transition to it. The above one is one way that this still can occur.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/why-oh-why-does-a-process-run-in-unlabeled_t/","loc":"https://blog.siphos.be/2013/05/why-oh-why-does-a-process-run-in-unlabeled_t/"},{"title":"A simple IPv6 setup","text":"For internal communication between guests on my workstation, I use IPv6 which is set up using the Router Advertisement \"feature\" of IPv6. The tools I use are dnsmasq for DNS/DHCP and router advertisement support, and dhcpcd as client. It might be a total mess (grew almost organically until it worked), but as far as I'm concerned, it is working... and that is all that matters (for now). I'll have to look deeper into the IPv6 stuff to understand it all better though. On the client side, dhcpcd is ran with the following options: dhcpcd_eth0=\"-t 5 -L --ipv6ra_own\" I had to enable --ipv6ra_own to get it to obtain its global address, otherwise it only got its link local one ( fe80:: something). I also added a hook into /lib/dhcpcd/dhcpcd-hooks to get it to trigger a hostname update for IPv6. $ cat 28-set-ip6-address if $ifup; then export new_ip_address=${ra1_prefix%%/64}; fi SELinux-policy wise, I had to enable dhcpc_t to write to the hostname proc file and set the system hostname. The first one (21) is needed because of the --ipv6ra_own parameter. # selocal -l | grep dhcpc_t 21: allow dhcpc_t self:rawip_socket create_socket_perms; # dhcpclient 22: kernel_rw_kernel_sysctl(dhcpc_t) # set hostname 23: allow dhcpc_t self:capability sys_admin; # set hostname Finally, in /etc/dhcpcd.conf , I removed the nohook lookup-hostname and set the force_hostname one: #nohook lookup-hostname env force_hostname=YES On the server side, I use the following configuration of dnsmasq (snippet): dhcp-range=2001:db8:81:e2::,ra-only enable-ra dhcp-option=option6:dns-server,[2001:db8:81:e2::26b5:365b:5072] As you can see, I use the documentation prefix for now (since it is meant for internal communication only, and makes it easier to copy/paste into documentation ;-) but when I am going to use full IPv6 access to the Internet, this prefix will of course change. Finally, I enabled IPv6 forwarding on the tap0 interface because otherwise I continuously got the following messages on the clients: May 12 18:43:07 test dhcpcd[3869]: eth0: adding default route via fe80::d848:19ff:fe0d:55c2 May 12 18:43:07 test dhcpcd[3869]: eth0: fe80::d848:19ff:fe0d:55c2 is no longer a router May 12 18:43:07 test dhcpcd[3869]: eth0: deleting default route via fe80::d848:19ff:fe0d:55c2 May 12 18:43:13 test dhcpcd[3869]: eth0: fe80::d848:19ff:fe0d:55c2 is unreachable, expiring it To enable IPv6 forwarding, you can use sysctl but I added it in the script that sets up the tap0 interface: tunctl -b -u swift -t tap0 ifconfig tap0 add 2001:db8:81:e2::26b5:365b:5072/64 vde_switch --numports 16 --mod 777 --group users --tap tap0 -d echo 1 > /proc/sys/net/ipv6/conf/tap0/forwarding","tags":"Documentation","url":"https://blog.siphos.be/2013/05/a-simple-ipv6-setup/","loc":"https://blog.siphos.be/2013/05/a-simple-ipv6-setup/"},{"title":"The weird \"audit_access\" permission","text":"While writing up the posts on capabilities, one thing I had in my mind was to give some additional information on frequently occurring denials, such as the dac_override and dac_read_search capabilities, and when they are triggered. For the DAC-related capabilities, policy developers often notice that these capabilities are triggered without a real need for them. So in the majority of cases, the policy developer wants to disable auditing of this: dontaudit <somedomain> self:capability { dac_read_search dac_override }; When applications wants to search through directories not owned by the user as which the application runs, both capabilities will be checked - first the dac_read_search one and, if that is denied (it will be audited though) then dac_override is checked. If that one is denied as well, it too will be audited. That is why many developers automatically dontaudit both capability calls if the application itself doesn't really need the permission. Let's say you allow this because the application needs it. But then another issue comes up when the application checks file attributes or access permissions (which is a second occurring denial that developers come across with). Such applications use access() or faccessat() to get information about files, but other than that don't do anything with the files. When this occurs and the domain does not have read, write or execute permissions on the target, then the denial is shown even when the application doesn't really read, write or execute the file. #include <stdio.h> #include <unistd.h> int main(int argc, char ** argv) { printf(\"%s: Exists (%d), Readable (%d), Writeable (%d), Executable (%d)\\n\", argv[1], access(argv[1], F_OK), access(argv[1], R_OK), access(argv[1], W_OK), access(argv[1], X_OK)); } $ check /var/lib/logrotate.status /var/lib/logrotate.status: Exists (0), Readable (-1), Writeable (-1), Executable (-1) $ tail -1 /var/log/audit.log ... type=AVC msg=audit(1367400559.273:5224): avc: denied { read } for pid=12270 comm=\"test\" name=\"logrotate.status\" dev=\"dm-3\" ino=2849 scontext=staff_u:staff_r:staff_t tcontext=system_u:object_r:logrotate_var_lib_t tclass=file This gives the impression that the application is doing nasty stuff, even when it is merely checking permissions. One way would be to dontaudit read as well, but if the application does the check against several files of various types, that might mean you need to include dontaudit statements for various domains. That by itself isn't wrong, but perhaps you do not want to audit such checks but do want to audit real read attempts. This is what the audit_access permission is for. The audit_access permission is meant to be used only for dontaudit statements: it has no effect on the security of the system itself, so using it in allow statements has no effect. The purpose of the permission is to allow policy developers to not audit access checks without really dontauditing other, possibly malicious, attempts. In other words, checking the access can be dontaudited while actually attempting to use the access (reading, writing or executing the file) will still result in the proper denial.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/the-weird-audit_access-permission/","loc":"https://blog.siphos.be/2013/05/the-weird-audit_access-permission/"},{"title":"Commandline SELinux policy helper functions","text":"To work on SELinux policies, I use a couple of functions that I can call on the shell (command line): seshowif , sefindif , seshowdef and sefinddef . The idea behind the methods is that I want to search ( find ) for an interface ( if ) or definition ( def ) that contains a particular method or call. Or, if I know what the interface or definition is, I want to see it ( show ). For instance, to find the name of the interface that allows us to define file transitions from the postfix_etc_t label: $ sefindif filetrans.*postfix_etc contrib/postfix.if: interface(`postfix_config_filetrans',` contrib/postfix.if: filetrans_pattern($1, postfix_etc_t, $2, $3, $4) Or to show the content of the corenet_tcp_bind_http_port interface: $ seshowif corenet_tcp_bind_http_port interface(`corenet_tcp_bind_http_port',` gen_require(` type http_port_t; ') allow $1 http_port_t:tcp_socket name_bind; allow $1 self:capability net_bind_service; ') For the definitions, this is quite similar: $ sefinddef socket.*create obj_perm_sets.spt:define(`create_socket_perms', `{ create rw_socket_perms }') obj_perm_sets.spt:define(`create_stream_socket_perms', `{ create_socket_perms listen accept }') obj_perm_sets.spt:define(`connected_socket_perms', `{ create ioctl read getattr write setattr append bind getopt setopt shutdown }') obj_perm_sets.spt:define(`create_netlink_socket_perms', `{ create_socket_perms nlmsg_read nlmsg_write }') obj_perm_sets.spt:define(`rw_netlink_socket_perms', `{ create_socket_perms nlmsg_read nlmsg_write }') obj_perm_sets.spt:define(`r_netlink_socket_perms', `{ create_socket_perms nlmsg_read }') obj_perm_sets.spt:define(`client_stream_socket_perms', `{ create ioctl read getattr write setattr append bind getopt setopt shutdown }') $ seshowdef manage_files_pattern define(`manage_files_pattern',` allow $1 $2:dir rw_dir_perms; allow $1 $3:file manage_file_perms; ') I have these defined in my ~/.bashrc (they are simple functions ) and are used on a daily basis here ;-) If you want to learn a bit more on developing SELinux policies for Gentoo, make sure you read the Gentoo Hardened SELinux Development guide.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/commandline-selinux-policy-helper-functions/","loc":"https://blog.siphos.be/2013/05/commandline-selinux-policy-helper-functions/"},{"title":"Looking at the local Linux kernel privilege escalation","text":"There has been a few posts already on the local Linux kernel privilege escalation, which has received the CVE-2013-2094 ID. arstechnica has a write-up with links to good resources on the Internet, but I definitely want to point readers to the explanation that Brad Spengler made on the vulnerability. In short, the vulnerability is an out-of-bound access to an array within the Linux perf code (which is a performance measuring subsystem enabled when CONFIG_PERF_EVENTS is enabled). This subsystem is often enabled as it offers a wide range of performance measurement techniques (see its wiki for more information). You can check on your own system through the kernel configuration ( zgrep CONFIG_PERF_EVENTS /proc/config.gz if you have the latter pseudo-file available - it is made available through CONFIG_IKCONFIG_PROC ). The public exploit maps memory in userland, fills it with known data, then triggers an out-of-bound decrement that tricks the kernel into decrementing this data (mapped in userland). By looking at where the decrement occurred, the exploit now knows the base address of the array. Next, it targets (through the same vulnerability) the IDT base (Interrupt Descriptor Table) and targets the overflow interrupt vector. It increments the top part of the address that the vector points to (which is 0xffffffff, becoming 0x00000000 thus pointing to the userland), maps this memory region itself with shellcode, and then triggers the overflow. The shell code used in the public exploit modifies the credentials of the current task, sets uid/gid with root and gives full capabilities, and then executes a shell. As Brad mentions, UDEREF (an option in a grSecurity enabled kernel) should mitigate the attempt to get to the userland. On my system, the exploit fails with the following (start of) oops (without affecting the system further) when it tries to close the file descriptor returned from the syscall that invokes the decrement: [ 1926.226678] PAX: please report this to pageexec@freemail.hu [ 1926.227019] BUG: unable to handle kernel paging request at 0000000381f5815c [ 1926.227019] IP: [] sw_perf_event_destroy+0x1a/0xa0 [ 1926.227019] PGD 58a7c000 [ 1926.227019] Thread overran stack, or stack corrupted [ 1926.227019] Oops: 0002 [#4] PREEMPT SMP [ 1926.227019] Modules linked in: libcrc32c [ 1926.227019] CPU 0 [ 1926.227019] Pid: 4267, comm: test Tainted: G D 3.8.7-hardened #1 Bochs Bochs [ 1926.227019] RIP: 0010:[] [] sw_perf_event_destroy+0x1a/0xa0 [ 1926.227019] RSP: 0018:ffff880058a03e08 EFLAGS: 00010246 ... The exploit also finds that the decrement didn't succeed: test: semtex.c:76: main: Assertion 'i<0x0100000000/4' failed. A second mitigation is that KERNEXEC (also offered through grSecurity) which prevents the kernel from executing data that is writable (including userland data). So modifying the IDT would be mitigated as well. Another important mitigation is TPE - Trusted Path Execution . This feature prevents the execution of binaries that are not located in a root-owned directory and owned by a trusted group (which on my system is 10 = wheel). So users attempting to execute such code will fail with a Permission denied error, and the following is shown in the logs: [ 3152.165780] grsec: denied untrusted exec (due to not being in trusted group and file in non-root-owned directory) of /home/user/test by /home/user/test[bash:4382] uid/euid:1000/1000 gid/egid:100/100, parent /bin/bash[bash:4352] uid/euid:1000/1000 gid/egid:100/100 However, even though a nicely hardened system should be fairly immune against the currently circling public exploit, it should be noted that it is not immune against the vulnerability itself. The methods above mentioned make it so that that particular way of gaining root access is not possible, but it still allows an attacker to decrement and increment memory in specific locations so other exploits might be found to modify the system. Now out-of-bound vulnerabilities are not new. Recently (february this year), a vulnerability in the networking code also provided an attack vector to get a local privilege escalation. A mandatory access control system like SELinux has little impact on such vulnerabilities if you allow users to execute their own code. Even confined users can modify the exploit to disable SELinux (since the shell code is ran with ring0 privileges it can access and modify the SELinux state information in the kernel). Many thanks to Brad for the excellent write-up, and to the Gentoo Hardened team for providing the grSecurity PaX/TPE protections in its hardened-sources kernel.","tags":"Security","url":"https://blog.siphos.be/2013/05/looking-at-the-local-linux-kernel-privilege-escalation/","loc":"https://blog.siphos.be/2013/05/looking-at-the-local-linux-kernel-privilege-escalation/"},{"title":"Gentoo Hardened spring notes","text":"We got back together on the #gentoo-hardened chat channel to discuss the progress of Gentoo Hardened , so it's time for another write-up of what was said. Toolchain GCC 4.8.1 will be out soon, although nothing major has occurred with it since the last meeting. There is a plugin header install problem in 4.8 and its not certain that the (trivial) fix is in 4.8.1, but it certainly is inside Gentoo's release. Blueness is also (still, and hopefully for a long time ;-) maintaining the uclibc hardened related toolchain aspects. Kernel and grSecurity/PaX The further progress on the XATTR_PAX migration was put on a lower level the past few weeks due to busy, busy... very busy weeks (but this was announced and known in advance). We still need to do XATTR copying in install for packages that do pax markings before src_install() and include the user.pax XATTR patch in the gentoo-sources kernel. This will silence the errors for non-hardened users and fix the loss of XATTR markings for those packages that do pax-mark before install. The set then needs to be documented further and tested on vanilla and hardened systems. Zorry asked if a separate script can be provided for those ebuilds that directly call paxctl . These ebuilds might want to switch to the eclass, but if they need to call paxctl or similar directly (for instance because the result is immediately used for further building), a separate script or tool should be made available. Blueness will look into this. On hardened-sources , we are now with stable 2.6.32-r160, 3.2.42-r1 and 3.8.6 due to some vulnerabilities in earlier versions (in networking code). There is still some bug (nfs-related) that is fixed in 3.2.44 so that part might need a bump as well soon. SELinux The selocal command is now available for Gentoo SELinux users, allowing them to easily enhance the policy without having to maintain their own SELinux policy modules (the script is a wrapper that does all that). The setools package now also uses the SLOT'ed swig , so no more dependency breakage. On SELinux userspace and policy, both have seen a new release last month, and both are already in the Gentoo portage tree. Finally, the SELinux policy ebuilds now also call epatch_user so users can customize the policies even further without having to copy ebuilds to their overlay. Now that tar supports XATTR well, we might want to look into SELinux stages again. Jmbsvicetto did some work on that, but the builds failed during stage1. We'll look into that later. Integrity Nothing much to say, we're waiting a bit until the patches proposed by the IMA team are merged in the main kernel. Profiles Two no-multilib fixes have been applied to the hardened/amd64/no-multilib profiles. One was a QA issue and quickly resolved, the other is due to the profile stacking within Gentoo profiles, where we missed a profile and thus were missing a few masks defined in that (missed) profile. But including the profile creates a lot of duplicates again, so we are going to copy the masks across until the duplicates are resolved in the other profiles. Blueness will also clean up the experimental 13.0 directory since all hardened profiles now follow 13.0. Docs The latest changes on SELinux have been added to the Gentoo SELinux handbook. Also, I've been slowly (but surely) adding topics to the SELinux tutorials listing on the Gentoo wiki. The grSecurity 2 document is very much out of date, blueness hopes to put some time in fixing that soon. So that's about it for the short write-up. Zorry will surely post the log later on the appropriate channels. Good work done (again) by all team members!","tags":"Gentoo","url":"https://blog.siphos.be/2013/05/gentoo-hardened-spring-notes/","loc":"https://blog.siphos.be/2013/05/gentoo-hardened-spring-notes/"},{"title":"Public support channels: irc","text":"I've said it before - support channels for free software are often (imo) superior to the commercial support that you might get with vendors. And although those vendors often try to use \"modern\" techniques, I fail to see why the old, but proven/stable methods would be wrong. Consider the \"Chat with Support\" feature that many vendors have on their site. Often, these services use a webbrowser, AJAX-driven method for talking with support engineers. The problem with this that I see is that it is difficult to keep track of the feedback you got over time (unless you manually copy/paste the information), and again that it isn't public. With free software communities, we still often redirect such \"online\" support requests to IRC. Internet Relay Chat has been around for ages ( 1988 according to wikipedia) and still quite active. Gentoo has all of its support channels on the freenode IRC network: a community-driven, active #gentoo channel with often crosses the 1000 users, a #gentoo-dev development-related channel where many developers communicate, the #gentoo-hardened channel for all questions and support regarding Gentoo Hardened specifics, etc. Using IRC has many advantages. One is that logs can be kept (either individually or by the project itself) that can be queried later by the people who want to provide support (to see if questions have already been popping up, see what the common questions are for the last few days, etc.) or get support (to see if their question was already answered in the past). Of course, these logs can be made public through web interfaces quite easily. For users, such log functionality is offered through the IRC client. Another very simple, yet interesting feature is highlighting : give the set of terms for which you want to be notified (usually through a highlight and a specific notification in the client), making it easier to be on multiple channels without having to constantly follow-up on all discussions. Another advantage is that there is such a thing like \"bots\". Most Gentoo related channels do not allow active bots on the channels except for the project-approved ones (such as willikens ). These bots can provide project-specific help to users and developers alike: Give one-line information about bugs reported on bugzilla (id, assignee, status, but also the URL where the user/developer can view the bug etc.) Give meta information about a package (maintainer, herd, etc.), herd (members), GLSA details, dependency information, etc. Allow users to query if a developer is away or not Create notes (messages) for users that are not online yet but for which you know they come online later (and know their nickname or registered username) Notify when commits are made, or when tweets are sent that match a particular expression, etc. Furthermore, the IRC protocol has many features that are very interesting to use in free software communities as well. You can still do private chats (when potentially confidential data is exchanged) for instance, or even exchange files (although that is less common to use in free software communities). There is also still some hierarchy in case of abuse (channel operators can remove users from the chat or even ban them for a while) and one can even quiet a channel when for instance online team meetings are held (although using a different channel for that might be an alternative). IRC also has the advantage that connecting to the IRC channels has a very low requirement (software-wise): one can use console-only chat clients (in case users cannot get their graphical environment to work - example is irssi) or even webbrowser based ones (if one wants to chat from other systems). Even smartphones have good IRC applications, like AndChat for Android. IRC is also distributed: an IRC network consists of many interconnected servers who pass on all IRC traffic. If one node goes down, users can access a different node and continue. That makes IRC quite high-available. IRC network operators do need to try and keep the network from splitting (\"netsplit\") which occurs when one part of the distributed network gets segregated from the other part and thus two \"independent\" IRC networks are formed. When that occurs, IRC operators will try to join them back as fast as possible. I'm not going to explain the details on this - it suffices to understand that IRC is a distributed manner and thus often much more available than the \"support chat\" sites that vendors provide. So although IRC looks archaic, it is a very good match for support channel requirements.","tags":"Free Software","url":"https://blog.siphos.be/2013/05/public-support-channels-irc/","loc":"https://blog.siphos.be/2013/05/public-support-channels-irc/"},{"title":"Overriding the default SELinux policies","text":"Extending SELinux policies with additional rules is easy. As SELinux uses a deny by default approach, all you need to do is to create a policy module that contains the additional (allow) rules, load that and you're all set. But what if you want to remove some rules? Well, sadly, SELinux does not support deny rules. Once an allow rule is loaded in memory, it cannot be overturned anymore. Yes, you can disable the module itself that provides the rules, but you cannot selectively disable rules. So what to do? Generally, you can disable the module that contains the rules you want to disable, and load a custom module that defines everything the original module did, except for those rules you don't like. For instance, if you do not want the skype_t domain to be able to read/write to the video device, create your own skype-providing module ( myskype ) with the exact same content (except for the module name at the first line) as the original skype module, except for the video device: dev_read_sound(skype_t) # dev_read_video_dev(skype_t) dev_write_sound(skype_t) # dev_write_video_dev(skype_t) Load in this policy, and you now have the skype_t domain without the video access. You will get post-install failures when Gentoo pushes out an update to the policy though, since it will attempt to reload the skype.pp file (through the selinux-skype package) and fail because it declares types and attributes already provided (by myskype ). You can exclude the package from being updated, which works as long as no packages depend on it. Or live with the post-install failure ;-) But there might be a simpler approach: epatch_user . Recently, I added in support for epatch_user in the policy ebuilds. This allows users to create patches against the policy source code that we use and put them in /etc/portage/patches in the directory of the right category/package. For module patches, the working directory used is within the policy/modules directory of the policy checkout. For base, it is below the policy checkout (in other words, the patch will need to use the refpolicy/ directory base). But because of how epatch_user works, any patch taken from the base will work as it will start stripping directories up to the fourth one. This approach is also needed if you want to exclude rules from interfaces rather than from the .te file: create a small patch and put it in /etc/portage/patches for the sec-policy/selinux-base package (as this provides the interfaces).","tags":"Gentoo","url":"https://blog.siphos.be/2013/05/overriding-the-default-selinux-policies/","loc":"https://blog.siphos.be/2013/05/overriding-the-default-selinux-policies/"},{"title":"Highlevel assessment of Cdorked and Gentoo Hardened/SELinux","text":"With all the reports surrounding Cdorked , I took a look at if SELinux and/or other Gentoo Hardened technologies could reduce the likelihood that this infection occurs on your system. First of all, we don't know yet how the malware gets installed on the server. We do know that the Apache binaries themselves are modified, so the first thing to look at is to see if this risk can be reduced. Of course, using an intrusion detection system like AIDE helps, but even with Gentoo's qcheck command you can test the integrity of the files: # qcheck www-servers/apache Checking www-servers/apache-2.2.24 ... * 424 out of 424 files are good If the binary is modified, this would result in something equivalent to: Checking www-servers/apache-2.2.24 ... MD5-DIGEST: /usr/sbin/apache2 * 423 out of 424 files are good I don't know if the modified binary would otherwise work just fine, I have not been able to find exact details on the infected binary to (in a sandbox environment of course) analyze this further. Also, because we don't know how they are installed, it is not easy to know if binaries that you built yourself are equally likely to be modified/substituted or if the attack checks checksums of the binaries against a known list. Assuming that it would run, then the infecting malware would need to set the proper SELinux context on the file (if it overwrites the existing binary, then the context is retained, otherwise it gets the default context of bin_t ). If the context is wrong, then starting Apache results in: apache2: Syntax error on line 61 of /etc/apache2/httpd.conf: Cannot load /usr/lib64/apache2/modules/mod_actions.so into server: /usr/lib64/apache2/modules/mod_actions.so: cannot open shared object file: Permission denied This is because the modified binary stays in the calling domain context ( initrc_t ). If you use a targeted policy, then this will not present itself as initrc_t is an unconfined domain. But with strict policies, initrc_t is not allowed to read httpd_modules_t . Even worse, the remainder of SELinux protections don't apply anymore, since with unconfined domains, all bets are off. That is why Gentoo focuses this hard on using a strict policy. So, what if the binary runs in the proper domain? Well then, from the articles I read, the malware can do a reverse connect. That means that the domain will attempt to connect to an IP address provided by the attacker (in a specifically crafted URL). For SELinux, this means that the name_connect permission is checked: # sesearch -s httpd_t -c tcp_socket -p name_connect -ACTS Found 20 semantic av rules: allow nsswitch_domain dns_port_t : tcp_socket { name_connect } ; DT allow httpd_t port_type : tcp_socket { name_connect } ; [ httpd_can_network_connect ] DT allow httpd_t ftp_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ] DT allow httpd_t smtp_port_t : tcp_socket { name_connect } ; [ httpd_can_sendmail ] DT allow httpd_t postgresql_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ] DT allow httpd_t oracledb_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ] DT allow httpd_t squid_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ] DT allow httpd_t mssql_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ] DT allow httpd_t kerberos_port_t : tcp_socket { name_connect } ; [ allow_kerberos ] DT allow nsswitch_domain ldap_port_t : tcp_socket { name_connect } ; [ authlogin_nsswitch_use_ldap ] DT allow httpd_t http_cache_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ] DT allow httpd_t http_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ] DT allow httpd_t http_port_t : tcp_socket { name_connect } ; [ httpd_graceful_shutdown ] DT allow httpd_t mysqld_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ] DT allow httpd_t ocsp_port_t : tcp_socket { name_connect } ; [ allow_kerberos ] DT allow nsswitch_domain kerberos_port_t : tcp_socket { name_connect } ; [ allow_kerberos ] DT allow httpd_t pop_port_t : tcp_socket { name_connect } ; [ httpd_can_sendmail ] DT allow nsswitch_domain ocsp_port_t : tcp_socket { name_connect } ; [ allow_kerberos ] DT allow httpd_t gds_db_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ] DT allow httpd_t gopher_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ] So by default, the Apache ( httpd_t ) domain is allowed to connect to DNS port (to resolve hostnames). All other name_connect calls depend on SELinux booleans (mentioned after it) that are by default disabled (at least on Gentoo). Disabling hostname resolving is not really feasible, so if the attacker uses a DNS port as port that the malware needs to connect to, SELinux will not deny it (unless you use additional networking constraints). Now, the reverse connect is an interesting feature of the malware, but not the main one. The main focus of the malware is to redirect customers to particular sites that can trick the user in downloading additional (client) malware. Because this is done internally within Apache, SELinux cannot deal with this. As a user, make sure you configure your browser not to trust non-local iframes and such (always do this, not just because there is a possible threat right now). The configuration of Cdorked is a shared memory segment of Apache itself. Of course, since Apache uses shared memory, the malware embedded within will also have access to the shared memory. However, if this shared memory would need to be accessed by third party applications (the malware seems to grant read/write rights on everybody to this segment) SELinux will prevent this: # sesearch -t httpd_t -c shm -ACTS Found 2 semantic av rules: allow unconfined_domain_type domain : shm { create destroy getattr setattr read write associate unix_read unix_write lock } ; allow httpd_t httpd_t : shm { create destroy getattr setattr read write associate unix_read unix_write lock } ; Only unconfined domains and the httpd_t domain itself have access to httpd_t labeled shared memory. So what about IMA/EVM? Well, those will not help here since IMA checks for integrity of files that were modified offline . As the modification of the Apache binaries is most likely done online, IMA would just accept this. For now, it seems that a good system integrity approach is the most effective until we know more about how the malware-infected binary is written to the system in the first place (as this is better protected by MAC controls like SELinux).","tags":"Security","url":"https://blog.siphos.be/2013/05/highlevel-assessment-of-cdorked-and-gentoo-hardenedselinux/","loc":"https://blog.siphos.be/2013/05/highlevel-assessment-of-cdorked-and-gentoo-hardenedselinux/"},{"title":"SECMARK and SELinux","text":"When using SECMARK, the administrator configures the iptables or netfilter rules to add a label to the packet data structure (on the host itself) that can be governed through SELinux policies. Unlike peer labeling, here the labels assigned to the network traffic is completely locally defined. Consider the following command: # iptables -t mangle -A INPUT -p tcp --src 192.168.1.2 --dport 443 -j SECMARK --selctx system_u:object_r:myauth_packet_t With this command, packets that originate from the 192.168.1.2 host and arrive on port 443 (typically used for HTTPS traffic) are marked as myauth_packet_t . SELinux policy writers can then allow domains to receive this type of packets (or send) through the packet class: # Allow sockets with mydomain_t context to receive packets labeled myauth_packet_t allow mydomain_t myauth_packet_t:packet recv; The SELinux policy modules enable this through the corenet_sendrecv_<type>_{client,server}_packets interfaces: corenet_sendrecv_http_client_packets(mybrowser_t) # allow mybrowser_t http_client_packet_t:packet { send recv }; As a common rule, packets are marked as client packets or server packets, depending on the role of the domain . In the above example, the domain is a browser, so acts as a web client. So, it needs to send and receive http_client_packet_t . A web server on the other hand would need to send and receive http_server_packet_t . Note that the packets that are sent over the wire do not have any labels assigned to them - this is all local to the system. So even when the source and destination use SELinux with SECMARK, on the source server the packets might be labeled as http_client_packet_t whereas on the target they are seen as http_server_packet_t . As far as I know, when you want to use SECMARK, you will need to set the contexts with iptables yourself (there is no default labeling), so knowing about the above convention is important. Again, Paul Moore has more information about this.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/secmark-and-selinux/","loc":"https://blog.siphos.be/2013/05/secmark-and-selinux/"},{"title":"Peer labeling in SELinux policy","text":"Allow me to start with an important warning: I don't have much hands-on experience with the remainder of this post. Its based on the few resources I found on the Internet and a few tests done locally which I've investigated in my attempt to understand SELinux policy writing for networking stuff. So, with that out of the way, let's look into peer labeling . As mentioned in my previous post , SELinux supports some more advanced networking security features than the default socket restrictions. I mentioned SECMARK and NetLabel before, but NetLabel is actually part of the family of peer labeling technologies. With this technology approach, all participating systems in the network must support the same labeling method. NetLabel supports CIPSO ( Commerial IP Security Option ) where hosts label their network traffic to be part of a particular \"Domain of Interpretation\". The labels are used by the hosts to identify where a packet should be for. NetLabel, within Linux, is then used to translate those CIPSO labels. SELinux itself labels the incoming sockets based on the NetLabel information and the context of the listening socket, resulting in a context that is governed policy-wise through the peer class. Since this is based on the information in the packet instead of defined on the system itself, this allows remote systems to have a say in how the packets are labeled. Another peer technology is the Labeled IPSec one. In this case the labels are fully provided by the remote system. I think they are based on the security association within the IPSec setup. In both cases, in the SELinux policies, three definitions are important to keep an eye out on: interface definitions, node definitions and peer definitions. Interface definitions allow users to (mainly) set the sensitivity that is allowed to pass the interface. Using semanage interface this can be controlled by the user. One can also assign a different context to the interface - by default, this is netif_t . The permissions that are checked on the traffic is ingress (incoming) and egress (outgoing) traffic, and most policies set this through the following call (comment shows the underlying SELinux rules, where tcp_send and tcp_recv are - I think - obsolete): corenet_tcp_sendrecv_generic_if(something_t) # allow something_t netif_t:netif { tcp_send tcp_recv egress ingress }; Node definitions define which targets (nodes, which can be IP addresses or subnets) traffic meant for a particular socket is allow to originate from ( recvfrom ) or sent to ( sendto ). Again, users can define their own node types and manage them using semanage node . The default node I already covered in the previous post ( node_t ) and is allowed by most policies by default through the following call (where the tcp_send and tcp_recv are probably deprecated as well): corenet_tcp_sendrecv_generic_node(something_t) # allow something_t node_t:node { tcp_send tcp_recv sendto recvfrom }; Finally, peer definitions are based on the labels from the traffic. If the system uses NetLabel, then the target label will always be netlabel_peer_t since the workings of CIPSO are mainly (only?) mapped towards sensitivity labels (in MLS policy). As a result, SELinux always displays the peer as being netlabel_peer_t . In case of Labeled IPSec, this isn't the case as the peer label is transmitted by the peer itself. For NetLabel support, policies generally include two methods - one is to support unlabeled traffic (only needed the moment you have support for labeled traffic) and one is to allow the NetLabel'ed traffic: corenet_all_recvfrom_unlabeled(something_t) # allow something_t unlabeled_t:peer recv; corenet_all_recvfrom_netlabel(something_t) # allow something_t netlabel_peer_t:peer recv; In case of IPSec for instance, the peer will have a provided label, as is shown by the call for accepting hadoop traffic: hadoop_recvfrom(something_t) # allow something_t hadoop_t:peer recv; However, this alone is not sufficient for labeled IPSec. We also need to allow the domain to be allowed to send anything towards an IPSec security association. There is an interface called corenet_tcp_recvfrom_labeled that takes two arguments which, amongst other things, enables sendto towards its association. corenet_tcp_recvfrom_labeled(some_t, thing_t) # allow { some_t thing_t} self:association sendto; # allow some_t thing_t:peer recv; # allow thing_t some_t:peer recv; # corenet_tcp_recvfrom_netlabel(some_t) # corenet_tcp_recvfrom_netlabel(thing_t) This interface is usually called within a *_tcp_connect() interface for a particular domain, like with the mysql_tcp_connect example: interface(`mysql_tcp_connect',` gen_require(` type mysqld_t; ') corenet_tcp_recvfrom_labeled($1, mysqld_t) corenet_tcp_sendrecv_mysqld_port($1) # deprecated corenet_tcp_connect_mysqld_port($1) corenet_sendrecv_mysqld_client_packets($1) ') When using peer labeling, the domain that is allowed something is based on the socket context of the application. Also, the rules when using peer labeling are in addition to the rules mentioned before (\"standard\" networking control): name_bind and name_connect are always checked. For more information, make sure you check Paul Moore's blog , such as the egress/ingress information. And if you know of resources that show this in a more practical setting (above is mainly to work with the SELinux policy) I'm all ears.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/peer-labeling-in-selinux-policy/","loc":"https://blog.siphos.be/2013/05/peer-labeling-in-selinux-policy/"},{"title":"SELinux policy and network controls","text":"Let's talk about how SELinux governs network streams (and how it reflects this into the policy). When you don't do fancy stuff like SECMARK or netlabeling, then the classes that you should keep an eye on are tcp_socket and udp_socket (depending on the protocol). There used to be node and netif as well, but the support (enforcement) for these have been removed a while ago for the \"old style\" network control enforcement. The concepts are still available though, and I believe they take effect when netlabeling is used. But let's first look at the regular networking aspects. The idea behind the regular network related permissions are that you define either daemon-like behavior (which \"binds\" to a port) or client-like behavior (which \"connects\" to a port). Consider an FTP daemon (domain ftpd_t ) versus FTP client (example domain ncftp_t ). In case of a daemon, the policy would contain the following (necessary) rules: corenet_tcp_bind_generic_node(ftpd_t) # Somewhat legacy but still needed corenet_tcp_bind_ftp_port(ftpd_t) corenet_tcp_bind_ftp_data_port(ftpd_t) corenet_tcp_bind_all_unreserved_ports(ftpd_t) # In case of passive mode This gets translated to the following \"real\" SELinux statements: allow ftpd_t node_t:tcp_socket node_bind; allow ftpd_t ftp_port_t:tcp_socket name_bind; allow ftpd_t ftp_data_port_t:tcp_socket name_bind; allow ftpd_t unreserved_port_type:tcp_socket name_bind; I mention that corenet_tcp_bind_generic_node as being somewhat legacy. When you use netlabeling, you can define different nodes (a \"node\" in that case is a label assigned to an IP address or IP subnet) and as such define policy-wise where daemons can bind on (or clients can connect to). However, without netlabel, the only node that you get to work with is node_t which represents any possible node. Also, the use of passive mode within the ftp policy is governed through the ftpd_use_passive_mode boolean. For a client, the following policy line would suffice: corenet_tcp_connect_ftp_port(ncftp_t) # allow ncftp_t ftp_port_t:tcp_socket name_connect; Well, I lied. Because of how FTP works, if you use active connections, you need to allow the client to bind on an unreserved port, and allow the server to connect to unreserved ports (cfr code snippet below), but you get the idea. corenet_tcp_connect_all_unreserved_ports(ftpd_t) corenet_tcp_bind_generic_node(ncftp_t) corenet_tcp_bind_all_unreserved_ports(ncftp_t) In the past, policy developers also had to include other lines, but these have by time become obsolete ( corenet_tcp_sendrecv_ftp_port for instance). These methods defined the ability to send and receive messages on the port, but this is no longer controlled this way. If you need such controls, you will need to look at SELinux and SECMARK (which uses packets with the packet class) or netlabel (which uses the peer class and peer types to send or receive messages from). And that'll be for a different post.","tags":"SELinux","url":"https://blog.siphos.be/2013/05/selinux-policy-and-network-controls/","loc":"https://blog.siphos.be/2013/05/selinux-policy-and-network-controls/"},{"title":"Gentoo metadata support for CPE","text":"Recently, the metadata.xml file syntax definition (the DTD for those that know a bit of XML) has been updated to support CPE definitions. A CPE (Common Platform Enumeration) is an identifier that describes an application, operating system or hardware device using its vendor, product name, version, update, edition and language. This CPE information is used in the CVE releases (Common Vulnerabilities and Exposures) - announcements about vulnerabilities in applications, operating systems or hardware. Not all security vulnerabilities are assigned a CVE number, but this is as close as you get towards a (public) elaborate dictionary of vulnerabilities. By allowing Gentoo package maintainers to enter (part of) the CPE information in the metadata.xml file, applications that parse the CVE information can now more easily match if software installed on Gentoo is related to a CVE. I had a related post to this not that long ago on my blog and I'm glad this change has been made. With this information at hand, we can start feeding CPE information to the packages and then easily match this with CVEs. I had a request to \"provide\" the scripts I used for the previous post. Mind you, these are taking too many assumptions (and probably wrong ones) for now (and I'm not really planning on updating them as I have different methods for getting information related to CVEs), but I'm planning on integrating CPE data in Gentoo's packages more and then create a small script that generates a \"watchlist\" that I can feed to cvechecker . But anyway, here are the scripts. First , I took all CVE information and put it in a simple CSV file. The CSV is the same one used by cvechecker, so check out the application to see where it fetches the data from (there is a CVE RSS feed and a simple XSL transformation). Second , I create a \"hitlist\" which generates the CPEs. With the recent change to metadata.xml this step can be simplified a lot. Third , I try to match the CPE data with the CVE data, depending on a given time delay of commits. In other words, you can ask possible CVE fixes for commits made in the last few XXX days.","tags":"Gentoo","url":"https://blog.siphos.be/2013/05/gentoo-metadata-support-for-cpe/","loc":"https://blog.siphos.be/2013/05/gentoo-metadata-support-for-cpe/"},{"title":"Enabling Kernel Samepage Merging (KSM)","text":"When using virtualization extensively, you will pretty soon hit the limits of your system (at least, the resources on it). When the virtualization is used primarily for testing (such as in my case), the limit is memory. So it makes sense to seek memory optimization strategies on such systems. The first thing to enable is KSM or Kernel Samepage Merging . This Linux feature looks for memory pages that the applications have marked as being a possible candidate for optimization (sharing) which are then reused across multiple processes. The idea is that, especially for virtualized environments (but KSM is not limited to that), some processes will have the same contents in memory. Without any sharing abilities, these memory pages will be unique (meaning at different locations in your system's memory). With KSM, such memory pages are consolidated to a single page which is then referred to by the various processes. When one process wants to modify the page, it is \"unshared\" so that there is no corruption or unwanted modification of data for the other processes. Such features are not new - VMWare has it named TPS ( Transparent Page Sharing ) and Xen calls it \"Memory CoW\" (Copy-on-Write). One advantage of KSM is that it is simple to setup and advantageous for other processes as well. For instance, if you host multiple instances of the same service (web service, database, tomcat, whatever) there is a high chance that several of its memory pages are prime candidates for sharing. Now before I do mention that this sharing is only enabled when the application has marked it as such. This is done through the madvise() method, where applications mark the memory with MADV_MERGEABLE , meaning that the applications explicitly need to support KSM in order for it to be successful. There is work on the way to support transparent KSM (such as UKSM and PKSM ) where no madvise calls would be needed anymore. But beyond quickly reading the home pages (or translated home pages in case of UKSM ;-) I have no experience with those projects. So let's get back to KSM. I am currently running three virtual machines (all configured to take at most 1.5 Gb of memory). Together, they take just a little over 1 Gb of memory (sum of their resident set sizes). When I consult KSM, I get the following information: # grep -H '' /sys/kernel/mm/ksm/pages_* /sys/kernel/mm/ksm/pages_shared:48911 /sys/kernel/mm/ksm/pages_sharing:90090 /sys/kernel/mm/ksm/pages_to_scan:100 /sys/kernel/mm/ksm/pages_unshared:123002 /sys/kernel/mm/ksm/pages_volatile:1035 The pages_shared tells me that 48911 pages are shared (which means about 191 Mb) through 90090 references ( pages_sharing - meaning the various processes have in total 90090 references to pages that are being shared). That means a gain of 41179 pages (160 Mb). Note that the resident set sizes do not take into account shared pages, so the sum of the RSS has to be subtracted with this to find the \"real\" memory consumption. The pages_unshared value tells me that 123002 pages are marked with the MADV_MERGEABLE advise flag but are not used by other processes. If you want to use KSM yourself, configure your kernel with CONFIG_KSM and start KSM by echo'ing the value \"1\" into /sys/kernel/mm/ksm/run . That's all there is to it.","tags":"Free Software","url":"https://blog.siphos.be/2013/05/enabling-kernel-samepage-merging-ksm/","loc":"https://blog.siphos.be/2013/05/enabling-kernel-samepage-merging-ksm/"},{"title":"The Linux \".d\" approach","text":"Many services on a Linux system use a *.d directory approach to make their configuration easily configurable by other services. This is a remarkably simple yet efficient method for exposing services towards other applications. Let's look into how this .d approach works. Take a look at the /etc/pam.d structure: services that are PAM-aware can place their PAM configuration files in this location, without needing any additional configuration steps or registration. Same with /etc/cron.d : applications that need specific cronjobs do not need to edit /etc/crontab directly (with the problem of concurrent access, overwriting changes, etc.) but instead can place their definitions in the cron.d directory. This approach is getting more traction, as can be seen from the available \"dot-d\" directories on a system: $ ls -d /etc/*.d /etc/bash_completion.d /etc/ld.so.conf.d /etc/pam.d /etc/sysctl.d /etc/conf.d /etc/local.d /etc/profile.d /etc/wgetpaste.d /etc/dracut.conf.d /etc/logrotate.d /etc/request-key.d /etc/xinetd.d /etc/env.d /etc/makedev.d /etc/sandbox.d /etc/cron.d /etc/init.d /etc/modprobe.d /etc/sudoers.d An application can place its configuration files in these directories, automatically \"plugging\" it in into the operating system and the services that it provides. And the more services adopt this approach, the easier it is for applications to be pluggable within the operating system. Even complex systems such as database systems can easily configure themselves this way. And for larger organizations, this is a very interesting approach. Consider the need to deploy a database server on a Linux system in a larger organization. Each organization has its standards for file system locations, policies for log file management, etc. With the *.d approach, these organizations only need to put files on the file system (a rather primitive feature that every organization supports) and manage these files instead of using specific, proprietary interfaces to configure the environment. But to properly control this flexibility, a few attention points need to be taken into account. The first is to use a proper naming convention . If the organization has a data management structure, it might have specific names for services. These names are then used throughout the organization to properly identify owners or responsibilities. When using the *.d directories, these naming conventions also allow administrators to easily know who to contact if a malfunctioning definition is placed. For instance, if a log rotation definition has a wrong entry, a file called mylogrotation does not reveal much information. However, CDBM-postgres-querylogs might reveal that the file is placed there by the customer database management team for a postgresql database. And it isn't only about knowing who to contact (because that could easily be done by comments as well), but also to ensure no conflicts occur. On a shared database system, it is much more likely that two different teams place a postgresql file (which would overwrite the file already there) unless they use a proper naming convention. The second is to use something identifying where the file comes from. A best practice when using Puppet for instance is to add in a comment to the file such as the following: # This file is managed by Puppet through the org-pgsl-def module # Please do not modify manually This informs the administrator how the file is put there; you might even want to include version information. A third one is when the order of configuration entries is important. Most *.d supporting tools do not really care about ordering, but some, like udev, do. When that is the case, the common consensus is to use numbers in the beginning of the file name. The numbers then provide a good ordering of the files. Not all services already offer *.d functionality, although it isn't that difficult to provide it as well. Consider the Linux audit daemon, whose rules are managed in the /etc/audit/audit.rules file. Not that flexible, isn't it? But one can create a /etc/audit/audit.rules.d location and have the audit init script read these files (in alphanumeric order), creating the same functionality. Given enough service adoption, software distribution can be sufficient to configure an application completely and integrate it with all services used by the operating system. And even services that do not support *.d directories can still be easily wrapped around so that their configuration file itself is generated based on the information in such directories. Consider a hypothetical AIDE configuration, where the aide.conf is generated based on the aide.conf.head , aide.d/* and aide.conf.tail files (similar to how resolv.conf is sometimes managed). The generation is triggered right before aide itself is called (perhaps all in a single script). Such an approach allows full integration: A PAM configuration file is placed, allowing the service authentication to be easily managed by administrators. Changes on the authentication (for instance, switch to an LDAP authentication or introduce some trust relation) is done by placing an updated file. A log rotation configuration file is placed, making sure that the log files for the service do not eventually fill the partitions A syslog configuration is provided, allowing for some events to be sent to a different server instead of keeping it local - or perhaps both A cron configuration is stored so that statistics and other house-cleaning jobs for the service can run at night An audit configuration snippet is added to ensure critical commands and configuration files are properly checked Intrusion detection rules are added when needed Monitoring information is placed on the file system, causing additional monitoring metrics to be automatically picked up Firewall definitions are extended based on the snippets placed on the system etc. And all this by only placing files on the file system. Keep It Simple, and efficient ;-)","tags":"Free Software","url":"https://blog.siphos.be/2013/05/the-linux-d-approach/","loc":"https://blog.siphos.be/2013/05/the-linux-d-approach/"},{"title":"Added \"predictable network interface\" info into the handbook","text":"Being long overdue - like many of our documentation-reported bugs :-( I worked on bug 466262 to update the Gentoo Handbook with information about Network Interface Naming . Of course, the installation instructions have also seen the necessary updates to refer to this change. With some luck (read: time) I might be able to fix various other documentation-related ones soon. I had some problems with the new SELinux userspace that I wanted to get fixed before, and then I worked on the new SELinux policies as well as trying to figure out how SELinux deals with network related aspects. Hence I saw time fly by at the speed of a neutrino... BTW, the 20130424 policies are in the tree.","tags":"Documentation","url":"https://blog.siphos.be/2013/05/added-predictable-network-interface-info-into-the-handbook/","loc":"https://blog.siphos.be/2013/05/added-predictable-network-interface-info-into-the-handbook/"},{"title":"Overview of Linux capabilities, part 3","text":"In previous posts I talked about capabilities and gave an introduction to how this powerful security feature within Linux can be used (and also exploited). I also covered a few capabilities, so let's wrap this up with the remainder of them. CAP_AUDIT_CONTROL Enable and disable kernel auditing; change auditing filter rules; retrieve auditing status and filtering rules CAP_AUDIT_WRITE Write records to kernel auditing log CAP_BLOCK_SUSPEND Employ features that can block system suspend CAP_MAC_ADMIN Override Mandatory Access Control (implemented for the SMACK LSM) CAP_MAC_OVERRIDE Allow MAC configuration or state changes (implemented for the SMACK LSM) CAP_NET_ADMIN Perform various network-related operations: - interface configuration - administration of IP firewall, masquerading and accounting - modify routing tables - bind to any address for transparent proxying - set type-of-service (TOS) - clear driver statistics - set promiscuous mode - enabling multicasting - use setsockopt() for privileged socket operations CAP_NET_BIND_SERVICE Bind a socket to Internet domain privileged ports (less than 1024) CAP_NET_RAW Use RAW and PACKET sockets, and bind to any address for transparent proxying CAP_SETPCAP Allow the process to add any capability from the calling thread's bounding set to its inheritable set, and drop capabilities from the bounding set (using prctl() ) and make changes to the securebits flags. CAP_SYS_ADMIN Very powerful capability, includes: - Running quota control, mount, swap management, set hostname, ... - Perform VM86_REQUEST_IRQ vm86 command - Perform IPC_SET and IPC_RMID operations on arbitrary System V IPC objects - Perform operations on trusted.* and security.* extended attributes - Use lookup_dcookie and many, many more. man capabilities gives a good overview of them. CAP_SYS_BOOT Use reboot() and kexec_load() CAP_SYS_CHROOT Use chroot() CAP_SYS_MODULE Load and unload kernel modules CAP_SYS_RESOURCE Another capability with many consequences, including: - Use reserved space on ext2 file systems - Make ioctl() calls controlling ext3 journaling - Override disk quota limits - Increase resource limits - Override RLIMIT_NPROC resource limits and many more. CAP_SYS_TIME Set system clock and real-time hardware clock CAP_SYS_TTY_CONFIG Use vhangup() and employ various privileged ioctl() operations on virtual terminals CAP_SYSLOG Perform privileged syslog() operations and view kernel addresses exposed with /proc and other interfaces (if kptr_restrict is set) CAP_WAKE_ALARM Trigger something that will wake up the system Now when you look through the manual page of the capabilities, you'll notice it talks about securebits as well. This is an additional set of flags that govern how capabilities are used, inherited etc. System administrators don't set these flags - they are governed by the applications themselves (when creating threads, forking, etc.) These flags are set on a per-thread level, and govern the following behavior: SECBIT_KEEP_CAPS Allow a thread with UID 0 to retain its capabilities when it switches its UIDs to a nonzero (non-root) value. By default, this flag is not set, and even if it is set, it is cleared on an execve call, reducing the likelihood that capabilities are \"leaked\". SECBIT_NO_SETUID_FIXUP When set, the kernel will not adjust the capability sets when the thread's effective and file system UIDs are switched between zero (root) and non-zero values. SECBIT_NOROOT If set, the kernel does not grant capabilities when a setuid-root program is executed, or when a process with an effective or real UID of 0 (root) calls execve . Manipulating these bits requires the CAP_SETPCAP capability. Except for the SECBIT_KEEP_CAPS security bit, the others are preserved on an execve() call, and all bits are inherited by child processes (such as when fork() is used). As a user or admin, you can also see capability-related information through the /proc file system: # grep &#94;Cap /proc/$$/status CapInh: 0000000000000000 CapPrm: 0000001fffffffff CapEff: 0000001fffffffff CapBnd: 0000001fffffffff $ grep &#94;Cap /proc/$$/status CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000001fffffffff The capabilities listed therein are bitmasks for the various capabilities. The mask 1FFFFFFFFF holds 37 positions, which match the 37 capabilities known (again, see uapi/linux/capabilities.h in the kernel sources to see the values of each of the capabilities). Again, the pscap can be used to get information about the enabled capabilities of running processes in a more human readable format. But another tool provided by the sys-libs/libcap is interested as well to look at: capsh . The tool offers many capability-related features, including decoding the status fields: $ capsh --decode=0000001fffffffff 0x0000001fffffffff=cap_chown,cap_dac_override,cap_dac_read_search, cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap, cap_linux_immutable,cap_net_bind_service,cap_net_broadcast, cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module, cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct, cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time, cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write, cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin, cap_syslog,35,36 Next to fancy decoding, capsh can also launch a shell with reduced capabilities. This makes it a good utility for jailing chroots even more.","tags":"Security","url":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-3/","loc":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-3/"},{"title":"Overview of Linux capabilities, part 2","text":"As I've (in a very high level) described capabilities and talked a bit on how to work with them , I started with a small overview of file-related capabilities. So next up are process-related capabilities (note, this isn't a conform terminology, more some categorization that I do myself). CAP_IPC_LOCK Allow the process to lock memory CAP_IPC_OWNER Bypass the permission checks for operations on System V IPC objects (similar to the CAP_DAC_OVERRIDE for files) CAP_KILL Bypass permission checks for sending signals CAP_SETUID Allow the process to make arbitrary manipulations of process UIDs and create forged UID when passing socket credentials via UNIX domain sockets CAP_SETGID Same, but then for GIDs CAP_SYS_NICE This capability governs several permissions/abilities, namely to allow the process to - change the nice value of itself and other processes - set real-time scheduling priorities for itself, and set scheduling policies and priorities for arbitrary processes - set the CPU affinity for arbitrary processes - apply migrate_pages to arbitrary processes and allow processes to be migrated to arbitrary nodes - apply move_pages to arbitrary processes - use the MPOL_MF_MOVE_ALL flag with mbind() and move_pages() The abilities related to page moving, migration and nodes is of importance for NUMA systems, not something most workstations have or need. CAP_SYS_PACCT Use acct() , to enable or disable system resource accounting for the process CAP_SYS_PTRACE Allow the process to trace arbitrary processes using ptrace() , apply get_robust_list() against arbitrary processes and inspect processes using kcmp() . CAP_SYS_RAWIO Allow the process to perform I/O port operations, access /proc/kcore and employ the FIBMAP ioctl() operation. Capabilities such as CAP_KILL and CAP_SETUID are very important to govern correctly, but this post would be rather dull (given that the definitions of the above capabilities can be found from the manual page) if I wouldn't talk a bit more about its feasibility. Take a look at the following C application code: #include <errno.h> #include <stdio.h> #include <string.h> #include <sys/capability.h> #include <sys/prctl.h> #include <sys/types.h> #include <unistd.h> int main(int argc, char ** argv) { printf(\"cap_setuid and cap_setgid: %d\\n\", prctl(PR_CAPBSET_READ, CAP_SETUID|CAP_SETGID, 0, 0, 0)); printf(\" %s\\n\", cap_to_text(cap_get_file(argv[0]), NULL)); printf(\" %s\\n\", cap_to_text(cap_get_proc(), NULL)); if (setresuid(0, 0, 0)); printf(\"setresuid(): %s\\n\", strerror(errno)); execve(\"/bin/sh\", NULL, NULL); } At first sight, it looks like an application to get root privileges ( setresuid() ) and then spawn a shell. If that application would be given CAP_SETUID and CAP_SETGID effectively, it would allow anyone who executed it to automatically get a root shell, wouldn't it? $ gcc -o test -lcap test.c # setcap cap_setuid,cap_setgid+ep test $ ./test cap_setuid and cap_setgid: 1 = cap_setgid,cap_setuid+ep = setresuid() failed: Operation not permitted So what happened? After all, the two capabilities are set with the +ep flags given. Then why aren't these capabilities enabled? Well, this binary was stored on a file system that is mounted with the nosuid option. As a result, this capability is not enabled and the application didn't work. If I move the file to another file system that doesn't have the nosuid option: $ /usr/local/bin/test cap_setuid and cap_setgid: 1 = cap_setgid,cap_setuid+ep = cap_setgid,cap_setuid+ep setresuid() failed: Operation not permitted So the capabilities now do get enabled, so why does this still fail? This now is due to SELinux: type=AVC msg=audit(1367393377.342:4778): avc: denied { setuid } for pid=21418 comm=\"test\" capability=7 scontext=staff_u:staff_r:staff_t tcontext=staff_u:staff_r:staff_t tclass=capability And if you enable grSecurity's TPE, we can't even start the binary to begin with: $ ./test -bash: ./test: Permission denied $ /lib/ld-linux-x86-64.so.2 /home/test/test /home/test/test: error while loading shared libraries: /home/test/test: failed to map segment from shared object: Permission denied # dmesg ... [ 5579.567842] grsec: From 192.168.100.1: denied untrusted exec (due to not being in trusted group and file in non-root-owned directory) of /home/test/test by /home/test/test[bash:4221] uid/euid:1002/1002 gid/egid:100/100, parent /bin/bash[bash:4195] uid/euid:1002/1002 gid/egid:100/100 When all these \"security obstacles\" are not enabled, then the call succeeds: $ /usr/local/bin/test cap_setuid and cap_setgid: 1 = cap_setgid,cap_setuid+ep = cap_setgid,cap_setuid+ep setresuid() failed: Success root@hpl tmp # This again shows how important it is to regularly review capability-enabled files on the file system, as this is a major security problem that cannot be detected by only looking for setuid binaries, but also that securing a system is not limited to one or a few settings: one always has to take the entire setup into consideration, hardening the system so it becomes more difficult for malicious users to abuse it. # filecap -a file capabilities /usr/local/bin/test setgid, setuid","tags":"Security","url":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-2/","loc":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-2/"},{"title":"Overview of Linux capabilities, part 1","text":"In the previous posts , I talked about capabilities and how they can be used to allow processes to run in a privileged fashion without granting them full root access to the system. An example given was how capabilities can be leveraged to run ping without granting it setuid root rights. But what are the various capabilities that Linux is, well, capable of? There are many, and as time goes by, more capabilities are added to the set. The last capability added to the main Linux kernel tree was the CAP_BLOCK_SUSPEND in the 3.5 series. An overview of all capabilities can be seen with man capabilities or by looking at the Linux kernel source code, include/uapi/linux/capability.h . But because you are all lazy, and because it is a good exercise for myself, I'll go through many of them in this and the next few posts. For now, let's look at file related capabilities. As a reminder, if you want to know which SELinux domains are \"granted\" a particular capability, you can look this up using sesearch . The capability is either in the capability or capability2 class, and is named after the capability itself, without the CAP_ prefix: $ sesearch -c capability -p chown -A CAP_CHOWN Allow making changes to the file UIDs and GIDs. CAP_DAC_OVERRIDE Bypass file read, write and execute permission checks. I came across a reddit post that was about this capability not that long ago. CAP_DAC_READ_SEARCH Bypass file read permission and directory read/search permission checks. CAP_FOWNER This capability governs 5 capabilities in one: - Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file (unless already granted through CAP_DAC_READ_SEARCH and/or CAP_DAC_OVERRIDE ) - Allow to set extended file attributes - Allow to set access control lists - Ignore directory sticky bit on file deletion - Allow specifying O_NOATIME for files in open() and fnctl() calls CAP_FSETID Do not clear the setuid/setgid permission bits when a file is modified CAP_LEASE Allow establishing leases on files CAP_LINUX_IMMUTABLE Allow setting FS_APPEND_FL and FP_IMMUTABLE_FL inode flags CAP_MKNOD Allow creating special files with mknod CAP_SETFCAP Allow setting file capabilities (what I did with the anotherping binary in the previous post) When working with SELinux (especially when writing applications), you'll find that the CAP_DAC_READ_SEARCH and CAP_DAC_OVERRIDE capability come up often. This is the case when applications are written to run as root yet want to scan through, read or even execute non-root owned files. Without SELinux, because these run as root, this is all granted. However, when you start confining those applications, it becomes apparent that they require this capability. Another example is when you run user applications, as root, like when trying to play a movie or music file with mplayer when this file is owned by a regular user: type=AVC msg=audit(1367145131.860:18785): avc: denied { dac_read_search } for pid=8153 comm=\"mplayer\" capability=2 scontext=staff_u:sysadm_r:mplayer_t tcontext=staff_u:sysadm_r:mplayer_t tclass=capability type=AVC msg=audit(1367145131.860:18785): avc: denied { dac_override } for pid=8153 comm=\"mplayer\" capability=1 scontext=staff_u:sysadm_r:mplayer_t tcontext=staff_u:sysadm_r:mplayer_t tclass=capability Notice the time stamp: both checks are triggered at the same time. What happens is that the Linux security hooks first check for DAC_READ_SEARCH (the \"lesser\" grants of the two) and then for DAC_OVERRIDE (which contains DAC_READ_SEARCH and more). In both cases, the check failed in the above example. The CAP_LEASE capability is one that I had not heard about before (actually, I had not heard of getting \"file leases\" on Linux either). A file lease allows for the lease holder (which requires this capability) to be notified when another process tries to open or truncate the file. When that happens, the call itself is blocked and the lease holder is notified (usually using SIGIO) about the access. It is not really to lock a file (since, if the lease holder doesn't properly release it, it is forcefully \"broken\" and the other process can continue its work) but rather to properly close the file descriptor or flushing caches, etc. BTW, on my system, only 5 SELinux domains hold the lease capability. There are 37 capabilities known by the Linux kernel at this time. The above list has 9 file related ones. So perhaps next I can talk about process capabilities.","tags":"Security","url":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/","loc":"https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/"},{"title":"Restricting and granting capabilities","text":"As capabilities are a way for running processes with some privileges, without having the need to grant them root privileges, it is important to understand that they exist if you are a system administrator, but also as an auditor or other security-related function. Having processes run as a non-root user is no longer sufficient to assume that they do not hold any rights to mess up the system or read files they shouldn't be able to read. The grsecurity kernel patch set, which is applied to the Gentoo hardened kernel sources, contains for instance CONFIG_GRKERNSEC_CHROOT_CAPS which, as per its documentation, \"restrcts the capabilities on all root processes within a chroot jail to stop module insertion, raw i/o, system and net admin tasks, rebooting the system, modifying immutable files, modifying IPC owned by another, and changing the system time.\" But other implementations might even use capabilities to restrict the users. Consider LXC (Linux Containers). When a container is started, CAP_SYS_BOOT (the ability to shutdown/reboot the system/container) is removed so that users cannot abuse this privilege. You can also grant capabilities to users selectively, using pam_cap.so (the Capabilities Pluggable Authentication Module). For instance, to allow some users to ping, instead of granting the cap_net_raw immediately ( +ep ), we can assign the capability to some users through PAM, and have the ping binary inherit and use this capability instead ( +p ). That doesn't mean that the capability is in effect, but rather that it is in a sort-of permitted set. Applications that are granted a certain permission this way can either use this capability if the user is allowed to have it, or won't otherwise. # setcap cap_net_raw+p anotherping # vim /etc/pam.d/system-login ... add in something like auth required pam_cap.so # vim /etc/security/capability.conf ... add in something like cap_net_raw user1 The logic used with capabilities can be described as follows (it is not as difficult as it looks): pI' = pI (***) pP' = fP | (fI & pI) pE' = pP' & fE [NB. fE is 0 or ~0] I=Inheritable, P=Permitted, E=Effective // p=process, f=file ' indicates post-exec(). So, for instance, the second line reads \"The permitted set of capabilities of the newly forked process is set to the permitted set of capabilities of its executable file, together with the result of the AND operation between the inherited capabilities of the file and the inherited capabilities of the parent process.\" As an admin, you might want to keep an eye out for binaries that have particular capabilities set. With filecap you can list which capabilities are in the effective set of files found on the file system (for instance, +ep ). # filecap file capabilities /bin/anotherping net_raw Similarly, with pscap you can see the capabilities set on running processes. # pscap -a ppid pid name command capabilities 6148 6152 root bash full It might be wise to take this up in the daily audit reports.","tags":"Security","url":"https://blog.siphos.be/2013/05/restricting-and-granting-capabilities/","loc":"https://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"},{"title":"Capabilities, a short intro","text":"Capabilities. You probably have heard of them already, but when you start developing SELinux policies, you'll notice that you come in closer contact with them than before. This is because SELinux, when applications want to do something \"root-like\", checks the capability of that application. Without SELinux, this either requires the binary to have the proper capability set, or the application to run in root modus. With SELinux, the capability also needs to be granted to the SELinux context (the domain in which the application runs). But forget about SELinux for now, and let's focus on capabilities. Capabilities in Linux are flags that tell the kernel what the application is allowed to do, but unlike file access, capabilities for an application are system-wide: there is no \"target\" to which it applies. Think about an \"ability\" of an application. See for yourself through man capabilities . If you have no additional security mechanism in place, the Linux root user has all capabilities assigned to it. And you can remove capabilities from the root user if you want to, but generally, capabilities are used to grant applications that tiny bit more privileges, without needing to grant them root rights. Consider the ping utility. It is marked setuid root on some distributions, because the utility requires the (cap)ability to send raw packets. This capability is known as CAP_NET_RAW . However, thanks to capabilities, you can now mark the ping application with this capability and drop the setuid from the file. As a result, the application does not run with full root privileges anymore, but with the restricted privileges of the user plus one capability, namely the CAP_NET_RAW . Let's take this ping example to the next level: copy the binary (possibly relabel it as ping_exec_t if you run with SELinux), make sure it does not hold the setuid and try it out: # cp ping anotherping # chcon -t ping_exec_t anotherping Now as a regular user: $ ping -c 1 127.0.0.1 PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data. 64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.057 ms $ anotherping -c 1 127.0.0.1 ping: icmp open socket: Operation not permitted Let's assign the binary with the CAP_NET_RAW capability flag: # setcap cap_net_raw+ep anotherping And tadaa: $ anotherping -c 1 127.0.0.1 PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data. 64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.054 ms What setcap did was place an extended attribute to the file, which is a binary representation of the capabilities assigned to the application. The additional information ( +ep ) means that the capability is p ermitted and e ffective. So long for the primer, I'll talk about the various capabilities in a later post.","tags":"Security","url":"https://blog.siphos.be/2013/05/capabilities-a-short-intro/","loc":"https://blog.siphos.be/2013/05/capabilities-a-short-intro/"},{"title":"SELinux mount options","text":"When you read through the Gentoo Hardened SELinux handbook , you'll notice that we sometimes update /etc/fstab with some SELinux-specific settings. So, what are these settings about and are there more of them? First of all, let's look at a particular example from the installation instructions so you see what I am talking about: tmpfs /tmp tmpfs defaults,noexec,nosuid,rootcontext=system_u:object_r:tmp_t 0 0 What the rootcontext= option does here is to set the context of the \"root\" of that file system (meaning, the context of /tmp in the example) to the specified context before the file system is made visible to the userspace. Because we do it soon, the file system is known as tmp_t throughout its life cycle (not just after the mount or so). Another option that you'll frequently see on the Internet is the context= option. This option is most frequently used for file systems that do not support extended attributes, and as such cannot store the context of files on the file system. With the context= mount option set, all files on that file system get the specified context. For instance, context=system_u:object_r:removable_t . If the file system does support extended attributes, you might find some benefit in using the defcontext= option. When set, the context of files and directories (and other resources on that file system) that do not have a SELinux context set yet will use this default context. However, once a context is set, it will use that context instead. The last context-related mount option is fscontext= . With this option, you set the context of the \"filesystem\" class object of the file system rather than the mount itself (or the files). Within SELinux, \"filesystem\" is one of the resource classes that can get a context. Remember the /tmp mount example from before? Well, even though the files are labeled tmp_t , the file system context itself is still tmpfs_t . It is important to know that, if you use one of these mount options, context= is mutually exclusive to the other options as it \"forces\" the context on all resources (including the filesystem class).","tags":"SELinux","url":"https://blog.siphos.be/2013/05/selinux-mount-options/","loc":"https://blog.siphos.be/2013/05/selinux-mount-options/"},{"title":"Qemu-KVM monitor tips and tricks","text":"When running KVM guests, the Qemu/KVM monitor is a nice interface to interact with the VM and do specific maintenance tasks on. If you run the KVM guests with VNC, then you can get to this monitor through Ctrl-Alt-2 (and Ctrl-Alt-1 to get back to the VM display). I personally run with the monitor on the standard input/output where the VM is launched as its output is often large and scrolling in the VNC doesn't seem to work well. I decided to give you a few tricks that I use often on the monitor to handle the VMs. When I do not start the VNC server associated with the VM by default, I can enable it on the monitor using change vnc while getting details is done using info vnc . To disable VNC again, use change vnc none . (qemu) info vnc Server: disabled (qemu) change vnc 127.0.0.1:20 (qemu) change vnc password Password: ****** (qemu) info vnc Server: address: 127.0.0.1:5920 auth: vnc Client: none Similarly, if you need to enable remote debugging, you can use the gdbserver option. Getting information using info is dead-easy, and supports a wide area of categories: balloon info, block devices, character devices, cpus, memory mappings, network information, etcetera etcetera... Just enter info to get an overview of all supported commands. To easily manage block devices, you can see the current state of devices using info block and then use change <blockdevice> <path> to update it. (qemu) info block virtio0: removable=0 io-status=ok file=/srv/virt/gentoo/hardened2selinux/selinux-base.img ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0 ide1-cd0: removable=1 locked=0 tray-open=0 io-status=ok [not inserted] floppy0: removable=1 locked=0 tray-open=0 [not inserted] sd0: removable=1 locked=0 tray-open=0 [not inserted] (qemu) change ide1-cd0 /srv/virt/media/systemrescuecd-x86-2.2.0.iso To powerdown the system, use system_powerdown . If that fails, you can use quit to immediately shut down (terminate) the VM. To reset it, use system_reset . You can also hot-add PCI devices and manipulate CPU states, or even perform live migrations between systems. When you use qcow2 image formats, you can take a full VM snapshot using savevm and, when you later want to return to this point again, use loadvm . This is interesting when you want to do potentially harmful changes on the system and want to easily revert back if things got broken. (qemu) savevm 20130419 (qemu) info snapshots ID TAG VM SIZE DATE VM CLOCK 1 20130419 224M 2013-04-19 12:05:16 00:00:17.294 (qemu) loadvm 20130419","tags":"Free Software","url":"https://blog.siphos.be/2013/04/qemu-kvm-monitor-tips-and-tricks/","loc":"https://blog.siphos.be/2013/04/qemu-kvm-monitor-tips-and-tricks/"},{"title":"photorec to the rescue","text":"Once again PhotoRec has been able to save files from a corrupt FAT USB drive. The application scans the partition, looking for known files (based on the file magic) and then restores those files. The files are not named as they were though, so there is still some manual work left, but the recovery works pretty well: PhotoRec 6.12, Data Recovery Utility, May 2011 Christophe GRENIER http://www.cgsecurity.org Disk /dev/sdc1 - 1000 GB / 931 GiB (RO) - WD My Book Partition Start End Size in sectors No partition 0 0 1 121600 253 63 1953520002 [Whole disk] Pass 1 - Reading sector 464342462/1953520002, 10738 files found Elapsed time 2h46m01s - Estimated time to completion 8h52m25 jpg: 7429 recovered txt: 961 recovered mp3: 558 recovered tx?: 373 recovered riff: 297 recovered gif: 218 recovered exe: 151 recovered ifo: 126 recovered mpg: 91 recovered pdf: 83 recovered others: 451 recovered In Gentoo, you can find the package as part of app-admin/testdisk . To recover the files, I ran the following command: $ photorec /log /d /path/to/recovery/dest /dev/sdc1 While skimming through the recovered files, I found a few ones that I deleted a long time ago but apparently never got overwritten (the data, that is). Scary to see how easy such recovery is... Makes me remember that, if you really want to delete files in a less recoverable manner, you can use shred for that. And for those out there yelling to backup this data - you're absolutely correct, but no. I backup my systems and important files daily, but this disk contained (mainly) raw picture images and videorecordings. The manipulated, finished images and recordings are backed up (or at least on a disk and somewhere online), but the raw images and recordings are often too much to introduce a backup for, and if I would really lost them, I wouldn't shed a tear (nor panic).","tags":"Free Software","url":"https://blog.siphos.be/2013/04/photorec-to-the-rescue/","loc":"https://blog.siphos.be/2013/04/photorec-to-the-rescue/"},{"title":"Securely handling libffi","text":"I've recently came across libffi again. No, not because it was mentioned during the Gentoo Hardened online meeting, but because my /var/tmp wasn't mounted correctly, and emerge (actually python) uses libffi. Most users won't notice this, because libffi works behind the scenes. But when it fails, it fails bad. And SELinux actually helped me quickly identify what the problem is. $ emerge --info segmentation fault The abbreviation \"libffi\" comes from Foreign Function Interface , and is a library that allows developers to dynamically call code from another application or library. But the method how it approaches this concerns me a bit. Let's look at some strace output: 8560 open(\"/var/tmp/ffiZ8gKPd\", O_RDWR|O_CREAT|O_EXCL, 0600) = 11 8560 unlink(\"/var/tmp/ffiZ8gKPd\") = 0 8560 ftruncate(11, 4096) = 0 8560 mmap(NULL, 4096, PROT_READ|PROT_EXEC, MAP_SHARED, 11, 0) = -1 EACCES (Permission denied) Generally, what libffi does, is to create a file somewhere where it can write files (it checks the various mounts on a system to get a list of possible target file systems), adds the necessary data (that it wants to execute) to it, unlinks the file from the file system (but keep the file descriptor open, so that the file cannot (easily) be modified on the system anymore) and then maps it to memory for executable access. If executing is allowed by the system (for instance because the mount point does not have noexec ), then SELinux will trap it because the domain (in our case now, portage_t ) is trying to execute an (unlinked) file for which it holds no execute rights on: type=AVC msg=audit(1366656205.201:2221): avc: denied { execute } for pid=8560 comm=\"emerge\" path=2F7661722F66666962713154465A202864656C6574656429 dev=\"dm-3\" ino=6912 scontext=staff_u:sysadm_r:portage_t tcontext=staff_u:object_r:var_t tclass=file When you notice something like this (an execute on an unnamed file), then this is because the file descriptor points to a file already unlinked from the system. Finding out what it was about might be hard (but with strace it is easy as ... well, whatever is easy for you). Now what happened was that, because /var/tmp wasn't mounted, files created inside it got the standard type ( var_t ) which the Portage domain isn't allowed to execute. It is allowed to execute a lot of types, but not that one ;-) When /var/tmp is properly mounted, the file gets the portage_tmp_t type where it does hold execute rights for. Now generally, I don't like having world-writeable locations without noexec . For /tmp , noexec is enabled, but for /var/tmp I have (well, had ;-) to allow execution from the file system, mainly because some (many?) Gentoo package builds require it. So how about this dual requirement, of allowing Portage to write (and execute) its own files, and allow libffi to do its magic? Certainly, from a security point of view, I might want to restrict this further... Well, we need to make sure that the location where Portage works with (the location pointed to by $PORTAGE_TMPDIR ) is specifically made available for Portage: have the directory only writable by the Portage user. I keep it labeled as tmp_t so that the existing policies apply, but it might work with portage_tmp_t immediately set as well. Perhaps I'll try that one later. With that set, we can have this mount-point set with exec rights (so that libffi can place its file there) in a somewhat more secure manner than allowing exec on world-writeable locations. So now my /tmp and /var/tmp (and /run and /dev/shm and /lib64/rc/init.d ) are tmpfs-mounts with the noexec (as well as nodev and nosuid ) bits set, with the location pointed towards by $PORTAGE_TMPDIR being only really usable by the Portage user: $ ls -ldZ /var/portage drwxr-x---. 4 portage root system_u:object_r:tmp_t 4096 Apr 22 21:45 /var/portage/ And libffi? Well, allowing applications to create their own executables and executing it is something that should be carefully governed. I'm not aware of any existing or past vulnerabilities, but I can imagine that opening the ffi* file(s) the moment they come up (to make sure you have a file descriptor) allows you to overwrite the content after libffi has created it but before the application actually executes it. By limiting the locations where applications can write files to (important step one) and the types they can execute (important step two) we can already manage this a bit more. Using regular DAC, this is quite difficult to achieve, but with SELinux, we can actually control this a bit more. Let's first see how many domains are allowed to create, write and execute files: $ sesearch -c file -p write,create,execute -A | grep write | grep create | grep execute | awk '{print $1}' | sort | uniq | wc -l 32 Okay, 32 target domains. Not that bad, and certainly doable to verify manually (hell, even in a scripted manner). You can now check which of those domains have rights to execute generic binaries ( bin_t ), possibly needed for command execution vulnerabilities or privilege escalation. Or that have specific capabilities. And if you want to know which of those domains use libffi, you can use revdep-rebuild to find out which files are linked to the libffi libraries. It goes to show that trying to keep your box secure is a never-ending story (please, companies, allow your system administrators to do their job by giving them the ability to continuously increase security rather than have them ask for budget to investigate potential security mitigation directives based on the paradigm of business case and return on investment using pareto-analytics blaaaahhhh....), and that SELinux can certainly be an important method to help achieve it.","tags":"Security","url":"https://blog.siphos.be/2013/04/securely-handling-libffi/","loc":"https://blog.siphos.be/2013/04/securely-handling-libffi/"},{"title":"How logins get their SELinux user context","text":"Sometimes, especially when users are converting their systems to be SELinux-enabled, their user context is wrong. An example would be when, after logon (in permissive mode), the user is in the system_u:system_r:local_login_t domain instead of a user domain like staff_u:staff_r:staff_t . So, how does a login get its SELinux user context? Let's look at the entire chain of SELinux context changes across a boot. At first, when the system boots, the kernel (and all processes invoked from it) run in the kernel_t domain (I'm going to ignore the other context fields for now until they become relevant). When the kernel initialization has been completed, the kernel executes the init binary. When you use an initramfs, then a script might be called. This actually doesn't matter that much yet, since SELinux stays within the kernel_t domain until a SELinux-aware init is launched. When the init binary is executed, init of course starts. But as mentioned, init is SELinux-aware, meaning it will invoke SELinux-related commands. One of these is that it will load the SELinux policy (as stored in /etc/selinux ) and then reexecute itself. Because of that, its process context changes from kernel_t towards init_t . This is because the init binary itself is labeled as init_exec_t and a type transition is defined from kernel_t towards init_t when init_exec_t is executed. Ok, so init now runs in init_t and it goes on with whatever it needs to do. This includes invoking init scripts (which, btw, run in initrc_t because the scripts are labeled initrc_exec_t or with a type that has the init_script_file_type attribute set, and a transition from init_t to initrc_t is defined when such files are executed). When the bootup is finally completed, init launches the getty processes. The commands are mentioned in /etc/inittab : $ grep getty /etc/inittab c1:12345:respawn:/sbin/agetty --noclear 38400 tty1 linux c2:2345:respawn:/sbin/agetty 38400 tty2 linux ... These binaries are also explicitly labeled getty_exec_t . As a result, the getty (or agetty ) processes run in the getty_t domain (because a transition is defined from init_t to getty_t when getty_exec_t is executed). Ok, so gettys run in getty_t . But what happens when a user now logs on to the system? Well, the getty's invoke the login binary which, you guessed it right, is labeled as something: login_exec_t . As a result (because, again, a transition is defined in the policy), the login process runs as local_login_t . Now the login process invokes the various PAM subroutines which follow the definitions in /etc/pam.d/login . On Gentoo systems, this by default points to the system-local-login definitions which points to the system-login definitions. And in this definition, especially under the sessions section, we find a reference to pam_selinux.so : session required pam_selinux.so close ... session required pam_selinux.so multiple open Now here is where some of the magic starts (see my post on Using pam_selinux to switch contexts for the gritty details). The methods inside the pam_selinux.so binary will look up what the context should be for a user login. For instance, when the root user logs on, it has SELinux checking what SELinux user root is mapped to, equivalent to running semanage login -l : $ semanage login -l | grep &#94;root root root In this case, the SELinux user for root is root , but this is not always the case (that login and user are the same). For instance, my regular administrative account maps to the staff_u SELinux user. Next, it checks what the default context should be for this user. This is done by checking the default_contexts file (such as the one in /etc/selinux/strict/contexts although user-specific overrides can be (and are) placed in the users subdirectory) based on the context of the process that is asking SELinux what the default context should be. In our case, it is the login process running as local_login_t : $ grep -HR local_login_t /etc/selinux/strict/contexts/* default_contexts:system_r:local_login_t user_r:user_t staff_r:staff_t sysadm_r:sysadm_t unconfined_r:unconfined_t users/unconfined_u:system_r:local_login_t unconfined_r:unconfined_t users/guest_u:system_r:local_login_t guest_r:guest_t users/user_u:system_r:local_login_t user_r:user_t users/staff_u:system_r:local_login_t staff_r:staff_t sysadm_r:sysadm_t users/root:system_r:local_login_t unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t users/xguest_u:system_r:local_login_t xguest_r:xguest_t Since we are verifying this for the root SELinux user, the following line of the users/root file is what matters: system_r:local_login_t unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t Here, SELinux looks for the first match in that line that the user has access to. This is defined by the roles that the user is allowed to access: $ semanage user -l | grep root root staff_r sysadm_r As root is allowed both the staff_r and sysadm_r roles, the first one found in the default context file that matches will be used. So it is not the order in which the roles are displayed in the semanage user -l output that matters, but the order of the contexts in the default context file. In the example, this is sysadm_r:sysadm_t : system_r:local_login_t unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t <-----------+-----------> <-------+-------> <------+------> <-----+-----> `- no matching role `- first (!) `- second `- no match Now that we know what the context should be, this is used for the first execution that the process (still login ) will do. So login changes the Linux user (if applicable) and invokes the shell of that user. Because this is the first execution that is done by login , the new context is set (being root:sysadm_r:sysadm_t ) for the shell. And that is why, if you run id -Z , it returns the user context ( root:sysadm_r:sysadm_t ) if everything works out fine ;-)","tags":"SELinux","url":"https://blog.siphos.be/2013/04/how-logins-get-their-selinux-user-context/","loc":"https://blog.siphos.be/2013/04/how-logins-get-their-selinux-user-context/"},{"title":"New SELinux userspace release","text":"A new release of the SELinux userspace utilities was recently announced. I have made the packages for Gentoo available and they should now be in the main tree (\\~arch of course). During the testing of the packages however, I made a stupid mistake of running the tests on the wrong VM, one that didn't contain the new packages. Result: no regressions (of course). My fault for not using in-ebuild tests properly, as I should . So you'll probably see me blogging about the in-ebuild testing soon ;-) In any case, the regressions I did find out (quite fast after I updated my main laptop with them as well) where a missing function in libselinux , a referral to a non-existing makefile when using \"semanage permissive\" and the new sepolicy application requiring yum python bindings . At least, with the missing function (hopefully correctly) resolved, all tests I usually do (except for the permissive domains) are now running well again. This only goes to show how important testing is. Of course, I reported the bugs on the mailinglist of the userspace utilities as well. Hopefully they can look at them while I'm asleep so I can integrate fixes tomorrow more easily ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/new-selinux-userspace-release/","loc":"https://blog.siphos.be/2013/04/new-selinux-userspace-release/"},{"title":"Gentoo protip: using buildpkgonly","text":"If you don't want to have the majority of builds run in the background while you are busy on the system, but you don't want to automatically install software in the background when you are not behind your desk, then perhaps you can settle for using binary packages . I'm not saying you need to setup a build server and such or do your updates first in a chroot. No, what this tip is for is to use the --buildpkgonly parameter of emerge at night, building some of your software (often the larger ones) as binary packages only (storing those in the ${PKGDIR} which defaults to /usr/portage/packages ). When you are then on your system, you can run the update with binary package included: # emerge -puDk world To use --buildpkgonly , all package(s) that Portage wants to build must have all their dependencies met. If not, then the build will not go through and you're left with no binary packages at all. So what we do is to create a script that looks at the set of packages that would be build, and then one for one building the binary package. When ran, the script will attempt to build binary packages for those that have no dependency requirements anymore. Those builds will then create a binary package but will not be merged on the system. When you later update your system, including binary packages, those packages that have been build during the night will be merged quickly, reducing the build load on your system while you are working on it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/sh LIST = $( mktemp ) ; emerge -puDN --color = n --columns --quiet = y world | awk '{print $2}' > ${ LIST } ; for PACKAGE in $( cat ${ LIST } ) ; do printf \"Building binary package for ${ PACKAGE } ... \" emerge -uN --quiet-build --quiet = y --buildpkgonly ${ PACKAGE } ; if [[ $? -eq 0 ]] ; then echo \"ok\" ; else echo \"failed\" ; fi done I ran this a couple of days ago when -uDN world showed 46 package updates (including a few hefty ones like chromium). After running this script, 35 of them had a binary package ready so the -uDN world now only needed to build 11 packages, merging the remainder from binary packages.","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/gentoo-protip-using-buildpkgonly/","loc":"https://blog.siphos.be/2013/04/gentoo-protip-using-buildpkgonly/"},{"title":"Using strace to troubleshoot SELinux problems","text":"When SELinux is playing tricks on you, you can just \"allow\" whatever it wants to do, but that is not always an option: sometimes, there is no denial in sight because the problem lays within SELinux-aware applications (applications that might change their behavior based on what the policy sais or even based on if SELinux is enabled or not). At other times, you get a strange behavior that isn't directly visible what the cause is. But mainly, if you want to make sure that allowing something is correct (and not just a corrective action), you need to be absolutely certain that what you want to allow is security-wise acceptable. To debug such issues, I often take the strace command to debug the application at hand. To use strace , I toggle the allow_ptrace boolean ( strace uses ptrace() which, by default, isn't allowed policy-wise) and then run the offending application through strace (or attach to the running process if it is a daemon). For instance, to debug a tmux issue we had with the policy not that long ago: # setsebool allow_ptrace on # strace -o strace.log -f -s 256 tmux The resulting log file (strace.log) might seem daunting at first to look at. What you see are the system calls that the process is performing, together with their options but also the return code of each call. This is especially important as SELinux, if it denies something, often returns something like EACCESS (Permission Denied). 7313 futex(0x349e016f080, FUTEX_WAKE_PRIVATE, 2147483647) = 0 7313 futex(0x5aad58fd84, FUTEX_WAKE_PRIVATE, 2147483647) = 0 7313 stat(\"/\", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 7313 stat(\"/home\", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 7313 stat(\"/home/swift\", {st_mode=S_IFDIR|0755, st_size=12288, ...}) = 0 7313 stat(\"/home/swift/.pki\", {st_mode=S_IFDIR|0700, st_size=4096, ...}) = 0 7313 stat(\"/home/swift/.pki/nssdb\", {st_mode=S_IFDIR|0700, st_size=4096, ...}) = 0 7313 statfs(\"/home/swift/.pki/nssdb\", 0x3c3cab6fa50) = -1 EACCES (Permission denied) Most (if not all) of the methods shown in a strace log are documented through manpages, so you can quickly find out that futex() is about fast user-space locking, stat() ( man 2 stat to see the information about the method instead of the application) is about getting file status and statfs() is for getting file system statistics. The most common permission issues you'll find are file related: 7313 open(\"/proc/filesystems\", O_RDONLY) = -1 EACCES (Permission denied) In the above case, you notice that the application is trying to open the /proc/filesystems file read-only. In the SELinux logs, this might be displayed as follows: audit.log:type=AVC msg=audit(1365794728.180:3192): avc: denied { read } for pid=860 comm=\"nacl_helper_boo\" name=\"filesystems\" dev=\"proc\" ino=4026532034 scontext=staff_u:staff_r:chromium_naclhelper_t tcontext=system_u:object_r:proc_t tclass=file Now the case of tmux before was not an obvious one. In the end, I compared the strace output's of two runs (one in enforcing and one in permissive) to find what the difference would be. This is the result: Enforcing: 10905 fcntl(9, F_GETFL) = 0x8000 (flags O_RDONLY|O_LARGEFILE) 10905 fcntl(9, F_SETFL, O_RDONLY|O_NONBLOCK|O_LARGEFILE) = 0 Permissive: 10905 fcntl(9, F_GETFL) = 0x8002 (flags O_RDWR|O_LARGEFILE) 10905 fcntl(9, F_SETFL, O_RDWR|O_NONBLOCK|O_LARGEFILE) = 0 You notice the difference? In enforcing-mode, one of the flags on the file descriptor has O_RDONLY whereas the one in permissive mode as O_RDWR . This means that the file descriptor in enforcing mode is read-only whereas in permissive-mode is read-write. What we then do in the strace logs is to see where this file descriptor (with id=9) comes from: 10905 dup(0) = 9 10905 dup(1) = 10 10905 dup(2) = 11 As the man-pages sais, dup() duplicates a file descriptor. And because, by convention, the first three file descriptors of an application correspond with standard input (0), standard output (1) and error output (2), we now know that the file descriptor with id=9 comes from the standard input file descriptor. Although this one should be read-only (it is the input that the application gets = reads), it seems that tmux might want to use this for writes as well. And that is what happens - tmux sends the file descriptor to the tmux server to check if it is a tty and then uses it to write to the screen. Now what does that have to do with SELinux? It has to mean something, otherwise running in permissive mode would give the same result. After some investigation, we found out that using newrole to switch roles changes the flags of the standard input (as then provided by newrole ) from O_RDWR to O_RDONLY (code snippet from newrole.c - look at the first call to open() ): /* Close the tty and reopen descriptors 0 through 2 */ if (ttyn) { if (close(fd) || close(0) || close(1) || close(2)) { fprintf(stderr, _(\"Could not close descriptors.\\n\")); goto err_close_pam; } fd = open(ttyn, O_RDONLY | O_NONBLOCK); if (fd != 0) goto err_close_pam; fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) & ~O_NONBLOCK); fd = open(ttyn, O_RDWR | O_NONBLOCK); if (fd != 1) goto err_close_pam; fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) & ~O_NONBLOCK); fd = open(ttyn, O_RDWR | O_NONBLOCK); if (fd != 2) goto err_close_pam; fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) & ~O_NONBLOCK); } Such obscure problems are much easier to detect and troubleshoot thanks to tools like strace .","tags":"SELinux","url":"https://blog.siphos.be/2013/04/using-strace-to-troubleshoot-selinux-problems/","loc":"https://blog.siphos.be/2013/04/using-strace-to-troubleshoot-selinux-problems/"},{"title":"SLOT'ing the old swig-1","text":"The SWIG tool helps developers in building interfaces/libraries that can be accessed from many other languages than the ones the library is initially written in or for. The SELinux userland utility setools uses it to provide Python and Ruby interfaces even though the application itself is written in C. Sadly, the tool currently requires swig-1 for its building of the interfaces and uses constructs that do not seem to be compatible with swig-2 (same with the apse package, btw). I first tried to patch setools to support swig-2, but eventually found regressions in the libapol library it provides so the patch didn't work out (that is why some users mentioned that a previous setools version did build with swig - yes it did, but the result wasn't correct). Recently, a post on Google Plus' SELinux community showed me that I wasn't wrong in this matter (it really does require swig-1 and doesn't seem to be trivial to fix). Hence, I have to fix the gentoo build problem where one set of tools requires swig-1 and another swig-2. Otherwise world-updates and even building stages for SELinux systems would fail as Portage finds incompatible dependencies. One way to approach this is to use Gentoo's support for SLOTs . When a package (ebuild) in Gentoo defines a SLOT, it tells the package manager that the same package but a different version might be installed alongside the package if that has a different SLOT version. In case of swig, the idea is to give swig-1 a different slot than swig-2 (which uses SLOT=\"0\" ) and make sure that both do not install the same files (otherwise you get file collisions). Luckily, swig places all of its files except for the swig binary itself in /usr/share/swig/<version> , so all I had left to do was to make sure the binary itself is renamed. I chose to use swig1.3 (similar as to how tools like ruby and python and for some packages even java is implemented on Gentoo). The result (through bug 466650 ) is now in the tree, as well as an adapted setools package that uses the new swig SLOT. Thanks to Samuli Suominen for getting me on the (hopefully ;-) right track. I don't know why I was afraid of doing this, it was much less complex than I thought (now let's hope I didn't break other things ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/sloting-the-old-swig-1/","loc":"https://blog.siphos.be/2013/04/sloting-the-old-swig-1/"},{"title":"Mitigating DDoS attacks","text":"Lately, DDoS attacks have been in the news more than I was hoping for. It seems that the botnets or other methods that are used to generate high-volume traffic to a legitimate service are becoming more and more easy to get and direct. At the time that I'm writing this post (a few days before its published though), the popular Reddit site is undergoing a DDoS attack which I hope will be finished (or mitigated) soon. But what can a service do against DDoS attacks? After all, DDoS is like gasping for air if you can't swim and are (almost) drowning: the air is the legitimate traffic, but the water is overwhelming and your mouth, pharynx and trachea just aren't made to deal with this properly. And unlike specific Denial-of-Service attacks that use a vulnerability or malcrafted URL, you cannot just install some filter or upgrade a component to be safe again. Methods for mitigating DDoS attacks (beyond increasing your bandwidth as that is very expensive and the botnets involved can go up to 130 Gbps , not a bandwidth you are probably willing to pay for if legitimate services on your site have enough with 10 Mbps) that come to mind are of all sorts of \"classes\"... Configure your servers and services that they stay alive under pressure . Look for the sweet spot where performance of the services is still stable where a higher load means performance degradation. If you have some experience with load testing, you know that throughput on a service initially goes up linearly with the load (first phase). Then, it slows down (but still rises - phase 2) up to a point that, when you increase the load even further just a bit, the service degrades (and sometimes doesn't even get back to its feed when you remove the additional load again - phase3). You need to look for the spot where load and performance is stable (somewhere at the middle of the second phase) and configure your systems so that additional load is dropped. Yes, this means that the DDoS will be more effective, but also means that your systems can easily get back up to their feet when the attack has finished (and you get a more predictable load and consequences). Investigate if you can have a backup service that has a higher throughput ability (with reduced functionality). If the DDoS attack focuses on the system resources rather than network resources involved, such a backup \"lighter\" service can be used to still provide basic functionality (for instance a more static website), but even in case of network resource consumption it can have the advantage that the network consumption that your servers are placing (while replying to the requests) are lower. Depending on the service you offer (and financial means you have at your disposal) you can look at redirecting traffic to more specialized services. Companies like Prolexic have systems that \"scrub\" the DDoS traffic from all traffic and only send legitimate requests to your systems. There are several methods for redirecting load, but a common one is to change the DNS records for your service(s) to point to the addresses of those specialized services instead. The lower the TTL (Time To Live) is of the records, the faster the redirect might take place. If you want to be able to handle an increase in load without specialized services, you might want to be able to redirect traffic to cloud services (where you host your service as well) which are generally capable of handling higher throughput than your own equipment (but this too comes at an additional cost). Some people mention that you can switch IP address . This is true only if the DDoS attack is targeting IP addresses and not (DNS-resolved) URIs. You could set up additional IP addresses that are not registered in DNS (yet) and during the attack, extend the service resolving towards the additional addresses as well. If you do not notice a load spread of the DDoS attack towards the new addresses, you can remove the old addresses from DNS. But again, this won't work generally - not only are most DDoS attacks using DNS-resolved URIs, most of the time attackers are actively involved in the attack and will quickly notice if such a \"failover\" has occurred (and react against it). Depending on your relationship with your provider or location service, you can ask if the edge routers (preferably those of the ISP) can have fallback source filtering rules available to quickly enable. Those fallback rules would then only allow traffic from networks that you know most (all?) of your customers and clients are at. This isn't always possible, but if you have a service that targets mainly people within your country, have the filter only allow traffic from networks of that country. If the DDoS attack uses geographically spread resources, it might be that the number of bots inside those allowed networks are low enough that your service can continue. Configure your firewalls (and ask that your ISP does the same) to not accept (drop) traffic not expected. If the services on your architecture do not use external DNS, then you can drop incoming DNS response packets (a popular DDoS attack method is by using spoofed addresses towards open DNS resolvers; called a DNS reflection attack). And finally, if you are not bound to a single data center, you might want to spread services across multiple locations . Although more difficult from a management point of view, a dispersed/distributed architecture allows other services to continue running while one is being attacked.","tags":"Security","url":"https://blog.siphos.be/2013/04/mitigating-ddos-attacks/","loc":"https://blog.siphos.be/2013/04/mitigating-ddos-attacks/"},{"title":"Introducing selocal for small SELinux policy enhancements","text":"When working with a SELinux-enabled system, administrators will eventually need to make small updates to the existing policy. Instead of building their own full policy (always an option, but most likely not maintainable in the long term) one or more SELinux policy modules are created (most distributions use a modular approach to the SELinux policy development) which are then loaded on their target systems. In the past, users had to create their own policy module by creating (and maintaining) the necessary .te file(s), building the proper .pp files and loading it in the active policy store. In Gentoo, from policycoreutils-2.1.13-r11 onwards, a script is provided to the users that hopefully makes this a bit more intuitive for regular users: selocal . As the name implies, selocal aims to provide an interface for handling local policy updates that do not need to be packaged or distributed otherwise. It is a command-line application that you feed single policy rules at one at a time. Each rule can be accompanied with a single-line comment, making it obvious for the user to know why he added the rule in the first place. # selocal --help Usage: selocal [] [] Command can be one of: -l, --list List the content of a SELinux module -a, --add Add an entry to a SELinux module -d, --delete Remove an entry from a SELinux module -M, --list-modules List the modules currently known by selocal -u, --update-dep Update the dependencies for the rules -b, --build Build the SELinux module (.pp) file (requires privs) -L, --load Load the SELinux module (.pp) file (requires privs) Options can be one of: -m, --module Module name to use (default: selocal) -c, --comment Comment (with --add) The option -a requires that a rule is given, like so: selocal -a \"dbadm_role_change(staff_r)\" The option -d requires that a line number, as shown by the --list, is given, like so: selocal -d 12 Let's say that you need to launch a small script you written as a daemon, but you want this to run while you are still in the staff_t domain (it is a user-sided daemon you use personally). As regular staff_t isn't allowed to have processes bind on generic ports/nodes, you need to enhance the SELinux policy a bit. With selocal , you can do so as follows: # selocal --add \"corenet_tcp_bind_generic_node(staff_t)\" --comment \"Launch local webserv.py daemon\" # selocal --add \"corenet_tcp_bind_generic_port(staff_t)\" --comment \"Launch local webserv.my daemon\" # selocal --build --load (some output on building the policy module) When finished, the local policy is enhanced with the two mentioned rules. You can query which rules are currently stored in the policy: # selocal --list 12: corenet_tcp_bind_generic_node(staff_t) # Launch local webserv.py daemon 13: corenet_tcp_bind_generic_port(staff_t) # Launch local webserv.py daemon If you need to delete a rule, just pass the line number: # selocal --delete 13 Having this tool around also makes it easier to test out changes suggested through bugreports. When I test such changes, I add in the bug report ID as the comment so I can track which settings are still local and which ones have been pushed to our policy repository. Underlyingly, selocal creates and maintains the necessary policy file in \\~/.selocal and by default uses the selocal policy module name. I hope this tool helps users with their quest on using SELinux. Feedback and comments are always appreciated! It is a small bash script and might still have a few bugs in it, but I have been using it for a few months so most quirks should be handled.","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/introducing-selocal-for-small-selinux-policy-enhancements/","loc":"https://blog.siphos.be/2013/04/introducing-selocal-for-small-selinux-policy-enhancements/"},{"title":"Transforming GuideXML to DocBook","text":"I recently committed an XSL stylesheet that allows us to transform the GuideXML documents (both guides and handbooks) to DocBook. This isn't part of a more elaborate move to try and push DocBook instead of GuideXML for the Gentoo Documentation though (I'd rather direct documentation development more to the Gentoo wiki instead once translations are allowed): instead, I use it to be able to generate our documentation in other formats (such as PDF but also ePub) when asked. If you're not experienced with XSL: XSL stands for Extensible Stylesheet Language and can be seen as a way of \"programming\" in XML. A stylesheet allows developers to transform one XML document towards another format (either another XML, or as text-like output like wiki ) while manipulating its contents. In case of documentation, we try to keep as much structure in the document as possible, but other uses could be to transform a large XML with only a few interesting fields towards a very small XML (only containing those fields you need) for further processing. For now (and probably for the foreseeable future), the stylesheet is to be used in an offline mode (we are not going to provide auto-generated PDFs of all documents) as the process to convert a document from GuideXML to DocBook to XML:FO to PDF is quite resource-intensive. But users that are interested can use the stylesheet as linked above to create their own PDFs of the documentation. Assuming you have a checkout of the Gentoo documentation, this process can be done as follows (example for the AMD64 handbook): $ xsltproc docbook.xsl /path/to/handbook-amd64.xml > /somewhere/handbook-amd64.docbook $ cd /somewhere $ xsltproc --output handbook-amd64.fo --stringparam paper.type A4 /usr/share/sgml/docbook/xsl-stylesheets/fo/docbook.xsl handbook-amd64.docbook $ fop handbook-amd64.fo handbook-amd64.pdf The docbook stylesheets are offered by the app-text/docbook-xsl-stylesheets package whereas the fop command is provided by dev-java/fop . I have an example output available (temporarily) at my dev space (amd64 handbook) but I'm not going to maintain this for long (so the link might not work in the near future).","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/transforming-guidexml-to-docbook/","loc":"https://blog.siphos.be/2013/04/transforming-guidexml-to-docbook/"},{"title":"Comparing performance with sysbench: performance analysis","text":"So in the past few posts I discussed how sysbench can be used to simulate some workloads, specific to a particular set of tasks. I used the benchmark application to look at the differences between the guest and host on my main laptop, and saw a major performance regression with the memory workload test. Let's view this again, using parameters more optimized to view the regressions: $ sysbench --test=memory --memory-total-size=32M --memory-block-size=64 run Host: Operations performed: 524288 (2988653.44 ops/sec) 32.00 MB transferred (182.41 MB/sec) Guest: Operations performed: 524288 (24920.74 ops/sec) 32.00 MB transferred (1.52 MB/sec) $ sysbench --test=memory --memory-total-size=32M --memory-block-size=32M run Host: Operations performed: 1 ( 116.36 ops/sec) 32.00 MB transferred (3723.36 MB/sec) Guest: Operations performed: 1 ( 89.27 ops/sec) 32.00 MB transferred (2856.77 MB/sec) From looking at the code (gotta love Gentoo for making this obvious ;-) we know that the memory workload, with a single thread, does something like the following: total_bytes = 0; repeat until total_bytes >= memory-total-size: thread_mutex_lock() total_bytes += memory-block-size thread_mutex_unlock() (start event timer) pointer -> buffer; while pointer <-> end-of(buffer) write somevalue at pointer pointer++ (stop event timer) Given that the regression is most noticeable when the memory-block-size is very small, the part of the code whose execution count is much different between the two runs is the mutex locking, global memory increment and the start/stop of event timer. In a second phase, we also saw that mutex locking itself is not impacted. In the above case, we have 524288 executions. However, if we run the mutex workload this number of times, we see that this hardly has any effect: $ sysbench --test=mutex --mutex-num=1 --mutex-locks=524288 --mutex-loops=0 run Host: total time: 0.0275s Guest: total time: 0.0286s The code for the mutex workload, knowing that we run with one thread, looks like this: mutex_locks = 524288 (start event timer) do lock = get_mutex() thread_mutex_lock() global_var++ thread_mutex_unlock() mutex_locks-- until mutex_locks = 0; (stop event timer) To check if the timer might be the culprit, let's look for a benchmark that mainly does timer checks. The cpu workload can be used, when we tell sysbench that the prime to check is 3 (as its internal loop runs from 3 till the given number, and when the given number is 3 it skips the loop completely) and we ask for 524288 executions. $ sysbench --test=cpu --cpu-max-prime=3 --max-requests=524288 run Host: total time: 0.1640s Guest: total time: 21.0306s Gotcha! Now, the event timer (again from looking at the code) contains two parts: getting the current time (using clock_gettime() ) and logging the start/stop (which is done in memory structures). Let's make a small test application that gets the current time (using the real-time clock as the sysbench application does) and see if we get similar results: $ cat test.c #include <stdio.h> #include <time.h> int main(int argc, char **argv, char **arge) { struct timespec tps; long int i = 524288; while (i-- > 0) clock_gettime(CLOCK_REALTIME, &tps); } $ gcc -lrt -o test test.c $ time ./test Host: 0m0.019s Guest: 0m5.030s So given that the clock_gettime() is ran twice in the sysbench, we already have 10 seconds of overhead on the guest (and only 0,04s on the host). When such time-related functions are slow, it is wise to take a look at the clock source configured on the system. On Linux, you can check this by looking at /sys/devices/system/clocksource/* . # cd /sys/devices/system/clocksource/clocksource0 # cat current_clocksource kvm-clock # cat available_clocksource kvm-clock tsc hpet acpi_pm Although kvm-clock is supposed to be the best clock source, let's switch to the tsc clock: # echo tsc > current_clocksource If we rerun our test application, we get a much more appreciative result: $ time ./test Host: 0m0.019s Guest: 0m0.024s So, what does that mean for our previous benchmark results? $ sysbench --test=cpu --cpu-max-prime=20000 run Host: 35,3049 sec Guest (before): 36,5582 sec Guest (now): 35,6416 sec $ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run Host: 1,8424 MB/sec Guest (before): 1,5591 MB/sec Guest (now): 1,5912 MB/sec $ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run Host: 3959,78 MB/sec Guest (before) 3079,29 MB/sec Guest (now): 3821,89 MB/sec $ sysbench --test=threads --num-threads=128 --max-time=10s run Host: 9765 executions Guest (before): 512 executions Guest (now): 529 executions So we notice that this small change has nice effects on some of the tests. The CPU benchmark improves from 3,55% overhead to 0,95%; fileio is the same (from 15,38% to 13,63%), memory improves from 22,24% overhead to 3,48% and threads remains about status quo (from 94,76% slower to 94,58%). That doesn't mean that the VM is now suddenly faster or better than before - what we changed was how fast a certain time measurement takes, which the benchmark software itself uses rigorously. This goes to show how important it is to understand fully how the benchmark software works and measures realize the importance of access to source code is not to be misunderstood know that performance benchmarks give figures, but do not tell you how your users will experience the system That's it for the sysbench benchmark for now (the MySQL part will need to wait until a later stage).","tags":"Free Software","url":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-3/","loc":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-3/"},{"title":"Comparing performance with sysbench: memory, threads and mutexes","text":"In the previous post, I gave some feedback on the cpu and fileio workload tests that sysbench can handle. Next on the agenda are the memory , threads and mutex workloads. When using the memory workload, sysbench will allocate a buffer (provided through the --memory-block-size parameter, defaults to 1kbyte) and each execution will read or write to this memory ( --memory-oper , defaults to write) in a random or sequential manner ( --memory-access-mode , defaults to seq uential). $ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run Host throughput, 1M: 3959,78 MB/sec Guest throughput, 1M: 3079,29 MB/sec The guest has a lower throughput (about 77% of the host), which is lower than what most online posts provide on KVM performance. We'll get back to that later. Let's look at the default block size of 1k (meaning that the benchmark will do a lot more executions before it reaches the total memory (in load): $ sysbench --test=memory --memory-total-size=1G run Host throughput, 1k: 1702,59 MB/sec Guest throughput, 1k: 23,67 MB/sec This is a lot worse: the guest' throughput is only 1,4% of the host throughput! The qemu-kvm process on the host is also taking up a lot of CPU. Now let's take a look at the other workload, threads . In this particular workload, you identify the number of threads ( --num-threads ), the number of locks ( --thread-locks ) and the number of times a thread should run its 'lock-yield..unlock' workload ( --thread-yields ). The more locks you identify, the less number of threads will have the same lock (each thread is allocated a single lock during an execution, but every new execution will give it a new lock so the threads do not always take the same lock). Note that parts of this is also handled by the other tests: mutex'es are used when a new operation (execution) for the thread is prepared. In case of the memory-related workload above, the smaller the buffer size, the more frequent thread operations are needed. In the last run we did (with the bad performance), millions of operations were executed (although no yields were performed). Something similar can be simulated using a single lock, single thread and a very high number of operations and no yields: $ sysbench --test=threads --num-threads=1 --thread-yields=0 --max-requests=1000000 --thread-locks=1 run Host runtime: 0,3267 s (event: 0,2278) Guest runtime: 40,7672 s (event: 30,6084) This means that the guest \"throughput\" problems from the memory identified above seem to be related to this rather than memory-specific regressions. To verify if the scheduler itself also shows regressions, we can run more threads concurrently. For instance, running 128 threads simultaneously, using the otherwise default settings, during 10 seconds: $ sysbench --test=threads --num-threads=128 --max-time=10s run Host: 9765 executions (events) Guest: 512 executions (events) Here we get only 5% throughput. Let's focus on the mutex again, as sysbench has an additional mutex workload test. The workload has each thread running a local fast loop (simple increments, --mutex-loops ) after which it takes a random mutex (one of --mutex-num ), locks it, increments a global variable and then releases the mutex again. This is repeated for the number of locks identified ( --mutex-locks ). If mutex operations would be the cause of the performance issues above, then we would notice that the mutex operations are a major performance regression on my system. Let's run that workload with a single thread (default), no loops and a single mutex. $ sysbench --test=mutex --mutex-num=1 --mutex-locks=50000000 --mutex-loops=1 run Host (duration): 2600,57 ms Guest (duration): 2571,44 ms In this example, we see that the mutex operations are almost at the same speed (99%) of the host, so pure mutex operations are not likely to be the cause of the performance regressions earlier on. So what does give the performance problems? Well, that investigation will be for the third and last post in this series ;-)","tags":"Free Software","url":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-2/","loc":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-2/"},{"title":"Another Gentoo Hardened month has passed","text":"Another month has passed, so time to mention again what we have all been doing lately ;-) Toolchain Version 4.8 of GCC is available in the tree, but currently masked. The package contains a fix needed to build hardened-sources, and a fix for the asan (address sanitizer). asan support in GCC 4.8 might be seen as an improvement security-wise, but it is yet unclear if it is an integral part of GCC or could be disabled with a configure flag. Apparently, asan \"makes building gcc 4.8 crazy\". Seeing that it comes from Google, and building Google Chromium is also crazy, I start seeing a pattern here. Anyway, it turns out that PaX/grSec and asan do not get along yet (ASAN assumes/uses hardcoded userland address space size values, which breaks when UDEREF is set as it pitches a bit from the size): ERROR: AddressSanitizer failed to allocate 0x20000001000 (2199023259648) bytes at address 0x0ffffffff000 Given that this is hardcoded in the resulting binaries, it isn't sufficient to change the size value from 47 bits to 46 bits as hardened systems can very well boot a kernel with and another kernel without UDEREF, causing the binaries to fail on the other kernel. Instead, a proper method would be to dynamically check the size of a userland address. However, GCC 4.8 also brings along some nice enhancements and features. uclibc profiles work just fine with GCC 4.8, including armv7a and mips/mipsel. The latter is especially nice to hear, since mips used to require significant effort with previous GCCs. Kernel and grSecurity/PaX More recent kernels have now been stabilized to stay close to the grSecurity/PaX upstream developments. The most recent stable kernel now is hardened-sources-3.8.3. Others still available are hardened-sources versions 3.2.40-r1 and 2.6.32-r156. The support for XATTR_PAX is still progressing, but a few issues have come up. One is that non-hardened systems are seeing warnings about pax-mark not being able to set the XATTR_PAX on tmpfs since vanilla kernels do not have the patch to support user.* extended attribute namespaces for tmpfs. A second issue is that the install application, as provided by coreutils , does not copy extended attributes. This has impact on ebuilds where pax markings are done before the install phase of a package. But only doing pax markings after the install phase isn't sufficient either, since sometimes we need the binaries to be marked already for test phases or even in the compile phase. So this is still something on the near horizon. Most likely the necessary tools will be patched to include extended attributes on copy operations. However, we need to take care only to copy over those attributes that make sense: user.pax does, but security ones like security.evm and security.selinux shouldn't as those are either recomputed when needed, or governed through policy. The idea is that USE=\"pax_kernel\" will enable the above on coreutils . SELinux The SELinux support in Gentoo has seen a fair share of updates on the userland utilities (like policycoreutils, setools, libselinux and such). Most of these have already made the stable tree or are close to be bumped to stable. The SELinux policy also has been updated a lot: most changes can be tracked through bugzilla, looking for the sec-policy r13 whiteboard. The changes can be applied to the system immediately if you use the live ebuilds (like selinux-base-9999 ), but I'm planning on releasing revision 13 of our policy set soon. System Integrity Some of the \"early adopter\" problems we've noticed on Gentoo Hardened have been integrated in the repositories upstream and are slowly progressing towards the main Linux kernel tree. Profiles All hardened profiles have been moved to the 13.0 base. Some people frowned when they noticed that the uclibc profiles do not inherit from any architecture-related profile. This is however with reason: the architecture profiles are (amongst other reasons) focusing on the glibc specifics of the architecture. Since the profile intended here is for uclibc, those changes are not needed (nor wanted). Hence, these are collapsed in a single profile. Documentation For SELinux, the SELinux handbook now includes information about USE=\"unconfined\" as well as the selinux_gentoo init script as provided by policycoreutils . Users who are already running with SELinux enabled can just look at the Change History to see which changes affect them. A set of tutorials (which I've blogged about earlier as well) have been put online at the Gentoo Wiki . Next to the SELinux tutorials, an article pertaining to AIDE has been added as well as it fits nicely within the principles/concepts of the System Integrity subproject. Media If you don't do it already, start following @GentooHardened ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/another-gentoo-hardened-month-has-passed/","loc":"https://blog.siphos.be/2013/04/another-gentoo-hardened-month-has-passed/"},{"title":"Comparing performance with sysbench: cpu and fileio","text":"Being busy with virtualization and additional security measures, I frequently come in contact with people asking me what the performance impact is. Now, you won't find the performance impact of SELinux here as I have no guests nor hosts that run without SELinux. But I did want to find out what one can do to compare system (and later application) performance, so I decided to take a look at the various benchmark utilities available. In this first post, I'll take a look at sysbench (using 0.4.12, released on March 2009 - unlike what you would think from the looks of the site alone) to compare the performance of my KVM guest versus host. The obligatory system information: the host is a HP Pavilion dv7 3160eb with an Intel Core i5-430M processor (dual-core with 2 threads per core). Frequency scaling is disabled - the CPU is fixed at 2.13 Ghz. The system has 4Gb of memory (DDR3), the internal hard disks are configured as a software RAID1 and with LVM on top (except for the file system that hosts the virtual guest images, which is a plain software RAID1). The guests run with the boot options given below, meaning 1.5Gb of memory, 2 virtual CPUs of the KVM64 type. The CFLAGS for both are given below as well, together with the expanded set given by gcc \\${CFLAGS} -E -v - &1 | grep cc1 . /usr/bin/qemu-kvm -monitor stdio -nographic -gdb tcp::1301 -vnc 127.0.0.1:14 -net nic,model=virtio,macaddr=00:11:22:33:44:b3,vlan=0 -net vde,vlan=0 -drive file=/srv/virt/gentoo/test/pg1.img,if=virtio,cache=none -k nl-be -m 1536 -cpu kvm64 -smp 2 # For host CFLAGS=\"-march=core2 -O2 -pipe\" #CFLAGS=\"-D_FORTIFY_SOURCE=2 -fno-strict-overflow -march=core2 -fPIE -O2 -fstack-protector-all\" # For guest CFLAGS=\"-march=x86-64 -O2 -pipe\" #CFLAGS=\"-fno-strict-overflow -march=x86-64 -fPIE -O2 -fstack-protector-all\" I am aware that the CFLAGS between the two are not the same (duh), and I know as well that the expansion given above isn't entirely accurate. But still, it gives some idea on the differences. Now before I go on to the results, please keep in mind that I am not a performance expert , not even a performance experienced or even performance wanna-be experienced person: the more I learn about the inner workings of an operating system such as Linux, the more complex it becomes. And when you throw in additional layers such as virtualization, I'm almost completely lost. In my day-job, some people think they can \"prove\" the inefficiency of a hypervisor by counting from 1 to 100'000 and adding the numbers, and then take a look at how long this takes. I think this is short-sighted, as this puts load on a system that does not simulate reality. If you really want to do performance measures for particular workloads, you need to run those workloads and not some small script you hacked up. That is why I tend to focus on applications that use workload simulations for infrastructural performance measurements (like HammerDB for performance testing databases). But for this blog post series, I'm first going to start with basic operations and later posts will go into more detail for particular workloads (such as database performance measurements). Oh, and BTW, when I display figures with a comma (\",\"), that comma means decimal (so \"1,00\" = \"1\"). The figures below are numbers that can be interpreted in many ways, and can prove everything. I'll sometimes give my interpretation to it, but don't expect to learn much from it - there are probably much better guides out there for this. The posts are more of a way to describe how sysbench works and what you should take into account when doing performance benchmarks. So the testing is done using sysbench , which is capable of running CPU, I/O, memory, threading, mutex and MySQL tests. The first run of it that I did was a single-thread run for CPU performance testing. $ sysbench --test=cpu --cpu-max-prime=20000 run This test verifies prime numbers by dividing the number with sequentially increasing numbers and verifying that the remainder (modulo calculation) is zero. If it is, then the number is not prime and the calculation goes on to the next number; otherwise, if none have a remainder of 0, then the number is prime. The maximum number that it divides by is calculated by taking the integer part of the square root of the number (so for 17, this is 4). This algorithm is very simple, so you should also take into account that during the compilation of the benchmark, the compiler might already have optimized some of it. Let's look at the numbers. Run Stat Host Guest 1.1 total 35,4331 37,0528 e.total 35,4312 36,8917 1.2 total 35,1482 36,1951 e.total 35,1462 36,0405 1.3 total 35,3334 36,4266 e.total 35,3314 36,2640 ================================ avg total 35,3049 36,5582 e.total 35,3029 36,3987 med total 35,3334 36,4266 e.total 35,3314 36,2640 On average (I did three runs on each system), the guest took 3,55% more time to finish the test than the host ( total ). If we look at the pure calculation (so not the remaining overhead of the inner workings - e.total ) then the guest took 3,10% more time. The median however (the run that wasn't the fastest nor the slowest of the three) has the guest taking 3,09% more time (total) and 2,64% more time (e.total). Let's look at the two-thread results. Run Stat Host Guest 1.1 total 17,5185 18,0905 e.total 35,0296 36,0217 1.2 total 17,8084 18,1070 e.total 35,6131 36,0518 1.3 total 18,0683 18,0921 e.total 36,1322 36,0194 ================================ avg total 17,5185 18,0965 e.total 35,0296 36,0310 med total 17,8084 18,0921 e.total 35,6131 36,0194 With these figures, we notice that the guest average total run time takes 1,67% more time to complete, and the event time only 1,23%. I was personally expecting that the guest would have a higher percentage than previously (gut feeling - never trust it when dealing with complex matter) but was happy to see that the difference wasn't higher. I'm not going to start analyze this in more detail and just go to the next test: fileio. In case of fileio testing, I assume that the hypervisor will take up more overhead , but keep in mind that you also need to consider the environmental factors: LVM or not, RAID1 or not, mount options, etc. Since I am comparing guests versus hosts here, I should look for a somewhat comparable setup. Hence, I will look for the performance of the host (software raid, LVM, ext4 file system with data=ordered) and the guest (images on software raid, ext4 file system with data=ordered and barrier=0, and LVM in guest). Furthermore, running a sysbench test suggests a file that is much larger than the available RAM. I'm going to run the tests on a 6Gb file size, but enable O_DIRECT for writes so that some caches (page cache) are not used. This can be done using --file-extra-flags=direct . As with all I/O-related benchmarks, you need to define which kind of load you want to test with. Are the I/Os sequential (like reading or writing a large file completely) or random? For databases, you are most likely interested in random reads (data, for selects) and sequential writes (into transaction logs). A file server usually has random read/write. In the below test, I'll use a combined r a nd om r ead/ w rite. $ sysbench --test=fileio --file-total-size=6G prepare $ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run $ sysbench --test=fileio --file-total-size=6G cleanup In the output, the throughput seems to be most important: Operations performed: 4348 Read, 2898 Write, 9216 Other = 16462 Total Read 67.938Mb Written 45.281Mb Total transferred 113.22Mb (1.8869Mb/sec) In the above case, the throughput is 1,8869 Mbps. So let's look at the (averaged) results: Host: 1,8424 Mbps Guest: 1,5591 Mbps The above figures (which are an average of 3 runs) tell us that the guest has a throughput of about 84,75% (so we take about 15% performance hit on random read/write I/O). Now I used sysbench here for some I/O validation of guest between host, but other usages apply as well. For instance, let's look at the impact of data=ordered versus data=journal (taken on the host): 6G, data=ordered, barrier=1: 1,8435 Mbps 6G, data=ordered, barrier=0: 2,1328 Mbps 6G, data=journal, barrier=1: 599,85 Kbps 6G, data=journal, barrier=0: 767,93 Kbps From the figures, we can see that the data=journal option slows down the throughput to a final figure about 30% of the original throughput (70% decrease!). Also, disabling barriers has a positive impact on performance, giving about 15% throughput gain. This is also why some people report performance improvements when switching to LVM, as - as far as I can tell (but finding a good source on this is difficult) - LVM by default disables barriers (but does honor the barrier=1 mount option if you provide it). That's about it for now - the next post will be about the memory and threads tests within sysbench.","tags":"Free Software","url":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench/","loc":"https://blog.siphos.be/2013/04/comparing-performance-with-sysbench/"},{"title":"Simple drawing for I/O positioning","text":"Instead of repeatedly trying to create an overview of the various layers involved with I/O operations within Linux on whatever white-board is in the vicinity, I decided to draw one up in Draw.io that I can then update as I learn more from this fascinating world. The drawing's smaller blocks within the layers are meant to give some guidance to what is handled where, so they are definitely not complete. So for those interested (or those that know more of it than I ever will and prepared to help me out): I hope it isn't too far from the truth.","tags":"Documentation","url":"https://blog.siphos.be/2013/04/simple-drawing-for-io-positionin/","loc":"https://blog.siphos.be/2013/04/simple-drawing-for-io-positionin/"},{"title":"What could SELinux have done to mitigate the postgresql vulnerability?","text":"Gentoo is one of the various distributions which supports SELinux as a Mandatory Access Control system to, amongst other things, mitigate the results of a succesfull exploit against software. So what about the recent PostgreSQL vulnerability ? When correctly configured, the PostgreSQL daemon will run in the postgresql_t domain. In SELinux-speak, a domain can be seen as a name granted to a set of permissions (what is allowed) and assigned to one or more processes. A process that \"runs in domain postgresql_t\" will be governed by the policy rules (what is and isn't allowed) for that domain. The vulnerability we speak of is about creating new files or overwriting existing files, potentially corrupting the database itself (when the database files are overwritten). Creating new files is handled through the create privilege on files (and add_name on directories), writing into files is handled through the write privilege. Given certain circumstances, one could even write commands inside files that are executed by particular users on the system (btw, the link gives a great explanation on the vulnerability). So let's look at what SELinux does and could have done. In the current situation, as we explained, postgresql_t is the only domain we need to take into account (the PostgreSQL policy does not use separate domains for the runtime processes). Let's look at what directory labels it is allowed to write into: $ sesearch -s postgresql_t -c dir -p add_name -SCATd Found 11 semantic av rules: allow postgresql_t postgresql_log_t : dir { add_name } ; allow postgresql_t var_log_t : dir { add_name } ; allow postgresql_t var_lock_t : dir { add_name } ; allow postgresql_t tmp_t : dir { add_name } ; allow postgresql_t postgresql_tmp_t : dir { add_name } ; allow postgresql_t postgresql_var_run_t : dir { add_name } ; allow postgresql_t postgresql_db_t : dir { add_name } ; allow postgresql_t etc_t : dir { add_name } ; allow postgresql_t tmpfs_t : dir { add_name } ; allow postgresql_t var_lib_t : dir { add_name } ; allow postgresql_t var_run_t : dir { add_name } ; So the PostgreSQL service is allowed to create files inside directories labeled with one of the following labels: postgresql_log_t , used for PostgreSQL log files ( /var/log/postgresql ) var_log_t , used for the generic log files ( /var/log ) var_lock_t , used for lock files ( /run/lock or /var/lock ) tmp_t , used for the temporary file directory ( /tmp or /var/tmp ) postgresql_tmp_t , used for the PostgreSQL temporary files/directories postgresql_var_run_t , used for the runtime information (like PID files) of PostgreSQL ( /var/run/postgresql ) postgresql_db_t , used for the PostgreSQL database files ( /var/lib/postgresql ) etc_t , used for the generic system configuration files ( /etc/ ) var_lib_t , used for the /var/lib data var_run_t , used for the /var/run or /run data Next to this, depending on the label of the directory, the PostgreSQL service is allowed to write into files with the following label assigned (of importance to both creating new files as well as overwriting existing ones): $ sesearch -s postgresql_t -c file -p write -SCATd Found 11 semantic av rules: allow postgresql_t postgresql_log_t : file { write } ; allow postgresql_t postgresql_lock_t : file { write } ; allow postgresql_t faillog_t : file { write } ; allow postgresql_t lastlog_t : file { write } ; allow postgresql_t postgresql_tmp_t : file { write } ; allow postgresql_t hugetlbfs_t : file { write } ; allow postgresql_t postgresql_var_run_t : file { write } ; allow postgresql_t postgresql_db_t : file { write } ; allow postgresql_t postgresql_t : file { write } ; allow postgresql_t security_t : file { write } ; allow postgresql_t etc_t : file { write } ; Found 6 semantic te rules: type_transition postgresql_t var_log_t : file postgresql_log_t; type_transition postgresql_t var_lock_t : file postgresql_lock_t; type_transition postgresql_t tmp_t : file postgresql_tmp_t; type_transition postgresql_t tmpfs_t : file postgresql_tmp_t; type_transition postgresql_t var_lib_t : file postgresql_db_t; type_transition postgresql_t var_run_t : file postgresql_var_run_t; If an exploit creates a new file, the add_name permission on the directory is needed. If otoh the exploit is overwriting existing files, I think the only permission needed here is the write on the files (also open but all the writes have open as well in the above case). Now accessing and being able to write files into the database file directory is expected - it is the functionality of the server, so unless we could separate domains more, this is a \"hit\" we need to take. Sadly though, this is also the label used for the PostgreSQL service account home directory here (not sure if this is for all distributions), making it more realistic that an attacker writes something in the home directory .profile file and hopes for the administrator to do something like su postgres - . Next, the etc_t write privileges also worry me, not mainly because it can write there, but also because I can hardly understand why - PostgreSQL is supposed to run under its own, non-root user (luckily) so unless there are etc_t labeled directories owned by the PostgreSQL service account (or world writeable - please no, kthx). And this isn't an \"inherited\" permission from something - the policy currently has files_manage_etc_files(postgresql_t) set, and has been since 2005 or earlier. I'm really wondering if this is still needed. But I digress. Given that there are no PostgreSQL-owned directories nor world-writeable ones in /etc , let's look at a few other ones. security_t is used for the SELinux pseudo file system, and is used for the SEPostgreSQL support. From the looks of it, only the root Linux user has the rights to do really harmful things on this file system (and only if he too has write permissions on security_t ), non-root should be limited to verifying if contexts exist or have particular rights. Still, I might investigate this further as I'm intrigued about many of the pseudo files in /sys/fs/selinux that I'm not fully sure yet what they deal with. tmp_t should not be a major concern. Most (if not all) daemons and services that use temporary files have file transitions to their own type so that access to these files, even if it would be allowed by regular permissions, is still prohibited by SELinux lastlog_t is also a weird one, again because it shouldn't be writeable for anyone else but root accounts; if succesfull, an attacker can overwrite the lastlog information which might be used by some as a means for debugging who was logged on when (part of forensics). Given the information above, it is a bit sad to see that SELinux can't protect PostgreSQL users from this particular vulnerability - most of the \"mitigation\" (if any) is because the process runs as non-root to begin with (which is another hint at users not to think SELinux is sufficient to restrict the permissions of processes). But could it have been different? In my opinion, yes, and I'll see if we can learn from it for the future. First of all, we should do more policy code auditing. It might not be easy to remove policy rules generally, but we should at least try. I use a small script that enables auditing (SELinux auditing, so auditallow statements) for the entire domain, and then selectively disables auditing until I get no hits anymore. The remainder of auditallow statements warrant a closer look to see if they are still needed or not. I'll get onto that in the next few days. Second, we might want to have service accounts use a different home directory, where they do have the necessary search privileges for, but no write privileges. Exploits that write stuff into a home directory (hoping for a su postgresql - ) are then mitigated a bit. Third, we might want to look into separating the domains according to the architecture of the service. This requires intimate knowledge of the ins and outs of PostgreSQL and might even require PostgreSQL patching, so is not something light. But if no patching is needed (such as when all process launches are done using known file executions) we could have a separate domain for the master process, server processes and perhaps even the various subfunction processes (like the WAL writer, BG writer, etc.). The Postfix service has such a more diverse (but also complex) policy. Such a subdomain structure in the policy might reduce the risk if the vulnerable process (I think this is the master process) does not need to write to database files (as this is handled by other processes), so no postgresql_db_t write privileges. If others have ideas on how we can improve service security (for instance through SELinux policy development) or knows of other exploits related to this vulnerability that I didn't come across yet, please give a comment on it below.","tags":"Security","url":"https://blog.siphos.be/2013/04/what-could-selinux-have-done-to-mitigate-the-postgresql-vulnerability/","loc":"https://blog.siphos.be/2013/04/what-could-selinux-have-done-to-mitigate-the-postgresql-vulnerability/"},{"title":"Integrity checking with AIDE","text":"As to at least do some progress in the integrity part of Gentoo Hardened (a subproject I'd like to extend towards greater heights), I dediced to write up a small guide on how to work with AIDE . The tool is simple enough (and it allowed me to test its SELinux policy module a bit) so you'll get by fairly quickly. However, what I'd like to know a bit more about is on how to use AIDE on a hypervisor level, scanning through the file systems of the guests, without needing in-guest daemons. I wrote a small part in the guide, but I need to test it more thoroughly. In the end, I'd like to have a configuration that AIDE is running on the host, mounting the guest file systems, scanning the necessary files and sending out reports, all one at a time (snapshot, mount, scan+report, unmount, destroy snapshot, next). If anyone has pointers towards such a setup, it'd be greatly appreciated. It provides, in my opinion, a secure way of scanning systems even if they are completely compromised (in other words you couldn't trust anything running inside the guest or running with the libraries or software within the guest).","tags":"Documentation","url":"https://blog.siphos.be/2013/04/integrity-checking-with-aide/","loc":"https://blog.siphos.be/2013/04/integrity-checking-with-aide/"},{"title":"Not needing run_init for password-less service management","text":"One of the things that has been bugging me was why, even with having pam_rootok.so set in /etc/pam.d/run_init , I cannot enjoy passwordless service management without using run_init directly: # rc-service postgresql-9.2 status Authenticating root. Password: # run_init rc-service postgresql-9.2 status Authenticating root. * status: started So I decided to strace the two commands and look for the differences. I found out that there is even a SELinux permission for being able to use the rootok setting for passwords! Apparently, pam_rootok.so is SELinux-aware and does some additional checks. Although I don't know the exact details of it, it looks for the context before the call (exec) of run_init occurred. Then it checks if this domain has the right for passwd { rootok } (unless SELinux is in permissive, in which case it just continues) and only then it allows the \"rootok\" to succeed. Now why doesn't this work without using run_init ? I think it has to do with how we integrate run_init in the scripts, because out of the trace I found that the previous context was also run_init_t (instead of sysadm_t ): 20451 open(\"/proc/self/task/20451/attr/current\", O_RDONLY) = 3 20451 read(3, \"root:sysadm_r:run_init_t\\0\", 4095) = 25 20451 close(3) = 0 20451 gettid() = 20451 20451 open(\"/proc/self/task/20451/attr/prev\", O_RDONLY) = 3 20451 read(3, \"root:sysadm_r:run_init_t\\0\", 4095) = 25 20451 close(3) Because there already is a transition to run_init_t upon calling the scripts, the underlying call to runscripts causes the \"previous\" attribute to be set to run_init_t as well, and only then is run_init called (which then causes the PAM functions to be called). But by prepending the commands with run_init (which quickly causes the PAM functions to be called) the previous context is sysadm_t . I tested on a system with the following policy update, and this succeeds nicely. policy_module(localruninit, 1.0) gen_require(` class passwd { passwd chfn chsh rootok }; type run_init_t; ') allow run_init_t self:passwd rootok; I'll probably add this in Gentoo's policy.","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/not-needing-run_init-for-password-less-service-management/","loc":"https://blog.siphos.be/2013/04/not-needing-run_init-for-password-less-service-management/"},{"title":"How far reaching vulnerabilities can go","text":"If you follow the news a bit, you know that PostgreSQL has had a significant security vulnerability. The PostgreSQL team announced it up front and communicated how they would deal with the vulnerability (which basically comes down to saying that it is severe, that the public repositories will be temporarily frozen as developers add in the necessary fixes and start building the necessary software for a new release, and at the release moment give more details about the vulnerability. The exploitability of the vulnerability was quickly identified, and we know that compromises wouldn't take long. A blog post from the schemaverse tells us that exploits won't take long (less than 24 hours) and due to the significance of the vulnerability, it cannot be stressed enough that patching should really be part of the minimal security requirements of any security-conscious organization. But patching alone isn't the only thing to consider. The notice that PostgreSQL mentions also that restricting access to the database through pg_hba.conf isn't sufficient, as the vulnerable code is executed before the pg_hba.conf file is read. So one of the mitigations for the vulnerability would be a firewall (hostbased or network) that restricts access to the database so only trusted addresses are allowed. I'm personally an advocate in favor of hostbased firewalls. But the thing that hits me the most, is the amount of applications that use \"embedded\" postgresql database services in their product. If you take part of a larger organization with a large portfolio of software titles running in the data center, you'll undoubtedly have seen lists (through network scans or otherwise) of systems that are running PostgreSQL as part of the product installation (and not as a \"managed\" database service). The HP GUIDManager or the NNMI components or the Systems Insight Manager use embedded PostgreSQL services. The cloudera manager can be easily set up with an \"embedded\" PostgreSQL (which doesn't mean it isn't a full-fledged PostgreSQL, but rather that the setup and management of the service is handled by the product instead of by \"your own\" DBA team). Same with Servoy. I don't disagree with all products providing embedded database platforms, and especially not with choosing for PostgreSQL which I consider a very mature, stable and feature-rich (and not to be forgotten, very active community) database platform. But I do hope that these products take up their responsibility and release updated versions or patches for their installations to their customers very soon. Perhaps I should ask our security operational team to take a scan to actively follow-up on these...","tags":"Security","url":"https://blog.siphos.be/2013/04/how-far-reaching-vulnerabilities-can-go/","loc":"https://blog.siphos.be/2013/04/how-far-reaching-vulnerabilities-can-go/"},{"title":"Separate puppet provider for Gentoo/SELinux?","text":"While slowly transitioning my playground infrastructure towards Puppet, I already am in process of creating a custom provider for things such as services. Puppet uses providers as \"implementations\" for the functions Puppet needs. For instance, for the service type (which handles init script services), there are providers for RedHat, Debian, FreeBSD, ... and it also has providers called gentoo and openrc . The openrc one uses the service scripts that Gentoo's OpenRC provides, such as rc-service and rc-status . On a SELinux-enabled system, and especially when using a decentralized Puppet environment (I dropped the puppet master set in favor of a decentralized usage of Puppet), if you call rc-service to, say, start a service, it will ask for the users' password. Of course, Puppet doesn't want this, so I have to prefix the commands with run_init and have a pam_rootok.so rule in run_init's PAM definition. So far that's a simple change - I just patched the openrc.rb file to do so. But then the second problem I'm facing is that Puppet wants to use return code based commands for checking the run-time state of services. Even though some of my services weren't running, Puppet either thought they were or called the start routine and consider the service started. Sadly that wasn't the case, as the rc-* scripts always return 0 (you'll need to parse the output). So what I did now is to create a simple script called runstatus which returns the state of services. It's crude, but seems to work: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/sh SERVICENAME = $1 ; # We need to exit: # 0 - if running # 1 - if dead but PID exists # 2 - if dead but lock file exists # 3 - if not running # 4 - if unknown rc-status -a -C | grep ${ SERVICENAME } | grep -q started && exit 0 ; rc-status -a -C | grep ${ SERVICENAME } | grep -q stopped && exit 3 ; exit 4 ; I then have the service provider (I now provide my own instead of patching the openrc one) call runstatus to get the state of a service, as well as call it after trying to start a service. But as this is quite basic functioning, I'm wondering if I'm doing things the right way or not. Who else has experience with Puppet and Gentoo, and did you have to tweak things to get services and such working?","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/separate-puppet-provider-for-gentooselinux/","loc":"https://blog.siphos.be/2013/04/separate-puppet-provider-for-gentooselinux/"},{"title":"Matching packages with CVEs","text":"I've come across a few posts on forums (Gentoo and elsewhere) asking why Gentoo doesn't make security-related patches on the tree. Some people think this is the case because they do not notice (m)any GLSAs, which are Gentoo's security advisories. However, it isn't that Gentoo doesn't push out security fixes - it is a matter of putting the necessary human resources against it to write down the GLSAs. Gentoo is often quick with creating the necessary ebuilds for newer versions of software. And newer versions often contain security fixes that mitigate problems detected in earlier versions. So by keeping your system up to date, you get those security fixes as well. But without GLSA, it is difficult to really know which packages are necessary and which aren't, let alone be aware that there are potential problems with your system. I already captured one of those needs through the cvechecker application, so I took a step further and wrote an extremely ugly script (it's so ugly, it would spontaneously become a joke of itself when published) which compiles a list of potential CPEs (identifiers for products used in CVEs) from the Gentoo package list (ugliness 1: it assumes that the package name is the product name). It then tries to assume what the version of that software is based on the ebuild version (ugliness 2: it just takes the a.b.c number). Then, it lists the CVEs affiliated with a particular package, and checks this list with the list of CVEs from an earlier version (ugliness 3: it requires the previous, vulnerable version to still be in the tree). If one of the CVEs has \"disappeared\", it will report that the given package might fix that CVE. Oh, and if the CVE has a CPE that contains more than just a version, the script ignores it (ugliness 4). And it probably ignores a lot of other things as well while not checking the input (ugliness 5 and higher). But if we ignore all that, what does that give for the Gentoo portage tree for the last 7 days? In other words, what releases have been made on the tree that might contain security fixes (and that do comply with the above ugliness)? app-editors/emacs-23.4-r5 might fix CVE-2010-0825 app-editors/emacs-24.2-r1 might fix CVE-2012-0035 app-editors/emacs-24.2-r1 might fix CVE-2012-3479 dev-lang/python-2.6.8-r1 might fix CVE-2010-3492 dev-lang/python-2.6.8-r1 might fix CVE-2011-1521 dev-lang/python-2.6.8-r1 might fix CVE-2012-0845 dev-lang/python-2.6.8-r1 might fix CVE-2012-1150 dev-lang/python-2.6.8-r1 might fix CVE-2008-5983 dev-php/smarty-2.6.27 might fix CVE-2009-5052 dev-php/smarty-2.6.27 might fix CVE-2009-5053 dev-php/smarty-2.6.27 might fix CVE-2009-5054 dev-php/smarty-2.6.27 might fix CVE-2010-4722 dev-php/smarty-2.6.27 might fix CVE-2010-4723 dev-php/smarty-2.6.27 might fix CVE-2010-4724 dev-php/smarty-2.6.27 might fix CVE-2010-4725 dev-php/smarty-2.6.27 might fix CVE-2010-4726 dev-php/smarty-2.6.27 might fix CVE-2010-4727 dev-php/smarty-2.6.27 might fix CVE-2012-4277 dev-php/smarty-2.6.27 might fix CVE-2012-4437 media-sound/rhythmbox-2.97 might fix CVE-2012-3355 net-im/empathy-3.6.3 might fix CVE-2011-3635 net-im/empathy-3.6.3 might fix CVE-2011-4170 sys-cluster/glusterfs-3.3.1-r2 might fix CVE-2012-4417 www-client/seamonkey-2.17 might fix CVE-2013-0788 www-client/seamonkey-2.17 might fix CVE-2013-0789 www-client/seamonkey-2.17 might fix CVE-2013-0791 www-client/seamonkey-2.17 might fix CVE-2013-0792 www-client/seamonkey-2.17 might fix CVE-2013-0793 www-client/seamonkey-2.17 might fix CVE-2013-0794 www-client/seamonkey-2.17 might fix CVE-2013-0795 www-client/seamonkey-2.17 might fix CVE-2013-0796 www-client/seamonkey-2.17 might fix CVE-2013-0797 www-client/seamonkey-2.17 might fix CVE-2013-0800 As you can see, there is still a lot of work to remove bad ones (and add matches for non-default ones), but at least it gives an impression (especially those that have CVEs of 2012 or even 2013 are noteworthy), which is the purpose of this post. It would be very neat if ebuilds, or the package metadata, could give pointers on the CPEs. That way, it would be much easier to check a system for known vulnerabilities through the (publicly) available CVE databases as we then only have to do simple matching. A glsa-check-ng (what's in a name) script would then construct the necessary CPEs based on the installed package list (and the metadata on it), check if there are CVEs against it, and if there are, see if a newer version of the same package is available that has no (or fewer) CVEs assigned to it. Perhaps someone can create a GSoC proposal out of that?","tags":"Gentoo","url":"https://blog.siphos.be/2013/04/matching-packages-with-cves/","loc":"https://blog.siphos.be/2013/04/matching-packages-with-cves/"},{"title":"Linux Sea and ePub update","text":"I just \"published\" a small update on the Linux Sea online book. Nothing major, some path updates (like the move to /etc/portage for the make.conf file). But I wouldn't put a blog post online if there wasn't anything else to say ;-) Recently I was made aware that the ePub versions I publish were broken. I don't use ePub readers myself, so all I do is read the ePubs through a Firefox plug-in and it's been a while that I did that on my own ePubs. Apparently, the stylesheets I used to convert the Docbook to ePub changes behavior (or my scripts abused an error in the previous stylesheets that are fixed now). So right now the ePub version should work again, and the code snippet below is what I use now to build it: xsltproc --stringparam base.dir linuxsea-epub/OEBPS/ /usr/share/sgml/docbook/xsl-stylesheets/epub3/chunk.xsl LINUXSEA.xml; cp -r /path/to/src/linux_sea/images linuxsea-epub/OEBPS; cd linuxsea-epub; zip -X0 linux_sea.epub mimetype; zip -r -X9 linux_sea.epub META-INF OEBPS; mv linux_sea.epub ../;","tags":"Documentation","url":"https://blog.siphos.be/2013/04/linux-sea-and-epub-update/","loc":"https://blog.siphos.be/2013/04/linux-sea-and-epub-update/"},{"title":"Fiddling with puppet apply","text":"As part of a larger exercise, I am switching my local VM set from a more-or-less scripted manual configuration towards a fully Puppet-powered one. Of course, it still uses a lot of custom modules and is most likely too ugly to expose to the wider internet, but it does seem to improve my ability to quickly rebuild images if I corrupt them somehow. One of the tricks I am using is to use a local apply instead of using a Puppet master server - mainly because that master server is again a VM that might need to be build up and consumes some resources that I'd rather have free for other VMs. So what I do now is akin to the following: ~# puppet apply --modulepath /mnt/puppet/modules /mnt/puppet/manifests/site.pp All I have to do is make sure that the /mnt/puppet location is a shared resource (in my case, an NFSv4 read-only mount) which I can just mount on a fresh image. Part of this exercise I noticed that Puppet by default uses the regular gentoo provider for the services. I'd like to use the openrc provider instead, as I can easily tweak that one to work with SELinux (I need to prepend run_init to the rc-service calls, otherwise SELinux wants to authenticate the user and Puppet doesn't like that; I have a pam_rootok.so statement in the run_init PAM file to allow unattended calls towards rc-service). A quick Google revealed that all I had to do was to add a provider => openrc in the service definitions, like so: service { \"net.eth0\": provider => openrc, ensure => running, } As mentioned, I still manually patch the openrc provider (located in /usr/lib64/ruby/site_ruby/1.9.1/puppet/provider/service) so that the run_init command is known as well, and that all invocations of the rc-service is prepended with run_init: ... commands :runinit => '/usr/sbin/run_init' commands :rcservice => '/sbin/rc-service' ... [command(:runinit), command(:rcservice), @resource[:name], :start ] And the same for the stop and status definitions. I might use Portage' postinst hook to automatically apply the patch so I don't need to do this manually each time.","tags":"Gentoo","url":"https://blog.siphos.be/2013/03/fiddling-with-puppet-apply/","loc":"https://blog.siphos.be/2013/03/fiddling-with-puppet-apply/"},{"title":"SELinux tutorial series, update","text":"Just a small update - the set of SELinux tutorials has been enhanced since my last blog post about it with information on SELinux booleans, customizable types, run-time modi (enforcing versus permissive), some bits about unconfined domains, information on policy loading, purpose of SELinux roles, SELinux users and an example on how a policy works regarding init scripts. The near future will give more information about the multi-level security aspect, about multi-category support, a review on the SELinux context (as we then have handled each field in the context string) and i'll also start with the second series that focuses more on policy enhancements and policy building. And probably a few dozen more. Happy reading!","tags":"SELinux","url":"https://blog.siphos.be/2013/03/selinux-tutorial-series-update/","loc":"https://blog.siphos.be/2013/03/selinux-tutorial-series-update/"},{"title":"SELinux tutorial series","text":"As we get a growing number of SELinux users within Gentoo Hardened and because the SELinux usage at the firm I work at is most likely going to grow as well, I decided to join the bunch of documents on SELinux that are \"out there\" and start a series of my own. After all, too much documentation probably doesn't hurt, and SELinux definitely deserves a lot of documentation. I decided to use the Gentoo Wiki for this endeavour instead of a GuideXML approach (which is the format used for Gentoo documentation on the main site). The set of tutorials that I already wrote can be found under the SELinux : Gentoo Hardened SELinux Tutorials location. Although of course meant to support the Gentoo Hardened SELinux users, I'm hoping to keep the initial set of tutorial articles deliberately distribution-independent so I can refer to them at work as well. For now (this is a week's work, so don't expect this amount of tutorials to double in the next few days) I wrote about the security context of a process, how SELinux controls file and directory accesses, where to find SELinux permission denial details, controlling file contexts yourself and how a process gets into a certain context. I hope I can keep the articles in good shape and with a gradual step-up in complexity. That does mean that most articles are not complete (for instance, when talking about domain transitions, I don't talk about constraints that might prohibit them, or about the role and type mismatches (invalid context) that you might get, etc.) and that those details will follow in later articles. Hopefully that allows users to learn step by step. At the end of each tutorial, you will find a \"What you need to remember\" section. This is a very short overview of what was said in the tutorial and that you will need to know in future articles. If you ever read a tutorial article, then this section might be sufficient for you to remember again what it was about - no need to reread the entire article. Consider it an attempt at a tl;dr for articles ;-) Enjoy your reading, and if you have any remarks, don't hesitate to contribute on the wiki or talk through the \"Talk\" pages.","tags":"SELinux","url":"https://blog.siphos.be/2013/03/selinux-tutorial-series/","loc":"https://blog.siphos.be/2013/03/selinux-tutorial-series/"},{"title":"Gentoo Hardened progress meeting of march 2013","text":"Another month has passed, so time for a new progress meeting... Toolchain GCC v4.7 has been unmasked, allowing a large set of users to test out the new GCC. It is also expected that GCC 4.8-rc1 will hit the tree next week. In the hardened-dev overlay, hardened support for x86, amd64 and arm has been added (SPEC updates) and the remainder of architectures will be added by the end of the week. Kernel and grSecurity/PaX Kernel 3.7.5 had a security issue (local root privilege escalation) so 3.7.5-r1 which held a fix for this was stabilized quickly. However, other (non-security) problems have been reported, such as one with dovecot, regarding the VSIZE memory size. This should be fixed in the 3.8 series, so these are candidate for a faster stabilization. This faster stabilization is never fun, as it increases the likelihood that we miss other things, but they are needed as the vulnerability in the previous stable kernel was too severe. Regarding XATTR_PAX, we are getting pretty close to the migration. The eclass is ready and will be announced for review on the appropriate mailinglists later this week. A small problem still remains on Paludis-using systems (Paludis does not record NEEDED.ELF.2 information - linkage information - so it is hard to get all the linkage information on a system). A different revdep-pax and migrate-pax toolset will be built that detects the necessary linkage information, but much slower than on a Portage-running system. SELinux The 11th revision of the policies are now stable, and work is on the way for the 12th revision which will hit the tree soon. Some work is on the way for setools and policycoreutils (one due to a new release - setools - and the other one due to a build failure if PAM is not set). Both packages will hit the hardened-dev overlay soon. A new \"edition\" of the selinuxnode virtual image has been pushed to the mirror system, providing a SELinux-enabled (enforcing) Gentoo Hardened system with grSecurity and PaX, as well as IMA and EVM enabled. Profiles The 13.0 profiles have been running fine for a while at a few of our developer systems. No changes have been needed (yet) so things are looking good. System Integrity The necessary userland utilities have been moved to the main tree. The documentation for IMA/EVM has been updated as well to reflec the current state of IMA/EVM within Gentoo Hardened. IMA, even with the custom policies, seems to be working well. EVM on the other hand has some issues, so you might need to run with EVM=fix for now. Debugging on this issue is on the way. Documentation Some of the user oriented documentation (integrity and SELinux) have been moved to the Gentoo Wiki for easier user contributions and simplified management. Other documents will follow soon.","tags":"Gentoo","url":"https://blog.siphos.be/2013/03/gentoo-hardened-progress-meeting-of-march-2013/","loc":"https://blog.siphos.be/2013/03/gentoo-hardened-progress-meeting-of-march-2013/"},{"title":"Uploading selinuxnode test VM","text":"At the time of writing (but I'll delay the publication of this post a few hours), I'm uploading a new SELinux-enabled KVM guest image. This is not an update on the previous image though (it's a reinstalled system - after all, I use VMs for testing, so it makes sense to reinstall from time to time to check if the installation instructions are still accurate). However, the focus remains the same: A minimal Gentoo Linux installation for amd64 (x86_64) as guest within a KVM hypervisor. The image is about 190 Mb in size compressed, and 1.6 Gb in size uncompressed. The file format is Qemu's QCOW2 so expect the image to grow as you work with it. The file systems are, in total, sized to about 50 Gb. The installation has SELinux enabled (strict policy, enforcing mode), various grSecurity settings enabled (including PaX and TPE), but now also includes IMA (Integrity Measurement Architecture) and EVM (Extended Verification Module) although EVM is by default started in fix mode. The image will not start any network-facing daemons by default (unlike the previous image) for security reasons (if I let this image stay around this long as I did with the previous, it's prone to have some vulnerabilities in the future, although I'm hoping I can update the image more frequently). This includes SSH, so you'll need access to the image console first after which you can configure the network and start SSH ( run_init rc-service sshd start does the trick). A couple of default accounts are created, and the image will display those accounts and their passwords on the screen (it is a test/play VM, not a production VM). There are still a few minor issues with it, that I hope to fix by the next upload: Bug 457812 is still applicable to the image, so you'll notice lots of SELinux denials on the mknod capability. They seem to be cosmetic though. At shutdown, udev somewhere fails with a SELinux initial context problem. I thought I had it covered, but I noticed after compressing the image that it is still there. I'll fix it - I promise ;) EVM is enabled in fix mode, because otherwise EVM is prohibiting mode changes on files in /run. I still have to investigate this further though - I had to use the EVM=fix workaround due to time pressure. When uploaded, I'll ask the Gentoo infrastructure team to synchronise the image with our mirrors so you can enjoy it. It'll be on the distfiles, under experimental/amd64/qemu-selinux (it has the 20130224 date in the name, so you can see for yourself if the sync has already occurred or not).","tags":"Gentoo","url":"https://blog.siphos.be/2013/02/uploading-selinuxnode-test-vm/","loc":"https://blog.siphos.be/2013/02/uploading-selinuxnode-test-vm/"},{"title":"Working on a new selinuxnode VM","text":"A long time ago, I made a SELinux enabled VM for people to play with, displaying a minimal Gentoo installation, including the hardening features it supports (PIE/PIC toolchain, grSecurity, PaX and SELinux). I'm currently trying to create a new one, which also includes IMA/EVM, but it looks like I still have many things to investigate further... First of all, I notice that many SELinux domains want to use the mknod capability, even for domains of which I have no idea whatsoever why they need it. I don't notice any downsides though, and running in permissive mode doesn't change the domain behavior. But still, I'm reluctant to mark them dontaudit as long as I'm not 100% sure. Second, the gettys (I think it is the getty) result in a \"Cannot change SELinux context: permission denied\" error, even though everything is running in the right SELinux context. I still have to confirm if it really is the getty process or something else (the last run I had the impression it was a udev-related process). But there are no denials and no SELinux errors in the logs. Third, during shutdown, many domains have problems accessing their PID files in /var/run (which is a link to /run). I most likely need to allow read privileges on all domains that have access to var_run_t towards the var_t symlinks. It isn't a problem per se (the processes still run correctly) but ugly as hell, and if you introduce monitoring it'll go haywire (as no PID files were either found, or were stale). Also, EVM is giving me a hard time, not allowing me to change mode and ownership in files on /var/run. I have received some feedback from the IMA user list on this so it is still very much a work-in-progress. Finally, the first attempt to generate a new VM resulted in a download of 817 MB (instead of the 158 MB of the previous release), so I still have to correct my USE flags and doublecheck the installed applications. Anyway, definitely to be continued. Too bad time is a scarce resource :-(","tags":"Gentoo","url":"https://blog.siphos.be/2013/02/working-on-a-new-selinuxnode-vm/","loc":"https://blog.siphos.be/2013/02/working-on-a-new-selinuxnode-vm/"},{"title":"Transforming GuideXML to wiki","text":"The Gentoo project has its own official wiki for some time now, and we are going to use it more and more in the next few months. For instance, in the last Gentoo Hardened meeting, we already discussed that most user-oriented documentation should be put on the wiki, and I've heard that there are ideas on moving Gentoo project pages at large towards the wiki. And also for the regular Gentoo documentation I will be moving those guides that we cannot maintain ourselves anymore easily towards the wiki. To support migrations of documents, I created a gxml2wiki.xsl stylesheet. Such a stylesheet can be used, together with tools like xsltproc , to transform GuideXML documents into text output somewhat suitable for the wiki. It isn't perfect (far from it actually) but at least it allows for a more simple migration of documents with minor editing afterwards. Currently, using it is as simple as invoking it against the GuideXML document you want to transform: ~$ xsltproc gxml2wiki.xsl /path/to/document.xml The output shown on the screen can then be used as a page. The following things still need to be corrected manually: Whitespace is broken, sometimes there are too many newlines. I had to make the decision to put in newlines when needed (which makes too many newlines) rather than a few newlines too few (which makes it more difficult to find where to add in). Links need to be double/triple checked, but i'll try to fix that in later editions of the stylesheet Commands will have \"INTERNAL\" in them - you'll need to move the commands themselves into the proper location and only put the necessary output in the pre-tags. This is because the wiki format has more structure than GuideXML in this matter, thus transformations are more difficult to write in this regard. The stylesheet currently automatically adds in a link towards a Server and security category, but of course you'll need to change that to the proper category for the document you are converting. Happy documentation hacking!","tags":"Gentoo","url":"https://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/","loc":"https://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"},{"title":"Gentoo Hardened goes onward (aka project meeting)","text":"It's been a while again, so time for another Gentoo Hardened online progress meeting. Toolchain GCC 4.8 is on development stage 4, so the hardened patches will be worked on next week. Some help on it is needed to test the patches on ARM, PPC and MIPS though. For those interested, keep a close eye on the hardened-dev overlay as those will contain the latest fixes. When GCC 4.9 starts development phase 1, Zorry will again try to upstream the patches. With the coming fixes, we might probably (need to) remove the various hardenedno* GCC profiles from the hardened Gentoo profiles. This shouldn't impact too many users as ebuilds add in the correct flags anyhow (for instance when needing to turn off PIE/PIC). Kernel, grSecurity and PaX The kernel release 3.7.0 that we have stable in our tree has seen a few setbacks, but no higher version is stable yet (mainly due to the stabilization period needed). 3.7.4-r1 and 3.7.5 are prime candidates with good track record, so we might be stabilizing 3.7.5 in the very near future (next week probably). On the PaX flag migration (you know, from ELF-header based marking to extended attributes marking), the documentation has seen its necessary upgrades and the userland utilities have been updated to reflect the use of xattr markings. The eclass we use for the markings will use the correct utility based on the environment. One issue faced when trying to support both markings is that some actions (like the \"paxctl -Cc\" which creates the PT_PAX header if it is missing) make no sense with the other (as there is no header when using XATTR_PAX). The eclass will be updated to ignore these flags when XATTR_PAX is selected. SELinux Revision 10 is stable in the tree, and revision 11 is waiting stabilization period. A few more changes have been put in the policy repository already (which are installed when using the live ebuilds) and will of course be part of revision 12. A change in the userland utilities was also pushed out to allow permissive domains (so run a single domain in permissive mode instead of the entire system). Finally, the SELinux eclass has been updated to remove SELinux modules from all defined SELinux module stores if the SELinux policy package is removed from the system. Before that, the user had to remove the modules from the store himself manually, but this is error-prone and easily forgotten, especially for the non-default SELinux policy stores. Profiles All hardened subprofiles are marked as deprecated now (you've seen the discussions on this on the mailinglist probably on this) so we now have a sane set of hardened profiles to manage. The subprofiles were used for things like \"desktop\" or \"server\", whereas users can easily stack their profiles as they see fit anyhow - so there was little reason for the project to continue managing those subprofiles. Also, now that Gentoo has released its 13.0 profile, we will need to migrate our profiles to the 13.0 ones as well. So, the idea is to temporarily support 13.0 in a subprofile, test it thoroughly, and then remove the subprofile and switch the main one to 13.0. System Integrity The documentation for IMA and EVM is available on the Gentoo Hardened project site. They currently still refer to the IMA and EVM subsystems as development-only, but they are available in the stable kernels now. Especially the default policy that is available in the kernel is pretty useful. When you want to consider custom policies (for instance with SELinux integration) you'll need a kernel patch that is already upstreamed but not applied to the stable kernels yet. To support IMA/EVM, a package called ima-evm-utils is available in the hardened-dev overlay, which will be moved to the main tree soon. Documentation As mentioned before, the PaX documentation has seen quite a lot of updates. Other documents that have seen updates are the Hardened FAQ, Integrity subproject and SELinux documentation although most of them were small changes. Another suggestion given is to clean up the Hardened project page; however, there has been some talk within Gentoo to move project pages to the Gentoo wiki. Such a move might make the suggestion easier to handle. And while on the subject of the wiki, we might want to move user guides to the wiki already. Bugs Bug 443630 refers to segmentation faults with libvirt when starting Qemu domains on a SELinux-enabled host. Sadly, I'm not able to test libvirt myself so either someone with SELinux and libvirt expertise can chime in, or we will need to troubleshoot it by bug (using gdb, strace'ing more, ...) which might take quite some time and is not user friendly... Media Various talks where held at FOSDEM regarding Gentoo Hardened, and a lot of people attended those talks. Also the round table was quite effective, with many users interacting with developers all around. For next year, chances are very high that we'll give a \"What has changed since last year\" session and a round table again. With many thanks to the usual suspects: Zorry, blueness, prometheanfire, lejonet, klondike and the several dozen contributors that are going to kill me for not mentioning their (nick)names.","tags":"Gentoo","url":"https://blog.siphos.be/2013/02/gentoo-hardened-goes-onward-aka-project-meeting/","loc":"https://blog.siphos.be/2013/02/gentoo-hardened-goes-onward-aka-project-meeting/"},{"title":"Why would paid-for support be better?","text":"Last Saturday evening, I sent an e-mail to a low-volume mailinglist regarding IMA problems that I'm facing. I wasn't expecting an answer very fast of course, being holidays, weekend and a low-volume mailinglist. But hey - it is the free software world, so I should expect some slack on this, right? Well, not really. I got a reply on sunday - and not just an acknowledgement e-mail, but a to-the-point answer. It was immediately correct and described why, and helped me figure out things further. And this is not a unique case in the free software world: because you are dealing with the developers and users that have written the code that you are running/testing, you get a bunch of very motivated souls, all looking at your request when they can, and giving input when they can. Compare that to commercial support from bigger vendors: in these cases, your request probably gets read by a single person whose state of mind is difficult to know (but from the communication you often get the impression that they either couldn't care less or they are swamped with request tasks so they cannot devote enough time on your request). In most cases, they check the request for containing the right amount of information in the right format on the right fields, or even ignore that you did all that right and just ask you for (the same) information again. And who knows how many times I had to \"state your business impact\". Now, I know that commercial support from bigger vendor has the burden of a huge overload in requests, but is that truely that different in the free software world? Mailinglists such as the Linux kernel mailinglist (for kernel development) gets hundreds (thousands?) mails a day, and those with request for feedback or with questions get a reply quite swiftly. Mailinglists for distribution users get a lot of traffic as well, and each and every request is handled with due care and responded to within a very good timeframe (24h or less most of the time, sometimes a few days if the user is using a strange or exotic environment that not everyone knows how to handle). I think one of the biggest advantages of the free software world is that the requests are public. That both teaches the many users on those mailinglists and fora on how to handle problems they haven't seen before, as well as allows users to first look for a problem before reporting it. Everybody wins with this. And because it is public, many users are happily answering more and more questions because they get the visibility (with acknowledgements) they deserve: they gain a specific position in that particular area that others respect, because we can see how much effort (and good results) they gave earlier on. So kudos to the free software world, a happy new year - and keep going forward.","tags":"Free Software","url":"https://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/","loc":"https://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/"},{"title":"IMA and EVM on Gentoo, part 2","text":"I have been playing with Linux IMA/EVM on a Gentoo Hardened (with SELinux) system for a while and have been documenting what I think is interesting/necessary for Gentoo Linux users when they want to use IMA/EVM as well. Note that the documentation of the Linux IMA/EVM project itself is very decent. It's all on a single wiki page, but it's decent and I learned a lot from it. That being said, I do have the impression that the method they suggest for generating IMA hashes for the entire system is not always working properly. It might be because of SELinux on my system, but for now I'm searching for another method that does seem to work well (I'm currently trying my luck with a find ... -exec evmctl based command). But once the hashes are registered, it works pretty well (well, there's a probably small SELinux problem where loading a new policy or updating the existing policies seems to generate stale rules and I have to reboot my system, but I'll find the culprit of that soon ;-) The IMA Guide has been updated to reflect recent findings - including how to load a custom policy, and I have also started on the EVM Guide . I think it'll take me a day or three to finish off the rough edges and then I'll start creating a new SELinux node (KVM) image that users can use with various Gentoo Hardened-supported technologies enabled (PaX, grSecurity, SELinux, IMA and EVM). So if you're curious about IMA/EVM and willing to try it out on Gentoo Linux, please have a look at those documents and see if they assist you (or confuse you even more).","tags":"Gentoo","url":"https://blog.siphos.be/2012/12/ima-and-evm-on-gentoo-part-2/","loc":"https://blog.siphos.be/2012/12/ima-and-evm-on-gentoo-part-2/"},{"title":"Gentoo Hardened IMA support","text":"Adventurous users, contributors and developers can enable the Integrity Measurement Architecture subsystem in the Linux kernel with appraisal (since Linux kernel 3.7). In an attempt to support IMA (and EVM and other technologies) properly, the System Integrity subproject within Gentoo Hardened was launched a few months ago. And now that Linux kernel 3.7 is out (and stable) you can start enjoying this additional security feature. With IMA (and IMA appraisal), you are able to protect your system from offline tampering: modifications made to your files while the system is offline will be detected as their hash values do not match the hash values stored in extended attributes (whereas the extended attributes are then protected through digitally signed values using the EVM technology). I'm working on integrating IMA (and later EVM) properly, which of course includes the necessary documentation: concepts and a ima guide for starters, with more to follow. Be aware though that the integration is still in its infancy, but any questions and feedback is greatly appreciated, and bugreports (like bug 448872 ) are definitely welcome.","tags":"Gentoo","url":"https://blog.siphos.be/2012/12/gentoo-hardened-ima-support/","loc":"https://blog.siphos.be/2012/12/gentoo-hardened-ima-support/"},{"title":"Switching policy types in Gentoo/SELinux","text":"When you are running Gentoo with SELinux enabled, you will be running with a particular policy type, which you can devise from either /etc/selinux/config or from the output of the sestatus command. As a user on our IRC channel had some issues converting his strict-policy system to mcs, I thought about testing it out myself. Below are the steps I did and the reasoning why (and I will update the docs to reflect this accordingly). Let's first see if the type I am running at this moment is indeed strict, and that the mcs type is defined in the POLICY_TYPES variable. This is necessary because the sec-policy/selinux-* packages will then build the policy modules for the other types referenced in this variable as well. test ~ # sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: strict Current mode: enforcing Mode from config file: enforcing Policy MLS status: disabled Policy deny_unknown status: denied Max kernel policy version: 28 test ~ # grep POLICY_TYPES /etc/portage/make.conf POLICY_TYPES=\"targeted strict mcs\" If you notice that this is not the case, update the POLICY_TYPES variable and rebuild all SELinux policy packages using emerge \\$(qlist -IC sec-policy) first. Let's see if I indeed have policies for the other types available and that they are recent (modification date): test ~ # ls -l /etc/selinux/*/policy /etc/selinux/mcs/policy: total 408 -rw-r--r--. 1 root root 417228 Dec 19 21:01 policy.27 /etc/selinux/strict/policy: total 384 -rw-r--r--. 1 root root 392168 Dec 19 21:15 policy.27 /etc/selinux/targeted/policy: total 396 -rw-r--r--. 1 root root 402931 Dec 19 21:01 policy.27 Great, we're now going to switch to permissive mode and edit the SELinux configuration file to reflect that we are going to boot (later) into the mcs policy. Only change the type - I will not boot in permissive mode so the SELINUX=enforcing can stay. test ~ # setenforce 0 test ~ # vim /etc/selinux/config [... set SELINUXTYPE=mcs ...] You can run sestatus to verify the changes, but be aware that - while the command does say that the mcs policy is loaded, this is not the case. The mcs policy is just defined as the policy to load: test ~ # sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: mcs Current mode: permissive Mode from config file: enforcing Policy MLS status: disabled Policy deny_unknown status: denied Max kernel policy version: 28 So let's load the mcs policy shall we? test ~ # cd /usr/share/selinux/mcs/ test mcs # semodule -b base.pp -i $(ls *.pp | grep -v base | grep -v unconfined) Next we are going to relabel all files on the file system, because the mcs policy adds in another component in the context (a sensitivity label - always set to 0 for mcs). We will also re-do the setfiles steps done initially while setting up SELinux on our system. This is because we need to relabel files that are \"hidden\" from the current file system because other file systems are mounted on top of it. test mcs # rlpkg -a -r Relabeling filesystem types: btrfs ext2 ext3 ext4 jfs xfs Scanning for shared libraries with text relocations... 0 libraries with text relocations, 0 not relabeled. Scanning for PIE binaries with text relocations... 0 binaries with text relocations detected. test mcs # mount -o bind / /mnt/gentoo test mcs # setfiles -r /mnt/gentoo /etc/selinux/mcs/contexts/files/file_contexts /mnt/gentoo/dev test mcs # setfiles -r /mnt/gentoo /etc/selinux/mcs/contexts/files/file_contexts /mnt/gentoo/lib64 test mcs # umount /mnt/gentoo Finally, edit /etc/fstab and change all rootcontext= parameters to include a trailing :s0 , otherwise the root contexts of these file systems will be illegal (in the mcs-sense) as they do not contain the sensitivity level information. test mcs # vim /etc/fstab [... edit rootcontext's to now include \":s0\" ...] There ya go. Now reboot and notice that all is okay, and we're running with the mcs policy loaded. test ~ # id -Z root:sysadm_r:sysadm_t:s0-s0:c0.c1023 test ~ # sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: mcs Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: denied Max kernel policy version: 28","tags":"Gentoo","url":"https://blog.siphos.be/2012/12/switching-policy-types-in-gentooselinux/","loc":"https://blog.siphos.be/2012/12/switching-policy-types-in-gentooselinux/"},{"title":"Another hardened month has passed...","text":"... so it's time for a new update ;-) Toolchain GCC 4.8 is still in its stage 3 development phase, so Zorry will send out the patches to the GCC development community when this phase is done. For Gentoo hardened itself, we now support all architectures except for IA64 (which never had SSP). Full uclibc support is now in place for amd64, i686, mips32r2: not only is their technological support ok, but stages are now also automatically built to support installations through the regular installation instructions. The next target to get stages automatically built for is armv7a. Kernel and grSecurity/PaX Stabilization on 3.6.x is still showing some difficulties. Until those are resolved, we're still stable in 3.5.4. We have a couple of panics in some odd cases, but these will need to be resolved before we can stabilize further. glibc-2.16 will also drop the declarations for PT_PAX (in elf.h) and the binutils will also not cover PT_PAX phdr anymore. So, we will standardize fully on xattr-based PaX flags. This will get some proper focus in the next period to ensure this is done correctly. Most work on this support is focusing on communication towards users and the pax-utils eclass support. There was some confusion if the tmpfs-xattr patch would or would not properly restrict access, but it looks like the PaX patch on mm/shmem.c was based upon the Gentoo patch and enhanced with the needed restrictions, so we can just keep the PaX code. On USE=\"pax_kernel\", which should enable some updates on userland utilities when applications are run under a PaX enabled kernel, prometheanfire tried to get this as a global USE flag (as many applications might eventually want to get a trigger on it). However, due to some confusion on the meaning of the USE flag, and potential need to depend on additional tools, we're going to stick with a local flag for now. SELinux schmitt953 will help in the testing and possible development of SELinux policies for Samba 4. Furthermore, the userspace utilities have been stabilized (except for the setools-3.3.7-r5+ due to some swig problems, but those have been worked around in setools-3.3.7-r6). Also, the rev8 policies are in the tree and no big problems were reported on them. They are currently still \\~arch, but will be stabilized in the next few days. A new rev9 release will be pushed to the hardened-dev overlay soon as well. Profiles nvidia is unmasked for the hardened profiles, but still has X and tools USE flags masked, and is only supported on kernels 3.0.x and higher. Also, the hardened/linux/uclibc/arm/armv7a profile is now available as a development profile. Profiles will be updated as the architectures for ARM are getting supported, so expect more in the next month. System Integrity We were waiting for kernel 3.7, which just got released, so we can now start integrating this further. Expect more updates by next meeting. Docs For SELinux, some information on USE=\"unconfined\" is added to the SELinux handbook. Blueness will also start documenting the xattr pax stuff.","tags":"Gentoo","url":"https://blog.siphos.be/2012/12/another-hardened-month-has-passed/","loc":"https://blog.siphos.be/2012/12/another-hardened-month-has-passed/"},{"title":"Using pam_selinux to switch contexts","text":"With SELinux managing the access controls of applications towards the resources on the system, a not-to-be forgotten important component on any Unix/Linux system is the authentication part. Most systems use or support PAM, the Pluggable Authentication Modules , and for SELinux this plays an important role. Applications that are PAM-enabled use PAM for the authentication of user activities. If this includes setting up an authenticated session, then the \"session\" part of the PAM configuration is also handled. And for SELinux, this is a nice-to-have, since this means applications that are not SELinux-aware can still enjoy transitions towards specified domains depending on the user that is authenticated. The \"not SELinux-aware\" here is important. By default, applications keep running in one security context for their lifetime. If they invoke a execve or similar call (which is used to start another application or command when used in combination with a fork ), then the SELinux policy might trigger an automatic transition if the holy grail of fourfold rules is set: a transition from the current context to the new one is allowed the label of the executed command/label is marked as an entrypoint for the new context the current context is allowed to execute that application an automatic transition rule is made from the current context to the new one over the command label Or, in SELinux policy terms, assuming the domains are source_t and destination_t with the label of the executed file being file_exec_t : allow source_t destination_t:process transition; allow destination_t file_exec_t:file entrypoint; allow source_t file_exec_t:file execute; type_transition source_t file_exec_t : process destination_t; If those four settings are valid, then (and only then) can the automatic transition be active. Sadly, for applications that run user actions (like cron systems, remote logon services and more) this is not sufficient, since there are two major downsides to this \"flexibility\": The rules to transition are static and do not depend on the identity of the user for which activities are launched. The policy can not deduce this identity from a file context either. The policy is statically defined: different transitions based on different user identities are not possibel. To overcome this problem, applications can be made SELinux-aware, linking with the libselinux library and invoking the necessary switches themselves (or running the commands with runcon ). Luckily, this is where the PAM system comes to play to aide us in setting up this policy behavior. When an application is PAM-enabled, it will invoke PAM calls to authenticate and possibly set up the user session. The actions that PAM invokes are defined by the PAM configuration files. For instance, for the at daemon: ## /etc/pam.d/atd # # The PAM configuration file for the at daemon # auth required pam_env.so auth include system-services account include system-services session include system-services I am not going to dive into the details of PAM in this blog post, so let's just jump to the session management part. In the above example file, if PAM sets up (or shuts down) a user session for the service (at in our case), it will go through the PAM services that are listed in the system-services definition, which looks like so: ## /etc/pam.d/system-services auth sufficient pam_permit.so account include system-auth session optional pam_loginuid.so session required pam_limits.so session required pam_env.so session required pam_unix.so session optional pam_permit.so Until now, nothing SELinux-specific is enabled. But if we change the session section of the at service to the following, then the SELinux pam module will be called as well: session optional pam_selinux.so close session include system-services session optional pam_selinux.so multiple open Now that the SELinux module is called, pam_selinux will try to switch the context of the process based on the definitions in the /etc/selinux/strict/contexts location (substitute strict with the policy type you use). The outcome of this switching can be checked with the getseuser application: ~# getseuser root system_u:system_r:crond_t seuser: root, level (null) Context 0 root:sysadm_r:cronjob_t Context 1 root:staff_r:cronjob_t By providing the contexts in configurable files in /etc/selinux/strict/contexts, a non-SELinux aware application suddenly becomes SELinux-aware (through the PAM support it already has) without needing to patch or even rebuild the application. All that is need is to allow the security context of the application to switch ids and roles (as that is by default not allowed), which I believe is offered through the following statements: domain_subj_id_change_exemption(atd_t) domain_role_change_exemption(atd_t) selinux_validate_context(atd_t) selinux_compute_access_vector(atd_t) selinux_compute_create_context(atd_t) selinux_compute_relabel_context(atd_t) selinux_compute_user_contexts(atd_t) seutil_read_config(atd_t) seutil_read_default_contexts(atd_t)","tags":"SELinux","url":"https://blog.siphos.be/2012/12/using-pam_selinux-to-switch-contexts/","loc":"https://blog.siphos.be/2012/12/using-pam_selinux-to-switch-contexts/"},{"title":"Using stunnel for mutual authentication","text":"Sometimes services do not support SSL/TLS, or if they do, they do not support using mutual authentication (i.e. requesting that the client also provides a certificate which is trusted by the service). If that is a requirement in your architecture, you can use stunnel to provide this additional SSL/TLS layer. As an example, I have a mail server running on localhost, and I want to provide SSMTP services with mutual authentication on top of this service, using stunnel. First of all, I provide two certificates and private keys that are both signed by the same CA, and keep the CA certificate close as well: client.key is the private key for the client client.pem is the certificate for the client (which contains the public key and CA signature) server.key and server.pem are the same but for the server root-genfic.crt is the certificate of the signing CA First of all, we setup the stunnel, listening on 1465 (as 465 requires the stunnel service to run as root, which I'd rather not) and fowarding towards 127.0.0.1:25: cert = /etc/ssl/services/stunnel/server.pem key = /etc/ssl/services/stunnel/server.key setuid = stunnel setgid = stunnel pid = /var/run/stunnel/stunnel.pid socket = l:TCP_NODELAY=1 socket = r:TCP_NODELAY=1 verify = 2 # This enables the mutual authentication CAfile = /etc/ssl/certs/root-genfic.crt [smtp] accept = 1465 connect = 127.0.0.1:25 To test out mutual authentication this way, I used the following command-line snippet. The delays between the lines are because the mail client is supposed to wait for the mail server to give its reply and if not, the data gets lost. I'm sure this can be made easier (with netcat I could just use \"-i 1\" to print a line with a one-second delay), but it works ;-) ~$ (sleep 1; echo \"EHLO localdomain\"; sleep 1; echo \"MAIL FROM:remote@test.localdomain\"; sleep 1; echo \"RCPT TO:user@localhost\"; sleep 1; echo \"DATA\"; sleep 1; cat TEMPFILE) | openssl s_client -connect 192.168.100.102:1465 -crlf -ign_eof -ssl3 -key client.key -cert client.pem The TEMPFILE file contains the email content (you know, Subject, From, To, other headers, data, ...). If the provided certificate isn't trusted, then you'll find the following in the log file (on Gentoo, thats /var/log/daemon.log by default but you can setup logging in stunnel as well): Dec 8 13:17:32 testsys stunnel: LOG7[20237:2766895953664]: Starting certificate verification: depth=0, /C=US/ST=California/L=Santa Barbara/O=SSL Server/OU=For Testing Purposes Only/CN=localhost/emailAddress=root@localhost Dec 8 13:17:32 testsys stunnel: LOG4[20237:2766895953664]: CERT: Verification error: unable to get local issuer certificate Dec 8 13:17:32 testsys stunnel: LOG4[20237:2766895953664]: Certificate check failed: depth=0, /C=US/ST=California/L=Santa Barbara/O=SSL Server/OU=For Testing Purposes Only/CN=localhost/emailAddress=root@localhost Dec 8 13:17:32 testsys stunnel: LOG7[20237:2766895953664]: SSL alert (write): fatal: bad certificate Dec 8 13:17:32 testsys stunnel: LOG3[20237:2766895953664]: SSL_accept: 140890B2: error:140890B2:SSL routines:SSL3_GET_CLIENT_CERTIFICATE:no certificate returned When a trusted certificate is shown, the connection goes through. Finally, if you not only want to validate if the certificate is trusted, but also only want to accept a given number of certificates, you can set the stunnel variable verify to 3. If you set it to 4, it will not check the CA and only allow a connection to go through if the presented certificate is one in the stunnel trusted certificates.","tags":"Security","url":"https://blog.siphos.be/2012/12/using-stunnel-for-mutual-authentication/","loc":"https://blog.siphos.be/2012/12/using-stunnel-for-mutual-authentication/"},{"title":"nginx as reverse SMTP proxy","text":"I've noticed that not that many resources are online telling you how you can use nginx as a reverse SMTP proxy. Using a reverse SMTP proxy makes sense even if you have just one mail server back-end, either because you can easily switch towards another one, or because you want to put additional checks before handing off the mail to the back-end. In the below example, a back-end mail server is running on localhost (in my case it's a Postfix back-end, but that doesn't matter). Mails received by Nginx will be forwarded to this server. user nginx nginx; worker_processes 1; error_log /var/log/nginx/error_log debug; events { worker_connections 1024; use epoll; } http { log_format main '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$gzip_ratio\"'; server { listen 127.0.0.1:8008; server_name localhost; access_log /var/log/nginx/localhost.access_log main; error_log /var/log/nginx/localhost.error_log info; root /var/www/localhost/htdocs; location ~ .php$ { add_header Auth-Server 127.0.0.1; add_header Auth-Port 25; return 200; } } } mail { server_name localhost; auth_http localhost:8008/auth-smtppass.php; server { listen 192.168.100.102:25; protocol smtp; timeout 5s; proxy on; xclient off; smtp_auth none; } } If you first look at the mail setting, you notice that I include an auth_http directive. This is needed by Nginx as it will consult this back-end service on what to do with the mail (the moment that it receives the recipient information). The URL I use is arbitrarily chosen here, as I don't really run a PHP service in the background (yet). In the http section, I create the same resource that the mails' auth_http wants to connect to. I then declare the two return headers that Nginx needs (Auth-Server and Auth-Port) with the back-end information (127.0.0.1:25). If I ever need to do load balancing or other tricks, I'll write up a simple PHP script and serve it from PHP-FPM or so. Next on the list is to enable SSL (not difficult) with client authentication (which isn't supported by Nginx for the mail module (yet) sadly, so I'll need to look at a different approach for that). BTW, this is all on a simple Gentoo Hardened with SELinux enabled. The following booleans were set to true: nginx_enable_http_server , nginx_enable_smtp_server and nginx_can_network_connect_http . This page has been translated into Spanish language by Maria Ramos from Webhostinghub.com/support/edu .","tags":"Free Software","url":"https://blog.siphos.be/2012/12/nginx-as-reverse-smtp-proxy/","loc":"https://blog.siphos.be/2012/12/nginx-as-reverse-smtp-proxy/"},{"title":"Why you need the real_* thing with genkernel","text":"Today it bit me. I rebooted my workstation, and all hell broke loose. Well, actually, it froze. Literally, if you consider my root file system. When the system tried to remount the root file system read-write, it gave me this: mount: / not mounted or bad option So I did the first thing that always helps me, and that is to disable the initramfs booting and boot straight from the kernel. Now for those wondering why I boot with an initramfs while it still works directly with a kernel: it's a safety measure. Ever since there are talks, rumours, fear, uncertainty and doubt about supporting a separate /usr file system I started supporting an initramfs on my system in case an update really breaks the regular boot cycle. Same because I use lvm on most file systems, and software RAID on all of them. If I wouldn't have an initramfs laying around, I would be screwed the moment userspace decides not to support this straight from a kernel boot. Luckily, this isn't the case (yet) so I could continue working without an initramfs. But I digress. Back to the situation. Booting without initramfs worked without errors of any kind. Next thing is to investigate why it fails. I reboot back with the initramfs, get my read-only root file system and start looking around. In my dmesg output, I notice the following: EXT4-fs (md3): Cannot change data mode on remount So that's weird, not? What is this data mode? Well, the data mode tells the file system (ext4 for me) how to handle writing data to disk. As you are all aware, ext4 is a journaled file system, meaning it writes changes into a journal before applying, allowing changes to be replayed when the system suddenly crashes. By default, ext4 uses ordered mode, writing the metadata (information about files and such, like inode information, timestamps, block maps, extended attributes, ... but not the data itself) to the journal right after writing data to the disk, after which the metadata is then written to disk as well. On my system though, I use data=journal so data too is written to the journal first. This gives a higher degree of protection in case of a system crash (or immediate powerdown - my laptop doesn't recognize batteries anymore and with a daughter playing around, I've had my share of sudden powerdowns). I do boot with the rootflags=data=journal and I have data=journal in my fstab. But the above error tells me otherwise. It tells me that the mode is not what I want it to be. So after fiddling a bit with the options and (of course) using Google to find more information, I found out that my initramfs doesn't check the rootflags parameter, so it mounts the root file system with the standard (ordered) mode. Trying to remount it later will fail, as my fstab contains the data=journal tag, and running mount -o remount,rw,data=ordered for fun doesn't give many smiles. The man page for genkernel however showed me that it uses real_rootflags . So I reboot with that parameter set to real_rootflags=data=journal and all is okay again. Edit: I wrote that even changing the default mount options in the file system itself (using tune2fs /dev/md3 -o journal_data ) didn't help. However, that seems to be an error on my part, I didn't reboot after toggling this, which is apparently required. Thanks to Xake for pointing that out.","tags":"Gentoo","url":"https://blog.siphos.be/2012/11/why-you-need-the-real_-thing-with-genkernel/","loc":"https://blog.siphos.be/2012/11/why-you-need-the-real_-thing-with-genkernel/"},{"title":"The hardened project continues going forward...","text":"This wednesday, the Gentoo Hardened team held its monthly online meeting, discussing the things that have been done the last few weeks and the ideas that are being worked out for the next. As I did with the last few meetings, allow me to summarize it for all interested parties... Toolchain The upstream GCC development on the 4.8 version progressed into its 3rd stage of its development cycle. Sadly, many of our hardened patches didn't make the release. Zorry will continue working on these things, hopefully still being able to merge a few - and otherwise it'll be for the next release. For the MIPS platform, we might not be able to support the hardenedno* GCC profiles [1] in time. However, this is not seen as a blocker (we're mostly interested in the hardened ones, not the ones without hardening ;-) so this could be done later on. Blueness is migrating the stage building for the uclibc stages towards catalyst, providing more clean stages. For the amd64 and i686 platforms, the uclibc-hardened and uclibc-vanilla stages are already done, and mips32r2/uclibc is on the way. Later, ARM stages will be looked at. Other platforms, like little endian MIPS, are also on the roadmap. Kernel The latest hardened-sources (\\~arch) package contains a patch supporting the user.* namespace for extended attributes in tmpfs, as needed for the XATTR_PAX support [2]. However, this patch has not been properly investigated nor tested, so input is definitely welcome. During the meeting, it was suggested to cap the length of the attribute value and only allow the user.pax attribute, as we are otherwise allowing unprivileged applications to \"grow data\" in the kernel memory space (the tmpfs). Prometheanfire confirmed that recent-enough kernels (3.5.4-r1 and later) with nested paging do not exhibit the performance issues reported earlier. SELinux The 20120725 upstream policies are stabilized on revision 5. Although a next revision is already available in the hardened-dev overlay, it will not be pushed to the main tree due to a broken admin interface. Revision 7 is slated to be made available later the same day to fix this, and is the next candidate for being pushed to the main tree. The september-released newer userspace utilities for SELinux are also going to be stabilized in the next few days (at the time of writing this post, they are ;-). These also support epatch_user so that users and developers can easily add in patches to try out stuff without having to repackage the application themselves. grSecurity and PaX The toolchain support for PT_PAX (the ELF-header based PaX markings) is due to be removed soon, meaning that the XATTR_PAX support will need to be matured by then. This has a few consequences on available packages (which will need a bump and fix) such as elfix, but also on the pax-utils.eclass file (interested parties are kindly requested to test out the new eclass before it reaches \"production\"). Of course, it will also mean that the new PaX approach needs to be properly documented for end users and developers. pipacs also mentioned that he is working on a paxctld daemon. Just like SELinux' restorecond daemon, this deamon will look for files and check them against a known database of binaries with their appropriate PaX markings. If the markings are set differently (or not set), the paxctld daemon will rectify the situation. For Gentoo, this is less of a concern as we already set the proper information through the ebuilds. Profiles The old SELinux profiles, which were already deprecated for a while, have been removed from the portage tree. That means that all SELinux-using profiles use the features/selinux inclusion rather than a fully build (yet difficult to maintain) profile definition. System Integrity A few packages, needed to support or work with ima/evm, have been pushed to the hardened-dev overlay. Documentation The SELinux handbook has been updated with the latest policy changes (such as supporting the named init scripts). We also documented SELinux policy constraints which was long overdue. So again a nice month of (volunteer) work on the security state of Gentoo Hardened. Thanks again to all (developers, contributors and users) for making Gentoo Hardened where it is today. Zorry will send out the meeting log later to the mailinglist, so you can look at the more gory details of the meeting if you want. [1] GCC profiles are a set of parameters passed on to GCC as a \"default\" setting. Gentoo hardened uses GCC profiles to support using non-hardening features if the users wants to (through the gcc-config application). [2] XATTR_PAX is a new way of handling PaX markings on binaries. Previously, we kept the PaX markings (i.e. flags telling the kernel PaX code to allow or deny specific behavior or enable certain memory-related hardening features for a specific application) as flags in the binary itself (inside the ELF header). With XATTR_PAX, this is moved to an extended attribute called \"user.pax\".","tags":"Gentoo","url":"https://blog.siphos.be/2012/11/the-hardened-project-continues-going-forward/","loc":"https://blog.siphos.be/2012/11/the-hardened-project-continues-going-forward/"},{"title":"Local policy management script","text":"I've written a small script that I call selocal which manages locally needed SELinux rules. It allows me to add or remove SELinux rules from the command line and have them loaded up without needing to edit a .te file and building the .pp file manually. If you are interested, you can download it from my github location . Its usage is as follows: You can add a rule to the policy with selocal -a \"rule\" You can list the current rules with selocal -l You can remove entries by referring to their number (in the listing output), like semodule -d 19 . You can ask it to build ( -b ) and load ( -L ) the policy when you think it is appropriate It even supports multiple modules in case you don't want to have all local rules in a single module set. So when I wanted to give a presentation on Tor, I had to allow the torbrowser to connect to an unreserved port. The torbrowser runs in the mozilla domain, so all I did was: ~# selocal -a \"corenet_tcp_connect_all_unreserved_ports(mozilla_t)\" -b -L At the end of the presentation, I removed the line from the policy: ~# selocal -l | grep mozilla_t 19. corenet_tcp_connect_all_unreserved_ports(mozilla_t) ~# selocal -d 19 -b -L I can also add in comments in case I would forget why I added it in the first place: ~# selocal -a \"allow mplayer_t self:udp_socket create_socket_perms;\" -c \"MPlayer plays HTTP resources\" -b -L This then also comes up when listing the current local policy rules: ~# selocal -l ... 40: allow mplayer_t self:udp_socket create_socket_perms; # MPlayer plays HTTP resources","tags":"SELinux","url":"https://blog.siphos.be/2012/11/local-policy-management-script/","loc":"https://blog.siphos.be/2012/11/local-policy-management-script/"},{"title":"Gentoo Hardened progress meeting","text":"Not that long ago we had our monthly Gentoo Hardened project meeting (on October 3rd to be exact). On these meetings, we discuss the progress of the project since the last meeting. For our toolchain domain, Zorry reported that the PIE patchset is updated for GCC, fixing bug #436924 . Blueness also mentioned that he will most likely create a separate subproject for the alternative hardened systems (such as mips and arm). This is mostly for management reasons (as the information is currently scattered throughout the Gentoo project at large). For the kernel domain, since version 3.5.4-r2 (and higher), the kernexec and uderef settings (for grSecurity) should no longer impact performance on virtualized platforms (when hardware acceleration is used of course), something that has been bothering Intel-based systems for quite some time already. Also, the problem with guest systems immediately reserving (committing) all memory on the host should be fixed with recent kernels as well. Of course, this is only true as long as you don't sanitize your memory, otherwise all memory gets allocated regardless. In the SELinux subproject, we now have live ebuilds allowing users to pull in the latest policy changes directly from the git repository where we keep our policy at. Also, we will see a high commit frequency in the next few weeks (or perhaps even months) as Fedora's changes are being merged with upstream. Another change is that our patchbundles no longer contain all individual patches, but a merged patch. This increases the deployment time of a SELinux policy package considerably (up to 30% faster since patching is now only a second or less). And finally, the latest userspace utilities are in the hardened-dev overlay ready for broader testing. grSecurity is still focusing on the XATTR-based PaX flags. The eclass (pax-utils) has been updated, and we will now be looking at supporting the PaX extended attributes for file systems such as tmpfs. For profiles , people will notice that in the next few weeks, we will be dropping the (extremely) old SELinux profiles as the current ones have been marked stable long time ago. In the system integrity domain, IMA is being worked on (packages and documentation) after which we'll move to the EVM support to protect extended attributes. And finally, klondike held a good talk about Gentoo Hardened at the Flossk conference in Kosovo. All in all a good month of work, again with many thanks to the volunteers that are keeping Gentoo Hardened alive and kicking!","tags":"Gentoo","url":"https://blog.siphos.be/2012/10/gentoo-hardened-progress-meeting/","loc":"https://blog.siphos.be/2012/10/gentoo-hardened-progress-meeting/"},{"title":"git patch apply","text":"I recently had to merge the changes made to an upstream project with a local repository. I took out the changes as patches through git format-patch (as the local repository isn't a clone of the remote one so I couldn't just create a branch and merge) and hoped to apply them with git am . Sadly, trying this resulted in an error equivalent with: error: test.txt: does not match index Git suggested to fix the index, and then continue with git am --resolved . But what the ... does it mean with fixing the index? Basically, it means that the change needs to be recorded by git in order to be applied, but why does the patch fail to recognize this? The test.txt file exists and is known by git. After some searching, I found a way to handle this - it might not be pretty, but it did the trick, and I succesfully merged about 200 commits in an hour or so. You can see this post as a \"backup\" for my memory ;-) First of all, I tried to apply the patch using git am 0001-some-stuff.patch . If it succeeds, continue. If it doesn't, apply the patch manually using patch < 0001-some-stuff.patch . Then make sure that the changed files (see git status ) are taking part of the commit (use git add ). When the changes are made and recorded, run git am --resolved . Or if you want to discard it, make sure no changes are made/recorded and run git am --skip . That's it. Some scripting made this a whole lot easier. Check the return code of git am . If it is zero, continue with the next patch. If it isn't, run patch and again check for the return code. If it is zero, remove all *.orig files (or change the patch command so it doesn't write orig files), add all (changed) files to the git index and run git am --resolved . And if the patch fails, have the user fix things manually and continue.","tags":"Documentation","url":"https://blog.siphos.be/2012/09/git-patch-apply/","loc":"https://blog.siphos.be/2012/09/git-patch-apply/"},{"title":"Perimeter security testing","text":"I've been asked a few times how I would do perimeter security testing. Personally, I'm not an offensive security guy, more a defensive one, meaning I'm more about security-related defensive methods rather than PEN testing of any kind. But still, even in a defensive position, having a \"view\" on how to do security testing is important. For me, I would use the following testing categorisation to look at IT architectures and see how they would react against certain attacks. I'm calling this one about perimeter testing as I am interested here in remote attacks (or differentiation), not local ones (which requires, in my opinion, a different way of looking at things). Eggs and a basket Overhead testing Protocol insecurity or misuse Application insecurity or misuse Client insecurity Correlation Eggs and a basket First of all, don't put all your eggs in the same basket. I would never trust myself enough to say things are secure. Always see if you can't benefit from other people's knowledge (or even other companies knowledge). If you are doing testing to choose a specific security-related technology, use analysis made by independent analysis firms or organizations to further steer your choice. But make sure that the organization is truely independent and doesn't give \"reports\" that are heavily in favor of whomever asked for them. Overhead testing Most technologies you use to counter certain threats will incur some overhead. This is true for application firewalls, network firewalls, isolation technologies, confidentiality technologies, access controls and more. You should set yourself a baseline of what you consider too much overhead and what not. Overhead comes in many layers, so it is important to be able to perform load testing based on real loads, not fake lab-specific situations. Running one thousand clients with the same client certificate, same hosts, same reaction times against one SSL resource has an entirely different performance profile than running one thousand clients with different certificates, using different encryption libraries (other ciphers and such) and different speeds/reaction times (including things like SSL handshake timings). And that's just one example. I always find it very important to be able to run load testing regularly. I would even go as far as recommend organizations to run load testing as a \"business as usual\" test, or at least allow your technology-inspired teams to easily request such loads against their new applications or technologies. But enough of that. Let's talk about attack methods (or categorisation). I tend to look first towards protocol insecurity, then application insecurity and finally client-level insecurity. Protocol insecurity is primarily about knowing how the protocol works (or should work) and finding ways to attack that. Some protocols are inherently insecure, and introducing proper protection against these is extremely important as the technology that implements the protocol might not be able to do that itself. Then I look at application-specific insecurity, which is more about knowing the application (vendor/product). And finally it is about client insecurity (such as browser-based attacks, ActiveX component attacks, and more). In each of these cases, I consider the following attack methods: Denial-of-Service - what could be done to disable the protocol or service behind it completely or partially Out-of-order Execution - can the protocol or application be tricked into executing tasks when it isn't meant to, which most of the time leads to either information leakage or the next attack method Privilege escalation - to get more rights/privileges (or switch from unauthenticated to authenticated access) Remote command execution - executing whatever the attacker wants on the remote system Application switching/routing - updating the behavior of the application to become a service that can be used to further expose/explore the remote servers' environment Protocol insecurity or misuse Many protocols are inherently insecure. Good security solutions will need to detect if a protocol is being used in a way that does not match the behavior expected. And this goes beyond the standard TCP/IP protocols and the application-level HTTP protocol. Consider SMTP and VoIP-related protocols as well as a nice example. Denial of service attacks against TCP/IP are widely documented. Be it the well-known SYN flooding, a low-rate tcp-targeted DoS or messing with the TCP stack itself (like with the Microsoft Windows TCP/IP Stack Vulnerabilities ), these attacks can be easily evaluated against your architecture. With TCP/IP, I would generally also look at how the stacks present their information. Can an attacker use TCP sequence prediction attacks ? Can he get information on when is the most feasible period to launch an attack (for instance from a reasonably stable TCP window size value reading)? And how about TCP session hijacking? Or if we look at HTTP, can attacks such as Slowloris or an HTTP POST DOS attack bring down the service? And what if a user comes to a certain page after an obscure redirection, where the attacker hopes that the user authenticates against? Perhaps an attacker might hijack an HTTP session, or force a user to use a non-secure connection. E-mail services too are particularly interesting to look at. Does it expose information (settings, or account identification)? Does it accept large time-outs (giving attackers time to just \"play\" with the service using netcat/telnet)? And in case of VoIP, have you checked common voip-based attacks lately? VoIP is (imo) a complex set of protocols and whomever implements it has to follow strict rules. I would be very surprised if this can't be heavily influenced. Application insecurity or misuse Of course, protocols are implemented by applications, and applications have their own set of problems. And if you're running software that isn't properly configured or up to date, you'll definitely need to take a good read at my blog posting series on mitigating risks . Consider Citrix for instance: a commonly found remote management toolsuite (well yeah, Citrix offers a lot more, I'm not going to delve into that right now). It has seen its share of vulnerabilities in the past, like DoS vulnerabilities, directory traversal or open proxy , command execution and more. And Citrix is far from an insecure platform. Just like with all other applications, it is extremely important to have a good view / knowledge of each product you expose. Some applications can even mimic other protocols (like Nginx handling HTTP, IMAP, POP3, SMTP, WebDAV, ... which, if exploited by an attacker, can provide a new fall-out base to work from. Client insecurity Finally, the last thing to consider is most likely the one most difficult to manage: client insecurity. Especially with internet-facing services, it is very hard to protect yourself from client systems that are not properly protected. How to deal with user authentication if the user could have a keystroke logger running in the background? A browser is a commonly used application for service access, but what about things like a Citrix client (especially if local drive mapping is enabled)? Correlation A good security system is integrated with the various security technologies in place. An attacked that did discovery or even tried out a few other attacks before should already alert most of your security components, possibly even invoking a temporary countermeasure against the users' location. It is not sufficient to block the IP address on the webserver when the attacker tried an HTTP-based attack only to have him try his luck on the next service that you expose... Now for each \"category\" I tend to look at the attack from a \"hit and run\" aspect (exploitable with a single attack or burst), \"build up\" (most of the slower attacks tend to be like this) or \"evaded\" (trying to work around detection of the previous ones), and this for a single host, a relayed host or distributed. All these factors combined give me enough things to consider while evaluating an architecture (or security technology implementation) for remote attacks.","tags":"Security","url":"https://blog.siphos.be/2012/08/perimeter-security-testing/","loc":"https://blog.siphos.be/2012/08/perimeter-security-testing/"},{"title":"Gentoo Hardened in August","text":"Last wednesday Gentoo Hardened held its monthly online meeting to discuss the progress of the various subprojects, reconfirm the current project leads, talk about potential new projects and discuss some bugs that were getting on our nerves... For the project leads, all current leads were reconfirmed: Zorry will keep tight ship as Gentoo Hardened project lead, and will also continue as the lead for the toolchain-related projects. Blueness keeps tackling the kernel, pax, grsec and rsbac subprojects, klondike the documentation and media and I will continue with the SELinux and integrity subprojects. On the toolchain progress, Zorry is working on the 4.8 patches and hopes to be able to submit them upstream later this month. Blueness continues maintaining the uclibc architectures mentioned last month and is working on the documentation related to it. On the kernel side, there were some reports submitted that were triggered by the integer overflow plugin. This plugin, called size_overflow aims to detect integer overflows where an increase of an integer value goes beyond its maximum and wraps around (resulting in either a negative or a small integer result). This is of course unwanted behavior, so a gcc plugin (by Emese Revfy) is used to detect such occurrences. Basically, this plugin will recalculate whatever is done with the integers on a double precision integer level and see if the logic result is the same. If it isn't, then an overflow has most likely occurred. This is of course overly simply explained, but from what I can fond in the interwebs, not that far from the truth. The reports are generally about network-related applications, like tor , which are terminated because something fishy occurred within the network handling code of the kernel (see for instance bug #430906 ). In the SELinux camp, the documentation has been updated to inform users on how to create a new role (see also an earlier post of mine) and a few patches to the setools package have been added to support Python-2.7-only systems as well as systems using the latest swig. Also, all userspace utilities for SELinux should support both Python 2.7 and Python 3.x - the only remaining aspect is the SELinux code within Portage (see bug #430488 ). Regarding grSecurity and PaX, blueness is working on the xattr PaX markings support in Gentoo, and a tracker bug has been opened to manage the changes needed. Vapier suggested to move towards xattr markings completely and drop the PT_PAX ELF header support, but this cannot be done until all file systems support user-level extended attributes. That being said, it is a good idea to do this in the long run though as extended attributes give greater flexibility and don't manipulate the binaries of an application. On the integrity subproject, the concepts and introduction documentation is online. I'm working on a few ebuilds that are needed to support IMA/EVM and should hopefully hit the hardened development overlay the next week. The primary focus now is to support creating a \"secure image\" which, when uploaded to a hosting service, would detect if the hosting service tampered with the image outside (i.e. by manipulating the image file itself). Finally, on documentation and media, we will need to look into updating the prelude/LIDS documentation (host intrusion prevention/detection documentation) as it is quite old and obsoleted currently. Klondike also recently gave a talk about Gentoo Hardened (put the stuff online Francisco !) but I don't recall anymore where - I'lll update when I see the meeting log ;-) All by all a nice month! Good going guys.","tags":"Gentoo","url":"https://blog.siphos.be/2012/08/gentoo-hardened-in-august/","loc":"https://blog.siphos.be/2012/08/gentoo-hardened-in-august/"},{"title":"Lots of work on supporting swig-2","text":"The SELinux setools package provides a few of the commands I used the most when working with SELinux: sesearch for looking through the policy and seinfo to get information on type/attribute/role/... from the currently loaded policy. This package uses swig , the Simplified (sic) Wrapper and Interface Generator to provide libraries that can be loaded by Python and used as regular Python modules, based on the C code that setools uses. Or, in other words: you write C code, and swig transforms it into libraries that can be loaded by a dozen higher generation languages such as Python. The change from swig-1 to swig-2 however broke the setools build. It seems that the swig interface code that setools uses doesn't work properly anymore with more recent swig versions. The last few days (yes, days) I have been trying to get setools to build again. The fixes that I put in were not extremely difficult, but very labour-intensive (beyond the point that I think I'm doing things wrong, but hey - this is my first time I'm working on swig stuff, and I'm glad I already got it to build again). The first thing I had to do was fix constructor/destructor logic. It looks like swig-1 supported the C shorthand notation for structures whereas swig-2 sees the name of the structure as its class (note that I just got it to build, I still need to see if things still work): typedef struct apol_ip {...} apol_ip_t; %extend apol_ip_t { - apol_ip_t(const char * str) { + apol_ip(const char * str) { ... - ~apol_ip_t() { + ~apol_ip() { ... } Without this, I got \"Illegal destructor\" errors (not related to the illegal destructor fix made to swig itself) and \"Method apol_ip_t requires a return type\" (because it doesn't see them as constructors). The second fix I had to introduce was to rename all functions that swig would generate which would then have the same name as the C-function. For instance, suppose a C function in the code has apol_vector_get_size then swig would, for the apol_vector class with method get_size in the swig interface, generate a function called apol_vector_get_size which of course collides with the already defined function, giving an error like \"Conflicting types for apol_vector_get_size\" followed by a \"previous declaration was here:\". Swig supports the %rename method for this, so I had to do: typedef struct apol_vector {} apol_vector_t; + %rename(apol_vector_get_size) apol_vector_wrap_get_size; + %rename(apol_vector_get_capacity) apol_vector_wrap_get_capacity; ... - size_t get_size() { + size_t wrap_get_size() { ... - size_t get_capacity() { + size_t wrap_get_capacity() { The patch that I had to add to finally get it to build again is 7019 lines (229 Kbyte). Too much manual labour. Now let's hope this really fixes things, and doesn't just masquerade the build failures but introduces runtime failures. Of course, if I think it works, I'll send it upstream so that, if it is indeed the right fix, other developers don't have to go through this...","tags":"SELinux","url":"https://blog.siphos.be/2012/08/lots-of-work-on-supporting-swig-2/","loc":"https://blog.siphos.be/2012/08/lots-of-work-on-supporting-swig-2/"},{"title":"Adding roles to the Gentoo Hardened SELinux policy","text":"I wrote a small section on how to create additional roles to the SELinux policy offered by Gentoo Hardened. Whereas the default policy that we provide only offers a few basic roles, any policy administrator can provide additional roles for the system. By using additional roles, you can grant users administrative rights to particular services without risking having them elevate their privileges to root (+ sysadmin). You should even allow them to get a root shell while remaining confined within their domain (and role).","tags":"Gentoo","url":"https://blog.siphos.be/2012/08/adding-roles-to-the-gentoo-hardened-selinux-policy/","loc":"https://blog.siphos.be/2012/08/adding-roles-to-the-gentoo-hardened-selinux-policy/"},{"title":"Kickstarting the Integrity subproject","text":"Now that Gentoo Hardened has its integrity subproject, I started with writing down the concepts (draft - will move to the project site when finished!) used within the subproject: what is integrity, how does trust fit into this, what kind of technologies will we look at, etc. I'm hoping that this document will help users in positioning this project as well as already identify a few areas where I think we need to work on. The guide starts with talking about hashes (since hashes are often used in integrity validation schemes), continuing towards HMAC (for authenticated hashes) and signed HMAC digests (for better protection of the cryptographic keys while verifying the integrity). It already talks a bit about trust (and trust chains) and how it works in both ways (top-down and bottom up - the latter especially when considering you are running services on platforms you do not manage yourself). I will be working further on this, describing how the trusted computing group's vision and the trusted platform module standard they developed fits into this as a possible implementation of trust validation (hopefully without getting to the religious part of it) as well as giving first highlights on other technologies we will look at as well.","tags":"Gentoo","url":"https://blog.siphos.be/2012/07/kickstarting-the-integrity-subproject/","loc":"https://blog.siphos.be/2012/07/kickstarting-the-integrity-subproject/"},{"title":"Gentoo Hardened on the move","text":"Gentoo Hardened is thriving and going forward. For those that don't exactly know what Gentoo Hardened is - it is a Gentoo project dedicated to bring Gentoo in a shape ready for highly secure, high stability production server environments. This is what we live by, and why we do what we do. To accomplish this goal, we use a great community of developers & users that work on several subprojects: the implementation of kernel hardening features such as grSecurity, memory-based protection schemes such as PaX, toolchain updates to harden against buffer overflows and memory attacks, mandatory access control schemes such as SELinux and RSBAC. In Gentoo Hardened we then integrate these technologies in Gentoo Linux so that it is usable by a larger community, well documented and supported. I'm myself heavily working on the SELinux integration & documentation aspects, and am hoping to contribute even further - but more about that in a minute. Today, we had an online meeting where developers present their current \"state of affairs\" and the upcoming things they are going to work on. This is done about once every month in the IRC chat channel #gentoo-hardened on the freenode network. Of course, most of the developers are available on the chat channel on an (almost) daily basis. Todays meeting gave us feedback on the following (and remind you, this is one month of volunteer-driven work)... Toolchain When we talk about the toolchain, we mean the set of tools and libraries needed to build a (hardened) system. We put most focus on the GCC compiler because it contains most of the changes we support (like stack smashing protection, position independent code/executable changes, etc.) but work on libraries like glibc and uclibc are on their way as well. Zorry (yeah, I'm going to use nicknames here so you know who you're talking to on IRC ;-) is working on getting our patches upstream (meaning that the main GCC development can incorporate our patches). Sending and working together with the main projects is very important as it provides not only continuity on the patches (once they are upstream, more people are maintaining the code than just you/us), but also gives a multi-eye view on the code: is it of high quality? Does it comply with the proper security guidelines? What about impact of the code on things we don't or haven't considered yet? On the library part, blueness (one of our Gentoo Hardened developers and - imho - an expert in many fields) has been working on Hardened support on ARM (armv7a) with uclibc. He has put up stage4 files for armv7a softfloat uclibc hardened and is working on those for hardfloat. This means that ARM with uclibc+hardened or ARM with glibc+hardened are working - he has even tested an xfce4 desktop on ARM with uclibc and hardened toolchain. ARM support is becoming more and more important in the technology field. Other major processor players like SPARC, Itanium, PowerPC, ... are slowly seeing less and less market share, whereas ARM - albeit currently still a very small player - is rapidly gaining momentum. You all know ARM from the smartphones and other embedded-like platforms, but ARM on servers is coming faster than you expect. Being a simple platform with low energy consumption and good commercial backing (both on CPU level as well as platform support), we can see ARM becoming a major player on this - and Gentoo Hardened is actively working towards it. Kernel Within Gentoo Hardened, we support the grSecurity and PaX kernel patches for a more hardened Linux kernel. But this additional hardening can also sometimes interfere with the normal functioning of systems. To help users in their configuration quest, grSecurity allows users to select a few \"prebuilt configuration types\" in the kernel build. Previously, these types where one of the following label: \"virtualization\", \"workstation\" or \"server\". Based on these labels, the security settings that did not negatively effect the functioning of the system were selected. Recently, the labels have changed into a question-based configuration: is it a server or not? will you use it for virtualization and if so, on host or guest? Is performance for you more important than security? These questions are now also integrated in our hardened-sources. While working and testing one of the kernel settings (KERNEXEC - kernel non-executable pages, to protect non-code containing memory pages from being used to run (potentially hostile/injected) code from) in a virtualized environment, prometheanfire (another Gentoo Hardened developer) noticed a possible regression on the performance of guests if the host had KERNEXEC set. A severe performance hit is to be expected if the host processor doesn't support hardware-assisted nested page tables (a method for supporting memory page virtualization), but this also seemed to occur on systems with nested page tables ( /proc/cpuinfo flag ept for Intel, or rvi for AMD). So more testing (from others as well) is therefore needed to confirm and work on this. SELinux One of Gentoo Hardened's subprojects (and one I'm most actively working on) is its support for SELinux or Security Enhanced Linux. It offers a Mandatory Access Control implementation for Linux, ensuring that users cannot change the security settings that an administrator has set (which is Discretionary Access Control if they can), but also enforce that services/processes can not be forced to do things they are not meant to do. This provides reasonable protection against things like remote code execution exploits, or just limit what an administrator wants particular processes to do. With SELinux, you can even define roles to properly identify and segregate tasks, providing a method for \"segregation of duties\" on OS level. Anyway, as I said, Gentoo Hardened is actively working on SELinux integration. First of all is stages support (providing a small, deployable system unit that users can use to install a SELinux-enabled system) as currently, users are forced to switch to SELinux after having installed Gentoo, which is a multi-step approach . By offering stages, we can simplify the deployment of Gentoo Hardened SELinux. Currently, building stages works but requires some manual steps (labeling mostly) which need to be removed before we can automatically build stages. The next steps here are to see if we can build SELinux stages on non-SELinux systems (as all we need is to link the proper files with the SELinux-supporting libraries, which should work regardless of SELinux being enabled or not). The fact that users need to relabel their system during deployment is just a minor inconvenience (and a one-command fix, so easy to document too). Another item of progression made is a SELinux-enabled (well, Gentoo Hardened grSecurity with PaX and SELinux enforcing enabled) virtual image called \"selinuxnode\". This Qemu/KVM image is a simple Gentoo base installation but with those security features already enabled, allowing users to take a first look at SELinux before trying it out on their own system. But this image has the potential (and now roadmap ;-) to become much more: Provide a play-ground for users to test things in. Try out hammering the SELinux policy, or reproducing potential issues before reporting them (to make sure they are easily reproduceable). Become a Proof-of-Concept location for new enhancements: not only updates on SELinux, but also on other hardening measures and technologies that Gentoo Hardened can support. Implementing the technologies in the VM allow other developers to test and work on it without needing to sacrifice one of their own systems. Become the main system for educational (course-like) documentation. If we develop HOWTO documents, using this VM as a base allows users to follow the instructions to the letter and try things out while keeping the documentation consistent. The documentation can, in the future, also contain instructions that users need to follow as a sort-of test. At the end of the test, a simple script can easily verify on the VM if the test was finished succesfully or not. Even further down the road, it might evolve into a system for building appliance-like, hardened services based on Gentoo Hardened. But that's a milestone too far for now. But you can always dream ;-) On the SELinux policy development side, I'm recently focusing on two aspects: the change towards /run (which already required a few \"urgent\" updates and will probably need a lot more) as well as confining popular attack surfaces like browsers. Not many SELinux users run their browser in a confined space, but I personally don't run anything in unconfined domains and feel that browsers are too popular in the security area to not put attention to. So I'm struggling to have the browsers (first focus is Chromium as that one has an open bug for it, and Firefox because that is my main browser platform) fully confined yet still flexible enough (using SELinux booleans) to support users that have other wishes on their browsers. Speaking of policy development, in the meeting it was also brought forward to support a change of stabilization of SELinux policies from the standard 30-days towards a 14-day stabilization period. In most cases, this doesn't harm users as policies are usually enhanced (allow something that was denied before) and less about reducing privileges (as it is quite hard to find out why a rule was enabled in the first place, hence our reluctant approach to \"quickly\" update policies). For such updates, We're suggesting a 14-day stabilization period, while retaining 30 days for larger updates such as domain policy rewrites (which are sometimes needed if an application changes too much - think init and systemd - or when its segregated into multiple parts that each need (or can have) their own SELinux domain. Finally, we gave a quick update on our status for upstream support (as I mentioned before, having patches supported and accepted upstream is very important for us): we have 116 changesets to the policy in comparison with the 20120215 refpolicy release (which is our \"upstream\"). Of those changesets, 45 have been accepted and implemented upstream, 12 are pending. 55 have not been sent yet (because they still need work or more documentation before they can be accepted) and 4 will not be sent (mostly because they are gentoo-only or deviate from upstream's acceptance guidelines but fit in Gentoo's approach). grSecurity's PaX Blueness worked on the xattr pax support implementation (using extended attributes to store and manage the PaX flags, rather than using the ELF header changes used in the past) within Gentoo Hardened. This is now production-ready, so the proper tools will be made generally available shortly whereas the older method (mainly chpax) will be decommissioned in the very near future. PaX markings allow the Linux kernel to toggle specific PaX settings on or off for processes so that the general state of the system can use the PaX protections while a very few set of programs that cannot work with these settings (often binary software or third party software, but some self-built software can have difficulties with PaX as well) can run without them (or with a lower set). This is much more flexible than an all-or-nothing approach. By using extended attributes, managing these markings can be done without modifying the binaries themselves. In Gentoo, proper support is also given through the paxctl-ng.eclass so developers can automatically set markings at deploy-time when needed. Profiles In Gentoo, users select \"profiles\" as a way to define the defaults for their system. Profiles define stuff like the default kernel, C library, specific USE flags, toolchain, etc. For instance, users that want to use a Gentoo Hardened system with SELinux on an x86_64 system with no-multilib (all 64-bit only) select the hardened/linux/amd64/no-multilib/selinux profile. In the last few weeks, blueness has been working on the uclibc-related profiles (hardened/linux/uclibc/\\${ARCH}) using a clean slate. Gentoo supports profile inheritance, so you can \"stack\" one profile on top of the other. This is great for manageability, but when the profile is to support systems that are quite different from what Gentoo developers are used to, it makes sense to use a clean setup and start from there. And this is the case for hardened uclibc systems. System integrity subproject On this meeting the initial kick-off (after approval) was given of a new hardened subproject called system integrity . This project will focus on the implementation and support of integrity-related technologies such as (well, mainly) Linux IMA/EVM and its supporting userspace utilities and documentation. Integrity validation & enforcement is an important aspect of system security and, since I already work with SELinux, feel this is a natural improvement (since you need a MAC to enforce runtime security and use integrity to enforce detection and prevention of offline tampering). We have great plans with IMA/EVM here, and can hopefully introduce the first few steps towards it in the selinuxnode virtual image soon ;-) Documentation Of course, technologies are great, but documentation is always needed (even if nobody reads it (sic)). I have been documenting hardening of some settings/services using the XCCDF/OVAL languages (part of the SCAP set of standards) since not only do they provide the means to generate guides (we can generate guides in every language, XCCDF is probably the least flexible of them all) but they also support the validation of the settings in an automated manner. By using XCCDF/OVAL-supporting software such as Open-SCAP ( app-forensics/openscap in Gentoo) you can interpret these guides in an unattended manner, generating reports on the state of your services compared to these guides, and even have specific profiles (one system uses a different set of hardening guidelines than another). Since Gentoo Hardened is about supporting secure & stable production environments, it is logical that we can offer best practices on how to handle Gentoo-provided/supported services. And by using these within the SCAP standard, the guides might even be leveraged further than a regular online HOWTO could. And all that from one project? Not really. Gentoo Hardened here plays several roles: integrator for technologies that are managed in other (free software) projects, and development for technologies or settings that are either specific to Gentoo or not available to the public to the extend Gentoo Hardened believes is needed. You must understand this is possible thanks to the tremendous effort that all these projects perform. Gentoo Hardened here plays the role that every Linux distribution has: making all these technologies and advancements fit in a way that the users can easily work with it - integrated and well supported. Thanks to the free software nature though, Gentoo Hardened does more than what \"commercial integrators\" do when they deal with closed, propriatary software: it updates the code, improves it and brings it back for broader re-use. As such, it also acts a bit as development within those projects to assist them in their quest. And in my book, users are more likely to believe in an integrator that can react code-wise rather than using workarounds or \"helping create a service request\". The full excerpts of this meeting (the meeting minutes - well, actually an IRC chat log excerpt) will be sent out soon by the Gentoo Hardened project lead, Zorry. Big thanks to him (and the rest of the crew) to make all this happen! I love to be part of it, and hope I can remain so for a long, long time. Edit: RSBAC , not grSecurity's RBAC.","tags":"Gentoo","url":"https://blog.siphos.be/2012/07/gentoo-hardened-on-the-move/","loc":"https://blog.siphos.be/2012/07/gentoo-hardened-on-the-move/"},{"title":"Dynamic transitions in SELinux","text":"In between talks on heap spraying techniques and visualization of data for fast analysis, I'm working on integrating the chromium SELinux policy that was offered in bug bug #412637 within Gentoo Hardened. If you take a look at the bug, you notice I'm not really fond of the policy because it uses dynamic transitions . That's not something the policy writer can do anything about if he can't access the source code of the application though, since it means that the application is SELinux aware and will trigger transitions when needed. So what's this dynamic transitioning? Well, in short, it means that a process can decide to switch domains whenever it pleases (hence the dynamic part) instead of doing this on fork/exec's. Generally, that sounds like a flexible feature - and it is. But it's also dangerous. Dynamic transitions might seem like a way to enhance security - the application knows it will start a \"dangerous\" or more risky piece of code, and thus transitions towards another domain with less privileges. Once the dangerous code is passed, it transitions back to the main domain. The problem with this is that the entire process is still live - anything that happened within the transitioned domain remains, and SELinux cannot prevent what happens within the domain itself (like memory accesses within the same process space). If the more risky code resulted in corruption or modification of memory, this remains regardless of the SELinux context transitioning back or not. Assume that some code is \"injected\" in the transitioned domain (which isn't allowed to execute other applications) the moment it transitions back to the main domain which is allowed to execute applications, this injected code can become active and do its thing. This is why I didn't allow the original code (which ran chromium in the main user domain and used dynamic transitions towards chromium_renderer_t ) to be used, asking to confine the browser itself within its own domain too ( chromium_t ) so that we have a more clear view on the allowed privileges (which is the set of the chromium domain and the renderer domain together). It is that policy that I'm now enhancing to work on a fully confined system (no unconfined domains). If you want to know more about dynamic transitions, it seems that the blog post Subject & Object Tranquility, part 2 (and don't forget to read the comments too) is a fine read.","tags":"SELinux","url":"https://blog.siphos.be/2012/07/dynamic-transitions-in-selinux/","loc":"https://blog.siphos.be/2012/07/dynamic-transitions-in-selinux/"},{"title":"Hardening the Linux kernel updates","text":"Thanks to a comment by Andy, the guide now has information about additional settings: stackprotector, read-only data, restrict access to /dev/mem, disable /proc/kcore and restrict kernel syslog (dmesg). One suggestion he made didn't make it to the guide (about CONFIG_DEBUG_STACKOVERFLOW) since I can't find any resources about the setting on how it would made the system more secure or more resilient against attacks. Underlyingly, the OVAL now correctly identifies unset variables (it previously searched for \"is not set\" strings in the kernel configuration, and now it searches for the key entry definition and validates if it doesn't find it - e.g. \"CONFIG_PROC_KCORE=\" - since that matches both the definition not being there, or \"# CONFIG_PROC_KCORE has not been set\").","tags":"Security","url":"https://blog.siphos.be/2012/07/hardening-the-linux-kernel-updates/","loc":"https://blog.siphos.be/2012/07/hardening-the-linux-kernel-updates/"},{"title":"Hardening the Linux kernel","text":"I have moved out the kernel configuration settings (and sysctl stuff) from the Hardening Gentoo Linux benchmark into its own Hardening the Linux kernel guide. It covers some common hardening-related kernel configuration entries (although I'm sure I'm missing a lot of them still) as well as grSecurity and PaX settings (which is something the Gentoo Hardened project works on), and finally the system controls (sysctl) that are commonly suggested for a more secure system. The overview of hardening guides now thus contains three guides: one for Gentoo, one for OpenSSH and one for the kernel. These ones were definitions I already had in the past so were \"quickly\" possible to write down. I'm going to look at BIND and DHCP next. But simultaneously, I'm looking at Linux IMA/EVM support in the hope I can have this supported in Gentoo as well. Looks like a promising technology, and if I can get it working, it'll definitely deserve its place within Gentoo Hardened!","tags":"Security","url":"https://blog.siphos.be/2012/07/hardening-the-linux-kernel/","loc":"https://blog.siphos.be/2012/07/hardening-the-linux-kernel/"},{"title":"Hardening OpenSSH","text":"A while ago I wrote about a Gentoo Security Benchmark which would talk about hardening a Gentoo Linux installation. Within that document, I was documenting how to harden specific services as well. However, I recently changed my mind and wanted to move the hardening stuff for the services in separate documents. The first one is now finished - Hardening OpenSSH is a benchmark informing you how to potentially harden your SSH installation further. It uses XCCDF / OVAL so that users of openscap (and other compliant tools) can test their system automatically, generating nice reports on the state of their SSH configuration. For now, the SSH stuff is also still part of the Gentoo document, but I'll move that out soon and refer to this new document. Hardened Gentoo's purpose is to make Gentoo viable for highly secure, high stability production server environments. Hence, hardening documents should be one of its deliverables as well. So, dear users, do you think it is wise for the Gentoo Hardened project to also focus on delivering hardening guides for services? If so, I'm sure we can draft up others...","tags":"Security","url":"https://blog.siphos.be/2012/07/hardening-openssh/","loc":"https://blog.siphos.be/2012/07/hardening-openssh/"},{"title":"Updated Gentoo Hardened/SELinux VM image","text":"I have updated the Gentoo Hardened/SELinux VM image, available on the mirrors under experimental/amd64/qemu-selinux . The new image now asks for the keyboard layout, has a short DHCP timeout value (5 seconds) and provides the nano editor. If you plan on running the image using qemu, please use -cpu kvm64 to use a 64-bit virtual processor.","tags":"Gentoo","url":"https://blog.siphos.be/2012/07/updated-gentoo-hardenedselinux-vm-image/","loc":"https://blog.siphos.be/2012/07/updated-gentoo-hardenedselinux-vm-image/"},{"title":"Gentoo Hardened/SELinux VM image","text":"A few weeks ago, I pushed out a VM image (Qemu QCOW2 format) to the /experimental/amd64/qemu-selinux/ location in our mirrors. This VM image (which is about 1.6 Gib large decompressed) provides a SELinux-enabled, Gentoo Hardened (with PaX and other grSecurity security settings) base installation. Thanks to the Qcow2 image format, only the used 1.6 Gib of data is taken on your disk, even though the image is made for a 50 Gib deployment). The purpose of this image is, eventually, to allow users to test our Gentoo Hardened with SELinux in a virtual environment, offering decent isolation (so you can mess things up if you want, it doesn't hurt your own system). I'm also contemplating of providing more serious SELinux-focused course material (self-teaching stuff) based on this image, so that users can learn about Gentoo Hardened (and SELinux) in a structured manner. But before all that, I first need to see if the image is usable by most people: Does it boot? It is an amd64 image for the Qemu KVM64 CPU, but the kernel uses paravirtualization for disk and network access, and I don't know if that's a safe bet to do or not. People that know KVM know that the paravirtualization support is needed for decent performance, but I'm not sure if it still makes the images sufficiently portable or not. Does it work? The build is done based on my own systems, but these are all built in a similar fashion (and use binhosts to simplify deployment) so in effect, I can only test the images on a single system type (multiple, but they're all the same, so doesn't matter). If I can get some comments on this (it boots, it doesn't boot, it sucks, ...) and can work out things, I hope I can have the images better for all of us. Edit: yes, keyboard layout is azerty, not qwerty. So your rootpass will be rootpqss. Updates-are-a-comin'","tags":"Gentoo","url":"https://blog.siphos.be/2012/07/gentoo-hardenedselinux-vm-image/","loc":"https://blog.siphos.be/2012/07/gentoo-hardenedselinux-vm-image/"},{"title":"Gentoo Summer of Documentation - Let's do it!","text":"The Gentoo Wiki folks have started a great idea (and immediately set a nice milestone), namely the Gentoo Wiki Summer of Documentation . By september, they want to double the amount of articles on the wiki. I'll surely help out and participate where I can, and perhaps we can even go far above the targeted 500 articles!","tags":"Gentoo","url":"https://blog.siphos.be/2012/06/gentoo-summer-of-documentation-lets-do-it/","loc":"https://blog.siphos.be/2012/06/gentoo-summer-of-documentation-lets-do-it/"},{"title":"Had to edit /etc/init.d/root","text":"For some reason, I had to edit my /etc/init.d/root file to use \"mount /dev/root -n -o remount,rw /\" instead of the standard \"mount -n -o remount,rw /\". Without this, it failed to remount the root file system in a read-write mode, which is of course not that funny...","tags":"Gentoo","url":"https://blog.siphos.be/2012/06/had-to-edit-etcinit-droot/","loc":"https://blog.siphos.be/2012/06/had-to-edit-etcinit-droot/"},{"title":"Overview of SELinux changes","text":"Most users of Gentoo hardly take a look at the (installation) documentation when their installation has finished. After all, being a rolling distribution, there is little need to take a look at the instructions again. And for most Gentoo users, changes that are needed to be reviewed by existing users are pushed to them through news items (see eselect news ) or ebuild warnings. For SELinux users, since we're still improving actively (but also often lagging behind in updating our policies to reflect recent changes) I've decided to keep track of changes in the SELinux Handbook as a separate chapter. With this information at hand, existing users should be able to see if there are any changes needed on their part to stay up to date. I haven't used news items here yet, since we're still too volatile, but I do plan on using them in the (near) future.","tags":"Gentoo","url":"https://blog.siphos.be/2012/06/overview-of-selinux-changes/","loc":"https://blog.siphos.be/2012/06/overview-of-selinux-changes/"},{"title":"Python 3 support for SELinux userland, tests and policy rev 10","text":"In the last few hours I pushed my local changes on the SELinux userland utilities towards the hardened-development overlay. The utilities not only include some bugfixes, but have now also seen a first set of tests towards Python 3.2. In the past, I've made a few attempts at making the tools support Python 3, but I failed miserably. Although chances are still high that I failed, at least I got quite a bit further. To make testing a bit easier, I previously made quite a few scripts that did all sorts of things, in order to catch regressions. However, along the way, I've started noticing I had to put lots of effort in streamlining these tests (cleanups), introducing dependency information (test A before B, or cleanup before test, ...) and parallellism (after all, if you have many, many tests, lots of cores but run in single-thread/process mode, it'll take a while). So I started looking at a good way to handle this for me. I switched my tests into a Makefile-driven approach. Why Makefiles? Well, first of all, Makefiles support dependencies . You can define a target and then say which other targets need to be ran first before this target can run. If you want to support these dependencies in a run-independent manner, you can use trigger files but I'm not going to do that yet. Another simple feature is that you can tell make to not show output ( silent mode ) when not necessary. And of course, with make, you can execute targets concurrently . By using a simple, yet manageable directory structure and traverse the Makefiles in them, I am able to easily add more tests and add them to the runqueue. But I'd like to hear from you what infrastructural testing tools you use because I can imagine Makefiles aren't the best solution here. In the mean time, I also pushed the 10th revision of our SELinux policies to the hardened-dev overlay. The most notable fix in it is to improve support for those running \\~arch systems (some fix on the kdevtmpfs support).","tags":"Gentoo","url":"https://blog.siphos.be/2012/05/python-3-support-for-selinux-userland-tests-and-policy-rev-10/","loc":"https://blog.siphos.be/2012/05/python-3-support-for-selinux-userland-tests-and-policy-rev-10/"},{"title":"Catching up, but stuff is piling...","text":"Those that are frequent the #gentoo-hardened chat channel know that I'm currently trying to get the SELinux related utilities working under Python 3. This has progressed quite far, but I'm still not there yet. I'm now hitting a weird bug which seems to come down to an incorrect free() on some memory (well, I don't know this, this is my current assumption) but which seems hard to catch. So I'm learning a lot (thanks to an active community) about debugging Python and memory issues. These past few weeks have been enlightening for me on the matter of Python 2 to 3 conversions. Enough that I can fully understand Diego's pain when dealing with Ruby upgrades ;-) I hope that, if Perl 6 ever comes out (right now, Perl 6 is the future - now and in the future ;-), that they think about the children... err, package maintainers. Because it takes some time to work on these matters, other reported SELinux issues have been piling up; I hope I can close down this Python migration in the near future and work on the remainder of bugs... Next to all this, I'm slowly going through some documentation related bugs, but also mentoring Devan Franchini in his GSoC project on a SELinux policy originator. And now that I linked his blog, he's going to feel obliged to blog on his progress! ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2012/05/catching-up-but-stuff-is-piling/","loc":"https://blog.siphos.be/2012/05/catching-up-but-stuff-is-piling/"},{"title":"Keeping /selinux","text":"Just a very quick paragraph on a just-reported issue: if you upgrade your SELinux utilities to the latest version and you switch from /selinux to /sys/fs/selinux as the mountpoint for the SELinux file system, you might get into issues. Apparently, init (which is responsible for mounting the SELinux file system through a call to libselinux) is trying to mount it on - well yes - /sys/fs/selinux but at that time, /sys is not mounted yet. I haven't been able to reproduce just yet, because I just recently had to move all my systems to use an initramfs (thank you you-need-an-initramfs-when-you-have-a-separate-usr-partition) which premounts /sys. But the current workaround should be to keep /selinux for now. The utilities support it still, and that gives me some time to look and investigate the issue.","tags":"Gentoo","url":"https://blog.siphos.be/2012/05/keeping-selinux/","loc":"https://blog.siphos.be/2012/05/keeping-selinux/"},{"title":"20120215 policies now stable","text":"Today I've stabilized the sec-policy/selinux-* packages that provide the 20120215 \"series\" of SELinux policies. Together with the stabilization, the more recent userspace tools (like the policycoreutils as well as libraries like libsemanage and libselinux) have been pushed out as well. I will be dropping the older policies and userspace tools soon (as they are now deprecated). The documentation has been updated to reflect this too. support for permissive domains (allowing users to mark one specific SELinux domain, such as mplayer_t, as permissive (even though the rest of the system is running in enforcing mode) support for file context translations, so we can now say \"/usr/lib64 (and below) should have the same contexts as /usr/lib\" support for role attributes, which means for policy developers, we now have similar freedom as with type attributes support for named file transitions, so a policy rule can say that domain A, if creating a file in a directory labeled B, then that specific file should have label C. Same for directories, btw. Although some of these enhancements were available as features individually, the policies we had were not aligned with it - and now, that has changed ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2012/04/20120215-policies-now-stable/","loc":"https://blog.siphos.be/2012/04/20120215-policies-now-stable/"},{"title":"Linux Sea now in ePub","text":"On request of Matthew Marchese, I now automatically build an ePub version of Linux Sea for those that like to read such resources on a digital reader. Thanks to the use of DocBook, this was simply a matter of using its xsl-stylesheets/epub/docbook.xsl stylesheet against the DocBook sources and zip the created directory structures (OEBPS and META-INF) to get to the ePub file.","tags":"Documentation","url":"https://blog.siphos.be/2012/04/linux-sea-now-in-epub/","loc":"https://blog.siphos.be/2012/04/linux-sea-now-in-epub/"},{"title":"Why both chroot and SELinux?","text":"In my previous post , a very valid question was raised by Alexander E. Patrakov: why still use chroot if you have SELinux? Both chroot (especially with the additional restrictions that grSecurity enables on chroots that make it more difficult to break out of a chroot) and SELinux try to isolate an application so it only has access to those resources it needs. Chroot does this on file-level basis (and a bit more with grSecurity), SELinux on more general resources. However, things that make SELinux strong (flexible and detailed policy language, fine-grained authorizations) are also its weakness (consolidating files into groups having the same file label), and chroot does have an advantage on this. Suppose that a flaw exists in BIND through which an attacker can read files on the host (through BIND). With SELinux, the domain in which BIND runs is prohibited from accessing and reading files whose label is not one of the labels that the policy thinks BIND should be able to read. More specifically, the BIND policy in the reference policy (which is what both Gentoo and RedHat base their policies on, and generally policies are only enlarged, never really shrinked): etc_runtime_t (read) means access to the files in /etc that are modified at runtime (like mtab, profile.env, gentoo's /etc/env.d) named_var_run_t (read) is access to /var/run/bind and /var/run/named (and a few other related locations) named_checkconf_exec_t (read/execute) is access to read and execute /usr/sbin/named-checkconf named_conf_t (read) to read the BIND-related configuration files dnssec_t (read) to read the DNSSEC keyfiles locale_t (read) to access /etc/localtime, /usr/share/locale/*, /usr/share/zoneinfo/* etc_t (read) to read the general configuration files in /etc (including passwd, fstab, ...) proc_t (read), proc_net_t (read) and sysfs_t (read) to access those pseudo filesystems udev_tbl_t (read) to access /dev/.udev and /var/run/udev (but I have no idea yet why this is in) named_log_t (read/write) for the log files of BIND net_conf_t (read) to access /etc/hosts (including deny/allow), resolv.conf, ... named_exec_t (read/execute) the BIND executables named_zone_t (read) to access the zone files, also write access in case of slave system cert_t (read) to read certificate information named_cache_t (read/write) to access its cache named_tmp_t (read/write) to work with temporary files Isolation provided by SELinux is as powerful as the width of its labeling. For instance, by giving the named daemon read access to /etc files like passwd, fstab, group, hosts, resolv.conf and more, a malicious user who can exploit this hypothetical vulnerability can obtain information that might help him in his further attempts. By chrooting BIND, the files placed in the chroot itself should not offer the information he might be looking for (for instance, the passwd file, if needed at all, is limited to just the named and root accounts, etc.) Chrooting, but not enabling SELinux, could lead to escalation. A chroot cannot restrict what a process is allowed to do beyond the regular access privileges that are given on the user. If a user can upload an exploit through BIND and have BIND execute it, he can use this as an attack vector for further activities. SELinux here prohibits BIND to write stuff it can also execute (there is no write and execute privilege defined here). It also ensures that the BIND daemon never exists his security domain (transitioning towards another domain with perhaps other privileges) as there are no transition rules from named_t to any other domain. Another MAC system that would be better suited to fit both is grSecurity's RBAC model. Iirc, it uses path definitions to say which files are allowed to access and which not. The weakness SELinux here has (aggregation into sets of files with the same label) doesn't exist for grSecurity. This debate on path-based versus label-based access controls have been going on for very long time now - just google it ;-) So, Alexander, in short: chroot further limits the SELinux-allowed privileges to a more fine-grained set of file system resources (files/directories).","tags":"Security","url":"https://blog.siphos.be/2012/04/why-both-chroot-and-selinux/","loc":"https://blog.siphos.be/2012/04/why-both-chroot-and-selinux/"},{"title":"Chrooted BIND for IPv6 with SELinux","text":"BIND, or Berkeley Internet Name Domain, is one of the Internet's most popular domain name service software (DNS). It has seen its set of security flaws in the past, which is not that strange as it is such a frequently used service on the Internet. In this post, I'll give a quick intro on how to use it in Gentoo Hardened (with PaX)... chrooted... for IPv6... with SELinux ;-) Installing is of course, as usual, dead easy on Gentoo (Hardened/SELinux). Make sure you have USE=\"ipv6\" set, and then emerge bind . Also install bind-tools as they contain some great tools to help with DNS troubleshooting. Then we're editing /etc/conf.d/named to set the CHROOT variable. I also set CHROOT_NOMOUNT so that Gentoo doesn't bind-mount the information in the chroot but instead uses the files in the chroot. CHROOT=\"/var/named/chroot\" CHROOT_NOMOUNT=\"1\" Now we need to either temporarily add some privileges in SELinux, or run the portage_t domain in permissive mode. If you go for privileges, then add the following: allow portage_t var_t:chr_file { create getattr setattr }; If you however want to temporarily run the portage_t domain in permissive mode, do that as follows: ~# semanage permissive -a portage_t We are doing this because we are now going to ask the BIND ebuild to prepare the chroot for us. Doing so however requires portage to work on our live file system (and not in the regular \"sandbox\" mode). SELinux however forces portage in the portage_t domain and only gives it the privileges it needs for building and installing software. ~# emerge --config bind When done, remove the previous SELinux allow rules again (or set the portage_t domain back in enforcing mode, through semanage permissive -d portage_t ). Next we need to relabel the files in the chroot. By default, all files are labeled by SELinux as var_t in that location because it isn't aware that it needs to see /var/named/chroot as a \"root\" location. ~# setfiles -r /var/named/chroot /etc/selinux/strict/contexts/files/file_contexts /var/named/chroot So far so good. Now let's create a simple named.conf file (in /var/named/chroot/etc/bind): options { directory \"/var/bind\"; pid-file \"/var/run/named/named.pid\"; statistics-file \"/var/run/named/named.stats\"; listen-on { 127.0.0.1; }; listen-on-v6 { 2001:db8:81:21::ac:98ad:5fe1; }; allow-query { any; }; zone-statistics yes; allow-transfer { 2001:db8:81:22::ae:6b01:e3d8; }; notify yes; recursion no; version \"[nope]\"; }; # Access to DNS for local addresses (i.e. genfic-owned) view \"local\" { match-clients { 2001:db8:81::/48; }; recursion yes; zone \"genfic.com\" { type master; file \"pri/com.genfic\"; }; zone \"1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa\" { type master; file \"pri/inv.com.genfic\"; }; }; The zone files referenced in the configuration file are located in /var/named/chroot/var/bind (in a subdirectory called pri - which I use for \"primary\"). The regular one would look similar to this: $TTL 1h ; $ORIGIN genfic.com. @ IN SOA ns.genfic.com. ns.genfic.com. ( 2012041101 1d 2h 4w 1h ) IN NS ns.genfic.com. IN NS ns2.genfic.com. IN MX 10 mail.genfic.com. IN MX 20 mail2.genfic.com. genfic.com. IN AAAA 2001:db8:81:80::dd:13ed:c49e; ns IN AAAA 2001:db8:81:21::ac:98ad:5fe1; ns2 IN AAAA 2001:db8:81:22::ae:6b01:e3d8; www IN CNAME genfic.com.; mail IN AAAA 2001:db8:81:21::b0:0738:8ad5; mail2 IN AAAA 2001:db8:81:22::50:5e9f:e569; ; (...) while the one for reverse lookups looks like so: $TTL 1h ; @ IN SOA 1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa ns.genfic.com. ( 2012041101 1d 2h 4w 1h ) IN NS ns.genfic.com. IN NS ns2.genfic.com. $ORIGIN 1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa. 1.e.f.5.d.a.8.9.c.a.0.0.0.0.0.0.1.2.0.0 IN PTR ns.genfic.com. 8.d.3.e.1.0.b.6.e.a.0.0.0.0.0.0.2.2.0.0 IN PTR ns2.genfic.com. ; (...) We can now start the init script: ~# rc-service named start On the slave, don't set the allow-transfer directive and set its type to \"slave\". In each zone, you will need to tell where the master is: zone \"genfic.com\" { type slave; masters { 2001:db8:81:21::ac:98ad:5fe1; } file \"sec/com.genfic\"; }; By default, the SELinux policy for BIND does not allow BIND to write stuff in its directories. On the slave system, you will need to change this. A SELinux boolean here does the trick: ~# setsebool -P named_write_master_zones on; There ya go ;-) Okay, all very condensely written, but it should give some feedback on how to proceed. I'm adding this information to the new online resource I'm writing - A Gentoo Linux Advanced Reference Architecture . Nothing really ready yet, just writing as I go forward with exploring these technologies...","tags":"Gentoo","url":"https://blog.siphos.be/2012/04/chrooted-bind-for-ipv6-with-selinux/","loc":"https://blog.siphos.be/2012/04/chrooted-bind-for-ipv6-with-selinux/"},{"title":"Documentation updates for initramfs needed?","text":"A quick help request from the community: if you know of any Gentoo documents that need updates in order for end users to know when and how to use initramfs, please file bugreports and have them block bug #407959 . Currently, we have updated the Gentoo Handbook, Gentoo Quickinstall guides and added an Initial ramfs Guide. The tracker bug is also used to check if and when the eventual roll-out of software can happen, and we want to make sure that we do not forget documentation (something we learned from the openrc migration). Not that the change is as large as was the case with openrc, but it is still nice to have updated documentation in time ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2012/04/documentation-updates-for-initramfs-needed/","loc":"https://blog.siphos.be/2012/04/documentation-updates-for-initramfs-needed/"},{"title":"Get your devtmpfs ready","text":"If you are using stable profiles, you might want to verify if you are already running a kernel with devtmpfs support enabled. Why? Well, currently you might not need it, but the upcoming openrc/udev packages require it and they currently do not fail at install time if you have it enabled or not. As a result, upgrading these packages might give you a system that might fail to boot (if you have no initramfs but separate /usr partition) or gives many errors (if you have an initramfs). To verify if it is enabled, check your kernel configuration: # zgrep DEVTMPFS /proc/config.gz # CONFIG_DEVTMPFS is not set If you get the output as described above, best update your kernel configuration to include it. The second devtmpfs-related option (to automatically mount it on /dev) is not needed afaik. And for those that have been with Gentoo for a while - devtmpfs is not devfs. Well, it is. But it isn't. Somewhat. Oh well, there's discussion on that which I'm not going to elaborate on. Safe to say that we're getting older if we start feeling \"Been there, done that, got the t-shirt\" ;-) Edit: as Robin mentioned in the comments, the udev ebuild does check at it. However, it doesn't fail an installation so you could miss the message. Apologies for the lies, Robin ;-) Post updated.","tags":"Gentoo","url":"https://blog.siphos.be/2012/04/get-your-devtmpfs-ready/","loc":"https://blog.siphos.be/2012/04/get-your-devtmpfs-ready/"},{"title":"More on initramfs and SELinux","text":"With the upcoming udev version not supporting separate /usr locations unless you boot with an initramfs, we are now starting to document how to create an initramfs to boot with. After all, systems with a separate /usr are not that uncommon. As I've blogged about before , getting an initramfs to work well with SELinux has not been an easy drift. In effect, I'm going to push out the FAQ (the Gentoo wiki already has it) that the user will need to boot in permissive mode, and have an init script in the boot runlevel that will reset the contexts of /dev and then switch to enforcing mode. And those that want to make sure SELinux stays on can then also enable the secure_mode_policyload SELinux boolean so that you cannot go back to permissive mode (without rebooting). For those interested, this is the init script I use on my guest systems (which are for development purposes, so they do not toggle the SELinux boolean): #!/sbin/runscript # Copyright (c) 2007-2009 Roy Marples # Released under the 2-clause BSD license. description=\"Switch into SELinux enforcing mode\" depend() { need localmount } start() { ebegin \"Restoring file contexts in /dev\" restorecon -R /dev eend 0 ebegin \"Switching to enforcing mode\" setenforce 1 eend \\$? } I call it selinux_enforce for a lack of imagination (and to make it more clear, because if I'd name it \"wookie\" I'll be scratching my head in a few weeks trying to figure out why I did that in the first place). With that enabled, I cannot provide a \"denial-free\" boot-up anymore (you'll see many denials from the init_t domain, amongst others, which are best not hidden). That is to say, until I take some time to patch the initramfs to handle SELinux. Oh, btw, this is for both dracut-generated as well as genkernel-generated initramfs's. At least the technologies are consistent there.","tags":"Gentoo","url":"https://blog.siphos.be/2012/03/more-on-initramfs-and-selinux/","loc":"https://blog.siphos.be/2012/03/more-on-initramfs-and-selinux/"},{"title":"Hunting fuser","text":"I am able to work on Gentoo and SELinux about one hour per day. It's more in total time, but being a bit exhausted makes me act a bit more slowly which boils down to about one hour per day. And one hour per day isn't bad, you're able to do many things in that hour. The last few days, I've been hunting SELinux denials. I've set my mind onto releasing the 2.20120215-r5 policy only when I've been able to boot a minimalistic Gentoo installation without any visible denials, so either dontaudit them or fix them. Of course, I only want to allow if I'm absolutely confident that they are needed on some systems, but I also only want to dontaudit when I understand what it is doing (and find that it isn't something needed). Some of the denials are driving me up the walls, often having an entire evening hunting for a single one... and this is why you haven't seen many updates since a week or so. The one I'm hunting now, is shown in the logs as follows: Mar 12 20:21:32 testsys kernel: [ 6.550618] type=1400 audit(1331580090.874:4): avc: denied { getattr } for pid=1512 comm=\"fuser\" path=\"socket:[3159]\" dev=sockfs ino=3159 scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=unix_stream_socket Mar 12 20:21:32 testsys kernel: [ 6.551232] type=1400 audit(1331580090.875:5): avc: denied { getattr } for pid=1513 comm=\"fuser\" path=\"socket:[3160]\" dev=sockfs ino=3160 scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=netlink_kobject_uevent_socket Mar 12 20:21:32 testsys kernel: [ 6.562005] type=1400 audit(1331580090.885:6): avc: denied { getattr } for pid=1530 comm=\"fuser\" path=\"socket:[3705]\" dev=sockfs ino=3705 scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=netlink_kobject_uevent_socket ... (these netlink_kobject_uevent_socket ones are repeated a few times) I have no idea who (or what) is executing fuser to find some information. The shown PIDs are those of fuser, and of course that isn't running anymore when the system is booted. The timeframe shown also doesn't seem to provide much information, because it is the time that it is logged by the system logger apparently (I once hoped I was wrong here, but repeated tests and introducing delays and such seems to confirm it). And because the target is on dev=sockfs, it's hardly something I'm able to actively search for. Or at least that I know of. The source context is initrc_t, so it is started from an init script. And the target is always udev_t, so it might be triggered by the udev (or a udev-related) init script (as it seems to only look for these of udev, but that might be a coincidence). But alas, I still don't know what is calling it as I can't find a script or udev rule that calls fuser :-( It doesn't affect the runtime behavior of my system (everything seems to work just fine) so I might go on and dontaudit it. But I so want to know what this is about. To be continued...","tags":"Gentoo","url":"https://blog.siphos.be/2012/03/hunting-fuser/","loc":"https://blog.siphos.be/2012/03/hunting-fuser/"},{"title":"Introducing 2.20120215 policies","text":"A few weeks after being released , we now have the 20120215-based policies available for our users (and also the newer userspace utilities). The packages currently reside in the hardened-dev overlay as they will need to see sufficient testing before we merge those to the main tree. For most users, nothing changes, albeit there are a few changes under the hood that you might get in contact with later... The selinux-base-policy package now depends on a new package called selinux-base . This is the \"real\" base policy package, and now only includes those modules that upstream (reference policy) marks as being base modules. The rest of the modules that we (Gentoo) originally included in base are now built by the selinux-base-policy package and inserted in the policy store together with the base policy. This change is done to make future development a bit more flexible, but also because the policy build fails when we include too many packages. The selinux-unconfined package loads in the unconfined module. Users that know the difference between the strict and targeted policy types: loading the unconfined module in a \"strict\" policy will make the system support domains like in \"targeted\" mode. Currently, there is little use in this module as we (err, I) still need to get this in a good shape. This change is needed to support unconfined domains later when we work with MCS or MLS. The older definitions (using targeted or strict) remain supported though. The pesky change we had to do to /lib64/rcscripts/addons/lvm-st{art,op}.sh is not necessary anymore. This has nothing to do with the tools, but more with an update on the policy itself. I have to give you some reason to upgrade, don't I ;-) Now that the new policy is in, we can start using named transitions as well as use translations so that our file contexts aren't cluttered with all those /lib64 + /lib definitions. These changes will go in later. For those interested in helping, please give these policies thorough testing. I had some work in \"forward-porting\" the patches we had that weren't included upstream yet because of changes in the underlying structure. I hope none are forgotten. If you do find regressions, either ping me on IRC or file a bugreport.","tags":"Gentoo","url":"https://blog.siphos.be/2012/02/introducing-2-20120215-policies/","loc":"https://blog.siphos.be/2012/02/introducing-2-20120215-policies/"},{"title":"Transitioning to MCS policies","text":"Since I started maintaining the SELinux policies for Gentoo Hardened , the policy types we supported were primarily strict and targeted . About half a year ago, we also started supported mcs and offered the possibility for using mls as well (but didn't really support that one). With the recent release of the newer userspace utilities, we found out that Gentoo Hardened is one of the few distributions that still really supports policy types without levels (MCS and MLS have support for levels, strict and targeted don't) as libsemanage had a failure when running simple activities on a system without level support. The fix is fairly trivial, but it does gave me the signal to start moving towards MCS. So, now that the new userspace utilities are in the hardened-dev overlay (please test them ;-) I will now focus on the 2.20120215 policy release, getting that in good shape (forward-porting the patches that haven't made it to the refpolicy repository yet) and then see how we can transition users from strict or targeted to MCS (documentation, upgrade guide and software or packages when needed) so that we are up to par with the majority of other distributions. I personally like the strict policy type as it is fairly simple to explain to users, but I'm sure I can deal with MCS and (in the future) MLS equally well ;-)","tags":"SELinux","url":"https://blog.siphos.be/2012/02/transitioning-to-mcs-policies/","loc":"https://blog.siphos.be/2012/02/transitioning-to-mcs-policies/"},{"title":"This months' stabilization done, more to come","text":"A small notification to tell you that the SELinux policies that were pushed to the main tree 30 days (or more) ago have now been stabilized (none of them introduced problems, although some of them have other bugs still open which are either fixed in \\~arch or will be fixed in the hardened-dev overlay soon). I'll be working on pushing an additional set of changes to hardened-dev overlay today as it includes fixes for openrc that are quite important, and might even push this to the tree faster than usual. The reference policy is also working on a new release, so the moment it is released we will be picking that up as well (give or take a month, since my availability will be a bit less the next month).","tags":"Gentoo","url":"https://blog.siphos.be/2012/01/this-months-stabilization-done-more-to-come/","loc":"https://blog.siphos.be/2012/01/this-months-stabilization-done-more-to-come/"},{"title":"Trying out initramfs with selinux and grsec","text":"I'm no fan of initramfs. All my systems boot up just fine without it, so I often see it as an additional layer of obfuscation. But there are definitely cases where initramfs is needed, and from the looks of it , we might be needing to push out some documentation and support for initramfs. Since my primary focus is to look at a hardened system, I started playing with initramfs together with Gentoo Hardened, grSecurity and SELinux. And what a challenge it was... But first, a quick introduction to initramfs. The Linux kernel supports initrd images for quite some time. These images are best seen as loopback-mountable images containing a whole file system that the Linux kernel boots as the root device. On this initrd image, a set of tools and scripts then prepare the system and finally switch towards the real root device. The initrd feature was often used when the root device is a network-mounted location or on a file system that requires additional activities (like an encrypted file system or even on LVM. But it also had some difficulties with it. Using a loopback-mountable image means that this is seen as a full device (with file system on it), so the Linux kernel also tries caching the files on it, which leads to some unwanted memory consumption. It is a static environment, so it is hard to grow or shrink it. Every time an administrator creates an initrd, he needs to carefully design (capacity-wise) the environment not to request too much or too little memory. Enter initramfs . The concept is similar: an environment that the Linux kernel boots as a root device which is used to prepare for booting further from the real root file systems. But it uses a different approach. First of all, it is no longer a loopback-mountable image, but a cpio archive that is used on a tmpfs file system. Unlike initrd, tmpfs can grow or shrink as necessary, so the administrator doesn't need to plan the capacity of the image. And because it is a tmpfs file system, the Linux kernel doesn't try to cache the files in memory (as it knows they already are in memory). There are undoubtedly more advantages to initramfs, but let's stick to the primary objective of this post: talk about its implementation on a hardened system. I started playing with dracut , a tool to create initramfs archives which is seen as a widely popular implementation (and suggested on the gentoo development mailinglist). It uses a simple, modular approach to building initramfs archives. It has a base, which includes a small init script and some device handling (based on udev ), and modules that you can add depending on your situation (such as adding support for RAID devices, LVM, NFS mounted file systems etc.) On a SELinux system (using a strict policy, enforcing mode) running dracut in the sysadm_t domain doesn't work, so I had to create a dracut_t domain (which has been pushed to the Portage tree yesterday). But other than that, it is for me sufficient to call dracut to create an initramfs: # dracut -f \"\" 3.1.6-hardened My grub then has an additional set of lines like so: title Gentoo Linux Hardened (initramfs) root (hd0,0) kernel /boot/vmlinuz-3.1.6-hardened root=/dev/vda1 console=ttyS0 console=tty0 initrd /boot/initramfs-3.1.6-hardened.img Sadly, the bugger didn't boot. The first problem I hit was that the Linux kernel I boot has chroot restrictions in it (grSecurity). These restrictions further tighten chroot environments so that it is much more difficult to \"escape\" a chroot. But dracut , and probably all others, use chroot to further prepare the bootup and eventually switch to the chrooted environment to boot up further. Having the chroot restrictions enabled effectively means that I cannot use initramfs environments. To work around, I enabled sysctl support for all the chroot restrictions and made sure that their default behavior is to be disabled. Then, when the system boots up, it enables the restrictions later in the boot process (through the sysctl.conf settings) and then locks these settings (thanks to grSecurity's grsec_lock feature) so that they cannot be disabled anymore later. But no, I did get further, up to the point that either the openrc init is called (which tries to load in the SELinux policy and then breaks) or that the initramfs tries to load the SELinux policy - and then breaks. The problem here is that there is too much happening before the SELinux policy is loaded. Files are created (such as device files) or manipulated, chroots are prepared, udev is (temporarily) ran, mounts are created, ... all before a SELinux policy is loaded. As a result, the files on the system have incorrect contexts and the moment the SELinux policy is loaded, the processes get denied all access and other privileges they want against these (wrongly) labeled files. And since after loading the SELinux policy, the process runs in kernel_t domain, it doesn't have the privileges to relabel the entire system, let alone call commands. This is currently where I'm stuck. I can get the thing boot up, if you temporarily work in permissive mode. When the openrc init is eventually called, things proceed as usual and the moment udev is started (again, now from the openrc init) it is possible to switch to enforcing mode. All processes are running by then in the correct domain and there do not seem to be any files left with wrong contexts (since the initramfs is not reachable anymore and the device files in /dev are now set again by udev which is SELinux aware. But if you want to boot up in enforcing straight away, there are still things to investigate. I think I'll need to put the policy in the initramfs as well (which has the huge downside that every update on the policy requires a rebuild of the initramfs as well). In that case I can load the policy early up the chain and have the initramfs work further running in an enforced situation. Or I completely regard the initramfs as an \"always trusted\" environment and wait for openrc's init to load the SELinux policy. In that case, I need to find a way to relabel the (temporarily created) /dev entries (like console, kmsg, ...) before the policy is loaded. Definitely to be continued...","tags":"SELinux","url":"https://blog.siphos.be/2012/01/trying-out-initramfs-with-selinux-and-grsec/","loc":"https://blog.siphos.be/2012/01/trying-out-initramfs-with-selinux-and-grsec/"},{"title":"Unix domain sockets are files","text":"Probably not a first for many seasoned Linux administrators, and probably not correct accordingly to more advanced users than myself, but I just found out that Unix domain sockets are files. Even when they're not. I have been looking at a weird SELinux denial I had occuring on my system: avc: denied { read write } for pid=10012 comm=\"hostname\" path=\"socket:[318867]\" dev=sockfs ino=318867 scontext=system_u:system_r:hostname_t tcontext=system_u:system_r:dhcpc_t tclass=unix_stream_socket I had a tough time trying to figure out why in earth the hostname application was trying to read/write to a socket that was owned by dhcpcd . Even more, I didn't see a connectto attempt, and there is nothing in my policy that would allow the hostname_t domain to connect to a unix_stream_socket of dhcpc_t . But moreover I was intrigued why the given path was no real path, even though it has an inode. So I dug up lsof , which returned the following on this socket: # lsof -p 10017 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME ... dhcpcd 10017 root 3u unix 0x0000000000000000 0t0 318867 socket dhcpcd 10017 root 4w REG 252,3 6 268749 /var/run/dhcpcd-eth1.pid dhcpcd 10017 root 5u unix 0x0000000000000000 0t0 318869 socket Still no luck in figuring out what that is. And even /proc/net/unix didn't give anything back: # grep 318867 /proc/net/unix Num RefCount Protocol Flags Type St Inode Path 0000000000000000: 00000002 00000000 00000000 0001 01 318867 So I started looking at Unix domain sockets, what they are, how they are used, etc. And I learned that Unix domain sockets are just files. Well, most of the time. To use a socket (from server-perspective), a programmer first calls socket() to create a socket descriptor, which is a special type of file descriptor. It then bind() 's the socket to a (socket)file on the file system, listen() 's for incoming connections and eventually accept() 's them. Clients also use socket() but then call connectto() to have its socket connected to a (socket)file and eventually read() and write() (or send() and recv() ). Linux supports an abstract namespace for sockets, so not all of these are actually bound/connected to a file. Instead, they connect to a \"name\" instead, which cannot be traced back to a file. For those interested, looking at /proc/net/unix or netstat -xa shows the abstract ones starting with an @ sign. Not all Unix sockets (actually almost the majority of sockets on my system) can be traced back to either a file or abstract name. And this latter is eating me up. I assume that these sockets were originally created on a file system, but immediately after they were bind() 'ed, the file is unlinked, making it harder (impossible?) to find what the socket file was called to begin with. I first thought it were sockets that were not bind() 'ed to, but many of them have the state CONNECTED displayed (in the netstat -xa output) so that's not a likely scenario. In any case, if you know how these sockets can have an inode without a known path, please let me know. But what has this to do with my previous investigation? Well, because the sockets are descriptors, they are passed when a process uses fork() and execve() . And looking at the source code of dhcpcd, I noticed that it does not close its file descriptors when it calls its hook scripts (through the exec_script() function of its sources). As a result, the open file descriptors (including the sockets) are passed on to the hook scripts - one of them calling hostname . So what I saw in the AVC denials was a leaked socket (so there was no connectto originating from the hostname_t domain since the connection was made by dhcpc in the dhcpc_t domain) that is for some reason being read/written to. A leaked unix stream socket.","tags":"SELinux","url":"https://blog.siphos.be/2011/12/unix-domain-sockets-are-files/","loc":"https://blog.siphos.be/2011/12/unix-domain-sockets-are-files/"},{"title":"Gentoo WiKi & Knowledge Base","text":"I have been playing with the Gentoo Wiki the last few days and am very impressed with the work that both the wiki teams as well as existing contributors have already done to the place. The look and feel is very slick and editing works just as expected. One of the changes I made was to move SELinux module information to the wiki. This documentation was originally intended to be published on the Gentoo SELinux Project page, but is easily accessible and maintainable on the wiki too. So I went a step further and dug up my original GLEP 0051 - Gentoo Knowledge Base proposal and checked how far I could use the Gentoo WiKi for this purpose. From the looks of it, the WiKi can offer a great deal of leverage for this idea and although not everything is supported through the WiKi (like natural search language and such), that might have been overshooting a bit. So we received a Gentoo WiKi Knowledge Base namespace under which the Knowledgebase entries can reside. Now what is the idea behind such a knowledge base? Well, first of all, the articles below this prefix should all follow the same structure (as explained in the main page ) and be sufficiently specific so that the title of the entry should leave little room for misinterpretation. But other than that, there is no limit as to what the Knowledge Base could hold. To that respect, the knowledge base section then provides a (hopefully) thorough listing of common and less common issues with a good explanation why the problem occurred and how to resolve it. For the time being, the location doesn't hold that many entries yet, but I will add them as they come along. And of course, feedback is always appreciated ;-) On a second note, I'd like to give my PoV on the wiki and its relation with the official Gentoo documentation. Unlike what might be circulating, I'm definitely not against the wiki for documentation, on the contrary. Wiki's have proven to be a good resource for documentation, and because we can never have enough documentation writers, every method for getting more documentation is welcome. But because of its online nature, offline documentation development (which I frequently do) is not possible. Also, keeping translations in sync might be a bit more challenging compared to a file-based solution with version control (otoh, I have little experience with WiKi translations so I might be wrong here). I strongly believe that the wiki will play a big role in Gentoo's documentation assets. Many of the documents currently managed by the GDP or the subprojects might be suited to be hosted on the WiKi, especially when those documents are too specific (and as such would require a very specific developer profile to maintain the documents). In such cases, the maintainers of those documents should be able to pick the most efficient method. But for very generic documents, this might not be an easy option. At least the Gentoo documents now support CC-BY-SA 3.0, so we can migrate documents from the wiki to the main site, and the 2.5 version currently used the most on the main site should be forward compatible with 3.0 (if I read the legalese text well) so we might be able to migrate documents from the main site to the wiki too. Edit: a3li created the \"Knowledge Base\" namespace on the wiki, so I updated the links in my post. Thanks for the work on the wiki, a3li!","tags":"Gentoo","url":"https://blog.siphos.be/2011/12/gentoo-wiki-knowledge-base/","loc":"https://blog.siphos.be/2011/12/gentoo-wiki-knowledge-base/"},{"title":"Supporting fix scripts for XCCDF content and maintaining the documents","text":"One of the features supported through OVAL (and Open-SCAP) is to generate fix scripts when a test has failed. The administrator can then verify this script (of course) and then execute it to correct wrong settings. So I decided to play around with this as well and enhanced the Gentoo Security Benchmark ( XCCDF source ) with some fixables (like for the sysctl settings). And lo and behold: the thing works ;-) After evaluating the XCCDF (together with the OVAL document) against my system, I had Open-SCAP generate a fix script: # oscap xccdf generate fix --result-id OSCAP-Test-Gentoo-Default xccdf-results.xml #!/bin/bash # OpenSCAP fix generator output for benchmark: Gentoo Security Benchmark # XCCDF rule: rule-sysctl-ipv4-forward echo 0 > /proc/sys/net/ipv4/ip_forward # generated: 2011-12-23T14:53:03+01:00 # END OF SCRIPT Now isn't that nice. But generating a fix script is one thing, maintaining the XCCDF and OVAL documents is a completely other picture. One of the downsides that I talked about earlier already is that OVAL has quite an extensible language (it's a large XML document). Although this extensibility is very flexible and powerful, when you want to add generic tests (like validating sysctl values or matching regular expressions in files) having to write over 30 lines of XML code for a single test is time-consuming at the least. So I quickly scripted something to help me maintain these settings. The Generating OVAL documents with genoval.sh document explains this script (which is retrievable from my git repository ) whose primary purpose is to transform a single line into the entire OVAL structure. With this script, I can now just say gentoo variable USE must contain ssl and it generates both the rules in the XCCDF as the OVAL statements in the OVAL document. Okay, it's a script, not a feature-full application, but at least it helps me (and perhaps others as well).","tags":"Gentoo","url":"https://blog.siphos.be/2011/12/supporting-fix-scripts-for-xccdf-content-and-maintaining-the-documents/","loc":"https://blog.siphos.be/2011/12/supporting-fix-scripts-for-xccdf-content-and-maintaining-the-documents/"},{"title":"SELinux Gentoo/Hardened state 2011-12-19","text":"On december 14th, the Gentoo Hardened project had its monthly online meeting to discuss the current state of affairs of its projects and subprojects. Amongst them, the updates on the SELinux-front were presented as well. Since last meeting, the follow topics passed the revue. sec-policy/selinux-base-policy , which is the \"master\" of our SELinux policies and contains those SELinux modules that are somewhat indivisible (hence the name, \"base\"), is now at revision 8. I tend to describe the changes on the gentoo-hardened mailinglist, and this is not different for rev 8 . I haven't stabilized the rev 6 one yet although I promised too, I'll try to find some time to do that this evening. We had a regression with newrole for some time. Luckily, Jory \"Anarchy\" Pratt found the issue. Drop the setuid bit from the binary, and the application works again as it should. This will be included in the next policycoreutils bump. The last available sudo package now builds with native SELinux support as well, which allows users to add ROLE= and TYPE= information in the sudoers file. As such, users do not need to call newrole when they need to transition to a specific role for just a single command - sudo can now take care of that. The older selinux/v2refpolicy/* profiles have been deprecated. If you want to use a SELinux-enabled profile, you need to use a profile that ends with /selinux , such as default/linux/amd64/10.0/selinux or hardened/linux/amd64/selinux . Of course we prefer you to use a hardened profile ;-) Documentation-wise, the Gentoo Hardened SELinux Handbook has been updated to reflect the profile changes the SELinux bugreporting guide has been put online to inform users what kind of information is needed for us to fix issues or denials that they might see the SELinux FAQ has been updated with the questions Applications do not transition on a nosuid partition and Why do I always need to re-authenticate when operating init scripts? . That's about it. Not a too busy month but progress anyhow.","tags":"Gentoo","url":"https://blog.siphos.be/2011/12/selinux-gentoohardened-state-2011-12-19/","loc":"https://blog.siphos.be/2011/12/selinux-gentoohardened-state-2011-12-19/"},{"title":"Supporting CC-BY-SA 3.0","text":"Until now, documents on the Gentoo website all had to be licensed under the Creative Commons Attribution/Share Alike license, version 2.5. Why? Because at the time of the license choice, that was probably the latest version at hand. In the XML code itself, the license tagging was done using a <license /> tag. Simple and efficient. But things change, and so do license versions. The folks over at Creative Commons have released version 3.0 somewhere in 2007. I'm not going to cover the differences here, but in general, the principle behind Gentoo's choice for the CC-BY-SA license remains. But we didn't change our licenses and there was no real need for it either. Recently however, the Official Gentoo Wiki was announced, which uses the CC-BY-SA license as well... but the 3.0 version of it. You can't blame them for taking the latest version, but that does made it a bit more difficult to share resources between the two repositories (wiki versus GuideXML-ified website). The solution? Support CC-BY-SA 3.0 for GuideXML too. A few commits in our repository made that change happen. Nothing big though: the DTD is updated to allow for <license version=\"3.0\"/> tags, the XSL is updated to support this attribute (and display the new license) and a few other files (supporting files, like the GuideXML Guide ) have received the necessary updates. The result of the change is that existing documents remain under the current 2.5 license (we are not allowed to bump versions of licenses as most documents are not copyrighted by Gentoo Foundation but by their respective authors) but new documents can now use the 3.0 license. Edit: Sebastian Pipping mailed me to say that in the legal code of the CC-BY-SA 2.5 license there is a clausule about \"... a later version of this license with the same License elements...\", so perhaps I might have a \"take two\" on this.","tags":"Gentoo","url":"https://blog.siphos.be/2011/11/supporting-cc-by-sa-3-0/","loc":"https://blog.siphos.be/2011/11/supporting-cc-by-sa-3-0/"},{"title":"SELinux Gentoo/Hardened state 2011-11-17","text":"A small write-down on the Gentoo Hardened SELinux state-of-affairs, largely triggered because there was an online meeting for the Gentoo Hardened project today. The SELinux policies offered in the sec-policy category are based on the latest refpolicy release. The older policies have been removed from the Portage tree. The patches that we include in our policies are sent upstream and are getting eventually merged. This way we ensure that we keep the policies manageable (larger development audience), secure (more eyes looking at policy changes) and usable for other SELinux-enabled distributions. The userspace utilities to manage SELinux are also the latest ones available upstream; the older ones have been removed from the tree as well as to keep the number of ebuilds small enough. The Gentoo profiles that enable SELinux support are currently the selinux/v2refpolicy ones and the hardened/*/selinux ones. The former are the older profiles and were a bit more difficult to maintain. The latter ones are the newer profiles which have been running for quite some time now. Alas, we will be deprecating the selinux/v2refpolicy profiles pretty soon now. The various SELinux-related documents as offered on our subproject page are regularly crosschecked to ensure that they are up-to-date with the latest SELinux state-of-affairs. An additional guide will be created on how to report SELinux policy bugs in bugzilla to ensure that we have the information that is needed to get a policy patch accepted upstream as well. On a HR-note: Matt Thode (known as \"prometheanfire\") has joined the ranks of SELinux developers in Gentoo Hardened. I've also taken over the position as Gentoo Hardened SELinux subproject lead from Chris Pebenito.","tags":"Gentoo","url":"https://blog.siphos.be/2011/11/selinux-gentoohardened-state-2011-11-17/","loc":"https://blog.siphos.be/2011/11/selinux-gentoohardened-state-2011-11-17/"},{"title":"Gentoo Security Benchmark with OVAL and Open-SCAP","text":"A while ago, I got referred to the Open Vulnerability and Assessment Language , which seems to be an open specification (or even standard) for defining security content/information and being able to document such things in a way that tools can interpret it. Actually, it is a set of these specifications. But first: tl;dr Gentoo Security Benchmark Guide with example report on automated tests based on XCCDF and OVAL , interpreted by Open-SCAP . There; now that we have that out of our way, let's continue on the somewhat more gory details. In this first post, I'd like to talk a bit about XCCDF and OVAL, which are both imo overly complex but interesting XML formats. The first one, XCCDF, is better known as the Extensible Configuration Checklist Description Format and is an XML format in which you document settings (what should a system look like). By itself, that doesn't warrant another XML format. However, the power of XCCDF is that you can define in the document profiles . Each of these profiles is then documented with the set of rules that applies to the profile. So you can have an XCCDF document on the configuration of BIND (the nameserver) and have two profiles: one for a single-server setup, and one for a multi-server (master/slave) setup. These rules also define checks that you can have a tool performed against your configuration. These checks are documented in an OVAL XML file ( Open Vulnerability and Assessment Language ) which can be interpreted by an OVAL-compliant tool. A very simply put statement could be: \"File /etc/ssh/sshd_config must have a line that matches 'PermitRootLogin no'\". Of course, XML doesn't use simple statements. In the case of OVAL, a specific form of normalization occurs. At the beginning, you define a definition that explains what you want to achieve (similar to the above statement) in plain text, and then refer to one or more criteria that needs to be passed if this line applies. Most checks in a configuration guide are simple criteria, but with OVAL you can create criteria like \"If my system is a Gentoo x86_64 one, and I use the hardened profile, then criteria A must apply, but if my system does not use a hardened profile, it is criteria B\". The criteria refers to a test that needs to be executed. This test can be a file expression match, partition information check, service state, installed software, etc. but does not allow executing commands that the user defines (it is not considered a safe practice that you execute commands that are stated in the XML file since most OVAL interpreters will run as root). This test is based on two additional aspects: The object refers to the object or resource that is checked. This can be a partition or a file, or a list of lines that match an expression in a file, etc. The state refers to the state that that object or resource should be in (or should match). With this OVAL language in place, you can now refer to several tests to enhance your XCCDF document, and allow OVAL interpreters to test the various rules on your system. For me, this was the major reason to look into the language, since I had my hopes up to update or rewrite the Gentoo Linux Security Handbook but with a way for users to validate if their system adheres to most/all statements made in that document. As a matter of exercise, I started making such a security benchmark of which you can find a HTML version online (it's a preview URL, so might change in the future). And since it is written with XCCDF and OVAL, I've also added an example report on automated tests too. The sources of these documents are available as well ( XCCDF and OVAL - download as txt but rename to XML then). For those adventurous enough to play with them: install app-forensics/openscap so that you can parse the files. To generate the guide itself, use oscap xccdf generate scap-gentoo-xccdf.xml > guide.html . To run the tests associated with it, use oscap xccdf eval --oval-results --profile Gentoo-Default --report report.html scap-gentoo-xccdf.xml . Also take a look at the Open-SCAP website which is a good resource as well (and the mailinglists are low traffic but with good response times!). So, what is the future on all this for me? First, I'm going to work a bit further on the OVAL statements, so that I can automatically test the majority of settings that I currently have in the benchmark/guide. Only when I'm far enough will I continue on the content of the guide (since it is far from finished) and also see if this isn't something that can be put on a somewhat more official location. If not, I'll still continue developing it, but it'll remain on my dev-page. When I'm somewhat satisfied with that, I might check if I can't have OVAL enhanced with some Gentoo-specific objects (there are already objects for RedHat-like and Debian-like distributions) so that we can write tests for Gentoo settings (like USE flags, profiles, enabled GCC specs, etc.) If we have that, then we can even write checks (XCCDF and OVAL based) to validate if a system is how it is supposed to be (wouldn't that be great, an automated test that tells you if your system is properly set up according to our documents). But XCCDF and OVAL isn't the end. There are other formats available as well, like CCE (for configuration entries), CVE/CPE (to check vulnerability information and its target software/platform) and more. I know RedHat is already actively using OVAL for its security advisories , and other sites like Center for Internet Security are also using XCCDF and OVAL to document and work with security guides. So why would Gentoo not get on that train as well?","tags":"Gentoo","url":"https://blog.siphos.be/2011/11/gentoo-security-benchmark-with-oval-and-open-scap/","loc":"https://blog.siphos.be/2011/11/gentoo-security-benchmark-with-oval-and-open-scap/"},{"title":"Centers of Excellence","text":"When dealing with software (I'll talk about software here, but the information is applicable to most technologies, such as appliances and operating systems) many organizations want to have \"centers of excellence\" with respect to the software. These teams are responsible for positioning the software within the organization, supporting the software and if necessary, act as a link between the internal customer and the software vendor. The approach on these \"centers of excellence\" is often described as a cost efficient way of handling the software within the organization. Sadly, many organizations go to the extreme and try to put as much support and services within those teams as possible, hoping that full consolidation of all service matters would yield an even better (financial) benefit. Such further consolidation however has a negative side which is often overlooked: centralized teams are less aware of the internal customers' requirements and situation . Most internal customers probably have their own IT teams that are much better informed about the criticality of the customers' systems and the services that the customer requests. Those teams are then responsible for getting the right services from those \"centers of excellence\". And that is where the difficulty lies. \"Centers of excellence\" are based on products and technical services . They want to provide the best-in-class service with their products and as such keep enhancing their services in the hope that they can serve all internal customers. But by doing so, they are becoming more and more of a product vendor. In the end, they either focus on the product completely, or they focus on some frameworks and tooling that they have designed and developed to support the integration of the product within the organization. For the IT teams of the internal customer however, the \"centers of excellence\" are less seen as part of the organization and more as a vendor (or broker). That doesn't mean that the concept of these \"centers of excellence\" is wrong though, but they need to keep the organization itself in mind when dealing with the product. The detail on services that they offer need to be aligned with the organizations' strategy and weighted to provide a cost efficient, yet qualitatively best-in-class service. Of course, all that is easier said than done. So let me suggest an approach on software service(s) within an organization. The lowest service that the organization must support for any software is assistance in the installation, upgrade, tracking and eventual removal of the software . Such a service is always needed and can be provided with little knowledge of the internal customer. A \"center of excellence\" should provide the means to (semi)automatically install the software - preferably using the organizations' standard software deployment methods, upgrade the software (both for major releases, minor releases as well as security patches), remove and (not to forget) track where the software is installed. Especially when dealing with proprietary software, tracking is almost mandatory for organizations to keep track of the licenses needed. This lowest service offering has almost only positive sides: IT teams that need the software can easily (and without further assistance of other teams) install the software. If the installation method is automated, it can even be done in a fast manner, which is always to the liking of the customer. The organization keeps its risks low by providing the security updates and product upgrades in a seem less manner. By tracking deployments, the organization can keep track of licenses used (for instance, the \"center of excellence\" can provide regular reporting towards the financial departments) and, if the tracking is done right, can even suggest improvements in the architecture or deployments to further minimize license cost. IT teams can freely focus on the solution that they are building for the customer without the need to duplicate software installation methods and different patching processes. By providing these services from a \"center of excellence\", you definitely reduce certain research & development costs - without this offering, each team would need to develop processes to deploy software and track its usage. This is independent of the internal customer and as such, consolidation is a definite win here. Once this service is offered, the \"center of excellence\" can focus on the services that they can provide and which are mandatory for all internal customers (not \"usable\", but mandatory) and for which little flexibility (in design or development) is possible. There are not that many cases here, and this is very specific to each technology and organization in which the software is made available. As a hypothetical example, consider an LDAP service. The \"center of excellence\" might want to provide auditing (and alignment with an organization standard auditing system) if the organization has a policy that auditing is mandatory, regardless of the project for which LDAP is used. Of course, if the \"center of excellence\" wants to stop at this service offering (i.e. the previously mentioned installation/tracking, and now auditing) then this is most likely a best practice document geared towards the IT teams that need to implement it. The benefit? The IT teams are in this case aware of the requirement (auditing must be enabled) and do not need to investigate how to do this anymore (it is already documented). The third level of service offering that I see is the reusable, customer-independent services that one wants to provide on the software. For a database, this might mean alignment with the organizations' backup infrastructure. For an LDAP, that might mean getting feeds from a central source (be it a central LDAP infrastructure, an Identity Management system, ...). When you consider providing this service (which is usually the case in larger environments), take special care that the service you want to offer is flexible enough so that any IT team can work with it. A service that is only applicable to 70% of your internal customers will not do it. For an LDAP service, this might mean that you provide out-of-the-box configuration templates, best practice information for its back-end infrastructure (which includes backup/restore operations), ... But make sure that you are not redesigning and re-developing what your product already provides. I have seen numerous cases where teams develop tools that should \"shield the complexity\" from the end user, but in effect are creating additional layers of clouds and complexity instead. If you want or need to abstract complexity from the user, make sure that this is only a single layer that you are introducing. The moment you are creating tools on top of previously written tools, you should reconsider your actions. Often, \"centers of excellence\" want to rewrite documentation for the end users. They feel that the documentation available from the vendor is too complex for IT teams to use. Although I can relate to that, they should not underestimate the expertise within the IT teams. If the IT teams do not want to gain the knowledge or experience through the product guides, then they are less likely to properly maintain and troubleshoot the product. In such cases, I would wager that it is more beneficial for the organization to look at their (human) resources and their relation to the software. In times like these, where cloud solutions play an important role, my suggestion would be to consolidate the software usage towards a SaaS principle ( Software-as-a-Service ) managed by an experienced team (or teams). This does not mean that the \"center of excellence\" has to play this role, but it does sound like a logical step for them. After all, if the \"center of excellence\" only defines additional services without actually consuming them, they might lose track of the product in real-case scenario's. Take a web server for example - Apache. You might have a \"center of excellence\" for Apache web servers, which provides easily deployable packages. They might provide example configuration files as well as pointers towards Apache's documentation (and best practices). They track the deployments and ensure that security patches are available as soon as possible. But they should guard over the rest of the services that they want to offer. Why would that team create a framework for auto-generating configuration files? What is the benefit of this for the entire organization, for each customer? If the IT team can take care of the configuration, let them be. But if those IT teams would rather not manage these web servers themselves, what is the point in creating additional frameworks to \"hide the complexity\"? It is, imo, much more efficient to see if you cannot provide web hosting services instead, and have the IT teams \"buy\" these (internal) web hosting services. There are many advantages to this: the web hosting environment is managed by a team of experts, can be consolidated to keep the TCO low, can provide default configurations that are fully aligned with the organization but still offer the flexibility that individual customers might require. Same goes for other software products: java application servers (like JBoss or IBM WebSphere AS), databases (Oracle, SQL Server, MySQL or Postgresql), messaging systems, log servers, LDAP services, file share services, ... So what about you - how do you position \"centre of excellence\" teams? Can you relate with such a \"back to basics\" approach, or would you rather see a fully integrated, standardized solution roll-out where IT teams only have experience with the organization-only frameworks?","tags":"Misc","url":"https://blog.siphos.be/2011/10/centers-of-excellence/","loc":"https://blog.siphos.be/2011/10/centers-of-excellence/"},{"title":"SELinux' 2011/07 releases now stable","text":"A few minutes ago, I stabilized both the 2.20110726 policies as well as the SELinux userspace utilities that were stable (upstream) on 20110727. With the change, I also updated the Gentoo SELinux Handbook with the changes I presented on our gentoo-hardened mailinglist. After some time, I'll remove the now obsoleted older policies and userspace utilities to keep the tree in a sane state. There are a few policy packages whose stabilized version isn't the latest (cfr earlier post), those are due within the proper designated period (about 1 month).","tags":"Gentoo","url":"https://blog.siphos.be/2011/10/selinux-201107-releases-now-stable/","loc":"https://blog.siphos.be/2011/10/selinux-201107-releases-now-stable/"},{"title":"Gentoo Hardened SELinux policies, rev 5","text":"I've pushed out selinux-base-policy version 2.20110726-r5 to the hardened-dev overlay. It does not hold huge changes, most of them are rewrites or updates on pre-existing patches (on the SELinux policies) to make them conform the refpolicy naming conventions and other guidelines. It includes preliminary support for the XDG Specification although changes there are still going to occur (as the policy is still under development). Other updates are primarily on the policies for user applications (pan, mozilla, skype), portage and asterisk. In related news, the Gentoo Hardened SELinux FAQ is updated with two entries: Portage fails to label files because \"setfiles\" does not work anymore , and Applications do not transition on a nosuid-mounted partition","tags":"Gentoo","url":"https://blog.siphos.be/2011/10/gentoo-hardened-selinux-policies-rev-5/","loc":"https://blog.siphos.be/2011/10/gentoo-hardened-selinux-policies-rev-5/"},{"title":"Upgrading GCC, revisited","text":"Gentoo has, since long, had a GCC Upgrading guide. A long time ago, upgrading GCC required quite a lot of side activities and was often considered a risky upgrade. But times change, and so do the GCC upgrade cycles. Improved compatibility as well as a better understood impact made GCC upgrades a lot less painful. Sadly, our documentation didn't reflect that as much - the last update on the GCC Upgrade guide was from 2008. Today, after some clarifications from gentoo-dev , I've updated the GCC Upgrading Guide to reflect the improved situation. I'm hoping this will help clear out the uncertainty surrounding GCC upgrades. As always, comments and feedback are always welcome.","tags":"Gentoo","url":"https://blog.siphos.be/2011/10/upgrading-gcc-revisited/","loc":"https://blog.siphos.be/2011/10/upgrading-gcc-revisited/"},{"title":"Mitigating risks, part 5 - application firewalls","text":"The last isolation-related aspect on risk mitigation is called application firewalls . Like more \"regular\" firewalls, its purpose is to be put in front of a service, controlling which data/connections get through and which don't. But unlike these regular firewalls, application firewalls work on higher-level protocols (like HTTP, FTP) that deal with user data rather than with connection routing. I'm going to call these firewalls \"network firewalls\", although most modern network firewalls have some application firewall functionality as well. The purpose and necessity of network firewalls is well known and understood: make sure that the service is only accessible from the right location, check if connections aren't abused (or too many connections are made), etc. But what if the connection itself is valid? After all, most abuse of services is not because they originate from the wrong location or try to access the wrong service. Instead, such abuse comes from valid access to the application, but with less kosher intentions. So what can application firewalls do in this case? Because they perform inspection of the data that is transferred itself, application firewalls can detect malicious data fragments or attempts to abuse the service. These detection rules can be based on general, heuristic rules (well-known examples are detection rules for cross-site scripting attacks (XSS) or SQL Injection) but can also be very specific to a particular application. Because all data is transferred through the firewall and the firewall has knowledge of the application itself, these firewalls offer advanced auditing features since they can detect authentication steps, user data, application-specific transactions and more. With knowledge of the users' session and behavior (application-level) and origin (network level), application firewalls can detect and prevent unauthorized sessions , such as the case with session hijacking or even man-in-the-middle attacks (based on behavior detection) Implementing an application firewall however doesn't only mean that you improve access controls on it. It has other advantages that make application firewalls an important part in many architectures: If all service access is forced through the application firewall (for instance through an IP filter on the service that only allows connections from the application firewalls) you can implement rules that deter known attacks/vulnerabilities without needing to fix the code itself (or if fixing is possible, lower the time pressure). For instance, for Apache-based services, such an application firewall could detect or even change the Range: header on malicious requests to lower the impact of this potentially nasty DoS vulnerability Depending on the complexity, some functional application bug fixing might even be possible. For instance changing content types on requests/replies (HTTP), adding a domain on an FTP accounts' login statement, ... Many application firewalls (or gateways) offer proxy functionality which might improve response times . This is not a sure-given, since most applications are session-aware so the advantage is only for session-agnostic requests (be it static content or specific SQL statements in case of a database firewall). But also in case of session-aware statements can an improvement be found. Consider a database firewall which translates SQL statements from an unsupported application towards better defined statements (for instance using proper indexes or materialized views). In some cases, you might even be able to upgrade a backend of an unsupported application (which previously required an outdated version of that database) by translating the backend requests when they are incompatible with the new backend version. So you can improve integration or support unsupported upgrades . In case of risk reduction, application firewalls also allow you to move a service elsewhere (even in the public cloud ) and still keep the access under control. Of course, it would be TGTBT (Too Good To Be True) if there isn't an (important) downside: maintaining the application firewall is a daunting task . Because of its flexibility, you'll need deep knowledge in the application firewall administration and development, keep track of all rules you have (and why you have them), do lots and lots of testing on each rule (since it might affect the functioning of the application) and still be aware that subtle differences introduced by the application firewall rules can pop up unexpectedly. Also, integrating an application firewall is another service between your customer and his service, which might influence performance but also makes the underlying architecture more complex. Finally, you'll need to consider that an application firewall requires lots of resources (CPU/memory), especially when it needs to perform SSL/TLS termination. Oh, and they're often expensive too. Still, even with these downsides, application firewalls are an important part of the service isolation strategy, which is a key aspect in the risk mitigation strategy which this series started with. We've focused on three now: service isolation (network-wise), process isolation (through mandatory access control ) and now access isolation through application firewalls. And with proper hardening in place, I believe that you have done all you can do to reduce the risks when running unsupported software (apart from upgrading it or switching towards supported software). If you have other ideas that benefit risk mitigation, with specific focus on unsupported software, I would be glad to hear about them.","tags":"Security","url":"https://blog.siphos.be/2011/10/mitigating-risks-part-5-application-firewalls/","loc":"https://blog.siphos.be/2011/10/mitigating-risks-part-5-application-firewalls/"},{"title":"Quickly setup a Gentoo system","text":"In order to verify if the installation instructions in the Gentoo Handbook are still valid, and to allow me to quickly seed new Gentoo installations in a virtual environment, I wrote a very ugly (really) script to automatically \"stage\" a Gentoo Linux installation in a KVM guest. This is not my intention to make this an \"unattended\" installation script, it is merely one of the many scripts out there to help some poor developer in working a bit more agile. I decided to document gensetup as a first step (cfr my earlier Catching up post) in my quest to document how to setup a virtual Gentoo Hardened (with SELinux) virtual architecture. The gensetup tool is just to provide a (semi)automated way to install Gentoo according to the instructions in the Gentoo Handbook. Later, I'll add documentation for the setup_*.sh scripts that I use to upgrade such a base installation to a specific server/service. You want a probably better working installation script, check out Andrew Gaffney's Quickstart for Gentoo . And if you know of other such scripts, I would be glad to hear from them, if not just to keep track of the various similar projects out there. Edit: The quickstart application does not seem to be maintained anymore. My bad. However, suggestions are made in the comments for more up-to-date systems ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2011/09/quickly-setup-a-gentoo-system/","loc":"https://blog.siphos.be/2011/09/quickly-setup-a-gentoo-system/"},{"title":"Power management guide updated","text":"The Gentoo Power Management Guide is now updated. It is a full rewrite, focusing currently on two main toolsets: Laptop Mode Tools and cpufreqd . I was pleasantly surprised by the number of features that the laptop mode tools package provided. Of course, this does not mean that the guide is now finished. Documentation never is, so do keep on suggesting improvements (and pointing to bugs) in Gentoo's bugzilla .","tags":"Gentoo","url":"https://blog.siphos.be/2011/09/power-management-guide-updated/","loc":"https://blog.siphos.be/2011/09/power-management-guide-updated/"},{"title":"Mitigating risks, part 4 - Mandatory Access Control","text":"I've talked about service isolation earlier and the risks that it helps to mitigate. However, many applications still run as highly privileged accounts, or can be abused to execute more functions than intended. Service isolation doesn't help there, and system hardening can only go that far. The additional countermeasures that you can take are application firewalls and mandatory access control. And now you know what part 5 will talk about ;-) Standard access control on most popular operating systems is based on a limited set of privileges (such as read, write and execute) on a limited scale (user, group, everyone else). Recent developments are showing an increase in the privilege flexibility, with the advent of manageable capabilities (Linux/Unix) or Group Policies (Windows). However, these still lack some important features: Users are still able to delegate their privilege to others. A user with read access on a particular file can copy that file to a public readable location so others can read it as well. Privileges on his own files and directories are fully manageable by the owner. For our risk mitigation approach on unsupported software, that means that a vulnerability might be exploited so that the service \"leaks\" information. It is especially important in an attack that uses a sequence of vulnerabilities (such as in an advanced persistent threat ) where low-risk vulnerabilities can be combined into a high-risk exploit. Privileges are still user-level privileges (including technical account users). In case of running services, this almost always means that the process has more privileges than it requires. Some software titles allow for dropping capabilities when not needed anymore. Most however are oblivious of the rights they possess. Abuse of the service (which includes use of features that the service offers but are not allowed policy-wise by the organization) cannot be prevented if hardening doesn't disable it. Privileges are managed by many actors (such as the system administrators) and are not that easy to audit. Privilege denials are often not audited, causing issues to only come up when they occur, rather then when the attempt to provoke issues is started. In many cases, a malicious (or \"playful, inventive\") user starts with investigating and trying out long before a way is found to abuse the service. In case of a mandatory access control system, a security administrator is responsible for writing and managing a security policy which is enforced by the operating system (well, higher level enforcement would be even better, but is currently not realistic). Once enforced, the policy ensures that privileges are not delegated (unless allowed). Also, in most MAC systems, the policy allows for a much more detailed privilege granularity . And recent server operating systems have support for MAC - I personally work with SELinux for the (GNU/)Linux operating system. But this more granular flexibility in privileges comes with some costs. First of all, it becomes much more complex to manage the policy . You'll need highly experienced administrators to work with a MAC-enabled system. Second, a MAC model has a negative influence on performance since the system has to check many more accesses and access rules. To make MAC-enabled systems workable, operating systems offer a default policy which already covers many services. Also, developers on the MAC technology are continuously safe-guarding performance - I personally do not notice a performance degradation when using SELinux, and more realistic benchmarks suggest that the impact of SELinux is between 3% and 12% depending on the policy level. But what does that mean towards the initial risk list that I identified in the beginning of this article series? Well, directly, very little: mandatory access control in this case is about reducing the impact of security vulnerabilities (and abuse of the service). It will not help you out in other ways. However, there are other things to gain from a mandatory access control than just threat reduction. An advantage is - again - that you get to know your application well, especially if you had to write a security policy for it. Since you need to define what files it can access, which kind of accesses it is allowed to do, which commands it can execute, etc, it will give you insight in how the application operates. Bugs in the application might be solved faster and you'll definitely learn more about how the application is integrated. Another one is that most mandatory access control systems have much more detailed auditing capabilities. Attempts to abuse the service will result in denials which are detected and on which you can then take proper action. Taking a higher-level look at mandatory access control will show you that, in case of risk mitigation, it is much more like service isolation, but then on the operating system level. You isolate the processes, governing the accesses they are allowed to do. But the one main issue - active exploits on the application service - cannot be hindered by neither service isolation (since the service is still accessible), hardening (although it might help) or mandatory access control (which reduces the actions an exploit can do). To make sure that vulnerabilities are less likely to be exploited, I'll talk about application firewalls in the next post.","tags":"Security","url":"https://blog.siphos.be/2011/09/mitigating-risks-part-4-mandatory-access-control/","loc":"https://blog.siphos.be/2011/09/mitigating-risks-part-4-mandatory-access-control/"},{"title":"Catching up","text":"As mentioned on the gentoo-doc mailinglist, all documentation bugs (that we know of) related to openrc have been fixed. It was already a week like so, but the last dependency on our \"tracker\" bug was an open one (asking if more needs to be done or not) from which we haven't received an answer in over a month. So I guess we're there. Now, the OpenRC transition wasn't an easy one documentation-wise. Since there is no full backwards compatibility, all changes would need to be done in an atomic way, but due to resource constraints, the documentation couldn't catch up on the changes in due time. Luckily, that's over now and we can hopefully start by improving our documentation once again. For SELinux too, OpenRC hasn't been a gift. The latest selinux-base-policy now in the Portage tree (20110726-r4) still includes some fixes to get OpenRC support fully working. However, I'm fairly confident that we will be able to tackle other bugs (if they arise) quickly now, since the basic policy definitions (like support for rc_exec_t ) are now in place. With the major changes done, let's look at the future. For documentation, I'm now working on a new Power Management Guide whereas for SELinux, I'll be focusing on the remaining bugs as well as documentation updates (the SELinux Handbook will have some major updates in the hope it becomes more useful and future-proof). Also, for GDP, I'm going to make a suggestion towards the Gentoo Documentation Policy , taking into account that the GDP resources are not as high as at the time the policy was written. Finally, I'm going to update my installation scripts that I use to seed the virtual servers so that I can enhance the SELinux policy testing. I consider this post to be a checklist - after all, now that I promised that I would do that, I guess I can't excuse myself from that anymore do I ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2011/09/catching-up/","loc":"https://blog.siphos.be/2011/09/catching-up/"},{"title":"Mitigating risks, part 3 - hardening","text":"While I'm writing this post, my neighbor is shouting. He's shouting so hard, that I was almost writing with CAPS on to make sure you could read me. But don't worry, he's not fighting - it is how he expresses his (positive) feelings about his religion. Security is, for some, also a religion. They see risks and vulnerabilities and what not everywhere. They're always thinking every system in the world is or will be hacked in the near future and are frantically trying to secure every service they are running - and more. But security is also a real-life issue. If you take a look at the compromised GlobalSign website (who mentions that the website is an isolated one - as I described earlier ) I hope that you look at security as being a functional requirement in architecturing and design (and not a non-functional one as many frameworks suggest). And as you can see from the example, isolating services is not sufficient to prevent a successful exploit of an insecure or unsupported software (the reason why I started with this series). One additional measure that you can take is hardening the server and service. The act of hardening a server and service is to configure the system so that it is as secure as possible, based on configuration entries. Many vendors and projects offer a security guide (like the Gentoo Security Handbook or the Fedora Security Guide ) although most of them add this as part of their standard administrative documents (like the PostgreSQL \"Server Setup and Operation\" chapter). But for some reason, you'll find that default installations - even when following the instructions of the vendor - are not as secure as you want it to be. As a matter of fact, if you come in contact with auditors, you'll probably fail any audit if you use a default installation. To help administrators to secure their services, you will find lots of third party sites offering advice on securing the operating system and the services running on it. These guides are what you will need to \"harden\" your system. OWASP , which stands for Open Web Application Security Project, hosts some hardening guides and suggestions together with test scenarios. For front-end application servers (mostly web application servers) you will find lots of interesting resources in the OWASP site (and surrounding community). Google is probably the best resource for finding hardening guides for your operating system or service. Just look for \"hardening foo\" and you will be reading for a week. CISecurity , or \"Center for Internet Security\", is another one with a larger portfolio on hardening guides. Not only does it offer these guides (which it calls \"benchmarks\"), but organizations can also become a member and as such benefit from tooling that CISecurity supports for the validation of benchmarks (i.e. test if the system/deployment is compliant towards a particular benchmark). It does that by developing the benchmarks in a open specification called OVAL (the Open Vulnerability and Assessment Language ) and XCCDF ( XML Configuration Checklist Data Format ). And CISecurity is not the only one there. Another such resource is the National Vulnerability Database (national for US residents, that is ;-) There you can find and download the OVAL/XCCDF resources for various software titles and operating systems. But as you can imagine from the abbreviations, the resources are XML files which are not made to be read by humans. Although you can use the tool(s) that CISecurity offers, another possibility is to use Open-SCAP , an open source framework for handling SCAP, OVAL, XCCDF and other such open specifications on a system. Its documentation offers a first glance at what it can support. However, this brings on he disadvantages of hardening services... Hardening a system and its services is a time consuming job. Its only purpose is to reduce the impact of exploited vulnerabilities and reduce the \"attack surface\" so that exploits on unused functions are not possible. Hardening a system and its services can impact the service . Make it too tight, and it might not behave anymore like you want it to. Also, since there are many, many resources \"out there\" on hardening, you will have to manage your hardening rules , document them for yourself. It is also advisable to document the rules you are not implementing, if not just for future's sake. The hardening guides also require quite some expertise on the service . If you are not experienced with the service but you need to harden it, you can be lucky and just implement what is suggested and hope for the best, but usually you will need to dive deeper in the subject and make (tough) choices. Although specifications like SCAP exist to help you in your hardening exercises, these are still difficult to manage (do not try to write OVAL/SCAP/XCCDF content in your favorite text editor). Its adoption however by Fedora and RedHat is showing a positive effect on the tools surrounding this specification. I will be writing about SCAP, OVAL and XCCDF later since I too see good use of it in organizations (or even free software projects). Does that mean that hardening is not beneficial? On the contrary: You gain lots of knowledge in the matter, and also forces you to think about integration aspects. Since you are responsible for the service (or the damage that could be made if the service is exploited) being knowledgeable is definitely a good thing. A considerable amount of vulnerabilities that are and will be reported on the service (check CVE details to find out about publicly known vulnerabilities, documented in the CVE specification) will not have their effect on a well hardened service. Or put another way, you will reduce the number of real vulnerabilities in your service. You will not be able to exclude all vulnerabilities, but the projected number is high - a fully hardened Windows or Linux system can mitigate up to 90% of the exploits on the operating system. It will considerably reduce the risks that you and your organization are taking. A well defined hardening guide will also offer the means to automatically audit or check if the system is still compliant to the hardening setup you envisioned. Scheduled regularly, this will ensure that your configurations are not drifting away, back to a more vulnerable setup, for whatever reason. By removing the functions that the service should not offer, you make sure that the use of the service is per the organizations' guidelines. (Internal) abuse of the service is made more difficult, so users are forced to take the regular way. Unlike service isolation, which allows you to keep track of data/service flows, hardening makes sure that side-functionality is not used without your consent. Or to put it more blunt, \"Yes I know Oracle DB can be used to schedule tasks on the operating system, but no, you're not allowed to use that function\". And who knows, perhaps by optimizing the configuration, it might run faster with a lower resource footprint ;-) If it does, that's perfect, since the next topic on risk mitigation will have a negative influence on performance: mandatory access control.","tags":"Security","url":"https://blog.siphos.be/2011/09/mitigating-risks-part-3-hardening/","loc":"https://blog.siphos.be/2011/09/mitigating-risks-part-3-hardening/"},{"title":"Mitigating risks, part 2 - service isolation","text":"Internet: absolute communication, absolute isolation \\~Paul Carvel The quote might be ripped out of its context completely, since it wasn't made when talking about risks and the assurance you might need to get in order to reduce risks. But it does give a nice introduction to the second part of this article series on risk mitigation . After all, if the unsupported software is offering services to the Internet, you really want to govern the communication and isolate the service. When you are dealing with a product or software that is unsupported (be it that it will not get any patches and updates from its authors or vendor, or there is no time/budget to support the environment properly), it is in my opinion wise to isolate the service from the rest. My first post on the matter gave a high-level introduction on the risks that you might be taking when you run unsupported (or out-of-support) systems. Service isolation helps in reducing the risks that others have when you run such software on a shared infrastructure (like in the same network or even data centre). By isolating the unsupported service from the rest, you create a sort-of quarantine environment where sudden mishaps are shielded from interfering with other systems. It provides insurance for others , knowing that their (supported) services cannot be influenced or jeopardized by issues with the unsupported ones. And if these services need to interact with the isolated service, the interface used is known and much more manageable (think about a well-defined TCP connection versus local communication or even Inter-Process Communication). But it goes beyond just providing insurance for others. Isolation forces you to learn about the application and its interaction with other services. It is this phase that makes it extremely important in an environment, because not knowing how an application works, behaves or interacts creates more problems later when you need to debug issues, troubleshoot performance problems and more. Integration failures, as described in my previous post, can only be dealt with swiftly if you know how the service integrates with others. Another advantage of proper service isolation is that you can fix its dependencies more easily. Remember that I talked about upgrade difficulties, where a necessary upgrade for one component impacted the functionalities of the other (unsupported) component? With good isolation, the dependencies are more manageable and controllable. Not only are (sub)component upgrades easier to schedule, it is also a lot easier to provide fall-back scenario's in case problems occur. After all, the isolated service is the only user so you have little to fear if you need to roll-back a change. But what is proper service isolation? First of all, it means that you focus on running the (unsupported) software alone on an operating system instance. Do not run other services on the same OS , not even if they too are unsupported. The only exception here is if the other services are tightly integrated with your service and cannot be installed on a separate OS. But usually, full service isolation is possible. Next, strip the operating system so it only runs what you need for managing the service. Put primary focus on services that are accepting incoming connections (\"listening\") and secondary focus on allowed outgoing protocols/sessions (and the tools that initiate them). See if you can virtualize the environment . In most cases, the service does not require many resources so it would be a waste running it on a dedicated system. However, in my opinion, a much better reason for virtualization is hardware abstraction. Sure, all operating systems tell you that they have some sort of Hardware Abstraction Layer in them and that they can deal with hardware changes without you noticing it. But if you are an administrator, you know this is only partially true. Virtualization offers the advantage that the underlying hardware is virtual and can remain the same, even if you move the virtualized system to a much more powerful host. Another advantage is that you might be able to offload certain necessary services from the OS (like backup) to the host (snapshotting). Shield the operating system, network-wise, from other systems. Yes, that means putting a firewall BEFORE the operating system guest (and definitely not on the OS) which governs all traffic coming in and out of the environment. Only allow connections that are legit. If your organization has a huge network to manage, they might work with network segment filtering instead of IP-level filtering. See if you can get an exception to that - managing the rules should not give too much overhead since the system, being unsupported and all, is a lot less likely to get many connectivity updates. But before finishing off, a hint about stripping an operating system. Stripping is much more than just removing services that are not used. It also means that you look for services that are needed, and see if you can externalize them. Common examples here are logging (send your logs to a remote system rather than keeping them local), e-mail (use simple \"direct-out\" mail) and backup (use a locally scheduled backup tool, or even offload to the host in virtualized systems), but many others exist. Of course, service isolation is not unknown to most people. If you run a large(r) network with Internet-facing services, you probably isolate those in a DMZ environment. That is quite frankly (also) for the same \"risk mitigation\" reason. In case of a security breach, service unavailability or otherwise, you want to reduce the risk that this fault spreads to other systems (be it getting to internal documents or putting more services down). Another aspect administrators do with systems in their DMZ is system hardening , which I will talk about in the third part.","tags":"Security","url":"https://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/","loc":"https://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/"},{"title":"Mitigating risks, part 1","text":"We are running Foobar 2.0 on Tomcat 4. We know that Tomcat 4 isn't supported, but hey - our (internal) customer is happy that the Foobar application works and would like to keep it that way. Upgrading to Tomcat 5 or higher is not possible - Foobar 2.0 only works on Tomcat 4. If we want to use a higher Tomcat version, we need to upgrade the application which costs a lot of money (which our (internal) customer doesn't want to pay) and requires lots of testing as it is a non-trivial upgrade. So... what can an IT department do to mitigate the risks here? This is not a hypothetical example (well, apart from the software titles used) for many organizations. Be it the application itself, its middleware, back-end or operating system: often you'll face an end-of-support deadline without the means to upgrade the application (because of budgetary issues, unwillingness of the responsible department or no alternative). Whatever the reason, you as an IT department have the responsibility to mitigate the risks involved with running out-of-support software (and communicate risks to all parties that are affected by it). So what are your options? In this series of posts, I'll cover a set of risk mitigation strategies that might help you reduce the issues that come up from running out-of-support software. But first, what are those risks? Security patches . It is the first risk that the operations department will say when they have to deal with unsupported software. Well, the risk isn't the security patches, but the result made by the lack of it. Software tends to have bugs. Some of these bugs can result in inappropriate functionality, such as granting access to unauthorized people or even executing (unwanted) commands on the server. Sounds improbable? Guess again . Especially when running out-of-support software this becomes a nightmare to manage, because security patches are not created anymore, and newly discovered vulnerabilities might still affect older versions - even when the vulnerabilities do not mention the older versions anymore. And the worst thing is that you might not even detect it . Functional bugs . If your customer tries something out and the application barfs, then there is little you can do to fix this. Either you dive in the code yourself (good luck with that) or you hope that a workaround exists. Getting a functional bug fix is not that feasible. Also, do not think that functional bugs will not pop up anymore \"because the application has been running fine for years\". A change on the system (update of the java runtime, kernel upgrade, update on a particular library) might be enough to trigger it. Non-functional bugs . The application starts dragging down? You notice inflated response times? Can the application only deal with 10 concurrent users, but your customer just hired 2 additional employees? Too bad. You might be able to work around this by duplicating the application and putting a load balancer in front of it, but with stateful applications that isn't always that easy to accomplish. Forget about service level agreements when the software is unsupported. You can't guarantee them anymore. Legal requirements . You might not know it, but many institutions are governed by specific IT requirements. Especially the financial sector (with the recent crisis and all) is and will get more and more regulatory compliance requests, and the IT infrastructure will not be spared. If you run unsupported software, you might be ignoring particular requirements that you have. Upgrade difficulties . Eventually you will need to upgrade. If the software you are upgrading from is unsupported, chances are very low that a good, flexible (and cost-efficient) upgrade trajectory exists. Migration scripts will probably not work and consultancy will fail. Anyone have experience with upgrading from Oracle 7.3 to Oracle 11g? Integration failures . Most applications are integrated in a larger architecture. Applications probably get authorization feeds or send out events to other components. As the external services that the application interacts with get updated, their interfaces update with them. And eventually you will get into a situation where the integration suddenly fails. I've seen an application use HTTP/1.0 whereas its external services suddenly only supported HTTP/1.1. Have fun explaining that to your customer (who might not even know that HTTP is a protocol). Customer support . If you use an internal help desk, then you might be able to educate them with troubleshooting the (unsupported) software. But if the help desk is external, you'll probably be facing a \"No\" after a while - or a nice additional fee. Some of these risks not only affect the product itself, but all other products / softwares that are installed on the server (or even on the network). If you ever face the request to continue supporting Foobar 2.0 on an unsupported Tomcat, now you have a checklist that you can tell your (requesting) customer about the risks he is introducing - and don't forget to tell the other customers about the risks they will be taking as well then. But I promised that I will be talking about risk mitigation... so just hold on for part 2 -- \"service isolation\".","tags":"Security","url":"https://blog.siphos.be/2011/09/mitigating-risks-part-1/","loc":"https://blog.siphos.be/2011/09/mitigating-risks-part-1/"},{"title":"Now using refpolicy 2.20110726","text":"A few days ago, I committed the SELinux policy modules that are based on the 2.20110726 set released upstream. For those that are using Gentoo Hardened with SELinux, you'll find them if you use the \\~arch set for the sec-policy category. When I talk about upstream, it usually is the reference policy as maintained by Tresys . This project, often abbreviated to refpolicy , tries to maintain a set of SELinux policies that are useful for the majority of Linux distributions. In fact, most (if not all) Linux distributions that support SELinux base their policies on the refpolicy. Now maintaining a reference policy for SELinux is not that easy, even with the contributions of many distributions and developers. Since the policy is used for many distributions (including RedHat Enterprise Linux ) it is vital that presented changes are only accepted if truly necessary (and do not present additional security risks). That means that patches should be well documented and easy to read. Patches that lack a proper motivation and that are not trivial are not accepted. When distributions want to push updates on the policy to the refpolicy, they need to send the patches to the refpolicy mailinglist . There they are picked up and analyzed and eventually added to the release. For Gentoo Hardened's SELinux project , getting (the majority of) our own patches in the reference policy is important, mainly because we currently lack the manpower to maintain a huge patch set ourselves. Every time a new release is made by the reference policy, we need to re-apply (and redevelop) our own patches. For a small set of patches, this isn't a lot of work, but the more changes you include, the more time-consuming this \"patch forwarding\" becomes. Of course, by quickly pushing out our patches we also get the confirmation (or rejection) of the patch, allowing us to be certain that we are on the right track. After all, it is a security policy that we are talking of. Now, the reference policy is just one of \"our upstreams\". A second important project - also governed by the Tresys organization - is what is called the SELinux Userspace . This project maintains the tools necessary to build the SELinux policy from readable text for humans to interpretable binary blobs for the Linux kernel. It maintains the tools that help us modify the policies' runtime behavior (using conditionals), manage file contexts and more. As this tool interacts intimately with the SELinux internals, development of these tools is discussed on the SELinux mailinglist offered by the NSA. It is the SELinux userspace project that provides tools like semanage , semodule , restorecon , chcon , etc. So next time you hear me talk about upstream, you know what it is.","tags":"Gentoo","url":"https://blog.siphos.be/2011/09/now-using-refpolicy-2-20110726/","loc":"https://blog.siphos.be/2011/09/now-using-refpolicy-2-20110726/"},{"title":"Use parted for large partitions","text":"A few bugs that were sitting in Gentoo's bugzilla for the documentation were related to large partitions (2 TB and higher). Previously, this wasn't as much as an issue since the number of users that have 2+ TB partitions are fairly slim. But of course time flies, hardware becomes cheaper and I have large partitions myself now too. So we had to update the docs. Since yesterday or so, the Gentoo Handbook now provides instructions on using parted for partitioning the disk (not for all architectures yet since I can't validate if it works on those as well). The use of the parted command, which is in the minimal LiveCDs for some time now, allows users to create GPT partition labels (instead of the old-style msdos ones). This partition system, which is supported by other operating systems as well, does not stop at 4 primary partitions, and supports partitions of a size I can currently only dream of (somewhere in the exabytes if I am not mistaken).","tags":"Gentoo","url":"https://blog.siphos.be/2011/08/use-parted-for-large-partitions/","loc":"https://blog.siphos.be/2011/08/use-parted-for-large-partitions/"},{"title":"Easy documentation updates thanks to the many contributions","text":"As mentioned previously, I took a stab at the Gentoo Guide to OpenLDAP Authentication , updating its configuration settings as well as give an introduction to its replication mechanism. Although I am no OpenLDAP guru at all, I set up a similar architecture for testing some SELinux policy changes. This test environment grew (okay, it's all KVM guests, so the only thing that grew was my resource consumption) and is currently entailing over 24 systems, ranging from BIND (in master/slave) to Apache/Nginx setups, reverse proxies to database clusters and what not. I'm hoping that I can manage the scripts I use to create those images (perform unattended installations of all these softwares as well as configuration aspects) and eventually provide some more documents for Gentoo on these matters. But until then, I'll focus more on fixing and helping the publication of documentation (a small list of changes contributed by various people are in the Gentoo Handbook which finally mentions ext4, has seen a whole slew of OpenRC fixes and updated kernel configuration information, or the Gentoo Guide to Mutt which has been rewritten from scratch). If you notice any errors or needs for corrections on Gentoo documentation, don't hesitate to file a bugreport or drop by on IRC's #gentoo-doc channel. Speaking of documentation, the SELinux Handbook has seen a few updates as well, and I have also started pushing some module-specific information (for instance on Portage ). This might help some users with their quest to get a particular software title to work on their system with the SELinux policies in place. Next to the documentation, you'll also find the SELinux policy modules based on the 2.20110726 version of the reference policy in the hardened-dev overlay. The base policy is currently in revision 2 with revision 3 on the way (asterisk, mutt and mozilla fixes). It now uses a cleaner patching process, something that is also part of the updated selinux-policy-2.eclass . I'm also hoping that I can introduce delivery of the SELinux policy interface documentation (a nicely formatted set of HTML pages showing which kind of interfaces - calls or privilege \"bundles\" if you like - are available), of course based on the availability of USE=\"doc\". Once this has been accomplished, I'll see that the new policy modules are migrated from the hardened-dev overlay to the main tree. Also, the majority of changes made to the policy are since revision 2 of the base policy in a more manageable format, allowing for faster pushing of the changes to the upstream reference policy.","tags":"Gentoo","url":"https://blog.siphos.be/2011/08/easy-documentation-updates-thanks-to-the-many-contributions/","loc":"https://blog.siphos.be/2011/08/easy-documentation-updates-thanks-to-the-many-contributions/"},{"title":"Ready, set, commit!","text":"Yesterday, I have entered the realms of Gentoo Development again. But as it was getting late then, I had to wait before the first commits happened. So this evening, things were done. The first couple of documentation bugs (mostly related to OpenRC) have been committed to the Gentoo CVS repository and I've also committed my first change on the gentoo-x86 CVS repository (a small change for the SELinux eclass, needed for the upcoming storm of packages. So what does that mean for Gentoo and Gentoo Hardened? Well, that means that I'll be taking on bugs myself now. You can ping me for documentation changes as well as SELinux policy changes. Within the next few hours, a little over 200 packages will be sent to the hardened-development overlay, containing the SELinux policy modules based on the upstream 2.20110726 release . These will linger there for a while, since I had some troubles getting them into the shape they are now - so some additional testing doesn't hurt. During the testing, most of the patches applied will also be submitted upstream for verification and inclusion. Simultaneously, a second set of patches will be prepared to squeeze out the remaining issues that are either left, or that have been reported since the push (I am expecting quite a few still, but luckily many users on #gentoo-hardened are helping out in testing SELinux). While we are on the SELinux policy development, I'll also be handling a few other documentation bugs. I'm hoping to take a stab at Gentoo's OpenLDAP HOWTO since I've been running a similar setup here for some time (including SELinux support of course ;-) Speaking about documentation, Anthony G. \"blueness\" Basile has pushed some documentation updates that I made in the hardened-docs repository to the main site. That means that users can now see how Gentoo Hardened supports MCS (and even talks about MLS for the brave ones out there). And since we now support MCS and have the latest userspace utilities in our repository, we can finally see if we can support SELinux sandboxing, a functionality that is already available in Fedora/RedHat but not fully supported through the upstream channels yet.","tags":"Gentoo","url":"https://blog.siphos.be/2011/08/ready-set-commit/","loc":"https://blog.siphos.be/2011/08/ready-set-commit/"},{"title":"checksec kernel security","text":"I have blogged about checksec.sh earlier before. Jono, one of the #gentoo-hardened IRC-members, kindly pointed me to its --kernel option. So I feel obliged to give its options a stab as well. So, here goes the next batch of OPE-style (One Paragraph Explanations). ~# checksec.sh --kernel * Kernel protection information: Description - List the status of kernel protection mechanisms. Rather than inspect kernel mechanisms that may aid in the prevention of exploitation of userspace processes, this option lists the status of kernel configuration options that harden the kernel itself against attack. Kernel config: /proc/config.gz GCC stack protector support: Enabled Strict user copy checks: Enabled Enforce read-only kernel data: Disabled Restrict /dev/mem access: Enabled Restrict /dev/kmem access: Enabled * grsecurity / PaX: Custom GRKERNSEC Non-executable kernel pages: Enabled Prevent userspace pointer deref: Disabled Prevent kobject refcount overflow: Enabled Bounds check heap object copies: Enabled Disable writing to kmem/mem/port: Enabled Disable privileged I/O: Enabled Harden module auto-loading: Enabled Hide kernel symbols: Enabled * Kernel Heap Hardening: No KERNHEAP The KERNHEAP hardening patchset is available here: https://www.subreption.com/kernheap/ In-kernel GCC stack protector support is the same as the Canary explanation I gave earlier, but now for the kernel code. Memory used by the stack (which contains both function variables as well as return addresses) is \"interleaved\" with specific data (canaries) which are checked before using a return address that is on the stack. If the canary doesn't match, you'll see a nice kernel panic. This is to prevent buffer overflows that might influence the in-kernel activity flow or overwrite data. When talking about Strict user copy checks , one can compare this with the FORTIFY_SOURCE explanation given earlier. Although not the same implementation-wise (since the latter is gcc/glibc bound, whereas the Linux kernel does not use glibc) this too enables the compiler to detect function calls with variable length data arguments to an extend that it can predict the (should-be) length of the argument. If this is the case, the function is switched with a(nother in-kernel) function that either continues the call, or break in case of a length mismatch. This is to prevent buffer overflows that might corrupt the stack (or other data locations). Enforce read-only kernel data marks specific kernel data sections as read-only to prevent accidental (or malicious) manipulations. When selecting Restrict /dev/mem access , the kernel does not allow applications (even those running as root) to access all of memory. Instead, they are only allowed to see device-mapped memory (and their own process memory). The same goes for Restrict /dev/kmem access , which is specifically for kernel memory. Non-executable kernel pages is similar to the NX explanation given earlier. It makes sure that pages marked as holding data can not contain executable code (and will as such never be \"jumped\" in) and pages marked as holding code will never be written to. To explain Prevent userspace pointer deref , first you need to understand the difference between a userland address and a kernel address . Each application holds its own, private virtual address space. Part of that virtual address space is \"reserved\" for most of the kernel data (in other words, the kernel data is available in each process' virtual address space), the rest is for the application. When interaction with the kernel occurs, a userland address is given to the kernel, which needs to translate it to a proper address (and treat it as data). With Prevent userspace pointer deref , specific checks are made to ensure that the kernel doesn't directly use userspace pointers, because that could be exploited by (malicious) software to trick the kernel into doing things it shouldn't. Reference counters in the Linux kernel are used to track users of specific objects or resources. A \"popular\" way to mistreat reference counters (or any counter per se) is to increment them that much until they overflow and wrap around, setting the counter to zero (or a negative number), leading to unexpected results (such as freeing memory that is in use). The Prevent kobject refcount overflow detects this for kobject resources and ensures that no wrap-around happens. The Bounds check heap object copies checks if particular memory copies use memory fragments within proper bounds. If the memory copy is for a fragment that crosses that bound (for instance because the fragment is too large) the copy fails. This offers some support against overflows, similar to (but not the same as) the use of the stack protector mentioned above. Disable writing to kmem/mem/port is similar to the Restrict /dev/(k)mem access settings, plus disable /dev/port from being opened. By selecting Disable privileged I/O , access to the kernel through functions like ioperm and iopl is prohibited. These functions are sometimes used by applications that need direct device access, like Xorg, but if you do not have such applications, it is wise to disable privileged I/O access. If not, any vulnerability in such an application might result in malicious code tampering with your devices. When Harden module auto-loading is set, processes that do not run as root will not be able to have particular kernel modules auto-loaded. Although this seems strange, it isn't. Suppose you have an application that wants to perform some IPv6 actions. Such applications can call request_module to ask the Linux kernel to load in a particular service. If the kernel supports IPv6 through a module, then it will load IPv6 support (you might have seen traces in your logs about net-pf-10 - well, that's the IPv6 support). You can disable auto-loading completely, but that might not be what you want. With this setting enabled, auto-loading is supported but only for root-running applications. The added security of Hide kernel symbols is not to prevent activities, but to prevent information to be leaked and (ab)used by malicious users. Kernel symbols are string representations of functions or variables that the kernel offers to kernel users (such as kernel modules and drivers). This is needed because the location of these functions/variables in memory cannot be provided in advance (this is no different from symbols used as explained in the RELRO security setting in my previous posting). By hiding these symbols from any user without sufficiently high privileges (and limit the exposure for high privileged process to well-known locations so these too can be protected by other means) it is far more difficult for malicious users to find out about available functions/variables on your system. Finally, Kernel Heap Hardening enhances the in-kernel dynamic memory allocator with additional hardening features (double-free protection, use-after-free protection, ...). It tries to ensure proper use of the allocated memory segments and protect against improper access. From reading all this, you probably imagine why this isn't all enabled by default. Well, many of the settings have implications on how the system behaves. Some restrict functionalities to the root user only (making it sometimes less user-friendly), some disable functionalities that are needed (like the I/O access) or are (ab)used (like the user space pointer deref which is used by many virtualization solutions) while others add some additional overhead (the more you check, the longer an action takes before it completes). To help users select the proper settings, Gentoo Hardened tries to differentiate settings based on workstation and virtualization usage. So you get most security settings for \"No Workstation, No Virtualization\" and less for each of those you enable. But of course, like always, Gentoo supports custom settings too so you don't have to follow the differentiation we suggest ;-) Find something incorrect in the above paragraphs? Or too much sales-speak and too little explanation? Give me a shout (and prove me wrong) ;-)","tags":"Security","url":"https://blog.siphos.be/2011/07/checksec-kernel-security/","loc":"https://blog.siphos.be/2011/07/checksec-kernel-security/"},{"title":"emerge-webrsync and gpg verification","text":"Gentoo has been working on its security from very early on. One of the (many) features it supports is to allow users to validate the state of the portage tree. Ebuild signing (where developers sign the Manifest file with their key) is one of the layers offered by Gentoo, but another one is full tree signing. When you use emerge-webrsync instead of emerge --sync , an archive containing a consistent state of the portage tree is downloaded and unpacked on your system. If you however set FEATURES=\"webrsync-gpg\" then this tool will check the GPG signature attached to the file with the public key used by Gentoo's infrastructure (0x239C75C4). If the archive does not contain a valid signature, then it is not used on the system. If you want to use this, here are the steps to do so. First, set up the location where you keep the key: ~# mkdir -p /etc/portage/gpg ~# gpg --homedir /etc/portage/gpg --keyserver subkeys.pgp.net --recv-keys 0x239C75C4 ~# gpg --homedir /etc/portage/gpg --edit-key 0x239C75C4 trust Next, edit /etc/make.conf and set the following parameters: FEATURES=\"webrsync-gpg\" PORTAGE_GPG_DIR=\"/etc/portage/gpg\" # Disable 'emerge --sync' so emerge-webrsync has to be used SYNC=\"\" With that done, you're all set. Just run emerge-webrsync . Happy Gentooing!","tags":"Gentoo","url":"https://blog.siphos.be/2011/07/emerge-webrsync-and-gpg-verification/","loc":"https://blog.siphos.be/2011/07/emerge-webrsync-and-gpg-verification/"},{"title":"Preliminary SELinux MCS support in Gentoo Hardened","text":"Users tracking the hardened-dev overlay for SELinux packages will notice yet another update on the selinux-base-policy package. This time however, the change is a little more than just a policy update. With this new revision, preliminary support for Multi-Category Security (aka MCS) is added. MCS is an update on the SELinux policy where domains and resources can be given a \"category\". This is especially useful on what is called multi-tenant systems, where multiple processes (but of the same application and hence the same domain definition) are running, servicing requests of different clients (or even customers). With MCS, these different processes, although using the same domain definitions, can still be isolated. The use of categories is well accepted for virtualization hosts (where virtual guests should be run isolated from each other) and web servers, but other uses can be found easily as well. Next to MCS, the update also supports MLS or Multi-Level Security . Like MCS, this supports multiple categories, but it also supports multiple sensitivity levels. On an MLS system, the security administrator can control how information of a certain sensitivity label \"flows\" through the system. Now, the MLS support within Gentoo Hardened is still very experimental so I don't recommend it yet, unless you are willing to help us get it in a workable shape. In order to use MCS, you need to use the POLICY_TYPES variable in /etc/make.conf (which allows Portage to build the policy type(s) you want) and the SELINUXTYPE variable in /etc/selinux/config. Whereas this previously was limited to \"strict\" or \"targeted\", they now support \"mls\" and \"mcs\" as well. Of course, this is documented in the Gentoo Hardened SELinux handbook (currently in the hardened-doc overlay). Now, this is still preliminary support for MCS. A small fix needs to happen on our eclass and it definitely needs lots of testing before it can be considered for production use. Also, the majority of development attention will continue in the \"strict\" policy type although MCS testing and support will grow.","tags":"Gentoo","url":"https://blog.siphos.be/2011/07/preliminary-selinux-mcs-support-in-gentoo-hardened/","loc":"https://blog.siphos.be/2011/07/preliminary-selinux-mcs-support-in-gentoo-hardened/"},{"title":"High level explanation on some binary executable security","text":"One very important functionality offered by Gentoo Hardened is a specific toolchain (compiler, libraries and more) that contains patches to make the built binaries a bit more protected from certain vulnerabilities. Explaining all those in detail is too much for a simple blog post like this, but some time ago the friendly folks of the Gentoo Hardened project told be about a script called checksec.sh that displays a few of those protections on a binary. So what can I find out of such a run? Let me show you the output on two binaries here: ~$ checksec.sh --file /opt/skype/skype RELRO STACK CANARY NX PIE FILE No RELRO Canary found NX disabled No PIE /opt/skype/skype ~$ checksec.sh --file /bin/bash RELRO STACK CANARY NX PIE FILE Full RELRO Canary found NX enabled PIE enabled /bin/bash It even comes with pretty colors (the \"No RELRO\" is red whereas \"Full RELRO\" is green). But beyond interpreting those colors (which should be obvious for the non-colorblind), what does that all mean? Well, let me try to explain them in one-paragraph entries (yes, I like such challenges ;-) Note that, if a protection is not found, then it probably means that the application was not built with this protection (the skype example, since this is a binary from Skype\\&#94;WMicrosoft, versus bash which is built by the Gentoo Hardened toolchain). RELRO stands for Relocation Read-Only , meaning that the headers in your binary, which need to be writable during startup of the application (to allow the dynamic linker to load and link stuff like shared libraries) are marked as read-only when the linker is done doing its magic (but before the application itself is launched). The difference between Partial RELRO and Full RELRO is that the Global Offset Table (and Procedure Linkage Table) which act as kind-of process-specific lookup tables for symbols (names that need to point to locations elsewhere in the application or even in loaded shared libraries) are marked read-only too in the Full RELRO . Downside of this is that lazy binding (only resolving those symbols the first time you hit them, making applications start a bit faster) is not possible anymore. A Canary is a certain value put on the stack (memory where function local variables are also stored) and validated before that function is left again. Leaving a function means that the \"previous\" address (i.e. the location in the application right before the function was called) is retrieved from this stack and jumped to (well, the part right after that address - we do not want an endless loop do we?). If the canary value is not correct, then the stack might have been overwritten / corrupted (for instance by writing more stuff in the local variable than allowed - called buffer overflow ) so the application is immediately stopped. The abbreviation NX stands for non-execute or non-executable segment. It means that the application, when loaded in memory, does not allow any of its segments to be both writable and executable. The idea here is that writable memory should never be executed (as it can be manipulated) and vice versa. Having NX enabled would be good. The last abbreviation is PIE , meaning Position Independent Executable . A No PIE application tells the loader which virtual address it should use (and keeps its memory layout quite static). Hence, attacks against this application know up-front how the virtual memory for this application is (partially) organized. Combined with in-kernel ASLR ( Address Space Layout Randomization , which Gentoo's hardened-sources of course support) PIE applications have a more diverge memory organization, making attacks that rely on the memory structure more difficult. But hold on, the checksec.sh application also supports detection for FORTIFY_SOURCE : ~$ checksec.sh --fortify-file /opt/skype/skype * FORTIFY_SOURCE support available (libc) : Yes * Binary compiled with FORTIFY_SOURCE support: Yes ------ EXECUTABLE-FILE ------- . -------- LIBC -------- FORTIFY-able library functions | Checked function names ------------------------------------------------------- printf | __printf_chk ... SUMMARY: * Number of checked functions in libc : 75 * Total number of library functions in the executable: 2468 * Number of FORTIFY-able functions in the executable : 25 * Number of checked functions in the executable : 0 * Number of unchecked functions in the executable : 25 In the given example, my system does support FORTIFY_SOURCE and the binary is supposedly built with this support as well, but the checks return that out of the 25 functions identified as FORTIFY -able, none of them were successfully verified as using a FORTIFY -ed library call. It goes without saying that for the /bin/bash binary, this yielded a bit more good results (12 out of 30 verified). Again, what is FORTIFY_SOURCE ? Well, when using FORTIFY_SOURCE , the compiler will try to intelligently read the code it is compiling / building. When it sees a C-library function call against a variable whose size it can deduce (like a fixed-size array - it is more intelligent than this btw) it will replace the call with a FORTIFY 'ed function call, passing on the maximum size for the variable. If this special function call notices that the variable is being overwritten beyond its boundaries, it forces the application to quit immediately. Note that not all function calls that can be fortified are fortified as that depends on the intelligence of the compiler (and if it is realistic to get the maximum size). If you do not agree with the explanation above, please comment... and try to explain it in a single paragraph without going too detailed (i.e. do not assume people are capable of writing their own compiler just for fun).","tags":"Security","url":"https://blog.siphos.be/2011/07/high-level-explanation-on-some-binary-executable-security/","loc":"https://blog.siphos.be/2011/07/high-level-explanation-on-some-binary-executable-security/"},{"title":"Some people on #selinux are ... dolphins","text":"A very useful resource for anyone working on or with SELinux policies is the #selinux chat channel on irc.freenode.net. People like Dominick Grift and Dan Walsh you would first think are IRC bots (being online all the time, answering questions), but I recently read that they must be ... dolphins. Yes, dolphins. Dolphins are known to not really sleep fully - only one part of their brain goes to a low-wave sleep so they're sufficiently conscious to keep track of what is happening around them. No seriously, thanks guys!","tags":"SELinux","url":"https://blog.siphos.be/2011/07/some-people-on-selinux-are-dolphins/","loc":"https://blog.siphos.be/2011/07/some-people-on-selinux-are-dolphins/"},{"title":"On the new SELinux profiles","text":"Ever since Anthony put in the new SELinux profiles - which was long due - they have seen quite a few tests and the necessary, evolutionary updates. No changes that broke things, no oddities that would give a WTF to whomever is using it. The latest updates were to remove some obsolete masks so that our visibility in the Gentoo QA Reports is down again. However, we are well aware that these profiles are still in the dev(elopment) phase but would like to stabilize these soon. For this to happen, we need SELinux users to give the new profiles a go. Become sysadm_r & root and switch your profile to whichever SELinux profile suits you the most (with the new profiles, we support SELinux on multilib and no-multilib and across various settings). All my local servers run with \"hardened/linux/amd64/no-multilib/selinux\" whereas my main workstation uses \"hardened/linux/amd64/selinux\" (since I still have some need for the multilib setup). We did some tests on non-hardened profiles too as well as on the x86 architecture with no problems whatsoever. So although we can't guarantee anything, I'm pretty convinced that the profiles will work for you too! So by all means, see if you can switch from the v2refpolicy/ profiles and give us your feedback. You're always welcome for a chat on #gentoo-hardened (irc.freenode.net) or on our mailinglists.","tags":"Gentoo","url":"https://blog.siphos.be/2011/07/on-the-new-selinux-profiles/","loc":"https://blog.siphos.be/2011/07/on-the-new-selinux-profiles/"},{"title":"Gentoo Hardened SELinux state","text":"Since last post, we've been working on the further stabilization and bug fixing of the SELinux policies within Gentoo Hardened. You might have noticed that we started working on the QA of the packages, like I promised in the last post. The binaries within selinux-base-policy are now published somewhere on blueness' developer page since he's proxy'ing all my commits until recruiters get the chance to pick up my recruitment bug . Other patches that are coming up will be published likewise as well if they get too big to be within the main Portage tree. Next to the binaries, I'm currently checking if the SELinux policy packages can become EAPI-4 compliant (they're currently still using EAPI-0). Same for the SELinux-specific packages, like policycoreutils, libsemanage, libselinux etc. During the last few days, I've tried to take a few stabs at supporting Python 2 and Python 3 simultaneously. It seems to work for policycoreutils and libsemanage (necessary fixes are in the gentoo-hardened overlay ) but any attempt to fix libselinux seems to give me hard walls. So for now, we're still stuck with Python 2 support when using Portage (note that you can still use Python 3 for all other things, but Portage requires Python 2 as it calls libselinux). This is currently still accomplished through a proper use.mask and use.force setting against Portage. Of course, the policies themselves are not silent either. I've updated the selinux-base-policy package so that Portage can now support NFS-mounted Portage trees and made quite a few openrc-related fixes as well (against the policy, not against openrc ;-) I promised to take a stab at MCS in the near future, and that's still the plan. Hopefully in the coming few weeks ;-)","tags":"Gentoo","url":"https://blog.siphos.be/2011/07/gentoo-hardened-selinux-state/","loc":"https://blog.siphos.be/2011/07/gentoo-hardened-selinux-state/"},{"title":"What's next after stabilization?","text":"The last few weeks have shown quite a few interesting improvements on Gentoo Hardened's SELinux state. We now have improved (simplified) Gentoo profile support, supporting SELinux on no-multilib (an often requested feature, now finally in), we stabilized the 2.20101213 policies that are in the tree and are cleaning up the old ones. The documentation is continuously updated ( handbook and FAQ ) and we are getting a nice stream of users helping out and reporting stuff on SELinux. So, besides the further stabilization and bug fixing, what else is on the horizon? Well, our first concern now will be to make the ebuilds more... correct. Some of them still violate a few QA rules and this needs to be fixed. If possible, we'll also start converting our ebuilds to a more recent EAPI. Then, we will take a first stab at MCS within Gentoo Hardened. Our primary concern here is support for the virtualization technologies which, if SELinux-aware, are often using MCS to shield off running guests from each other. So interesting times are ahead. And of course, while we're at it, we'll continue improving policies and submitting our own patches upstream.","tags":"Gentoo","url":"https://blog.siphos.be/2011/06/whats-next-after-stabilization/","loc":"https://blog.siphos.be/2011/06/whats-next-after-stabilization/"},{"title":"Policy 25, 26","text":"Recently I've seen quite a few messages on IRC pop up about policy.25 or even policy.26 so I harassed the guys in the chat channel to talk about it. Apparently, these new binary policy formats add support for filename transitions and non-process role transitions. Currently, when you initiate a type transition, you would use something like so: type_transition mysqld_t mysql_db_t:sock_file mysqld_var_run_t; This statement sais that, if a process running in the mysqld_t domain creates a socket in a directory labelled with mysql_db_t , then this socket gets the mysqld_var_run_t label. In other words, the type transitions from mysql_db_t (parent label) to mysqld_var_run_t . What will be supported from version 25 onwards is that you can add another argument, the file name (well, actually it is called \"last name component\" and should be seen as what basename /path/to/something returns). That allows processes running in the same domain and writing files in directories labelled with the same type to still have these files labelled specifically. A non-existing example: type_transition puppet_t etc_t:file locale_t timezone; type_transition puppet_t etc_t:file net_conf_t resolv.conf; In the above example, if the puppet_t domain creates files in /etc (which is labelled etc_t ) then based on the file it is creating, this file gets a different label ( /etc/timezone gets labelled locale_t whereas /etc/resolv.conf gets labelled net_conf_ ). The second change (valid since policy version 26) is that role transitions now also support non-process class transitions. A lengthy post that Harry Ciao made helps to describe it. The role_transition support in SELinux was previously used in the following way: role_transition roleA_r some_exec_t roleB_r; What this statement indicates is that a domain running within roleA_r and that is executing some_exec_t will change its runtime role to roleB_r . If by calling some_exec_t a domain transition occurs as well (which is most common when a role transition is supported as well) then this domain will run with the roleB_r runtime role. The added functionality is now that this isn't limited to processes anymore. You can now define non-process classes as well. If the source domain creates something new of a particular class and a role transition is declared for that, then the resulting new object will have the specified role assigned to it (rather than the default object_r ). So for instance: role_transition sysadm_r cron_spool_t:file sysadm_r; If a domain running within the sysadm_r role creates a file in a directory labelled cron_spool_t , then the resulting file will have the role sysadm_r rather than object_r . This opens up more support for role-based access controls (similar to the UBAC functionality that I described earlier, but in some cases more flexible). I'm pretty sure that the crontab management for vixie-cron will be one of the first ones that can benefit greatly from this ;-)","tags":"SELinux","url":"https://blog.siphos.be/2011/06/policy-25-26/","loc":"https://blog.siphos.be/2011/06/policy-25-26/"},{"title":"SELinux file contexts","text":"If you have been working with SELinux for a while, you know that file contexts are an important part of the policy and its enforcement. File contexts are used to inform the SELinux tools which type a file, directory, socket, ... should have. These types are then used to manage the policy itself, which is based on inter-type permissions. When dealing with file contexts, you either use chcon (mostly) if you are trying out stuff as a chcon -set security context doesn't stick after a file system relabel operation (customizable types notwithstanding, and even then) restorecon if you want to reset the file context of a file or set of files semanage through the semanage fcontext -a -t your_type \"regular_expression\" method, which enhances the SELinux known file contexts with the appropriate information so that relabel operations are survived policy improvements by editing and enhancing the *.fc files that take part in the policy definition When you look at the policy, or the output of semanage fcontext -l , you'll notice that the policy uses regular expressions very often. Of course, without regular expression support, the file context rules themselves would be impossible to manage. However, it immediately brings up the question about what SELinux does when two or more lines are appropriate for a particular file. Let's look at a few lines for configuration related locations... /etc/.* all files system_u:object_r:etc_t /etc/HOSTNAME regular file system_u:object_r:etc_runtime_t /etc/X11/[wx]dm/Xreset.* regular file system_u:object_r:xsession_exec_t /etc/X11/wdm(/.*)? all files system_u:object_r:xdm_rw_etc_t In the above examples, you'll notice that there is quite some overlap. To start, the first line already matches all other lines as well. So how does SELinux handle this? Well, SELinux uses the following logic to find the most specific match, and uses the most specific match then (extract taken from a pending update to the Gentoo Hardened SELinux FAQ ): If line A has a regular expression, and line B doesn't, then line B is more specific. If the number of characters before the first regular expression in line A is less than the number of characters before the first regular expression in line B, then line B is more specific If the number of characters in line A is less than in line B, then line B is more specific If line A does not map to a specific SELinux type, and line B does, then line B is more specific So in case of /etc/HOSTNAME , the second line is most specific because it does not contain a regular expression. In case of /etc/X11/wdm/Xreset.sh , SELinux will use the xdm_rw_etc_t type and not the xsession_exec_t one. This is because the first regular expression in the xsession_exec_t line ( [wx] ) comes sooner than the first regular expression in the xdm_rw_etc_t line ( (/.*)? ). You can validate this - even if you do not have such file - with matchpathcon : ~# matchpathcon /etc/X11/wdm/Xreset.sh /etc/X11/wdm/Xreset.sh system_u:object_r:xdm_rw_etc_t If you want to know which line in the semanage fcontext -l output is used, you can use findcon to show which lines match. That together with the output of matchpathcon can help you deduce which line is causing the label to be set: ~# matchpathcon /etc/X11/wdm/Xreset.sh /etc/X11/wdm/Xreset.sh system_u:object_r:xdm_rw_etc_t ~# findcon /etc/selinux/strict/contexts/files/file_contexts -p /etc/X11/wdm/Xreset.sh /.* system_u:object_r:default_t /etc/.* system_u:object_r:etc_t /etc/X11/[wx]dm/Xreset.* -- system_u:object_r:xsession_exec_t /etc/X11/wdm(/.*)? system_u:object_r:xdm_rw_etc_t In many cases, the last output line of findcon is the line you are looking for, but I have not find a source that confirms this behavior so do not trust this.","tags":"SELinux","url":"https://blog.siphos.be/2011/05/selinux-file-contexts/","loc":"https://blog.siphos.be/2011/05/selinux-file-contexts/"},{"title":"SELinux Gentoo profile updates","text":"The SELinux support within Gentoo Hardened is continuing to go forward. Anthony G. Basile has been working on the new SELinux Gentoo profiles which were in dire need of updates. With the rework, we'll also support the AMD64 no-multilib environment properly. With the new profiles we'll also make USE=\"open_perms\" enabled by default. This will enable the \"open\" permission within the SELinux policies. And of course we'll remove that FEATURES=\"loadpolicy\" . Not in git yet, but close, are further updates on the policy ebuilds. Revision 14 of our selinux-base-policy package will fail when the base policy couldn't be loaded properly. This will ensure that a successful installation means that the policies are loaded successfully as well. If we wouldn't do this, then users might assume that the policies are constantly being updated while in reality their system has always been working with older policies. I am also going to, based on the QA reports , update the sec-policy/* packages so they are not mentioned in those reports anymore. In other related news, the Gentoo Hardened SELinux FAQ will get updates on UBAC, cron issues and a few errors (cosmetic or not) that you might have when working with Gentoo Hardened SELinux. I'm also constantly updating the Gentoo Hardened SELinux Handbook (PDF) with the latest information and developments.","tags":"Gentoo","url":"https://blog.siphos.be/2011/05/selinux-gentoo-profile-updates/","loc":"https://blog.siphos.be/2011/05/selinux-gentoo-profile-updates/"},{"title":"SELinux User-Based Access Control","text":"Within the reference policy, support is given to a feature called UBAC constraints . Here, UBAC stands for User Based Access Control . The idea behind the constraint is that any activity between two types (say foo_t and bar_t ) can be prohibited if the user contexts of the resources that are using those types are different. So even though foo_t can read files with label bar_t , a process running as user1:user_r:foo_t will not be able to read a file labeled user2:user_r:bar_t . The policy defines the constraint like so (taken from policy/constraints and rewritten in a more readable code): Action is okay if user1 == user2, or user1 == system_u, or user2 == system_u, or type1 is not a UBAC constrained type, or type2 is not a UBAC constrained type So the constraint only denies an activity if the users involved are not system_u (that would render your system useless), not the same, and both types are ubac constrained types . The latter is, within the policy, set using type attributes: ~$ seinfo -aubac_constrained_type -x ubac_constrained_type screen_var_run_t admin_crontab_t links_input_xevent_t ... Some domains are also UBAC exempt (currently I know of the sysadm_t domain - cfr the ubacproc and ubacfile attributes), meaning that activities started from the sysadm_t domain will not trigger the constraint. UBAC gives some additional control on information flow between resources. But it isn't perfect. One major downside is that the error you get when the constraint is hit is a simple AVC denial where most users would just check the inter-type privileges, without paying attention to the difference in SELinux user identities. Another is that it might be difficult for users or administrators that use different SELinux user identities to still work properly with UBAC constrained domains. Work is on the way in the SELinux development to improve the role-based access control (RBAC) by allowing files and directories to have a role as well (rather than the object_r placeholder used currently) and then work on those roles. You can then grant the users that need access to a particular resource the necessary role rather than requiring those users to use the same SELinux user id. This would take at least one major downside of UBAC away and I'm hoping that the logging will improve on this as well. Of course, I do not ramble about UBAC here because it is fun (well yes, yes it is fun) but because in Gentoo, we've hit one UBAC-related issue. When a user starts vixie-cron, the root crontab would fail to be loaded. What gives? The root crontab has the SELinux identity of staff_u (as it is created by a regular staff user that su(do)'ed) whereas the cronjob_t process would have the SELinux identity of root . Bang. Dead. No error beyond what vixie-cron gives. Of course this can be easily worked around. chcon -u root /var/spool/cron/crontabs/root works, or you can recreate the crontab as a console-logged-on root user. We could also change the default context used by cronjob_t to use staff_u:sysadm_r:cronjob_t for root. But we can also take a look at how other distributions do this. What gives: most distributions disable UBAC within the policy. Their reasons might vary, but manageability of the policy comes to mind, as well as reducing the number of (difficult to debug) problems. Most are keen to include the RBAC at some point in the future though. Some discussion on #gentoo-hardened and #selinux later, and I decided to use a USE flag called \"ubac\" to optionally enable UBAC within the policy. How very Gentoo, isn't it? At least users have the choice of using UBAC or not (I know I'm going to enable it) and when RBAC is available, we'll definitely make sure that support for RBAC is available too. Currently in the hardened overlay, sec-policy/selinux-base-policy-2.20101213-r13 . Take your pick on it, give it a try and report any bugs you have on Bugzilla . And if you enable USE=\"ubac\", you get user based access control for free. PS I'm also going to reapply for Gentoo developer-ship and, amongst other things, help out the hardened team with SELinux policies and documentation.","tags":"SELinux","url":"https://blog.siphos.be/2011/05/selinux-user-based-access-control/","loc":"https://blog.siphos.be/2011/05/selinux-user-based-access-control/"},{"title":"SELinux and noatsecure, or why portage complains about LD_PRELOAD and libsandbox.so","text":"If you're fiddling with SELinux policies, you will eventually notice that the reference policy by default hides certain privilege requests (which are denied). One of them is noatsecure. But what is noatsecure? To describe noatsecure, I first need to describe what atsecure is. And to describe what that is, we first need to give a small talk about ELF auxiliary vectors. As you probably know, when an application instantiates another application, it calls the execve function right after fork'ing to load the new application in memory. The actual task to load the new application in memory is done by the C library, more specifically the binary loader. For Linux, this is the ELF loader. Now, ELF auxiliary vectors are parameters or flags that are set (or at least managed) by the ELF loader to allow the application and program interpreter to get some OS-specific information. Examples of such vectors are AT_UID and AT_EUID (real uid and effective uid) and AT_PAGESZ (system page size). One of the vectors that glibc supports is AT_SECURE . This particular parameter (which is either \"0\" (default) or \"1\") tells the ELF dynamic linker to unset various environment variables that are considered potentially harmful for your system. One of these is LD_PRELOAD (I mention this one specifically because it was the source of my small investigation). Normally, this environment sanitation is done when a setuid/setgid application is called (to prevent the obvious vulnerabilities). However, SELinux enhances the use of this sanitation... Whenever an application is called which triggers a domain transition in SELinux (say sysadm_t to mozilla_t through a binary labelled mozilla_exec_t ), SELinux sets the AT_SECURE flag for the loaded application (in the example, mozilla/firefox). In other words, every time a domain transition occurs, the environment for this application is sanitized. As you can imagine now the noatsecure permission disables the environment sanitation activity for a particular transition. You can do this through the following allow statement (applied to the above example): allow sysadm_t mozilla_t:process { noatsecure }; if every domain transition for which this permission isn't allowed would log its denial, our audit logs would be filled with noise. That is why the reference policy by default hides ( dontaudit ) these calls. But knowing what they are for is important, because you might sometimes come into contact with it, like I did: >>> Installing (1 of 1) net-dns/host-991529 >>> Setting SELinux security labels ERROR: ld.so: object 'libsandbox.so' from LD_PRELOAD cannot be preloaded: ignored. This error message is when Portage (running in portage_t ) wants to relabel the files that it just placed on the system through setfiles (which will run in setfiles_t ). As this involves a domain transition, AT_SECURE is set for setfiles, but LD_PRELOAD was set as part of Portage' sandboxing feature. This environment variable is disabled, and the loader warns the user that it cannot preload libsandbox.so . Although we can just set noatsecure here, it would open up a (small) window for exploits (although they would need to be provided through Portage, because when a user calls Portage, a domain transition is done there as well so the user-provided environment variables are already sanitized by then). By not allowing noatsecure , we are disabling a few functionalities provided by the libsandbox.so library for the file labeling activity (this is very important to understand: it does not disable the sandboxing for the builds and merges, only for the file relabeling). As we already run setfiles in its own, confined domain, I believe that we are best served by keeping the secure environment sanitation here. That does mean that the warning will stay as we cannot control that from within SELinux. If you want to allow noatsecure here, create a simple module and load it: ~# cat > portage_noatsecure.te << EOF module portage_noatsecure 1.0; require { type portage_t; type setfiles_t; class process { noatsecure }; } allow portage_t setfiles_t:process { noatsecure }; EOF ~# checkmodule -m -o portage_noatsecure.mod portage_noatsecure.te ~# semodule_package -o portage_noatsecure.pp -m portage_noatsecure.mod ~# semodule -i portage_noatsecure.pp","tags":"SELinux","url":"https://blog.siphos.be/2011/04/selinux-and-noatsecure-or-why-portage-complains-about-ld_preload-and-libsandbox-so/","loc":"https://blog.siphos.be/2011/04/selinux-and-noatsecure-or-why-portage-complains-about-ld_preload-and-libsandbox-so/"},{"title":"cvechecker 3.0","text":"I'm pleased to announce the immediate availability of cvechecker 3.0 . It contains two major feature enhancements: watchlists and MySQL support. watchlists allow cvechecker to track and report on CVEs for software that cvechecker didn't detect on the system (or perhaps even isn't installed on the system). You can use watchlists to stay informed of potential vulnerabilities in software used at work on servers where you are not allowed (or do not want) to run cvechecker on. To use watchlists, create a text file containing the CPE identifiers for the software that you want to watch out for, and add it to the database: ~$ cat watchlist.txt cpe:/a:microsoft:excel:2003::: ~$ cvechecker -d -w watchlist.txt Adding CPE entries - Added watch for cpe:/a:microsoft:excel:2003::: The second major feature is support for MySQL. This is the first server-oriented RDBMS that cvechecker supports (earlier versions worked with sqlite only) although sqlite support remains available as well. I hope to extend the number of supported databases in the future (say PostgreSQL, Oracle, SQL Server, ...). With support for server RDBMSes came of course the requirement that multiple cvechecker clients are able to use the same server (as the CVE and CPE data itself can be shared). With the 3.0 release, this is supported as each client now \"adds\" to the data both his hostname as well as an (optional) user defined value (which can be anything you like). If unset, this user value is set to the hostname, but you can use things like the systems' serial ID or asset ID. I'm hoping all users have fun with it - I know I have while writing it. Feedback, remarks, feature requests, bugs and other criticism is always very much appreciated.","tags":"Security","url":"https://blog.siphos.be/2011/04/cvechecker-3-0/","loc":"https://blog.siphos.be/2011/04/cvechecker-3-0/"},{"title":"cvechecker updates","text":"The in-svn version of cvechecker has seen quite a few changes in the last few days. I'm adding support for MySQL to it. This support will be added in three steps: support the same features as cvechecker currently does using sqlite streamline the database code so that duplicate code in the sqlite implementation and mysql implementation is removed support multi-node systems with a single master database The latter is something I've been meaning to implement for quite some time: have a single system dedicated to download and store the latest CVE entries in the database (as well as CPE definitions) whereas several systems can use the database by storing their own system information and getting a mapping from that information against the CVE database. Even more so, it would allow you to query the database asking on which systems a particular software was detected, or which systems still have vulnerable software installed. When the MySQL support is implemented, I'm going to work a bit on the versions.dat file, because it doesn't really support many services currently. I'm going to use it against my \"virtual network\" (a combination of KVM guests running bind (master/slave), ldap (multi-master), postfix, apache, squirrelmail, courier, postgresql, mysql and more) and enhance it so that it detects all those components as well. Oh, btw, I had a request to include support for just telling cvechecker which components/software to look for (rather than it scanning the files and deducing it from regular expressions and the like). The in-svn version supports it, so it will definitely be part of the 3.0 release.","tags":"Security","url":"https://blog.siphos.be/2011/03/cvechecker-updates/","loc":"https://blog.siphos.be/2011/03/cvechecker-updates/"},{"title":"Restoring configuration files on Gentoo","text":"If you work with Gentoo, you're probably aware of tools like etc-update and dispatch-conf . If you use dispatch-conf , you might know that it supports rcs for version control of the changes it makes. But if you have enabled it, you might be wondering how to actually restore configuration files with it. Well, dispatch-conf stores its version control information in /etc/config-archive . To restore a configuration file to a previous version, first find out what versions there are in the version control system: ~$ rlog -zLT /etc/config-archive/etc/protocols,v ... The output of the rlog command should allow you to find the revision you are interested in. The -zLT option displays date/time in the current timezone (instead of UTC). Once you have found the revision you are looking for, restore the file by redirecting the output of the co command: ~$ co -p -r1.1.1 /etc/config-archive/etc/protocols,v > /etc/protocols","tags":"Gentoo","url":"https://blog.siphos.be/2011/03/restoring-configuration-files-on-gentoo/","loc":"https://blog.siphos.be/2011/03/restoring-configuration-files-on-gentoo/"},{"title":"Updates on SELinux docs, added FAQ","text":"As you're probably noticing from my twitter feed and the various posts earlier in my blog, I'm helping out with the Gentoo Hardened folks to get the SELinux support state up to par. Today, the Gentoo Hardened/SELinux Handbook had a few updates, but the most important change is that there is now a Gentoo Hardened SELinux FAQ (in draft). I'm hoping that, at the next IRC meeting, we can vote on having it pushed to the main site. Also, the latest changes I made to various SELinux policy ebuilds have been pushed to the main tree. I'm now focusing on running various servers in KVM guests to test the SELinux policies. Following the Gentoo Virtual Mailhosting HOWTO creates a working system, although a few SELinux-specific steps had to be added (if you follow the guide exactly to the letter, you won't finish it). The issues are minor though: selinux-sasl needs to be installed manually (it isn't pulled in as a dependency), the PIDFILE and SSLPIDFILE variables in /etc/courier-imap/* need to point to /var/run/courier (and that location needs to be created) to match the file context definitions as suggested by upstream, you need to run postalias /etc/mail/aliases instead of newaliases and during the installation of Apache, you might need to chcon -t bin_t /usr/share/build-1/mkdir.sh as you'll get a permission denied otherwise.","tags":"SELinux","url":"https://blog.siphos.be/2011/03/updates-on-selinux-docs-added-faq/","loc":"https://blog.siphos.be/2011/03/updates-on-selinux-docs-added-faq/"},{"title":"Portage fails to build due to SELinux?","text":"If you're having troubles getting Portage to build packages due to SELinux, then the reason usually is that it is unable to transition to the proper portage domains. You'll get a nice OSError back with an ugly backtrace, saying somewhere that \"setexeccon\" is misbehaving. Now, the real issue (not being able to transition) means that the current domain you are in (check id -Z ) has no right to transition to the portage_fetch_t , portage_t or portage_sandbox_t domains. You can verify that with seinfo : ~# id -Z unconfined_u:unconfined_r:unconfined_t ~# seinfo -runconfined_r -x | grep portage The above example shows it for the unconfined_t domain, but the same is true if your current domain is a more illogical local_login_t (hint: check your PAM settings) or initrc_t . Now, if you want to fix these things, we will eventually ask you to reemerge some things - which was the first reason why you came asking how to fix things. There are two ways to handle this situation: the proper way (disabling SELinux and reenabling later) or the ugly way (hack Portage to ignore). In the first way, you need to edit /etc/selinux/config , set SELINUX=disabled , reboot, emerge whatever you need, edit the file again restoring SELINUX to what you had before, reboot, relabel your entire filesystem ( rlpkg -a -r ) and perhaps even reboot again. In the second method, edit /usr/lib(64)/portage/pym/portage/_selinux.py and go to line 79. It reads: if selinux.setexeccon(ctx) < 0: Comment out that line (so it isn't lost) and substitute it with if selinux.setexeccon(\"\\n\") < 0: Now you should be able to install software without hitting the error. But note that this is only to help you fix the real problem as we're circumventing SELinux integration in Portage a bit.","tags":"SELinux","url":"https://blog.siphos.be/2011/03/portage-fails-to-build-due-to-selinux/","loc":"https://blog.siphos.be/2011/03/portage-fails-to-build-due-to-selinux/"},{"title":"Updates on the Gentoo Hardened SELinux state","text":"For those following the progress of SELinux support in Gentoo Hardened... In the hardened-development overlay, the selinux-base-policy package has been updated, hopefully fixing a nasty issue with support for the targeted policy (up to today, I only tested strict policies so I missed that). It also fixes an issue with dhcpcd not functioning properly. If you use SELinux and don't have the hardened-development overlay yet, please use layman -a hardened-development , synchronize ( layman -S ) and update your system to get the latest base policy. Also, please report bugs on Gentoo Bugzilla (and perhaps immediately add selinux@gentoo.org to the Cc-list. The Gentoo Hardened SELinux Handbook , still in draft, has gotten a few updates. It now documents the use of the gentoo_try_dontaudit boolean which the Gentoo Hardened SELinux developers use to hide potential cosmetic denials. If they are truly cosmetic, they will be reported upstream later to be included in the reference policy. If they are not, then users can simple toggle the boolean ( setsebool gentoo_try_dontaudit off ) to see the denials that the developers hid. The Gentoo Hardened SELinux Policy now includes the naming convention on the SELinux policy packages with a very short explanation why this particular convention was chosen. The discussion on it can be found on the gentoo-hardened mailing list and the last online meeting .","tags":"Gentoo","url":"https://blog.siphos.be/2011/03/updates-on-the-gentoo-hardened-selinux-state/","loc":"https://blog.siphos.be/2011/03/updates-on-the-gentoo-hardened-selinux-state/"},{"title":"Temporary script for Gentoo Hardened SELinux users","text":"If you are currently using Gentoo Hardened with SELinux, you might have noticed that we are currently lacking the proper dependencies within our Portage tree upon the SELinux policies (or, in other words, installing a package doesn't guarantee that the SELinux policy needed for that package is pulled in as well). As the current SELinux policy is still in \\~arch phase, it is also not really feasible to ask other package maintainers to add the proper dependency information as that might stall potential stability requests in general. So, for the time being, I'm using a simple script (which I call genmodoverview ) which tells me on my systems which SELinux policy modules i might still be missing. Based on the output of that script, I can then continue to install the sec-policy/selinux-* package(s) for those modules. It's usage is simple. Download the genmodoverview.sh and LISTING files, make the first one executable and run ./genmodoverview.sh -c LISTING .","tags":"SELinux","url":"https://blog.siphos.be/2011/02/temporary-script-for-gentoo-hardened-selinux-users/","loc":"https://blog.siphos.be/2011/02/temporary-script-for-gentoo-hardened-selinux-users/"},{"title":"About time...","text":"I was just wondering why \"UTC\" stood for \"Coordinated Universal Time\". Apparently (okay, citing Wikipedia here, so be critical), it's of two main reasons: English and French speaking folks that were participating in that discussion wanted their language to be presented in the abbreviation (English wants \"CUT - Coordinated Universal Time\", French \"TUC\" - Temps Universel Coordonné), so a consensus was taken to use \"UTC\" which gladly followed another time convention (UT1, UT2, ... for universal time notations - which is the second reason). Little did I know that it is UTC that has leap seconds (sometimes you hear in the news that the last minute of a year lasts 61 seconds) whereas UT1 (which is the modern notation for GMT) doesn't. All that because I read about a meeting taking place at 2000 UTC...","tags":"Misc","url":"https://blog.siphos.be/2011/02/about-time/","loc":"https://blog.siphos.be/2011/02/about-time/"},{"title":"cvechecker update","text":"A while ago, I got the request to enhance cvechecker with support for providing a list of installed software (or software you want to watch over with cvechecker) even if cvechecker isn't able to detect that software on your system. I've implemented this and it is currently available in the SVN repository. The next release of cvechecker will support this, but I'm hoping to add support for other databases with it as well (currently, it uses a local sqlite database but I'm hoping to support at least MySQL and postgresql too).","tags":"Security","url":"https://blog.siphos.be/2011/02/cvechecker-update/","loc":"https://blog.siphos.be/2011/02/cvechecker-update/"},{"title":"File System Labels in Linux Sea","text":"I have added some information on file system labels in Linux Sea ( PDF ). If you don't know what labels are (or UUIDs), here is a quick summary. Most, if not all file systems, assign a universally unique identifier (UUID) which looks like a random hexadecimal string to each file system. On a Gentoo system, you can get an overview of all UUIDs detected using a simple ls -l /dev/disk/by-uuid , courtesy of the gentoo udev rules. Users can also assign a specific label to a file system, either when they create it (like mkfs.ext4 -L ROOT /dev/sda2 ) or afterwards ( e2label /dev/sda2 ROOT ). This is also not limited to the content file systems - you can also assign a label to a swap file system. This information can then be used to uniquely identify the file system, even if you don't know what the device file (/dev/sda2) is called. A huge advantage is for those devices that often change device file (removable media, but also SATA or SCSI disks on systems where the admin loves adding and removing disks ;-) as you can keep your fstab configuration static: the fstab file doesn't need to be changed, even when the device files themselves change. A simple fstab line ( /dev/sda2 / ext4 defaults,noatime 0 0 ) can then easily be transformed to use the LABEL=\"...\" syntax (like LABEL=ROOT / ext4 defaults,noatime 0 0 ). Some people might also think they can use this mechanism for their kernel boot parameter (like using root=LABEL=ROOT instead of root=/dev/sda2 ). Sadly, the Linux kernel doesn't offer this functionality. It is possible, but only when you use an initramfs (which I don't).","tags":"Documentation","url":"https://blog.siphos.be/2011/02/file-system-labels-in-linux-sea/","loc":"https://blog.siphos.be/2011/02/file-system-labels-in-linux-sea/"},{"title":"SELinux for Gentoo Hardened","text":"Recently, most of the SELinux-related ebuilds from the hardened overlay have been moved to the official Portage tree. Hopefully, this will trigger more people / organizations to try Gentoo Hardened with SELinux and help us improve the ebuilds. They're still marked as \\~arch (as they should be). The draft SELinux handbook has been updated to reflect this and I'm currently performing a few greenfield installations using the documentation again to verify if everything still works as it is supposed to. In the meantime, I'm scripting these things so that I can automate these tests and run those at night in the future ;-)","tags":"SELinux","url":"https://blog.siphos.be/2011/02/selinux-for-gentoo-hardened/","loc":"https://blog.siphos.be/2011/02/selinux-for-gentoo-hardened/"},{"title":"\"Gentoo in production?\" Oh no, not again...","text":"I think it is that time of the year again, where people get some crazy ideas. Again I discussed the what must be the gazillion-th time I've been asked \"Do you think Gentoo is ripe for use in production?\". Honestly, I always tell myself to ignore those discussions but I've never managed to actually do that - ignore, that is. So to give me some leverage the gazillion-th plus one time I get that same question, let me vent my opinion about the subject right here and allow me to hurdle the permalink to whomever tries to start another heated discussion. Your question is wrong . It is never about a technology being \"ripe enough\" or \"stable enough\". What you need to ask yourself (or get acquainted with) is what you, your organization or your company expects from a technology to be introduced in your (organization/company) infrastructure. This includes, but is not limited to: What kind of bugfixing and security fixing support do you want for the technology? What kind of knowledge support do you want for the technology? How important is certification of (other) technologies with respect to the technology or operating system? How far do you implement an operating systems' release cycle ? What level of experience do you expect from your internal support team (or yourself)? As you can see, the questions are not about technology itself or pretty features. It is about how you work with that technology. And one shouldn't look at these questions as having a single phrase as answer. To properly answer the first question alone, you'll need to take a look at delivery times (how fast do you want a bug to be fixed), follow-up (how fast does the technology issue security announcements, do they follow CVE closely, ...), responsibilities, eventual legal or contractual obligations you might need to cover your ___, the ability of the provider to reproduce issues etc. Internal experience is also not to be underestimated. How quick do you (organization/company) want to be able to resolve problems? How experienced are you with analyzing logs? How well are you able to integrate a technology within your existing architecture? What auditing does you(r organization/company) require and do you know how to get that from the technology? I mean, come on, you're talking about \"production\". That's not the same as saying \"I've installed it for my parents\".","tags":"Gentoo","url":"https://blog.siphos.be/2011/01/gentoo-in-production-oh-no-not-again/","loc":"https://blog.siphos.be/2011/01/gentoo-in-production-oh-no-not-again/"},{"title":"Confining user applications","text":"Ever since I started using SELinux, I'm getting more and more fond of what it can do for (security) administrators. Lately, I've started confining user applications (like skype ) in the idea that I do not want any application connecting to the Internet or working with content received from untrusted sources to work inside the main user domain ( user_t or staff_t in my case). This particular exercise has been quite interesting, not only to learn more on SELinux, not only to get acquainted with the reference policy which Gentoo basis its policies upon. No, it's been interesting because you learn how applications work underneith... Take the skype application for example. Little did I know it read stuff from my firefox configuration (like the sec8.db and prefs.js file), most likely to see if the skype firefox plugin is installed. With SELinux, I saw that it did all that - and also denied it. But it isn't easy to find out why an application behaves as it does. After all, these aren't questions that average joe asks. It also isn't easy to deduce if you want to allow it or not. If it was purely for my own system, I wouldn't hesitate for long, but the idea is that the modules should work for the majority of people - and who knows, perhaps even be included in the reference policy in the future. Perhaps Gentoo Hardened can write up some rules on the SELinux policies and how they should be made for the distribution. Do we want to deny as much as possible, only allowing those things developers can safely verify need to be allowed? Or do we want to allow everything that the application already does (but nothing more) so that no AVC denials are shown anymore? And if Gentoo Hardened chooses \"deny as much as possible\", do we configure the policy to not audit those things we don't think we need (hiding it) or do we expect the security administrator to manage his own dontaudit rules? Well, guess I'll ask the hardened folks and see what they think ;-) During the quest, I'll try to update the Gentoo Hardened SELinux handbook draft . It's far from finished, but should be usable for most interested parties. If you're interested in SELinux and want to give it a try with Gentoo Hardened, this might be the document you are looking for.","tags":"SELinux","url":"https://blog.siphos.be/2011/01/confining-user-applications/","loc":"https://blog.siphos.be/2011/01/confining-user-applications/"},{"title":"Why I have backups","text":"You often read stories about people who have data loss and did not keep any (recent) backups, and are now fully equipped with a state-of-the-art backup mechanism. So no - no such failure story here but an example why backups are important. Yesterday I had a vicious RAID/LVM failure. Due to my expeditions in the world of SELinux, for some odd reason when I booted with SELinux enforcing on, my RAID-1 where an LVM volume group (with /usr , /var , /opt and /home ) was hosted (coincidentally the only RAID-1 with 1.2 metadata version, the others are at 0.90) suddenly decided to split itself into two (!) degraded RAID-1 systems: /dev/md126 and /dev/md127 . During detection, LVM found two devices (the two RAID metadevices) but with the same meta data on it (same physical volume signature), so randomly picked one as its physical volume. Found duplicate PV Lgrl5nNfenRUg9bIwM20q1hfMrWylyyL: using /dev/md126 not /dev/md127 Result: after a few reboots (no, I didn't notice it at first - why would I, everything seemed to work well so I didn't look at the dmesg output) I started noticing that changes I made were suddenly gone (for instance, ebuild updates that I made) which almost immediately triggers for me a \" remount read-only, check logs and take emergency backup \"-adrenaline surge. And then I noticed that there were I/O errors in my logs, together with the previously mentioned error message. So I quickly made an emergency backup of my most critical file system locations ( /home as well as /etc and some files in var ) and then tried to fix the problem (without having to reinstall everything). The first thing I did - and that might have been the trigger for real pandemonium - was to try and found out which RAID (md126 or md127) is being used. The vgdisplay and other commands showed me that only md127 was used at that time. Also, /proc/mdstats showed that md126 was in a auto read-only state, meaning it wasn't touched since my system booted. So I decided to drop md126 and add its underlying partitions to the md127 RAID device. Once added, I would expect that the degraded array would start syncing, but no: the moment the partition was added, the RAID was shown to be fully operational. So I rebooted my system, only to find out it couldn't mount md127 . File system checks, duplicate inodes, deleted blocks, the whole shebang. Even running multiple fsck -y commands didn't help. The volume group was totally corrupted and my system almost totally gone. At that time, it was around 1am and knowing I wouldn't be able to sleep before my system is back operational - and knowing that I cannot sleep long as my daughter will wake up at her usual hour - I decided to remove the array, recreate it and pull back my original backup (not the one I just took as it might already have corrupted files). As I take daily backups (they are made at 6 o'clock or during first boot, whatever comes first) I quickly had most of my /home recovered (the backup doesn't take caches, git/svn/cvs snapshots etc. into account). A quick delta check between the newly restored /home and the backup I took yielded a few files which I have changed since, so those were recovered as well. But it also showed lost changes, lost files and just corrupted files so I'm glad I have my original backups. I don't take backups of my /usr as it is only a system-rebuild away. As /etc wasn't corrupted, I recovered my /var , threw in a Gentoo stage snapshot (but not the full tarball as that would overwrite clean files) and ran a emerge -pe world --keep-going . When I woke up, my system was almost fully recovered with only a few failed installs - which were identified and fixed in the next hour. Knowing that my backup system is rudimentary (an rsync command which uses hardlinks for incremental updates towards a second system plus a secure file upload to a remote system for really important files) I was quite happy to have only lost a few changes which I neglected to commit/push. So, what did I learn? Keep taking backups (and perhaps start using binpkg for fast recovery), Use 0.90 raid metadata version, Commit often, and Install a log checking tool that warns me the moment something weird might be occurring","tags":"Gentoo","url":"https://blog.siphos.be/2010/12/why-i-have-backups/","loc":"https://blog.siphos.be/2010/12/why-i-have-backups/"},{"title":"cvechecker 2.0 released","text":"Okay, enough play - time for a new release. Since cvechecker 1.0 was released, a few important changes have been made to the cvechecker tools : You can now tell cvechecker to only check newly added files, or remove a set of files from its internal database. Previously, you had to have cvechecker scan the entire system again. cvechecker can now also report if vulnerabilities have been found in software versions that are higher than the version you currently have installed. This can help you find seriously outdated software, but also help you identify possible vulnerabilities if the CVE itself doesn't contain all vulnerable versions, just the \"latest\" vulnerable version. The toolset now contains a command called cverules which, on a Gentoo system, will attempt to generate version matching rules for software that is currently not detected by cvechecker yet. Very useful as I myself cannot install every possible software on my system to enhance the version matching rules. If you want to help out, run the cverules command and send me the output. Some needed performance enhancements have been added as well One thing I wanted to include as well was a tool that validates cvechecker output against the distribution security information. Some distributions patch software (to fix a vulnerability) rather than ask the user to upgrade to a non-vulnerable software. The cvechecker tools often cannot differentiate between the vulnerable and non-vulnerable binaries (as they both mention the same version), but often you can check against some meta data files of the distribution if and which CVEs have been resolved in which versions of a distribution package. The cvechecker tarball contains a script (see the scripts/ folder for cvepkgcheck_gentoo ) for Gentoo that tries to get this information from the GLSAs, but it is far from ready. I should try setting up a KVM instance with an \"old\" Gentoo installation just to validate if the command works, but even if it does, I'm not happy with how it is written. Seems to me a lot of trouble, and if it cannot be done simply, I'm afraid I'm doing it wrong ;-) Anyhow, I hope you enjoy version 2.0 of cvechecker.","tags":"Security","url":"https://blog.siphos.be/2010/12/cvechecker-2-0-released/","loc":"https://blog.siphos.be/2010/12/cvechecker-2-0-released/"},{"title":"Helping with version detection rules in cvechecker","text":"The new development snapshot, available from the cvechecker project site , contains a helper script that returns potential version detection rules for your system if the current cvechecker database doesn't detect your software. The script is currently available for Gentoo (called cverules_gentoo ) but other distributions can be easily added. The actual logic for detection is distribution-agnostic (the script cvegenversdat ) so it shouldn't be too much of a problem for other distributions to be supported as well. Note that the script isn't very fast (it's not intended to be) nor has a very high accuracy rate. After all, it does use generic regular expressions to try. The idea is that deployments on systems that have software I don't have on my system can help me with the development of the version detection rules by sending me the output of the helper script. Next up: tool to auto-generate (part of) the acknowledgements file for reporting purposes - getting information from distribution-specific information. Once that is in, I'll tag it version 2.0 of cvechecker.","tags":"Security","url":"https://blog.siphos.be/2010/11/helping-with-version-detection-rules-in-cvechecker/","loc":"https://blog.siphos.be/2010/11/helping-with-version-detection-rules-in-cvechecker/"},{"title":"Delta processing in cvechecker","text":"The cvechecker application will support delta file processing as well as higher version matching with its next release. The functionality is currently in version control and I still have to work out quite a few things before they can go live, but the functionality is there. Now why would these functions be interesting? Well, first of all, by supporting delta file processing I am able to use cvechecker with Portage' hooks. Every time a package is unmerged, cvechecker will remove the files from its database (so that it doesn't get picked up in vulnerability reports anymore). Similarly, every time a package is emerged, the files are stored in the database. There is no need to perform a full system scan every time the system has been updated. Second, being able to report on higher version vulnerabilities the tool can now also trap potential issues with reports that do not contain the exact version as detected by cvechecker but can be relevant. For instance, a version detection of Linux 2.6.35-hardened-r1 might otherwise not be noticed (for instance because no CVE is reported on the hardened-r1 release) yet a CVE report on 2.6.35 or even 2.6.36-rc4 might be of interest. By using the higher version reporting, you'll be notified of this as well. Same goes for vulnerability reports on an entire branch (say Python 2.4 ), especially when those branches are not actively being developed anymore (so the vulnerability remains). And another benefit is that you might be informed about higher versions of particular software being available ;-) Now, a very quick warning before everybody cheers and does the penguin dance: enabling higher version reports will give you lots of false hits: First of all, detecting if a version is higher than another version isn't easy. The tool is able to put 0.9.8 - 0.9.8a - 0.9.8b in the right order, as well as 0.5.1_alpha - 0.5.1_beta - 0.5.1 , but the same algorithm will make 2.6.35-hardened-r1 be less than 2.6.35 , and a secure 0.9.8 version will be seen vulnerable when 1.0.0_alpha has a vulnerability. Second of all, official CVE entries don't always provide a good version match themselves. For instance, CVE-2008-4609 has been configured that Linux Kernel 390 and Linux Kernel 3.25 (I know those are not correct version numbers - my point exactly) are vulnerable. So yes, 390 is (a lot) higher than 2.6.35 ... Third, many tools use parallel development branches. Take Python for instance: even when version 2.6.5 would have no vulnerabilities and 2.7 or 3.2 alpha releases do, it will still report the 2.6.5 one as having a potential vulnerability. This seems to give (for me at least) the most false positives of all. I still don't know how to deal with this huge amount of false positives - comments are always welcome.","tags":"Security","url":"https://blog.siphos.be/2010/11/delta-processing-in-cvechecker/","loc":"https://blog.siphos.be/2010/11/delta-processing-in-cvechecker/"},{"title":"SELinux enforcing for console activity","text":"I'm now able to boot into my system with SELinux in enforcing mode (without unconfined domains), do standard system administration tasks as root / sysadm_r (including the relevant Portage activities) and work as a regular user as long as I don't want to run in Xorg. I'm not going to focus on Xorg pretty soon now as there is a bunch of other things to do (like other applications, writing policies, patching etc.), but here is a very quick summary on the activities I had to do (apart from those in the Gentoo Hardened SELinux Handbook ): Use a more recent reference policy to start from. I fiddled with a live ebuild first, but am now falling back to the latest reference policy release of Tresys , versioned 2.20100524 . The implementing package ( sec-policy/selinux-base-policy ) can be found in my overlay (sjvermeu) . I use a meta package sec-policy/selinux-policy which pulls in the base policy as well as policies that you definitely need, but seem to work well when loaded as a module. Currently, that only includes sec-policy/selinux-portage but others may follow later. The main reason is that I like the modular approach and this way, I can update/patch these modules without requiring a base rebuild/reload Speaking of patching, the sec-policy/selinux-portage ebuild contains a patch for those who have /tmp and/or /var/tmp as a tmpfs filesystem I had to update /lib64/rcscripts/addons/lvm-start.sh so that the lvm locks are placed in /etc/lvm/lock rather than /dev/.lvm I also had to update /lib/dhcpcd/dhcpcd-hooks/50-dhcpcd-compat to put the *.info files in /var/lib/dhcpcd rather than /var/lib . Many binaries in /bin (part of sys-apps/net-tools ) are hard links (same inode) but different name. This gives issues with SELinux' file contexts. Quick fix is to copy rather than hardlink (for instance, cp hostname hostname.old ). After this, I ran rlpkg net-tools . Many packages need to be unmasked (from ~amd64 ) as the current stable packages either don't work or are too old. The \"unstable\" ones seem to work pretty well though. I know much development is being put in the SELinux state of Gentoo Hardened (just visit #gentoo-hardened if you have questions) so I'm sure things will be improving soon.","tags":"SELinux","url":"https://blog.siphos.be/2010/10/selinux-enforcing-for-console-activity/","loc":"https://blog.siphos.be/2010/10/selinux-enforcing-for-console-activity/"},{"title":"Risk identification","text":"Risk identification is a difficult subject. Analysts need it to defend mitigation strategies or to suggest investments. Yet risk identification is often a subjective method, especially in the IT industry. How do you give a number on a certain risk? When do you believe that that number exceeds a threshold? Because of the ambiguous definition of risks, it is often overlooked or substituted with impact analysis. Now impact analysis is not much better, but it is easier to comprehend. Impact analysis describes what happens when something has occurred. It doesn't state how often it can occur, or what the chances are of the event to occur, but gives an estimation on how big the impact would be when it occurs. Most of the time one describes an impact with financial loss. The highest impact a company has is that its survival is threatened. If you're a simple shop and you don't take an insurance on your stock, the impact of a fire or explosion in your shop might be that you're put out of business. If you do have insurance, your impact is more limited. In the majority of cases, impact analysis is more straightforward. IT risks are a prime example of difficult exercises. Quantifying the IT risks that a company is taking is difficult. In this post, I'm introducing a more straightforward method - even if it isn't fail-safe, it might still give some interesting sights on the matter. It is of course not something I invented myself (there's enough information on the internet about risk analysis or risk identification), merely a combination of several methods and ideas which I find useful (and decided to write up about). The method identifies a risk within four levels: low, medium, high and very high. To get inside a level, it uses two metrics: impact analysis which we've discussed before, and chance of occurrence . The latter is the most difficult to identify, but let's first show how to map the two onto the risk level: chance of occurrence &#94; 4 + M M VH VH 3 + M M VH VH 2 + L L H H 1 + L L H H x--+--+--+--+--> impact analysis 1 2 3 4 As you can see, I give each axis four values, going from low (low impact, or low probability) onto high. The resulting point lays within one of four quadrants - Low, Medium, High or Very High. I identify an event as having a high(er) risk when its probability is low but impact high, whereas an event that has a high probability but low impact is considered to be less of a risk: when something with a somewhat low(er) impact occurs, you should still have some breathing space to make sure it won't happen again (or find a way to reduce the impact) whereas an occurrence of something with a high(er) impact will most likely leave you hurt, fragile and licking your wounds. So, how to measure the chance of occurrence of an event? Well, let's do this in two stages: do an initial assessment, and then elevate the chance using particular checks. Within IT, an often described threat is the threat of someone trying to achieve personal gain (financial or public) from the system(s). Note that, for every possible threat, one will need to make a risk identification - you can't just say that a system has a risk. It is always a particular threat which is assigned a risk. So how large is the chance of such a \"hack\" attempt occurring? First, how \"wide\" is the access vector towards your system? Can the entire world (read: Internet) access the system (level = 4), is it everyone in the company (level = 3), everyone within an affiliated department (in most cases it means \"IT department\", level = 2) or limited to a very small set of people (level = 1)? Second, if one of the people identified earlier would want to perform malicious activity with eye on personal gain (financial or PR), would he succeed by his own (level + 1) or would he need at least one accomplice? Third, if the activity was performed, would it be traceable to the person or not (if not: level + 1)? As an example (purely hypothetical): read access to the access logs of a web application server which contains HTTP session information (logging of SESSIONID) as well as username (authentication) and origin (IP address) as well as other information. The threat: using this information to hijack an active session. First, if a hijack would be successful, the impact would be considered a level-3 (with 1 being low and 4 being very high): the company might suffer huge financial losses or PR would be a difficult beast to tame (because the application is an internal stock application with features to perform financial operations). But how high is the chance of occurrence? Well, say that the log files themselves can only be read by IT staff (level = 2), but that someone of the IT staff cannot hijack the session easily with this information alone as he would either require firewall changes (for instance because the application can only be reached through trusted middleware components) or have access to the machine the user is working on, and such changes or access require more than a single person in the situation. Also, if he did, audit trails would lead the changes (firewall changes or machine access) to the person. As a result, the chance of this event occurring given the circumstances is considered a level 2. At the quadrant, this would yield a level of \"High\". The risk of occurrence is relatively low, but the impact is too high to ever consider this a \"Medium\" or \"Low\". Now if we were to reduce the risk, we could focus on lowering the chance of occurrence (only few people access to the given information - say keep SESSIONID information in a separate, inaccessible logfile only to be used for general, automated metric collection - after all, if there's no point in keeping track of SESSIONID's, they wouldn't be logged anyhow). This is a lot easier to accomplish than to try and lower the impact analysis. On the other hand, if the impact analysis could be lowered (say by requesting a stronger authentication method for validating particular steps within the application, such as approvals) a session hijack would give less impact - say level 2 or even 1. In that case, the current risk would be lowered from \"High\" to \"Low\".","tags":"Security","url":"https://blog.siphos.be/2010/10/risk-identification/","loc":"https://blog.siphos.be/2010/10/risk-identification/"},{"title":"cvechecker 1.0 released","text":"With only a few small bugfixes between this release and the previous one, cvechecker 1.0 has finally been released. It runs fine on my few systems and I have not gotten any bugreports from other users anymore. It can definitely need more rules to identify installed software (those rules are released separately) which is what I will focus on the few upcoming weeks. Once that has been accomplished, I will start with the alpha releases for the 2.0 series using the feature requests as a guideline. I plan on maintaining the earlier versions of cvechecker for two consecutive releases - one including small functional enhancements (as long as they can be applied on the development release and the stable release easily) and one just for security and bug fixes.","tags":"Security","url":"https://blog.siphos.be/2010/10/cvechecker-1-0-released/","loc":"https://blog.siphos.be/2010/10/cvechecker-1-0-released/"},{"title":"SELinux quicky","text":"I've been using SELinux for a few days now (in permissive mode, just to get to know things) and have learned a few interesting commands (or other nice-to-know's) for using SELinux. Since I'm going to forget those the moment all is running well, I'll \"document\" them here ;-) I'm not going to talk about the -Z switches in ps or ls , that has been documented sufficiently on the Internet already. With sesearch you can query through the loaded policy. For instance, you want to know why you can execute sudo as a user (and not just due to the DAC permissions): ~$ sesearch -s user_t -t sudo_exec_t -p execute -c file -A Of course, this is only one of the three requirements for a transition from user_t to user_sudo_t , for that you still need process transition and entrypoint: ~$ sesearch -s user_t -t user_sudo_t -p transition -A ~$ sesearch -s user_sudo_t -t sudo_exec_t -p entrypoint -A Now, sometimes you find a rule that you didn't expect: ~$ sesearch -s user_t -t dmesg_exec_t -p execute -A Found 1 semantic av rules: allow user_t application_exec_type : file { ioctl read getattr lock execute execute_no_trans open } ; This is because dmesg_exec_t has the application_exec_type attribute set. You can see the list of types that have an attribute set (or vice versa) with seinfo : (Show list of types that have the application_exec_type attribute) ~$ seinfo -aapplication_exec_type -x (Show list of attributes given to the dmesg_exec_t type) ~$ seinfo -tdmesg_exec_t -x Now, during my browsing through the SELinux activities on my system, I noticed that I could run /usr/sbin/dnsmasq as root, without generating an error in the avc log. Yet sesearch didn't give any results. I've almost killed a few kittens by searching for possibilities (perhaps types with exec_type automatically have application_exec_type - not, or perhaps the domain transitions to another domain first without me knowing - not, I would see that the process runs as a different domain then, which wasn't the case). Luckily, dgrift on #selinux gave me the hint of checking the dontaudit rules as well: ~$ sesearch --dontaudit -s sysadm_t -t dnsmasq_exec_t ... dontaudit sysadm_t exec_type : file { execute execute_no_trans } ; So there we had it - it was being denied, just not logged. And because I ran in permissive mode, it gets executed anyhow. I disabled the dontaudit rules and got the avc denial I was expecting: (Disable dontaudit rules) ~$ semodule -DB (Reenable dontaudit rules) ~$ semodule -B","tags":"SELinux","url":"https://blog.siphos.be/2010/09/selinux-quicky/","loc":"https://blog.siphos.be/2010/09/selinux-quicky/"},{"title":"Switching to hardened","text":"Yesterday (and this night) I successfully converted my system to a Gentoo Hardened system. In my case, this currently means that PaX has been enabled and I am currently running the system (which is an x86_64 laptop) with SELinux in permissive mode (so it won't enforce the policies yet, but report violations so I can see in my logs if enforcement is possible or not). The permissive mode will be on for quite some time I would assume, as getting SELinux active on the system involved quite a few \\~amd64 packages and I'm not too fond of using that branch (I'm more of a stability guy).","tags":"Gentoo","url":"https://blog.siphos.be/2010/09/switching-to-hardened/","loc":"https://blog.siphos.be/2010/09/switching-to-hardened/"},{"title":"prezi presentations","text":"While doing some research on current rich internet applications / web application platforms, I discovered an online presentation site/tool called Prezi . This online application allows you to make dynamic presentations differently from the standard presentation software like OpenOffice.org's Impress . A nice example can be found online as well of course. I think I'm going to try this out in the near future ;-)","tags":"Misc","url":"https://blog.siphos.be/2010/09/prezi-presentations/","loc":"https://blog.siphos.be/2010/09/prezi-presentations/"},{"title":"cvechecker 0.6 released","text":"This release makes me quite happy, because it resolves one major PITA I had (performance), but you know how things go. If it works fine for the developer, it's probably an abomination for the rest of the world. Anyhow, cvechecker version 0.6 is now available. It improves reporting performance tremendously if your sqlite library is sufficiently up-to-date, now supports reporting on found software (regardless if it matches a CVE entry or not) and adds quite a few bug fixes along the way.","tags":"Security","url":"https://blog.siphos.be/2010/09/cvechecker-0-6-released/","loc":"https://blog.siphos.be/2010/09/cvechecker-0-6-released/"},{"title":"Linux Sea last content chapter","text":"The last chapter in Linux Sea focuses on Using A Shell . This seems to me like a nice last chapter, as it confronts the user with the exciting world of shell scripts. I hope that the chapters in the book are sufficiently stuffed so that beginners (who are not afraid to learn) can more easily start off with (Gentoo) Linux. Of course, this is just the beginning. The existing content needs to be sharpened, extended where needed, updated, etc. so expect a few updates coming along!","tags":"Documentation","url":"https://blog.siphos.be/2010/09/linux-sea-last-content-chapter/","loc":"https://blog.siphos.be/2010/09/linux-sea-last-content-chapter/"},{"title":"devops - how hard can it/it can be","text":"Dieter made a good reference to devops and the open source community and (correctly) points out that, even in a more collaborative scene such as the free software communities', there is still distinction between development and operations. And it isn't hard to see commonalities between enterprise organizations and free software communities in that respect. But is the comparison correct? If you look at a distribution as an enterprise, then surely the distinction between upstream (project development) and \"downstream\" (distribution) should be compared with the relations between an enterprise and its ISVs, not its internal development / operational divisions. If we look at internal divisions, then distributions tend to provide better integration between (internal) projects and the distribution. I cannot talk for every distribution, but in those I do know, the infrastructure team (\"operations\") has a firm grip on the infrastructure, yet leaves out sufficient space for development to do their releases/production activity: uploading files, changing documentation, ... This works, if the provided interface does not allow for developers to harm the principles that infrastructure has. This is what many (enterprise) organizations are still lacking, but there is no simple solution for this. Often, the operations team has principles that are difficult to match with the goals of development. Finding the correct balance between development and operations in that respect is quite a challenge - usually, free software communities can get there faster, often because their mass is sufficiently low. With a total 'employee' count of a few hundreds it is statistically easier to find a balance than within enterprises of a few thousand employees. I believe that both teams should write down their principles, policies and standards, and see if they can find matches (which is good) and mutually exclusive distinctions (which is challenging) where more investigation can be done. Both teams should be allowed to question decisions made by the other (but without pretending to know better) and make suggestions. This should lead to the emergence of interfaces where a team has sufficient freedom to get to their own goals autonomously. With such interfaces, people will start thinking that devops is growing apart (after all, they're starting to work autonomously and independently of each other). That isn't true. In my opinion, devops is about interacting on a high level (which is less time-delimited) so that interactions on a low level (which is very time-limited and focused on releasing, releasing, releasing) aren't necessary. Less interaction means that the teams that are responsible for getting to a specific, short time-framed goal, can cooperate closely and have a better grip on resources and requirements.","tags":"Free Software","url":"https://blog.siphos.be/2010/09/devops-how-hard-can-itit-can-be/","loc":"https://blog.siphos.be/2010/09/devops-how-hard-can-itit-can-be/"},{"title":"Linux Sea: log file management and backups","text":"I've added two more chapters to the Linux Sea book. The first one is about Log file management , the second one about Taking Backups . They're far from finished, but I thought that those two topics are important for day-to-day Gentoo usage and shouldn't be left out of the Linux Sea saga.","tags":"Documentation","url":"https://blog.siphos.be/2010/09/linux-sea-log-file-management-and-backups/","loc":"https://blog.siphos.be/2010/09/linux-sea-log-file-management-and-backups/"},{"title":"cvechecker 0.5 released","text":"A new intermediate release of cvechecker is now released. The tool is reported to build properly on NetBSD and FreeBSD as well (although much user experience there is still welcome), introduces a cvereport command ( example output ), has lowered its initial dependency requirements and pullcves now only loads the CVE XML changes in the database, rather than iterating across all CVE XML entries. Many thanks to Nigel Horne for his continuous testing/hammering on the tool.","tags":"Security","url":"https://blog.siphos.be/2010/09/cvechecker-0-5-released/","loc":"https://blog.siphos.be/2010/09/cvechecker-0-5-released/"},{"title":"qemu monitor cd change","text":"I've been playing around with kvm (which uses qemu) to try out other operating systems and Linux distributions. Up until now, little progress on that part (not because it is difficult, just little time) but there are a few things worth mentioning. For this post, let's start with a quicky on CD changes. qemu's integrated monitor is very nice and powerful. To go to the monitor from inside the vnc session, press Ctrl+Alt+2 (to go back, use Ctrl+Alt+1). Then you can query for attached hardware, add new devices, remove others, cpu's, etc. Something I found necessary was to switch CD/DVD images. With info block I found the device. I then ran eject ide1-cd0 followed by change ide1-cd0 /path/to/new/image et voila, new CD available.","tags":"Free Software","url":"https://blog.siphos.be/2010/08/qemu-monitor-cd-change/","loc":"https://blog.siphos.be/2010/08/qemu-monitor-cd-change/"},{"title":"Added \"iw\" support to Linux Sea","text":"The wireless driver developers are actively working on a new wireless toolset called \"iw\" , slowly deprecating the older wireless-tools toolset (which contains the \"iwconfig\" command). Kasumi_Ninja reported to me in the Gentoo Forums that it would be nice to add information on iw to Linux Sea , so I did. I must admit though that my systems don't (or hardly) support iw, so I had to be lead by examples throughout the Internet. Apart from the iw addition, I've reorganized the sections on Software Management as it was becoming too crowded. I've also started a ChangeLog for those who want to track the changes to the document. Now if anyone can recommend me a good spell- and grammar checker...","tags":"Documentation","url":"https://blog.siphos.be/2010/08/added-iw-support-to-linux-sea/","loc":"https://blog.siphos.be/2010/08/added-iw-support-to-linux-sea/"},{"title":"cvechecker 0.4 released","text":"Albeit with less updates than 0.3 had, cvechecker 0.4 brings in internal project files reorganization (more to the liking of the GNU autoconf/automake standards - I think), fixes a databaseleak (instead of memoryleak ;-) bug and introduces a teenie weenie bit more intelligent pullcves command (with multiple return code behavior to improve automation efforts) as was mentioned in the feature request list. All documentation is also updated and a pullcves manual page has been added.","tags":"Security","url":"https://blog.siphos.be/2010/08/cvechecker-0-4-released/","loc":"https://blog.siphos.be/2010/08/cvechecker-0-4-released/"},{"title":"I remain impressed by the free software community","text":"My current personal projects, Linux Sea and cvechecker , are actively being watched by the free software community. For the Linux Sea book, I get nice feedback and ideas on the Gentoo Forums and on the cvechecker application, people such as Nigel Horne are helping out in various ways - including feature requests of all sorts. I must admit, I remain impressed. Small changes have already been squeezed in in the Linux Sea document. A larger change (the use of the iw tools for wireless connectivity) is being investigated (sadly, my broadcom-sta device doesn't support the new nl80211 API so the documentation change is slower to integrate than expected). I'm also planning to make some updates on the software management chapter as it is currently becoming quite crowded. In the cvetool, most changes are bugfixes and output enhancements as expected. I'm not going to add more functionality now - I first want to get a stable 1.0 release out there. But first continue to squash bugs and add rules to the versions.dat file so that it is usable on various systems (release 0.4 is around the corner).","tags":"Documentation","url":"https://blog.siphos.be/2010/08/i-remain-impressed-by-the-free-software-community/","loc":"https://blog.siphos.be/2010/08/i-remain-impressed-by-the-free-software-community/"},{"title":"cvechecker userguide","text":"Just a quick note, I've created and uploaded the cvechecker userguide .","tags":"Security","url":"https://blog.siphos.be/2010/08/cvechecker-userguide/","loc":"https://blog.siphos.be/2010/08/cvechecker-userguide/"},{"title":"cvechecker 0.3 released","text":"Time for a new intermediate cvechecker release, so here it is. Changes include (beyond the usual bugfixes) different CSV output (with some sort of version support) so that it can be easily used for reporting purposes, removal of debugging/verbose items and added example files for reporting.","tags":"Security","url":"https://blog.siphos.be/2010/08/cvechecker-0-3-released/","loc":"https://blog.siphos.be/2010/08/cvechecker-0-3-released/"},{"title":"cvechecker 0.2 released","text":"I've made version 0.2 available of cvechecker . It fixes some build warnings and also supports the normal \"make install\" step. The pullcves command now also pulls in the latest versions.dat file. Special thanks to Per Andersson for reporting that the ./configure didn't fail if sqlite3 or libconfig wasn't available - that should be fixed now as well. In my Gentoo overlay , cvechecker-0.2.ebuild has been put available. Thanks there to webkiller71 for helping out with a more sane approach to cvechecker-0.1.ebuild - I hope I didn't screw up his changes in 0.2 too much ;-)","tags":"Security","url":"https://blog.siphos.be/2010/08/cvechecker-0-2-released/","loc":"https://blog.siphos.be/2010/08/cvechecker-0-2-released/"},{"title":"cvechecker 0.1 released","text":"cvechecker version 0.1 is out. This is the first publicly available development release, so it's still far from production-ready yet. However, it is usable so it can now be publicly analyzed to remove all icky bugs and such. I'm not planning (m)any new features (apart from the reporting script as mentioned on the tools' homepage ) before the first general available release, but any request will be gladly documented and taken in scope in future versions. What is cvechecker? Well, it is a tool that strives to scan your system for installed software. For each software it detects, it attempts to discover which version you have. This information is stored in a local database. The tool then matches this information with the (publicly available) CVE data (security vulnerability information). If a CVE entry mentions the software/version you have, the tool will report this to you. Who needs cvechecker? Noone needs it, but it can be interesting nevertheless. Users that install lots of software themselves and don't use the Linux distribution's package manager might benefit from this as the tool will then help them verify if a security update is needed or not. Users of LinuxFromScratch can do some security validation tests on their installations. Developers of particular packages (or even tools) can use the tool to be notified when one of their software's has a CVE (which most likely results in a new version of the software to be made available). Who needs cvechecker? No, this is not a duplicate paragraph - cvechecker needs input. Most of the work goes in detecting the available software and version. The method cvechecker uses is very rudimentary: run a (predefined) regular expression against the file (which is parsed with the strings command as this command understands ELF structures) and if the expression matches, it will extract the version (which is found using the expressions' groups) and store this in the database. The rules are defined in the versions.dat file (also available from sourceforge ), but this file is currently microscopicly filled - so lots and lots of additional rules need to be added. I'll be adding more and more rules as I encounter them (or have immediate need for), but I can definitely use additional help here. If you are interested in enhancing the versions.dat file, check out the cvechecker manual page - it describes the format and how it is used as well as some examples. And yes, an ebuild is available in my overlay , but I'm no ebuild developer (it's just easy to have them so Portage can track it) and the ebuild is butt-ugly (and probably also violates all QA policies).","tags":"Security","url":"https://blog.siphos.be/2010/08/cvechecker-0-1-released/","loc":"https://blog.siphos.be/2010/08/cvechecker-0-1-released/"},{"title":"HP webcam on Linux","text":"Okay, getting the HP webcam running on Linux wasn't hard at all. Enable Video For Linux (CONFIG_VIDEO_DEV) which can be found in the Linux kernel configuration at Device Drivers, Multimedia Support. Then, select Video capture adapters and inside that menu, select V4L USB devices and then USB Video Class (UVC). Once installed, reboot (or load the kernel module) and ensure that your user is in the video group. You'll then be able to activate the webcam (for instance, using mplayer tv:// or using skype). The integrated microphone is also no problem. It is by default muted, so using alsamixer , press F4 (to get to the capture menu) and unmute the channels + enable capturing (press spacebar).","tags":"Free Software","url":"https://blog.siphos.be/2010/08/hp-webcam-on-linux/","loc":"https://blog.siphos.be/2010/08/hp-webcam-on-linux/"},{"title":"New laptop, time to play","text":"I gave myself a nice treat and bought a new laptop. After some consideration, I decided to go with the HP Pavilion DV7 3150EB. Years ago, I didn't take an HP laptop as the reviews were not that satisfying. However, it looks as if that is past. So I first took a stab at the Gentoo Quickinstall for LVM2 and RAID and gave myself a (software) RAID-1 system with everything except / and /boot using LVM2. To my satisfaction, following the guide was a breeze and it worked out just fine. The real hurdle, that I just won, was to get the wireless up and running on WPA2. I noticed earlier (before I bought the laptop) that getting the Broadcom 43255 wifi (Broadcom Corporation Device 4357 (rev 01)) might be a challenge. Well, the open-source b43 driver didn't detect the wifi card, but the (closed-source) Broadcom STA driver (as supported by Broadcom itself) does. To install it on Gentoo, it was as easy as unmasking broadcom-sta and installing it. It worked immediately, but not for WPA/WPA2 networks (and I am not going to put my wireless in non-WPA2 mode). Luckily, it was easy to discover that it was wpa_supplicant itself that was giving the card a hard time as non-WPA networks worked flawlessly. A quick stab at the wpa_supplicant.conf file gave me the final success I needed: ap_scan=2 did the trick. Tomorrow: getting the webcam working...","tags":"Free Software","url":"https://blog.siphos.be/2010/08/new-laptop-time-to-play/","loc":"https://blog.siphos.be/2010/08/new-laptop-time-to-play/"},{"title":"Linux Sea sources online, cvechecker still in development","text":"First of all, I've put the sources for Linux Sea online at GitHub . Not only does that safeguard any latest changes from not hitting my backup in time before my laptop dies (it's terminal, but I can't let him go yet ;-) but it also allows people who want to help with it (or translate it) to pull in the sources. Note that it is still not finished (no spelling and grammar check done yet, still need to add some exercises, etc); once it is, I will tag the sources appropriately. On the cvechecker state, it is also still under development, but progress is going nicely. Most of the work now is in updating the versions.dat file with information on how to obtain the current version of a package/tool. It is an easy activity - most of the work is in finding out how CVE entries would label a tool (what vendor and product name would be chosen) and because I am too lazy, I am currently only adding those that already have CVE entries assigned to them (so I can just take a look at the correct values). It is also my first attempt at using autotools. Quite some overkill for such a small project, but why not. At least it allows me to try to do some new things here ;-)","tags":"Security","url":"https://blog.siphos.be/2010/07/linux-sea-sources-online-cvechecker-still-in-development/","loc":"https://blog.siphos.be/2010/07/linux-sea-sources-online-cvechecker-still-in-development/"},{"title":"cvechecker in development mode","text":"A while ago I had the idea to create a simple tool that checks the CVE database against my current system. It would allow me to check if my system is somewhat up to date (no pending security vulnerabilities), but also to get an automated overview of the various software packages (and versions) using a distribution-agnostic method. So I started coding. The idea is to have a tool which can interprete CVE data, gather current version information from the system and match the CVEs against these versions and report the results to me. I have created a sourceforge project to host the source code and preliminary documentation for the tool. Although the tool runs, it is still far from finished. On the site, you can check out the progress of the development (there's a first todo-list on the main page). Do you think this is a good idea? I'd be happy to hear it.","tags":"Security","url":"https://blog.siphos.be/2010/07/cvechecker-in-development-mode/","loc":"https://blog.siphos.be/2010/07/cvechecker-in-development-mode/"},{"title":"OVAL, SCAP, CVE, CPE, ...","text":"For a personal POC I wanted to see if it is possible to generate, based on the collection of CVE entries publicly available, a report informing a system administrator about possible vulnerabilities. Nothing fancy, just based upon versions. A simple example: tool detects Perl, acquires installed Perl version, then matches the collection of CVE entries against this Perl version. If at least one CVE is found, report it. The idea is then to make this as generic as possible (not specific for an operating system or Linux distribution), so not use a package version but really the tool version (or library version). Of course, whenever I am planning such minor POCs, I search the Internet for possible existing tools (just like kev009 describes - \"But First, Write No Code\" ). And I found out that there are already quite some \"foundation components\" available... CPE is a structured way of naming software (vendor, title, version ...) OVAL is a method for performing structured tests (like regular expression matches in text) for reporting purposes Many more of these efforts are linked through the Mitre sites. The above two are the most important ones though - it seems that it might be possible to use OVAL to describe the tests I wanted for the POC. To be continued...","tags":"Security","url":"https://blog.siphos.be/2010/06/oval-scap-cve-cpe/","loc":"https://blog.siphos.be/2010/06/oval-scap-cve-cpe/"},{"title":"Listing files of (not) installed software","text":"Everyone that has been using Gentoo for a while now knows about tools such as qlist that show you the list of files installed by an (installed) package, or qfile that allows you to find which package provided a particular file on your system. One thing lacking is to be able to find out which package would provide a file. Unlike the previous tools, this tool cannot rely on the information found on your system as the package isn't installed yet. There have been projects in the past that attempted to provide such functionality, almost always through an online queryable database. Many haven't survived, due to too high expectations or little server infrastructure resources. But it seems like PortageFileList is to stay for a while. The project not only offers an online interface for querying information, it also provides a package ( app-portage/pfl ) that allows you to query their infrastructure from the command line. The package provides a tool called e-file which supports SQL-like syntax for the queries. ~$ e-file '%bin/xdm' The above command will then display, using the well-known emerge/Portage output, which package provides the file (as well as which file was matched by the query). Definitely a nice tool to have around. Thanks guys of PortageFileList !","tags":"Gentoo","url":"https://blog.siphos.be/2010/06/listing-files-of-not-installed-software/","loc":"https://blog.siphos.be/2010/06/listing-files-of-not-installed-software/"},{"title":"GSE TWS BeLux 2010","text":"Today, IBM generously hosted the GSE TWS BeLux 2010 conference . Although it was organized together with the GSE DB2 conference (which I would also have loved to attend) I must say I was pretty impressed with the topics given, especially those after the lunch. For me, personally, the topic on TWS 8.5.1 with broker functionality was most impressive. Not that the features are extremely innovative, but they are very useful (especially in an enterprise context). I don't know if you have (production) experience with TWS Broker (or even 8.5.1's broker functionality) - if you do, I'd love to hear about it!","tags":"Misc","url":"https://blog.siphos.be/2010/06/gse-tws-belux-2010/","loc":"https://blog.siphos.be/2010/06/gse-tws-belux-2010/"},{"title":"Question yourself v3","text":"Another update to Quizzer , now at version 3. But more importantly, updates to the Linux Sea related chapters are made available online - get a taste for it at the online quizzer set . Feedback is, as always, very much appreciated.","tags":"Documentation","url":"https://blog.siphos.be/2010/05/question-yourself-v3/","loc":"https://blog.siphos.be/2010/05/question-yourself-v3/"},{"title":"Question yourself v2","text":"A new version of the Quizzer webscript is available. The demo has also been updated with quick tests on the first few chapters of Linux Sea. More exercises on the following chapters will follow soon. Updates to the script include visual accept/reject of single-choice and multiple choice answers and improved support for Internet Explorer (which I don't have at home to validate).","tags":"Documentation","url":"https://blog.siphos.be/2010/05/question-yourself-v2/","loc":"https://blog.siphos.be/2010/05/question-yourself-v2/"},{"title":"Question yourself","text":"Do you ever write down things in the hope you never forget them, but still think it would be better if you could somehow take a test of that subject from time to time to make sure you don't forget? I do, and I found it quite difficult to keep the knowledge live without having to reread the things every now and then. For that purpose, I started writing a simple JavaScript/XML/XSL fileset that allowed me to present questions (randomly if necessary) from a structured set of questions. In the beginning, it was too simple to share (string matching) but quickly grew to something more elaborate: regular expression support, multiple string-answer support, in-paragraph answer boxes and single/multiple choice answers. With this fileset in place, I can keep track of things I would most likely otherwise forget: just select the category which I want to take a test from, and start with a (lot of) random question(s). I've decided to put this fileset online (including demo files) and will extend the demo file with questions regarding my book, Linux Sea , allowing readers of the book to take online tests after they've finished a chapter.","tags":"Documentation","url":"https://blog.siphos.be/2010/05/question-yourself/","loc":"https://blog.siphos.be/2010/05/question-yourself/"},{"title":"SAI and N-O-SQL","text":"Yesterday (argh, the day before yesterday) I went to a SAI conference on nosql. In Belgium, SAI is a non-profit organization for IT people which focuses on knowledge sharing. The conference that day was on nosql. The presentation given by OuterThought was very good and offered a nice introduction to the \"new types of database architectures\" that are being actively developed as we speak. Although the use of these nosql databases within KBC (where I work for) is limited (I'm not aware of any application that is already using this technology) it would be plain wrong to discard the technology as \"too immature\". With the recent developments that we face in the IT industry, applications are nowadays quick in adopting such new technologies and I suspect that off-the-shelf applications will soon come with such nosql database technology as part of the solution. For large enterprises, this does face some (hard?) challenges: how do you control your network usage (some of the technologies are easy to use, but hard to tune), how do you design your architecture, where is your data, how can you ensure that you do not \"lock in\" into a single nosql technology (i.e. how do you ensure interoperability and migrations between technologies), do you still need SAN-based replication or will you now let the technology handle this for you, etc. So yes, the nosql technology is nice to look into (and definitely something to follow up on) but make sure you don't introduce it in your organization without thinking about the entire integration and management aspect.","tags":"Databases","url":"https://blog.siphos.be/2010/04/sai-and-n-o-sql/","loc":"https://blog.siphos.be/2010/04/sai-and-n-o-sql/"},{"title":"A dozen pages added","text":"Just a quick heads-up that a dozen pages in the Linux Sea book have been added. Nothing spectacular, just a few more paragraphs on services/runlevels, a few updates on software management and on boot failure resolutions.","tags":"Documentation","url":"https://blog.siphos.be/2010/04/a-dozen-pages-added/","loc":"https://blog.siphos.be/2010/04/a-dozen-pages-added/"},{"title":"License support in Gentoo","text":"It's a bit sad that Gentoo didn't promote this more, but Gentoo users now have support for license-based masking. What does this mean? Well, previously, Gentoo already supported various masking reasons (like stable versus staging - the x86 versus \\~x86 saga, package.mask'ing - for security reasons or critical bugs, ...). Now, a new feature is added: license masking. By default, Portage accepts all non-EULA licenses. If a package uses a EULA license, you'll get a failure message stating that the license is 'masked'. Now, what good does this do for users? Well, you can now ask Portage only to accept certain licenses (like @FSF-APPROVED , which is a list of all FSF-approved licenses) and deny the installation of others. Nice, isn't it? I've added information regarding package license states (and the global as well as per-package unmasking support through /etc/portage/package.license) to the Linux Sea document.","tags":"Gentoo","url":"https://blog.siphos.be/2010/02/license-support-in-gentoo/","loc":"https://blog.siphos.be/2010/02/license-support-in-gentoo/"},{"title":"Executing, but only when you're home","text":"Sometimes you want to execute a particular command, but only when you're at home. Examples would be running fetchmail (or fetchnews) through cron, but you don't want this to run when you're in the train, connected to the Internet through GPRS... My idea here would be to create a script (say \"athome.sh\") which returns 0 if you're at home, and 1 otherwise. The key of the script is that the MAC address of your (default) gateway is unique. 1 2 3 4 5 6 7 8 9 10 11 #!/bin/sh GW = $( /sbin/ip route | awk '/default/ {print $3}' ) ; MGW = $( /sbin/arp -e | grep ${ GW } | awk '{print $3}' ) ; if [ \" ${ MGW } \" = \"00:11:22:33:44:55\" ] then exit 0 ; else exit 1 ; fi With this script, you can then run athome.sh && fetchmail . If you aren't home, athome.sh will return 1 and the fetchmail command will never be executed. When you are, the command returns 0 and fetchmail is launched.","tags":"Free Software","url":"https://blog.siphos.be/2010/01/executing-but-only-when-youre-home/","loc":"https://blog.siphos.be/2010/01/executing-but-only-when-youre-home/"},{"title":"Switching to database architecture","text":"It's finally committed: I'm going to dive into the realms of database architecture. It's with some sentiment that I'm leaving the expertise field of Apache, J(2)EE and WebSphere, but seeing the database architecture field makes it up well. I'm starting to get acquainted with Oracle DB as first platform and IBM DB2 + Microsoft SQL Server are pending. Exciting times are coming!","tags":"Databases","url":"https://blog.siphos.be/2009/12/switching-to-database-architecture/","loc":"https://blog.siphos.be/2009/12/switching-to-database-architecture/"},{"title":"Translations to \"Linux Sea\"","text":"A few people have contacted me if they were allowed to translate the online book I'm writing ( Linux Sea ). Of course they are, the license allows it. However, I recommend to wait a bit. At this moment, I'm not going to release the docbook sources (I'm not writing it in DocBook, but I'm generating from another XML into DocBook) until I'm happy with the final result. I'm glad to see that the document is well received. There is still lots of work on it (more excercises, a thorough spelling / grammar check, elaborate on certain topics, ...) so stay tuned for further updates. Why are those updates \"slow\"? Well, let's say that I use a \"fair share scheduling\" principle on all my hobbies ;-)","tags":"Documentation","url":"https://blog.siphos.be/2009/12/translations-to-linux-sea/","loc":"https://blog.siphos.be/2009/12/translations-to-linux-sea/"},{"title":"Small updates on Linux Sea","text":"A few updates have made it to the Linux Sea book: Information regarding ndiswrapper Some information about udev and the symlinks that it creates The PDF version has been updated as well.","tags":"Documentation","url":"https://blog.siphos.be/2009/10/small-updates-on-linux-sea/","loc":"https://blog.siphos.be/2009/10/small-updates-on-linux-sea/"},{"title":"Online image gallery","text":"If you're not up to the various free image gallery sites, you might want to try out ZenPhoto . Quite powerful, easy to use and well themeable. Requires PHP / MySQL.","tags":"Free Software","url":"https://blog.siphos.be/2009/10/online-image-gallery/","loc":"https://blog.siphos.be/2009/10/online-image-gallery/"},{"title":"Added quota information","text":"I've added quota support information to the Linux Sea book as well as information about the eclean command for cleaning distfiles and packages. The part on building a Linux kernel has been moved into its own chapter , the chapter on hardware support now has a bit more information about dealing with sound cards (ALSA support) and will contain information about sound servers in the near future. This chapter will also be used to configure the various other hardware things as they come by (printers, scanners, ...).","tags":"Gentoo","url":"https://blog.siphos.be/2009/09/added-quota-information/","loc":"https://blog.siphos.be/2009/09/added-quota-information/"},{"title":"Draft PDF for Linux Sea","text":"I've added a draft PDF version of my Linux Sea document. If you don't mind the A4 papersize and the bad typesetting of the text boxes (I still have lots of overflows to correct) it is quite usable.","tags":"Gentoo","url":"https://blog.siphos.be/2009/08/draft-pdf-for-linux-sea/","loc":"https://blog.siphos.be/2009/08/draft-pdf-for-linux-sea/"},{"title":"Darwin Information Typing Architecture","text":"Having documented a lot in LaTeX (back in the old days at the university), GuideXML (Gentoo's document markup language) and DocBook ( Linux Sea ) I'm now pointing my arrows at DITA, the Darwin Information Typing Architecture . DITA \"forces\" the technical writer in separating the content of his document in specialized subjects: reference, task or concept, or a specialized version of any of those which you can create/define yourself. By separating content in those three subjects, you can more easily manage your technical documentation (write concepts as individual topics, tasks as end-user procedures and references for affiliated topics or command information). Once all these documents are written, you bind them together using a DITA map (a metadocument which holds references to all related concepts/tasks/references) et voila: your documentation is ready. Well, not really, you need to build it to something end users can read - you can use dita-ot for that. It supports building for Eclipse Infocenter, RTF, XHTML and PDF out of the box.","tags":"Documentation","url":"https://blog.siphos.be/2009/04/darwin-information-typing-architecture/","loc":"https://blog.siphos.be/2009/04/darwin-information-typing-architecture/"},{"title":"Linux Sea is progressing slowly but surely","text":"My everlasting document, Linux Sea , is progressing slowely but surely. I've started a few new chapters and also initiated a chapter on Installing Gentoo (which is more a shortlist of tasks with pointers to earlier chapters). I also took a different CSS (docbook.css file used by the FreeBSD handbook) as it looks a lot better.","tags":"Gentoo","url":"https://blog.siphos.be/2009/02/linux-sea-is-progressing-slowly-but-surely/","loc":"https://blog.siphos.be/2009/02/linux-sea-is-progressing-slowly-but-surely/"},{"title":"Extremely simple task manager","text":"At work, I am often busy with quite a few projects. Yet, at times, I have no outstanding tasks because all of my tasks can only start when an event has occurred (like a server which is made available, or a budget that is approved) or another task has finished. To keep track of my work, I write an extremely simple task manager: an XML file (for the data), XSL file (for the rendering) and HTML/CSS file (to render and use the browsers' XSL capabilities). I call it taskviewer due to lack of more imagination ;-) It is a simple manager with no user interface for managing it at all - so you'll need to edit the XML file yourself. However, the HTML/CSS file, together with the XSL file, renders the content of the XML file in such a way that you have a nice overview of your tasks. It's \"features\" are simple: keep track of events you are waiting for keep track of a tasks' dependencies (events or other tasks) get an overview of tasks that can immediately start versus that are blocked, waiting for its dependencies to finish There is an example available online with some hypothetical data. If you know of a simple program (preferably java or one available for both Windows and Linux) that has similar features (especially tracking tasks depending on certain events), please do tell me. I've looked at tools like taskjuggler but couldn't find one that remains simple yet has these features.","tags":"Misc","url":"https://blog.siphos.be/2008/12/extremely-simple-task-manager/","loc":"https://blog.siphos.be/2008/12/extremely-simple-task-manager/"},{"title":"hex2passwd, a password generator","text":"I know that repeatable password generators are less secure than random character generators. After all, if you want a strong password, you can simply perform head -c 8 /dev/urandom | mimencode to obtain a nice, random password string. However, in certain cases you might want to generate passwords given a particular entry which always returns the same password. For instance, for low-profile web sites. Most people use mneumonics (such as username reversed and appended with domainname abbreviation to give an example) but mneumonics can be quite insecure, especially if you use a mneumonic that, once someone sees one of your passwords, he can deduce all passwords. An example would be the above-given algorithm, which yields for the following sites: bugs.gentoo.org, user foobar, password raboofbgo forums.gentoo.org, user bleh, password helbfgo www.sourceforge.net, user mynick, password kcinymwsn I'm sure you can find the password for other sites I would show you, so this kind of passwords are not that secure. Enter hex2passwd , a tool which generates (the same) password for the same input over and over again. With the tool you can make your mneumonic a bit more secure as it uses hashfunctions to create a pseudorandom sequence and a character mapping to convert the hash result into a possible password. An example for the above sites / mneumonic would yield: For bugs.gentoo.org, user foobar $ echo raboofbgo | sha1sum | hex2passwd -n 8 XqXgOYce For forums.gentoo.org, user bleh $ echo helbfgo | sha1sum | hex2passwd -n 8 l8U.tdzg For www.sourceforge.net, user mynick $ echo kcinymwsn | sha1sum | hex2passwd -n 8 70z4Bu3k Of course, the tool offers some more flexibility, such as choosing your own character maps or scrambling the maps before you use them. In any case, if you think such a tool is useful for you as well, don't hesitate to download, compile and install it - it's a simple C program, probably too ugly to show ;-)","tags":"Free Software","url":"https://blog.siphos.be/2008/09/hex2passwd-a-password-generator/","loc":"https://blog.siphos.be/2008/09/hex2passwd-a-password-generator/"},{"title":"Adding exercises and resources","text":"As stated earlier, I'm now focusing on the existing content of my (work-in-progress) ebook called Linux Sea ( PDF , HTML ). I'm going to add more text where appropriate, add exercises to each chapter as well as references to online resources. When that's finished, I'll probably be writing a chapter on installing Gentoo Linux as that's a major end-user topic that isn't discussed yet (but luckily, there's still the Gentoo Handbook ).","tags":"Gentoo","url":"https://blog.siphos.be/2008/09/adding-exercises-and-resources/","loc":"https://blog.siphos.be/2008/09/adding-exercises-and-resources/"},{"title":"Linux Sea - Updates on graphical environment chapter","text":"I've updated the chapter on graphical environments a bit to reflect how applications, window managers, X server and widget toolkits work together. Hopefully it isn't a big lie that I wrote there ;-) I'll probably be doing a bit of clean ups the coming days before I start out with more chapters...","tags":"Gentoo","url":"https://blog.siphos.be/2008/08/linux-sea-updates-on-graphical-environment-chapter/","loc":"https://blog.siphos.be/2008/08/linux-sea-updates-on-graphical-environment-chapter/"},{"title":"Playing with gqview","text":"Some time ago I received a digital camera; however, due to diskspace shortage I need to clean up my home directory. One of the directories that eats most of my sectors is one where I store all my pictures. I know I have a lot of duplicate pictures, pictures deduced from master pictures (lower resolution, some editing) and similar pictures (same scene taken 4 or 5 times with different camera settings, hoping to get at least one good shot) but managing them wasn't easy. I now played a bit with gqview and this tool seems to provide some features I find very interesting; one of them is the \"find duplicates\" where you can even search for pictures with \"similar\" content and I must say that it does work. Of course, nothing is perfect, but I've managed to clean up the picture directory so it works for me.","tags":"Free Software","url":"https://blog.siphos.be/2008/08/playing-with-gqview/","loc":"https://blog.siphos.be/2008/08/playing-with-gqview/"}]};