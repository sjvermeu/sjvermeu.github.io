<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art... - infrastructure</title><link href="https://blog.siphos.be/" rel="alternate"></link><link href="https://blog.siphos.be/tag/infrastructure/feed/atom.xml" rel="self"></link><id>https://blog.siphos.be/</id><updated>2022-05-21T13:00:00+02:00</updated><entry><title>Containers are the new IaaS</title><link href="https://blog.siphos.be/2022/05/containers-are-the-new-iaas/" rel="alternate"></link><published>2022-05-21T13:00:00+02:00</published><updated>2022-05-21T13:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-05-21:/2022/05/containers-are-the-new-iaas/</id><summary type="html">&lt;p&gt;At work, as with many other companies, we're actively investing in new
platforms, including container platforms and public cloud. We use Kubernetes
based container platforms both on-premise and in the cloud, but are also very
adamant that the container platforms should only be used for application
workload that is correctly designed for cloud-native deployments: we do not
want to see vendors packaging full operating systems in a container and
then shouting they are now container-ready.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At work, as with many other companies, we're actively investing in new
platforms, including container platforms and public cloud. We use Kubernetes
based container platforms both on-premise and in the cloud, but are also very
adamant that the container platforms should only be used for application
workload that is correctly designed for cloud-native deployments: we do not
want to see vendors packaging full operating systems in a container and
then shouting they are now container-ready.&lt;/p&gt;


&lt;p&gt;Sadly, we notice more and more vendors abusing containerization to wrap their
products in and selling it as 'cloud-ready' or 'container-ready'. For many
vendors, containers allow them to bundle everything as if it were an
appliance, but without calling it an appliance - in our organization, we
have specific requirements on appliances to make sure they aren't just
pre-build systems that lack the integration, security, maintainability and
supportability capabilities that we would expect from an appliance.&lt;/p&gt;
&lt;p&gt;Even developers are occasionally tempted to enlarge container images with a
whole slew of middleware and other services, making it more monolithic
solutions than micro-services, just running inside a container because they
can. I don't feel that this evolution is beneficial (or at least not yet),
because the maintainability and supportability of these images can be very
troublesome.&lt;/p&gt;
&lt;p&gt;This evolution is similar to the initial infrastructure-as-a-service
offerings, where the focus was on virtual machines: you get a platform on top
of which your virtual machines run, but you remain responsible for the virtual
machine and its content. But unlike virtual machines, where many organizations
have standardized management and support services deployed for, containers are
often shielded away or ignored. But the same requirements should be applied to
containers just as to virtual machines.&lt;/p&gt;
&lt;p&gt;Let me highlight a few of these, based on my &lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;Process view of
infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost and licensing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Be it on a virtual machine or in a container, the costs and licensing of the
products involved must be accounted for. For virtual machines, this is often
done through license management tooling that facilitates tracking of software
deployments and consumption. These tools often use agents running on the
virtual machines (and a few run at the hypervisor level so no in-VM agents are
needed).&lt;/p&gt;
&lt;p&gt;Most software products also use licensing metrics that are tailored to
(virtual) hardware (like processors) or deployments (aka nodes, i.e. a
per-operating-system count). Software vendors often have the right to audit
software usage, to make sure companies do not abuse their terms and
conditions. &lt;/p&gt;
&lt;p&gt;Now let's tailor that to a container environment, where platforms like
Kubernetes can dynamically scale up the number of deployments based on the
needs. Unlike more static virtual machine-based deployments, we now have a
more dynamic environment. How do you measure software usage here? Running
software license agents inside containers isn't a good practice. Instead, we
should do license scanning in the images up-front, and tag resources
accordingly. But not many license management tooling is already
container-aware, let alone aligned with a different way of working.&lt;/p&gt;
&lt;p&gt;But "our software license management tooling is not container-ready yet" is
not an adequate answer to software license audits, nor will the people in the
organization that are responsible for license management be happy with such
situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Product lifecycle&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the licensing part, companies also want to track which software
versions are being used: not just for vulnerability management purposes, but
also to make sure the software remains supported and fit for purpose.&lt;/p&gt;
&lt;p&gt;On virtual machines, regular software scanning and inventory setup can be done
to report on the software usage. And while on container environments this can
be easily done at the image level (which software and versions are available in
which containers) this often provides a pre-deployment view, and doesn't tell
us if a certain container is being used or not, nor if additional deployments
have been triggered since the container is launched.&lt;/p&gt;
&lt;p&gt;Again, deploying in-container scanning capabilities seems to be
contra-productive here. Having an end-to-end solution that detects and
registers software titles and products based on the container images, and then
provides insights into runtime deployments (and history) seems to be a better match.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authorization management (and access control)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When support teams need to gain access to the runtime environment (be it for
incident handling, problem management, or other operational tasks) most
companies will already have a somewhat finer-grained authorization system in
place: you don't want to grant full system administrator rights if they aren't
needed.&lt;/p&gt;
&lt;p&gt;For containers, this is often not that easy to accomplish: the design of
container platforms is tailored to situations where you don't want to
standardize on in-container access: runtimes are ephemeral, and support is
handled through logging and metric, with adaptation to the container images
and rolling out new versions. If containers are starting to get used for more
classical workloads, authorization management will become a more active field
to work out.&lt;/p&gt;
&lt;p&gt;Consider a database management system within the container alongside the
vendor software. Managing this database might become a nightmare, especially
if it is only locally accessible (within the container or pod). And before you
yell how horrible such a setup would be for a container platform... yes, but
it is still a reality for some.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Auditing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Auditing is a core part of any security strategy, logging who did what, when,
from where, on what, etc. For classical environments, audit logging, reporting
and analysis are based upon static environment details: IP addresses,
usernames, process names, etc.&lt;/p&gt;
&lt;p&gt;In a container environment, especially when using container orchestration,
these classical details are not as useful. Sure, they will point to the
container platform, but IP addresses are often shared or dynamically assigned.
Usernames are dynamically generated or are pooled resources. Process
identifiers are not unique either.&lt;/p&gt;
&lt;p&gt;Auditing for container platforms needs to consider the container-specific
details, like namespaces. But that means that all the components involved in
the auditing processes (including the analysis frameworks, AI models, etc.)
need to be aware of these new information types.&lt;/p&gt;
&lt;p&gt;In the case of monolithic container usage, this can become troublesome as the
in-container logging often has no knowledge of the container-specific nature,
which can cause problems when trying to correlate information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I only touched upon a few processes here. Areas such as quality assurance and
vulnerability management are also challenges for instance, as is data
governance. None of the mentioned processes are impossible to solve, but
require new approaches and supporting services, which make the total cost of
ownership of these environments higher than your business or management might
expect.&lt;/p&gt;
&lt;p&gt;The rise of monolithic container usage is something to carefully consider. In
the company I work for, we are strongly against this evolution as the enablers
we would need to put in place are not there yet, and would require significant
investments. It is much more beneficial to stick to container platforms for
the more cloud-native setups, and even in those situations dealing with ISV
products can be more challenging than when it is only for internally developed
products.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1527975405730336768"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="kubernetes"></category><category term="container"></category><category term="iaas"></category><category term="infrastructure"></category><category term="virtual-machine"></category></entry><entry><title>Not sure if TOSCA will grow further</title><link href="https://blog.siphos.be/2021/06/not-sure-if-TOSCA-will-grow-further/" rel="alternate"></link><published>2021-06-30T14:30:00+02:00</published><updated>2021-06-30T14:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-30:/2021/06/not-sure-if-TOSCA-will-grow-further/</id><summary type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;


&lt;p&gt;But while I do see some adoption of TOSCA, I get the feeling that it is
struggling with its position against the various infrastructure-as-code
(IaC) frameworks that are out there. While many of these frameworks do
not inherently support the abstraction that TOSCA has, it is not all that
hard to apply similar principles and use those frameworks to facilitate
multi-cloud deployments.&lt;/p&gt;
&lt;p&gt;Before considering the infrastructural value of TOSCA further, let's
see what TOSCA is about first.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplifying and abstracting cloud deployments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TOSCA offers a model where you can declare how an application should be
hosted in the cloud, or in a cloud-native environment (like a Kubernetes
cluster). For instance, you might want to describe a certain document
management system, which has a web application front-end deployed on 
a farm of web application servers with a load balancer in front of it,
a backend processing system, a database system and a document storage
system. With TOSCA, you can define these structural elements with their
resource requirements.&lt;/p&gt;
&lt;p&gt;For instance, for the database system, we could declare that it has to
be a PostgreSQL database system with a certain administration password,
and within the database system we define two databases with their
own user roles:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;topology_template:
  ...
  node_templates:
    db_server:
      type: tosca.nodes.Compute
      ...
    postgresql:
      type: tosca.nodes.DBMS.PostgreSQL
      properties:
        root_password: &amp;quot;...&amp;quot;
      requirements:
        host: db_server
    db_filemeta:
      type: tosca.nodes.Database.PostgreSQL
      properties:
        name: db_filemeta
        user: fmusr
        password: &amp;quot;...&amp;quot;
      artifacts:
        db_content:
          file: files/file_server_metadata.txt
          type: tosca.artifacts.File
      requirements:
        - host: postgresql
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The parameters can, and should be further parameterized. TOSCA supports
declaring inputs that are provided upon deployment so you can safely
publicize the TOSCA structure without putting passwords in there
for instance. Furthermore, TOSCA allows you to add scripts to execute
when a resource is created, which is a common requirement for database
systems.&lt;/p&gt;
&lt;p&gt;But that's not all. Within TOSCA, you then further define the relationship
between the different systems (nodes), including connectivity requirements.
Connections can then be further aligned with virtual networks to model
the network design of the application.&lt;/p&gt;
&lt;p&gt;One of the major benefits of TOSCA is that it also provides abstraction on
the requirements. While the above example explicitly pushes for a PostgreSQL
database hosted on a specific compute server, we could also declare that we
need a database management system, or for the network part we need firewall
capabilities. The TOSCA interpreter, when mapping the model to the target
cloud environment, can then either suggest or pick the technology service
itself. The TOSCA model can then have different actions depending on the
selected technology. For the database for instance, it would have different
deployment scripts.&lt;/p&gt;
&lt;p&gt;The last major benefit that I would like to point out are the workflow
and policy capabilities of TOSCA. You can declare how for instance a 
backup process should look like, or how to cleanly stop and start the
application. You can even model how a rolling upgrade of the application
or database could be handled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is not just theoretical&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Standards can often be purely theoretical, with one or a few reference
implementations out there. That is not the case with TOSCA. While reading
up on TOSCA, it became clear that it has a strong focus on Network
Functions Virtualization (NFV), a term used to denote the shift
from appliance-driven networking capabilities towards an environment
that has a multitude of solutions running in virtual environments, and
where the infrastructure adopts to this virtualized situation with
(for network) virtual routers, firewalls, etc. Another standards body,
namely the European Telecommunications Standards Institute (ETSI), seems
to be the driving force behind the NFV architecture.&lt;/p&gt;
&lt;p&gt;TOSCA has a simple profile for NFV, which aligns with ETSI's NFV and 
ensures that TOSCA parsers that support this profile can easily be used
to set up and manage solutions in virtualized environments (and thus
also cloud environments). The amount of online information about TOSCA
with respect to the NFV side is large, although still in my opinion
strongly vendor-oriented (products that support TOSCA) and standards-oriented
(talks about how well it fits). On TOSCA's &lt;a href="https://www.oasis-open.org/tosca-implementation-stories/"&gt;implementation stories&lt;/a&gt;
page, two of the three non-vendor stories are within the telco industry.&lt;/p&gt;
&lt;p&gt;There are a few vendors that heavily promote
TOSCA: &lt;a href="https://cloudify.co/"&gt;Cloudify&lt;/a&gt; and &lt;a href="https://designer.otc-service.com"&gt;Ubicity&lt;/a&gt;
offer multi-cloud orchestrators that are TOSCA-based. Many vendors, 
including the incumbent network technology vendors like Cisco and Nokia,
embrace TOSCA NFV. But most information from practical TOSCA usage out
there is in open source solutions. The list of &lt;a href="https://github.com/oasis-open/tosca-community-contributions/wiki/Known-TOSCA-Implementations"&gt;known TOSCA implementations&lt;/a&gt;
mentions plenty of open source products. One of the solutions that I
am considering of playing around with is &lt;a href="https://turandot.puccini.cloud/"&gt;turandot&lt;/a&gt;,
which uses TOSCA to compose and orchestrate Kubernetes workloads.&lt;/p&gt;
&lt;p&gt;As an infrastructure architect, TOSCA could be a nice way of putting
initial designs into practice: after designing solutions in a language
like ArchiMate, which is in general not 'executable', the next step could
be to move the deployment specifications into TOSCA and have the next
phases of the project use and enhance the TOSCA definition. But that
easily brings me to what I consider to be shortcomings of the current
situation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inhibitors for growth potential&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are a number of issues I have with the current state of TOSCA.&lt;/p&gt;
&lt;p&gt;TOSCA's ecosystem &lt;em&gt;seems&lt;/em&gt; to be lacking sufficient visualization support.
I did come across &lt;a href="https://projects.eclipse.org/projects/soa.winery"&gt;Winery&lt;/a&gt;
but that seems to be it. I would really like to see a solution that reads
TOSCA and generates an architectural overview. For instance, for the
example I started this blog with, something like the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visualization of a deployment" src="https://blog.siphos.be/images/202106/tosca-archimate.png"&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, my impression is that TOSCA is strongly and mostly Infrastructure
as a Service (IaaS) oriented. The company I currently work for strongly
focuses on platform services, managed cloud services rather than the
more traditional infrastructure services where we would still have to do
the blunt of the management ourselves. Can TOSCA still play a role
in solutions that are fully using platform services? Perhaps the answer
here is "no", as those managed services are often very cloud-vendor specific,
but that isn't always the case, and can often also be tackled using the
abstraction and implementation specifics that TOSCA supports.&lt;/p&gt;
&lt;p&gt;I also have to rely too much on impressions. While the TOSCA ecosystem
has plenty of open source solutions out there, I find it hard to get
tangible examples: TOSCA definitions of larger-scale definitions that
not only show an initial setup, but are actively maintained to show
maintenance evolution of the solution. If TOSCA is so great for vendors
to have a cloud-independent approach, why do I find it hard to find
vendors that expose TOSCA files? If the adoption of TOSCA stops
at the standards bodies and too few vendors, then it is not likely
to grow much further.&lt;/p&gt;
&lt;p&gt;TOSCA orchestration engines often are in direct competition with
general IaC orchestration like Terraform. Cloudify has a post that
&lt;a href="https://cloudify.co/blog/terraform-vs-cloudify/"&gt;compares Terraform with their solution&lt;/a&gt;
but doesn't look into how Terraform is generally used in CI/CD
processes that join Terraform with the other services that create a
decent end-to-end orchestration for cloud deployments. For Kubernetes,
it competes with Helm and the likes - not fully, as TOSCA has other 
benefits of course, but if you compare how quickly Helm is taking
the lead in Kubernetes you can understand the struggle that TOSCA in
my opinion has.&lt;/p&gt;
&lt;p&gt;Another inhibitor is TOSCA's name. If you search for information on
TOSCA, you need to exclude &lt;a href="https://www.tricentis.com/resources/tricentis-tosca-overview/"&gt;Tricentis'&lt;/a&gt;
continuous testing platform, the &lt;a href="https://en.wikipedia.org/wiki/Tosca"&gt;1900's Opera&lt;/a&gt;,
and several other projects, films, and other entities that use the same
name. You'll need to explicitly mention OASIS and/or cloud as well if
you want to find decent articles about TOSCA, knowing well that there
can be pages out there that are missed because of it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I appreciate the value TOSCA brings, I feel that it might not grow
to its fullest potential. I hope to be wrong of course, and I would
like to see big vendors publish their reference architecture TOSCA material
so that large-scale solutions are shown to be manageable using TOSCA and
that solution architects do not need to reinvent the wheel over and
over again, as well as link architecture with the more operations
side of things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To learn more about TOSCA, there are a few resources that I would recommend
here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=tosca"&gt;OASIS TOSCA Technical Committee&lt;/a&gt;
  has a number of resources linked. The &lt;a href="https://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.3/os/TOSCA-Simple-Profile-YAML-v1.3-os.pdf"&gt;TOSCA Simple Profile in YAML Version 1.3&lt;/a&gt;
  PDF is a good read which gradually explains the structure of a TOSCA YAML
  file with plenty of examples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.etsi.org/technologies/nfv"&gt;Network Functions Virtualisation (NFV)&lt;/a&gt;
  is the ETSI site on NFV. Given the focus on NFV that I find around when
  looking at TOSCA (and is even referenced on this page) understanding what
  NFV is about clarifies a bit more how valuable TOSCA is/can be in this
  environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NHYRESmE6uA"&gt;OCB: AMA on TOSCA the Topology and Orchestration Specification for Cloud Applications - Tal Liron&lt;/a&gt;
  is an hour-long briefing that covers TOSCA not only in theory but also applies
  it in practice, and covers some of the new features that are coming up.&lt;/li&gt;
&lt;/ul&gt;</content><category term="architecture"></category><category term="cloud"></category><category term="TOSCA"></category><category term="OASIS"></category><category term="topology"></category><category term="orchestration"></category><category term="infrastructure"></category><category term="IaC"></category><category term="NFV"></category></entry><entry><title>Abstracting infrastructure complexity</title><link href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/" rel="alternate"></link><published>2020-12-25T23:00:00+01:00</published><updated>2020-12-25T23:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-12-25:/2020/12/abstracting-infrastructure-complexity/</id><summary type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Complexity is a challenging beast&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If someone were to attempt drawing out how the IT infrastructure of a larger
IT environment looks like in reality, it would soon become very, very large and
challenging to explain. Perhaps not chaotic, but definitely complicated.&lt;/p&gt;
&lt;p&gt;One of the challenges is the amount of "something" that is out there. That can
be the amount of devices you have, the amount of servers in the network, the
amount of flows going through firewalls or gateways, the amount of processes
running on a server, the amount of workstations and end user devices in use,
the amount of containers running in the container platform, the amount of cloud
platform instances that are active... &lt;/p&gt;
&lt;p&gt;The "something" can even be less tangible than the previous examples such as
the amount of projects that are being worked on in parallel or the amount of
changes that are being prepared. However, that complexity is not one I'll deal
with in this post.&lt;/p&gt;
&lt;p&gt;Another challenge is the virtualized nature of IT infrastructure, which has
a huge benefit for the organization and simplifies infrastructure services
for its own consumers, but does make it more, well, complicated to deal with.&lt;/p&gt;
&lt;p&gt;Virtual networks (vlans), virtual systems (hypervisors), virtual firewalls,
virtual applications (with support for streaming desktop applications to the
end user device without having the applications installed on that device),
virtual storage environments, etc. are all wonderful technologies which allow
for much more optimized resource usage, but does introduce a higher complexity
of the infastructure at large.&lt;/p&gt;
&lt;p&gt;To make sense of such larger structures, we start making abstractions of what
we see, structuring it in a way that we can more easily explain, assess or analyze
the environment and support changes properly. These abstract views do reflect
reality, but only to a certain extend. Not every question that can be asked can
be answered satisfactory with the same abstract view, but when it can, it is very
effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstracting service complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my day-to-day job, I'm responsible for the infrastructure of a reasonably
large environment. With "responsible" I don't want to imply that I'm the one
and only party involved of course - responsibilities are across a range of
people and roles. I am accountable for the long-term strategy on
infrastructure and the high-level infrastructure architecture and its offerings,
but how that plays out is a collaborative aspect.&lt;/p&gt;
&lt;p&gt;Because of this role, I do want to keep a close eye on all the services that
we offer from infrastructure side of things. And hence, I am often confronted
with the complexity mentioned earlier. To resolve this, I try to look at all
infastructure services in an abstract way, and document it in the same way so
that services are more easily explained.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An Archimate based view on the abstractions listed" src="https://blog.siphos.be/images/202012/abstracting-infrastructure-complexity-kvm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 - A possible visualization of the abstraction model, here in Archimate&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The abstraction I apply is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with &lt;em&gt;components&lt;/em&gt;, building blocks that are used and which refer
  to a single product or technology out there. A specific Java product can
  be considered such a component, because by itself it hardly has any value.&lt;/li&gt;
&lt;li&gt;Components are put together to create a &lt;em&gt;solution&lt;/em&gt;. This is something that
  is intended to provide value to the organization at large, and is the level
  at which something is documented, has an organizational entity responsible
  for it, etc. Solutions are not yet instantiated though. An example of a
  solution could be a Kafka-based pub/sub solution, or an OpenLDAP-based
  directory solution.&lt;/li&gt;
&lt;li&gt;Solutions are used to create &lt;em&gt;services&lt;/em&gt;. A service is something that has
  an SLA attached to it. In most cases, the same solution is used to create
  multiple services. We can think of the Kafka-based pub/sub solution that
  has three services in the organization: a regular non-production one,
  a regular production one, and a highly available production service.&lt;/li&gt;
&lt;li&gt;Services are supported through one or more &lt;em&gt;clusters&lt;/em&gt;. These are a
  way for teams to organize resources in support of a service. Some services
  might be supported by multiple clusters, for instance spread across
  different data centers. An OpenLDAP-based service might be supported by
  a single OpenLDAP cluster with native synchronization support spread across
  two data centers, or by two OpenLDAP clusters with a different
  synchronization mechanism between the two clusters.&lt;/li&gt;
&lt;li&gt;Clusters exist out of one or more &lt;em&gt;instances&lt;/em&gt;. These are the actual deployed
  technology processes that enable the cluster. In an OpenLDAP cluster, you
  could have two master processes (&lt;code&gt;slapd&lt;/code&gt; processes) running, which are the
  instances within the cluster.&lt;/li&gt;
&lt;li&gt;On top of the clusters, we enable &lt;em&gt;containers&lt;/em&gt; (I call those containers, but
  they don't have anything to do with container technology like Docker containers).
  The containers are what the consumers are actually interested in. That could
  be an organization unit in an LDAP structure, a database within an RDBMS, 
  a set of topics within a Kafka cluster, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are the basic abstractions I apply for most of the technologies, allowing
me to easily make a good view on the environment. Let's look at a few examples
here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: Virtualization of Wintel systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a large, virtualized environment, you generally have a specific hypervisor
software being used: be it RHV (Red Hat Virtualization) based upon
KVM, Microsoft HyperV, VMWare vSphere or something else - the technology used
is generally well known. That's one of the components being used, but that is
far from the only component.&lt;/p&gt;
&lt;p&gt;To better manage the virtualized environment the administration teams might
use an orchestration engine like Ansible, Puppet or Saltstack. They might also
have a component in use for automatically managing certificates and what not.&lt;/p&gt;
&lt;p&gt;All these components are needed to build a full virtualization solution. For
me, as an architect, knowing which components are used is useful for things
like lifecycle management (which components are EOL, which components can be
easily replaced with a different one versus components that are more lock-in
oriented, etc.) or inventory management (which component is deployed where,
which version is used), which supports things like vulnerability management
(if we can map components to their Common Platform Enumeration (CPE) then
we can easily see which vulnerabilities are reported through the Common
Vulnerabilities and Exposure (CVE) reports).&lt;/p&gt;
&lt;p&gt;The interaction between all these components creates a sensible solution,
which is the virtualization solution. At this level, I'm mostly interested
in the solution roadmap, the responsibilities and documentation associated
with it, the costs, maturity of the offering within the organization, etc.
It is also on the solution level that most architectural designs are made,
and the best practices (and malpractices) are documented.&lt;/p&gt;
&lt;p&gt;The virtualization solution itself is then instantiated within the
organization to create one or more services. These could be different
services based on the environment (a lab/sandbox virtualization service
with low to no SLA, a non-production one with standard SLA, a non-production
one with specific disaster recovery requirements, a production one with
standard SLA (and standard disaster recovery requirements), a high-performance
production one, etc.&lt;/p&gt;
&lt;p&gt;These services are mostly important for other architects, project leads
or other stakeholders that are going to make active use of the virtualization
services - the different services (which one could document as "service
plans") make it more obvious on what the offering is, and what differentiation
is supported.&lt;/p&gt;
&lt;p&gt;Let's consider a production, standard SLA virtualization service. The
system administrators of the virtualization environment might enable this
service across multiple clusters. This could be for several reasons: this
could be due to limits (maximum number of hosts per cluster), or because
of particular resource requirements (different CPU architecture requirements
- yes even with virtualization this is still a thing), or to make
things manageable for the administrators in general.&lt;/p&gt;
&lt;p&gt;While knowing which cluster an application is on is, in general, not
that important, it can be very important when there are problems, or when
limits are being reached. As an architect, I'm definitely interested in
knowing why multiple clusters are made (what is the reasoning behind it) as
it gives a good view on what the administrators are generally dealing with.&lt;/p&gt;
&lt;p&gt;Within a cluster (to support the virtualization) you'll find multiple hosts.
Often, a cluster is sized to be able to deal with one or two host fall-outs
so that the virtual machines (which are hosted on the cluster - these are
the "containers" that I spoke of) can be migrated to another host with only
a short downtime as a consequence (if their main host crashed) or no downtime
at all (if it is scheduled maintenance of the host). These hosts are the
instances of the cluster.&lt;/p&gt;
&lt;p&gt;By using this abstraction, I can "map" the virtualization environment in
a way that I have a good enough view, without proclaiming to be anything
more than an informed architect, on this setup to support my own work,
and to be able to advice management on major investment requirements,
challenges, strategic evolutions and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More than just documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the above method is used for documenting the environment in which
I work (and which works well for the size of the environment I have to deal
with), it can be used for simplifying management of the technologies as
well. This level of abstraction can easily be used in environments that
push self-servicing forward.&lt;/p&gt;
&lt;p&gt;Let's take the &lt;a href="https://www.openservicebrokerapi.org/"&gt;Open Service Broker API&lt;/a&gt;
as an example. This is an API that defines how to expose (infrastructure)
services to consumers that can easily create (provision) and destroy 
(deprovision) their own services. Brokers that support the API will
then automatically handle the service management. This model can easily
be put in to support the previous abstraction.&lt;/p&gt;
&lt;p&gt;Take the virtualization environment again. If we want to enable self-servicing
on a virtualized environment, we can think of an offering where internal customers
can create new virtual machines (provision) either based on a company-vetted
template, or through an image (like with virtual appliances). The team that
manages the virtualization environment has a number of services, which they
describe in the service plans exposed by the API. An internal customer, when
privisioning a virtual machine, is thus creating a "container" for the right
service (based on their selected service plan) and on the right cluster
(based upon the parameters that the internal customer passes along with the
creation of its machine).&lt;/p&gt;
&lt;p&gt;We can do the same with databases: a certain database solution (say PostgreSQL)
has its own offerings (exposed through service plans linked to the service), and
internal customers can create their own database ("container") on the right
cluster through this API.&lt;/p&gt;
&lt;p&gt;I personally have a few scripts that I use at home myself to quickly set
up a certain technology, using the above abstraction level as the foundation.
Rather than having to try and remember how to set up a multi-master OpenLDAP
service, or a replicated Kafka setup, I have scripts that create this based
upon this abstraction: the script parameters always use the service, cluster,
instance and container terminology and underlyingly map this to the
technology-specific approach.&lt;/p&gt;
&lt;p&gt;It is my intention to also promote this abstraction usage within my
work environment, as I believe it allows us to more easily explain what
all the infrastructure is used for, but also to more easily get new employees
known to our environment. But even if that isn't reached, the abstraction is
a huge help for me to assess and understand the multitude of technologies
that are out there, be it our mainframe setup, the SAN offerings, the
network switching setup, the databases, messaging services, cloud
landing zones, firewall setups, container platforms and more.&lt;/p&gt;</content><category term="infrastructure"></category><category term="archimate"></category></entry></feed>