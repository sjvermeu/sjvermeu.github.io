<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art... - performance</title><link href="http://blog.siphos.be/" rel="alternate"></link><link href="http://blog.siphos.be/tag/performance/feed/atom.xml" rel="self"></link><id>http://blog.siphos.be/</id><updated>2013-04-19T16:22:00+02:00</updated><entry><title>Comparing performance with sysbench: performance analysis</title><link href="http://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-3/" rel="alternate"></link><published>2013-04-19T16:22:00+02:00</published><updated>2013-04-19T16:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-3/</id><summary type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with
the &lt;em&gt;memory&lt;/em&gt; workload test. Let's view this again, using parameters more
optimized to view the regressions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=32M --memory-block-size=64 run
Host:
  Operations performed: 524288 (2988653.44 ops/sec)
  32.00 MB transferred (182.41 MB/sec)

Guest:
  Operations performed: 524288 (24920.74 ops/sec)
  32.00 MB transferred (1.52 MB/sec)

$ sysbench --test=memory --memory-total-size=32M --memory-block-size=32M run
Host:
  Operations performed: 1 (  116.36 ops/sec)
  32.00 MB transferred (3723.36 MB/sec)

Guest:
  Operations performed: 1 (   89.27 ops/sec)
  32.00 MB transferred (2856.77 MB/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From looking at the code (gotta love Gentoo for making this obvious ;-)
we know that the memory workload, with a single thread, does something
like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;total_bytes = 0;
repeat until total_bytes &amp;gt;= memory-total-size:
  thread_mutex_lock()
  total_bytes += memory-block-size
  thread_mutex_unlock()

  (start event timer)
  pointer -&amp;gt; buffer;
  while pointer &amp;lt;-&amp;gt; end-of(buffer)
    write somevalue at pointer
    pointer++
  (stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Given that the regression is most noticeable when the memory-block-size
is very small, the part of the code whose execution count is much
different between the two runs is the mutex locking, global memory
increment and the start/stop of event timer.&lt;/p&gt;
&lt;p&gt;In a second phase, we also saw that mutex locking itself is not
impacted. In the above case, we have 524288 executions. However, if we
run the mutex workload this number of times, we see that this hardly has
any effect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=524288 --mutex-loops=0 run
Host:      total time:        0.0275s
Guest:     total time:        0.0286s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code for the mutex workload, knowing that we run with one thread,
looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mutex_locks = 524288
(start event timer)
do
  lock = get_mutex()
  thread_mutex_lock()
  global_var++
  thread_mutex_unlock()
  mutex_locks--
until mutex_locks = 0;
(stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To check if the timer might be the culprit, let's look for a benchmark
that mainly does timer checks. The &lt;em&gt;cpu&lt;/em&gt; workload can be used, when we
tell sysbench that the prime to check is 3 (as its internal loop runs
from 3 till the given number, and when the given number is 3 it skips
the loop completely) and we ask for 524288 executions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=3 --max-requests=524288 run
Host:  total time:  0.1640s
Guest: total time: 21.0306s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Gotcha! Now, the event timer (again from looking at the code) contains
two parts: getting the current time (using &lt;code&gt;clock_gettime()&lt;/code&gt;) and
logging the start/stop (which is done in memory structures). Let's make
a small test application that gets the current time (using the real-time
clock as the sysbench application does) and see if we get similar
results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cat test.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;time.h&amp;gt;

int main(int argc, char **argv, char **arge) {
  struct timespec tps;
  long int i = 524288;
  while (i-- &amp;gt; 0)
    clock_gettime(CLOCK_REALTIME, &amp;amp;tps);
}

$ gcc -lrt -o test test.c
$ time ./test
Host:  0m0.019s
Guest: 0m5.030s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So given that the &lt;code&gt;clock_gettime()&lt;/code&gt; is ran twice in the sysbench, we
already have 10 seconds of overhead on the guest (and only 0,04s on the
host). When such time-related functions are slow, it is wise to take a
look at the clock source configured on the system. On Linux, you can
check this by looking at &lt;code&gt;/sys/devices/system/clocksource/*&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cd /sys/devices/system/clocksource/clocksource0
# cat current_clocksource
kvm-clock
# cat available_clocksource
kvm-clock tsc hpet acpi_pm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Although kvm-clock is supposed to be the best clock source, let's switch
to the tsc clock:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# echo tsc &amp;gt; current_clocksource
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we rerun our test application, we get a much more appreciative
result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ time ./test
Host:  0m0.019s
Guest: 0m0.024s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, what does that mean for our previous benchmark results?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
Host:            35,3049 sec
Guest (before):  36,5582 sec
Guest (now):     35,6416 sec

$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
Host:            1,8424 MB/sec
Guest (before):  1,5591 MB/sec
Guest (now):     1,5912 MB/sec

$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host:            3959,78 MB/sec
Guest (before)   3079,29 MB/sec
Guest (now):     3821,89 MB/sec

$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:            9765 executions
Guest (before):   512 executions
Guest (now):      529 executions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So we notice that this small change has nice effects on some of the
tests. The CPU benchmark improves from 3,55% overhead to 0,95%; fileio
is the same (from 15,38% to 13,63%), memory improves from 22,24%
overhead to 3,48% and threads remains about status quo (from 94,76%
slower to 94,58%).&lt;/p&gt;
&lt;p&gt;That doesn't mean that the VM is now suddenly faster or better than
before - what we changed was how fast a certain time measurement takes,
which the benchmark software itself uses rigorously. This goes to show
how important it is to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;understand fully how the benchmark software works and measures&lt;/li&gt;
&lt;li&gt;realize the importance of access to source code is not to be
    misunderstood&lt;/li&gt;
&lt;li&gt;know that performance benchmarks give figures, but do not tell you
    how your users will experience the system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it for the sysbench benchmark for now (the MySQL part will need
to wait until a later stage).&lt;/p&gt;</content><category term="Free Software"></category><category term="performance"></category><category term="sysbench"></category></entry><entry><title>Comparing performance with sysbench: memory, threads and mutexes</title><link href="http://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-2/" rel="alternate"></link><published>2013-04-19T04:11:00+02:00</published><updated>2013-04-19T04:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-2/</id><summary type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each execution will read or write to this memory
(&lt;em&gt;--memory-oper&lt;/em&gt;, defaults to write) in a random or sequential manner
(&lt;em&gt;--memory-access-mode&lt;/em&gt;, defaults to &lt;strong&gt;seq&lt;/strong&gt;uential).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host throughput, 1M:  3959,78 MB/sec
Guest throughput, 1M: 3079,29 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The guest has a lower throughput (about 77% of the host), which is lower
than what most online posts provide on KVM performance. We'll get back
to that later. Let's look at the default block size of 1k (meaning that
the benchmark will do a lot more executions before it reaches the total
memory (in load):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=1G run
Host throughput, 1k:  1702,59 MB/sec
Guest throughput, 1k:   23,67 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is a lot worse: the guest' throughput is only 1,4% of the host
throughput! The &lt;code&gt;qemu-kvm&lt;/code&gt; process on the host is also taking up a lot
of CPU.&lt;/p&gt;
&lt;p&gt;Now let's take a look at the other workload, &lt;em&gt;threads&lt;/em&gt;. In this
particular workload, you identify the number of threads
(&lt;em&gt;--num-threads&lt;/em&gt;), the number of locks (&lt;em&gt;--thread-locks&lt;/em&gt;) and the number
of times a thread should run its 'lock-yield..unlock' workload
(&lt;em&gt;--thread-yields&lt;/em&gt;). The more locks you identify, the less number of
threads will have the same lock (each thread is allocated a single lock
during an execution, but every new execution will give it a new lock so
the threads do not always take the same lock).&lt;/p&gt;
&lt;p&gt;Note that parts of this is also handled by the other tests: mutex'es are
used when a new operation (execution) for the thread is prepared. In
case of the memory-related workload above, the smaller the buffer size,
the more frequent thread operations are needed. In the last run we did
(with the bad performance), millions of operations were executed
(although no yields were performed). Something similar can be simulated
using a single lock, single thread and a very high number of operations
and no yields:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=1 --thread-yields=0 --max-requests=1000000 --thread-locks=1 run
Host runtime:    0,3267 s  (event:    0,2278)
Guest runtime:  40,7672 s  (event:   30,6084)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This means that the guest "throughput" problems from the memory
identified above seem to be related to this rather than memory-specific
regressions. To verify if the scheduler itself also shows regressions,
we can run more threads concurrently. For instance, running 128 threads
simultaneously, using the otherwise default settings, during 10 seconds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:   9765 executions (events)
Guest:   512 executions (events)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we get only 5% throughput.&lt;/p&gt;
&lt;p&gt;Let's focus on the mutex again, as sysbench has an additional mutex
workload test. The workload has each thread running a local fast loop
(simple increments, &lt;em&gt;--mutex-loops&lt;/em&gt;) after which it takes a random mutex
(one of &lt;em&gt;--mutex-num&lt;/em&gt;), locks it, increments a global variable and then
releases the mutex again. This is repeated for the number of locks
identified (&lt;em&gt;--mutex-locks&lt;/em&gt;). If mutex operations would be the cause of
the performance issues above, then we would notice that the mutex
operations are a major performance regression on my system.&lt;/p&gt;
&lt;p&gt;Let's run that workload with a single thread (default), no loops and a
single mutex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=50000000 --mutex-loops=1 run
Host (duration):   2600,57 ms
Guest (duration):  2571,44 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this example, we see that the mutex operations are almost at the same
speed (99%) of the host, so pure mutex operations are not likely to be
the cause of the performance regressions earlier on. So what does give
the performance problems? Well, that investigation will be for the third
and last post in this series ;-)&lt;/p&gt;</content><category term="Free Software"></category><category term="memory"></category><category term="mutex"></category><category term="performance"></category><category term="sysbench"></category><category term="threading"></category><category term="threads"></category></entry><entry><title>Comparing performance with sysbench: cpu and fileio</title><link href="http://blog.siphos.be/2013/04/comparing-performance-with-sysbench/" rel="alternate"></link><published>2013-04-18T21:31:00+02:00</published><updated>2013-04-18T21:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-18:/2013/04/comparing-performance-with-sysbench/</id><summary type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out what one can do to compare system (and later application)
performance, so I decided to take a look at the various benchmark
utilities available. In this first post, I'll take a look at
&lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; (using 0.4.12, released on March 2009
- unlike what you would think from the looks of the site alone) to
compare the performance of my KVM guest versus host.&lt;/p&gt;
&lt;p&gt;The obligatory system information: the host is a HP Pavilion dv7 3160eb
with an Intel Core i5-430M processor (dual-core with 2 threads per
core). Frequency scaling is disabled - the CPU is fixed at 2.13 Ghz. The
system has 4Gb of memory (DDR3), the internal hard disks are configured
as a software RAID1 and with LVM on top (except for the file system that
hosts the virtual guest images, which is a plain software RAID1). The
guests run with the boot options given below, meaning 1.5Gb of memory, 2
virtual CPUs of the KVM64 type. The CFLAGS for both are given below as
well, together with the expanded set given by &lt;strong&gt;gcc \${CFLAGS} -E -v -
&lt;/dev&gt;&amp;amp;1 | grep cc1&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/bin/qemu-kvm -monitor stdio -nographic -gdb tcp::1301   
 -vnc 127.0.0.1:14   
 -net nic,model=virtio,macaddr=00:11:22:33:44:b3,vlan=0   
 -net vde,vlan=0   
 -drive file=/srv/virt/gentoo/test/pg1.img,if=virtio,cache=none   
 -k nl-be -m 1536 -cpu kvm64 -smp 2

# For host
CFLAGS=&amp;quot;-march=core2 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-D_FORTIFY_SOURCE=2 -fno-strict-overflow -march=core2   
        -fPIE -O2 -fstack-protector-all&amp;quot;
# For guest
CFLAGS=&amp;quot;-march=x86-64 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-fno-strict-overflow -march=x86-64 -fPIE -O2   
        -fstack-protector-all&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I am aware that the CFLAGS between the two are not the same (duh), and I
know as well that the expansion given above isn't entirely accurate. But
still, it gives some idea on the differences.&lt;/p&gt;
&lt;p&gt;Now before I go on to the results, please keep in mind that I am &lt;em&gt;not a
performance expert&lt;/em&gt;, not even a &lt;em&gt;performance experienced&lt;/em&gt; or even
&lt;em&gt;performance wanna-be experienced&lt;/em&gt; person: the more I learn about the
inner workings of an operating system such as Linux, the more complex it
becomes. And when you throw in additional layers such as virtualization,
I'm almost completely lost. In my day-job, some people think they can
"prove" the inefficiency of a hypervisor by counting from 1 to 100'000
and adding the numbers, and then take a look at how long this takes. I
think this is short-sighted, as this puts load on a system that does not
simulate reality. If you really want to do performance measures for
particular workloads, you need to run those workloads and not some small
script you hacked up. That is why I tend to focus on applications that
use workload simulations for infrastructural performance measurements
(like &lt;a href="http://hammerora.sf.net"&gt;HammerDB&lt;/a&gt; for performance testing
databases). But for this blog post series, I'm first going to start with
basic operations and later posts will go into more detail for particular
workloads (such as database performance measurements).&lt;/p&gt;
&lt;p&gt;Oh, and BTW, when I display figures with a comma (","), that comma means
decimal (so "1,00" = "1").&lt;/p&gt;
&lt;p&gt;The figures below are numbers that can be interpreted in many ways, and
can prove everything. I'll sometimes give my interpretation to it, but
don't expect to learn much from it - there are probably much better
guides out there for this. The posts are more of a way to describe how
&lt;strong&gt;sysbench&lt;/strong&gt; works and what you should take into account when doing
performance benchmarks.&lt;/p&gt;
&lt;p&gt;So the testing is done using &lt;strong&gt;sysbench&lt;/strong&gt;, which is capable of running
CPU, I/O, memory, threading, mutex and MySQL tests. The first run of it
that I did was a single-thread run for CPU performance testing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This test verifies prime numbers by dividing the number with
sequentially increasing numbers and verifying that the remainder (modulo
calculation) is zero. If it is, then the number is not prime and the
calculation goes on to the next number; otherwise, if none have a
remainder of 0, then the number is prime. The maximum number that it
divides by is calculated by taking the integer part of the square root
of the number (so for 17, this is 4). This algorithm is very simple, so
you should also take into account that during the compilation of the
benchmark, the compiler might already have optimized some of it.&lt;/p&gt;
&lt;p&gt;Let's look at the numbers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   35,4331   37,0528
     e.total   35,4312   36,8917
1.2    total   35,1482   36,1951
     e.total   35,1462   36,0405
1.3    total   35,3334   36,4266
     e.total   35,3314   36,2640
================================
avg    total   35,3049   36,5582
     e.total   35,3029   36,3987
med    total   35,3334   36,4266
     e.total   35,3314   36,2640
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On average (I did three runs on each system), the guest took 3,55% more
time to finish the test than the host (&lt;code&gt;total&lt;/code&gt;). If we look at the pure
calculation (so not the remaining overhead of the inner workings -
&lt;code&gt;e.total&lt;/code&gt;) then the guest took 3,10% more time. The median however (the
run that wasn't the fastest nor the slowest of the three) has the guest
taking 3,09% more time (total) and 2,64% more time (e.total).&lt;/p&gt;
&lt;p&gt;Let's look at the two-thread results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   17,5185   18,0905
     e.total   35,0296   36,0217
1.2    total   17,8084   18,1070
     e.total   35,6131   36,0518
1.3    total   18,0683   18,0921
     e.total   36,1322   36,0194
================================
avg    total   17,5185   18,0965
     e.total   35,0296   36,0310
med    total   17,8084   18,0921
     e.total   35,6131   36,0194
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With these figures, we notice that the guest average total run time
takes 1,67% more time to complete, and the event time only 1,23%. I was
personally expecting that the guest would have a higher percentage than
previously (gut feeling - never trust it when dealing with complex
matter) but was happy to see that the difference wasn't higher. I'm not
going to start analyze this in more detail and just go to the next test:
fileio.&lt;/p&gt;
&lt;p&gt;In case of fileio testing, I assume that the hypervisor will take up
&lt;a href="http://www.linux-kvm.org/page/Virtio/Block/Latency"&gt;more overhead&lt;/a&gt;, but
keep in mind that you also need to consider the environmental factors:
LVM or not, RAID1 or not, mount options, etc. Since I am comparing
guests versus hosts here, I should look for a somewhat comparable setup.
Hence, I will look for the performance of the host (software raid, LVM,
ext4 file system with data=ordered) and the guest (images on software
raid, ext4 file system with data=ordered and barrier=0, and LVM in
guest).&lt;/p&gt;
&lt;p&gt;Furthermore, running a sysbench test suggests a file that is much larger
than the available RAM. I'm going to run the tests on a 6Gb file size,
but enable O_DIRECT for writes so that some caches (page cache) are not
used. This can be done using &lt;em&gt;--file-extra-flags=direct&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As with all I/O-related benchmarks, you need to define which kind of
load you want to test with. Are the I/Os sequential (like reading or
writing a large file completely) or random? For databases, you are most
likely interested in random reads (data, for selects) and sequential
writes (into transaction logs). A file server usually has random
read/write. In the below test, I'll use a combined &lt;strong&gt;r&lt;/strong&gt;a&lt;strong&gt;nd&lt;/strong&gt;om
&lt;strong&gt;r&lt;/strong&gt;ead/&lt;strong&gt;w&lt;/strong&gt;rite.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=fileio --file-total-size=6G prepare
$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
$ sysbench --test=fileio --file-total-size=6G cleanup
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the output, the throughput seems to be most important:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Operations performed:  4348 Read, 2898 Write, 9216 Other = 16462 Total
Read 67.938Mb  Written 45.281Mb  Total transferred 113.22Mb  (1.8869Mb/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the above case, the throughput is 1,8869 Mbps. So let's look at the
(averaged) results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Host:  1,8424 Mbps
Guest: 1,5591 Mbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above figures (which are an average of 3 runs) tell us that the
guest has a throughput of about 84,75% (so we take about 15% performance
hit on random read/write I/O). Now I used sysbench here for some I/O
validation of guest between host, but other usages apply as well. For
instance, let's look at the impact of &lt;code&gt;data=ordered&lt;/code&gt; versus
&lt;code&gt;data=journal&lt;/code&gt; (taken on the host):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;6G, data=ordered, barrier=1: 1,8435 Mbps
6G, data=ordered, barrier=0: 2,1328 Mbps
6G, data=journal, barrier=1: 599,85 Kbps
6G, data=journal, barrier=0: 767,93 Kbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the figures, we can see that the &lt;code&gt;data=journal&lt;/code&gt; option slows down
the throughput to a final figure about 30% of the original throughput
(70% decrease!). Also, disabling barriers has a positive impact on
performance, giving about 15% throughput gain. This is also why some
people report performance improvements when switching to LVM, as - as
far as I can tell (but finding a good source on this is difficult) - LVM
&lt;em&gt;by default&lt;/em&gt; disables barriers (but does honor the &lt;code&gt;barrier=1&lt;/code&gt; mount
option if you provide it).&lt;/p&gt;
&lt;p&gt;That's about it for now - the next post will be about the memory and
threads tests within sysbench.&lt;/p&gt;</content><category term="Free Software"></category><category term="cpu"></category><category term="hypervisor"></category><category term="io"></category><category term="kvm"></category><category term="performance"></category><category term="sysbench"></category></entry></feed>