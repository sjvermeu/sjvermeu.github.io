<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art... - virtualization</title><link href="https://blog.siphos.be/" rel="alternate"></link><link href="https://blog.siphos.be/tag/virtualization/feed/atom.xml" rel="self"></link><id>https://blog.siphos.be/</id><updated>2021-06-03T10:10:00+02:00</updated><entry><title>Virtualization vs abstraction</title><link href="https://blog.siphos.be/2021/06/virtualization-vs-abstraction/" rel="alternate"></link><published>2021-06-03T10:10:00+02:00</published><updated>2021-06-03T10:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-03:/2021/06/virtualization-vs-abstraction/</id><summary type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Abstraction: common and simplified interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The term "abstraction" has slightly different connotations based on the
context in which the term is used. Generally speaking, an abstraction
provides a less detailed view on an object and shows the intrinsic qualities
upon which one looks at that object. Let's say we have a PostgreSQL database
and a MariaDB database. An abstract view on it could find that it has a lot
of commonalities, such as tabular representation of data, a network-facing
interface through which their database clients can interact with the
database, etc.&lt;/p&gt;
&lt;p&gt;We then further generalize this abstraction to come to the generalized
"relational database management system" concept. Furthermore, rather than
focusing on the database-specific languages of the PostgreSQL database and
the MariaDB database (i.e. the commands that database clients send to the
database), we abstract the details that are not shared across the two, and
create a more common set of commands that both databases support.&lt;/p&gt;
&lt;p&gt;Once you standardize on this common set of commands, you get more freedom in
exchanging one database technology for the other. This is exactly what
happened several dozen years ago, and resulted in the SQL standard
(ISO/IEC 9075). This standard is a language that, if all your relational
database technologies support it, allows you - as an organization - to work
with a multitude of database technologies while still having a more efficient
and standardized way of dealing with it.&lt;/p&gt;
&lt;p&gt;Now, the SQL language standard is one example. IT is filled with many other
examples, some more formally defined as standards than others. Let's look at
a more recent example, within the area of application containerization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRI and OCI are abstraction implementations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the Docker project, now supported through the Docker company, started
with enabling application containerization in a more efficient way, it leaned
upon the capabilities that the Linux kernel offered on hiding information and
isolating resources (namespaces and control groups) and expanded on it to
make it user friendly. It was an immediate hit, and has since then resulted
in a very competitive market.&lt;/p&gt;
&lt;p&gt;With Docker, applications could be put in more isolated environments and run
in parallel on the same system, without these applications seeing the other
ones. Each application has its own, private view on the system. With these
containers, the most important service that is still shared is the kernel,
with the kernel offering only those services to the containers that it can
keep isolated.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Container runtime abstraction" src="https://blog.siphos.be/images/202106/container-runtimes.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/"&gt;https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, while Docker can be easily attributed to bringing this to the wider
public, other initiatives quickly followed suit. Multiple container
technologies were coming to life, and started to bid for a place in the
containerization market. To be able to compete here, many of these attempted
to use the same interfaces (be it system calls, commands or other) as Docker
used, so the users can more easily switch. But while trying to copy and
implement the same interfaces is a possible venue, it is still strongly
controlled by the evolution that Docker is taking.&lt;/p&gt;
&lt;p&gt;Since then, larger projects like Kubernetes have started introducing an
abstraction between the container runtime (which implements the actual
containerization) and the container definitions and management (which uses
the containerization). Within Kubernetes for instance, this is through the
Common Runtime Interface (CRI), and the Open Container Interface (OCI) is
used to link the container runtime management with the underlying container
technologies.&lt;/p&gt;
&lt;p&gt;Introducing such an abstraction is a common way to establish a bit more
foothold in the market. Rather than trying to copy the market leader
verbatim, you create an intermediate layer, with immediate implementation
for the market leader as well, but with the promise that anyone that uses
the intermediate layer will be less tied to a single vendor or project: it
abstracts that vendor or project specifics away and shows mainly the
intrinsic qualities needed.&lt;/p&gt;
&lt;p&gt;If that abstraction is successful, other implementations for this abstraction
layer can easily come in and replace the previous technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstraction is not virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The introduction of abstraction layers, abstract technologies or abstract
languages should not be misunderstood for virtualization. Abstraction does
not hide or differently represent the resources beneath. It does not represent
itself as something else, but merely leaves out details that might make
interactions with the different technologies more complex.&lt;/p&gt;
&lt;p&gt;Virtualization on the other hand takes a different view. Rather than removing
the specific details, it represents a resource as something that it isn't
(or isn't completely). Hypervisors like KVM create a virtual hardware view,
and translates whatever calls towards the virtual hardware into calls to the
actual hardware - sometimes to the same type of hardware, but often towards
the CPU or resources that simulate the virtualized hardware further.&lt;/p&gt;
&lt;p&gt;Abstraction is a bit like classification, and defining how to work with a
resource through the agreed upon interfaces for that class. If you plug in
a USB device like a USB stick or USB keyboard or mouse, operating systems
will be able to interact with it regardless of its vendor and product,
because it uses the abstraction offered by the device classes: the USB mass
storage device class for the USB stick, or the USB human interface device
class for the keyboard and mouse. It abstracts away the complexity of
dealing with multiple implementations, but the devices themselves still
need to be classified as such.&lt;/p&gt;
&lt;p&gt;On hypervisors, you can create a virtual USB stick which in reality is just
a file on the hypervisor host or on a network file share. The hypervisor
virtualizes the view towards this file as if it is a USB stick, but in reality
there is no USB involved at all. Again, this doesn't have to be the case,
the hypervisor might as well enable virtualization of the USB device and
still eventually interact with an actual USB device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLANs are virtualized networks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another example of a virtualization is network virtualization through the
use of VLANs. In a virtual local area network (VLAN), all systems that
interact with this VLAN will see each other on the network as if they are
part of the same broadcast domain. Well, they are part of the same broadcast
domain. But if you look at the physical network implementation, this does not
imply that all these systems are attached to the same switch, and that no
routers are put in between to facilitate the communication.&lt;/p&gt;
&lt;p&gt;In larger enterprises, the use of VLANs is ubiquitous. Network virtualization
enables the telco teams and departments to optimize the actual physical
network without continuously impacting the configurations and deployments of
the services that use the network. Teams can create hundreds or thousands of
such VLANs while keeping the actual hardware investments under control, and
even be able to change and manage the network without impacting services.&lt;/p&gt;
&lt;p&gt;This benefit is strongly tied to virtualization, as we see the same in
hardware virtualization for server and workstation resources. By offering
virtualized systems, the underlying hardware can be changed, replaced or
switched without impact on the actual software that is running within the
virtualized environment. Well, mostly without impact, because not all
virtualization technologies or implementations are creating a full
virtualized view - sometimes shortcuts are created to improve efficiency
and performance. But in general, it works Just Fine (tm).&lt;/p&gt;
&lt;p&gt;Resource optimization and consolidation is easily accomplished when using
virtualization. You need far fewer switches in a virtualized network, and
you need far fewer servers for a virtualized server farm. But, it does come
at a cost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Virtualization introduces different complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you introduce a virtualization layer, be it for network, storage,
hardware or application runtimes, you introduce a layer that needs to be
actively managed. Abstraction is often much less resource intensive, as it
is a way to simplify the view on the actual resources while still being 100%
aligned with those underlying resources. Virtualization means that you need
to manage the virtualized resources, and keep track of how these resources
map to the actual underlying resources.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vSphere services" src="https://blog.siphos.be/images/202106/vsphere.png"&gt;
&lt;em&gt;Source: &lt;a href="https://virtualgyaan.com/vmkernel-components-and-functionality/"&gt;https://virtualgyaan.com/vmkernel-components-and-functionality/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's look at virtualized hardware for servers. On top of it, you have to
run and maintain the hypervisor, which represents the virtual hardware to
the operating systems. Within those (virtually running) operating systems,
you have a virtual view on resources: CPU, memory, etc. The sum of all
(virtual) CPUs is often not the same as the sum of all (actual) CPUs
(depending on configuration of course), and in larger environments the
virtual operating systems might not even be running on the same hardware
as they did a few hours ago, even though the system has not been restarted.&lt;/p&gt;
&lt;p&gt;Doing performance analysis implies looking at the resources within (virtual)
as well as mapped on the actual resources, which might not be of the same
type. A virtual GPU representation might be mapped to an actual GPU (and if
you want performance, I hope it is) but doesn't have to be. I've done
investigations on a virtual Trusted Platform Module (TPM) within a virtual
system running on a server that didn't have a TPM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assessing which standardization to approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I'm confronted with an increase in IT complexity, I will often be
looking at a certain degree of standardization to facilitate this in the
organization. But what type of standardization to approach depends strongly
on the situation.&lt;/p&gt;
&lt;p&gt;Standardization by rationalization is often triggered by cost optimization
or knowledge optimization. An organization that has ten different relational
database technologies in use could benefit of a rationalization in the number
of technologies to support. However, unless there is also sufficient
abstraction put in place, this rationalization can be intensive. Another
rationalization example could be on public cloud, where an organization
chooses to only focus on a single or two cloud providers but not more.&lt;/p&gt;
&lt;p&gt;While rationalization is easy to understand and explain, it does have adverse
consequences: you miss the benefits of whatever you're rationalized away,
and unless another type of standardization is put in place, it will be hard
to switch later on if the rationalization was ill-advised or picked the
wrong targets to rationalize towards.&lt;/p&gt;
&lt;p&gt;Standardization by abstraction focuses more on simplification. You are
introducing technologies that might have better interoperability through
this abstraction, but this can only be successful if the abstraction is
comprehensive enough to still use the underlying resources in an optimal
manner.&lt;/p&gt;
&lt;p&gt;My own observation on abstraction is that it is not commonly accepted by
engineers and developers at face value. It requires much more communication
and explanation than rationalization, which is often easy to put under "cost
pressure". Abstraction focuses on efficiency in a different way, and thus
requires different communication. At the company I currently work for,
we've introduced the Open Service Broker (OSB) API as an abstraction for
service instantiation and service catalogs, and after even more than a
year, including management support, it is still a common endeavor to
explain and motivate why we chose this.&lt;/p&gt;
&lt;p&gt;Virtualization creates a highly efficient environment and supports resource
optimizations that aren't possible in other ways. Its benefits are much easier
to explain to the organization (and to management), but has a downside that
is often neglected: it introduces complexity. Hence, virtualization should
only be pursued if you can manage the complexity, and that it isn't much
worse than the complexity you want to remove. Virtualization requires
organizational support which is more platform-oriented (and thus might be
further away from the immediate 'business value' IT often has to explain),
in effect creating a new type of technology within the ever increasing
catalog of IT services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software-defined infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While virtualization has been going around in IT for quite some time (before
I was born), a new kid on the block is becoming very popular: software-defined
infrastructure. The likes of Software Defined Network (SDN), Compute (SDC) and
Storage (SDS) are already common or becoming common. Other implementations,
like the Software Defined Perimeter, are getting jabbed by vendors as well.&lt;/p&gt;
&lt;p&gt;Now, SDI is not its own type of standardization. It is a way of managing
resources through code, and thus is a way of abstracting the infrastructure.
But unlike using a technology-agnostic abstraction, it pulls you into a
vendor-defined abstraction. That has its pros and cons, and as an architect
it is important to consider how to approach infrastructure-as-code, as SDI
implementations are not the only way to accomplish this.&lt;/p&gt;
&lt;p&gt;Furthermore, SDI does not imply virtualization. Certainly, if a technology
is virtualized, then SDI will also easily interact with it, and help you
define and manage the virtualized infrastructure as well as its underlay
infrastructure. But virtualization isn't a prerequisite for SDI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you're confronted with chaos and complexity, don't immediately start
removing technologies under the purview of "rationalization". Consider your
options on abstraction and virtualization, but be aware of the pros and cons
of each.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="virtualization"></category><category term="abstraction"></category></entry><entry><title>Enabling Kernel Samepage Merging (KSM)</title><link href="https://blog.siphos.be/2013/05/enabling-kernel-samepage-merging-ksm/" rel="alternate"></link><published>2013-05-09T03:50:00+02:00</published><updated>2013-05-09T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-09:/2013/05/enabling-kernel-samepage-merging-ksm/</id><summary type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The first thing to enable is KSM or &lt;em&gt;Kernel
Samepage Merging&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This Linux feature looks for memory pages that the applications have
marked as being a possible candidate for optimization (sharing) which
are then reused across multiple processes. The idea is that, especially
for virtualized environments (but KSM is not limited to that), some
processes will have the same contents in memory. Without any sharing
abilities, these memory pages will be unique (meaning at different
locations in your system's memory). With KSM, such memory pages are
consolidated to a single page which is then referred to by the various
processes. When one process wants to modify the page, it is "unshared"
so that there is no corruption or unwanted modification of data for the
other processes.&lt;/p&gt;
&lt;p&gt;Such features are not new - VMWare has it named TPS (&lt;em&gt;Transparent Page
Sharing&lt;/em&gt;) and Xen calls it "Memory CoW" (Copy-on-Write). One advantage
of KSM is that it is simple to setup and advantageous for other
processes as well. For instance, if you host multiple instances of the
same service (web service, database, tomcat, whatever) there is a high
chance that several of its memory pages are prime candidates for
sharing.&lt;/p&gt;
&lt;p&gt;Now before I do mention that this sharing is only enabled when the
application has marked it as such. This is done through the &lt;em&gt;madvise()&lt;/em&gt;
method, where applications mark the memory with &lt;em&gt;MADV_MERGEABLE&lt;/em&gt;,
meaning that the applications explicitly need to support KSM in order
for it to be successful. There is work on the way to support transparent
KSM (such as
&lt;a href="http://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTEzMTI"&gt;UKSM&lt;/a&gt; and
&lt;a href="https://code.google.com/p/pksm/"&gt;PKSM&lt;/a&gt;) where no &lt;em&gt;madvise&lt;/em&gt; calls would
be needed anymore. But beyond quickly reading the home pages (or
translated home pages in case of UKSM ;-) I have no experience with
those projects.&lt;/p&gt;
&lt;p&gt;So let's get back to KSM. I am currently running three virtual machines
(all configured to take at most 1.5 Gb of memory). Together, they take
just a little over 1 Gb of memory (sum of their resident set sizes).
When I consult KSM, I get the following information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; # grep -H &amp;#39;&amp;#39; /sys/kernel/mm/ksm/pages_*
/sys/kernel/mm/ksm/pages_shared:48911
/sys/kernel/mm/ksm/pages_sharing:90090
/sys/kernel/mm/ksm/pages_to_scan:100
/sys/kernel/mm/ksm/pages_unshared:123002
/sys/kernel/mm/ksm/pages_volatile:1035
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;pages_shared&lt;/code&gt; tells me that 48911 pages are shared (which means
about 191 Mb) through 90090 references (&lt;code&gt;pages_sharing&lt;/code&gt; - meaning the
various processes have in total 90090 references to pages that are being
shared). That means a gain of 41179 pages (160 Mb). Note that the
resident set sizes do not take into account shared pages, so the sum of
the RSS has to be subtracted with this to find the "real" memory
consumption. The &lt;code&gt;pages_unshared&lt;/code&gt; value tells me that 123002 pages are
marked with the &lt;code&gt;MADV_MERGEABLE&lt;/code&gt; advise flag but are not used by other
processes.&lt;/p&gt;
&lt;p&gt;If you want to use KSM yourself, configure your kernel with &lt;code&gt;CONFIG_KSM&lt;/code&gt;
and start KSM by echo'ing the value "1" into &lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt;.
That's all there is to it.&lt;/p&gt;</content><category term="Free Software"></category><category term="cow"></category><category term="ksm"></category><category term="kvm"></category><category term="linux"></category><category term="virtualization"></category></entry></feed>