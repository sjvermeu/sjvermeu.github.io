<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Comparing Hadoop with mainframe - Simplicity is a form of art...</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="http://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/">

        <meta name="author" content="Sven Vermeulen" />
        <meta name="keywords" content="hadoop,mainframe" />
        <meta name="description" content="At my work, I have the pleasure of being involved in a big data project that uses Hadoop as the primary platform for several services. As an architect, I try to get to know the platform&#39;s capabilities, its potential use cases, its surrounding ecosystem, etc. And although the implementation at …" />

        <meta property="og:site_name" content="Simplicity is a form of art..." />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Comparing Hadoop with mainframe"/>
        <meta property="og:url" content="http://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/"/>
        <meta property="og:description" content="At my work, I have the pleasure of being involved in a big data project that uses Hadoop as the primary platform for several services. As an architect, I try to get to know the platform&#39;s capabilities, its potential use cases, its surrounding ecosystem, etc. And although the implementation at …"/>
        <meta property="article:published_time" content="2016-06-15" />
            <meta property="article:section" content="Hadoop" />
            <meta property="article:tag" content="hadoop" />
            <meta property="article:tag" content="mainframe" />
            <meta property="article:author" content="Sven Vermeulen" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="http://blog.siphos.be/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="http://blog.siphos.be/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="http://blog.siphos.be/theme/css/pygments/native.css" rel="stylesheet">
    <link href="http://blog.siphos.be/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="http://blog.siphos.be/theme/css/style.css" type="text/css"/>

        <link href="http://blog.siphos.be/feed/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Simplicity is a form of art... ATOM Feed"/>

        <link href="http://blog.siphos.be/feed/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Simplicity is a form of art... RSS Feed"/>


        <link href="http://blog.siphos.be/category/hadoop/feed/atom.xml" type="application/atom+xml" rel="alternate"
              title="Simplicity is a form of art... Hadoop ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="http://blog.siphos.be/" class="navbar-brand">
Simplicity is a form of art...            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="http://blog.siphos.be/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="http://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/"
                       rel="bookmark"
                       title="Permalink to Comparing Hadoop with mainframe">
                        Comparing Hadoop with mainframe
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2016-06-15T20:55:00+02:00"> Wed 15 June 2016</time>
    </span>


            <span class="label label-default">By</span>
            <a href="http://blog.siphos.be/pages/about.html"><i class="fa fa-user"></i> Sven Vermeulen</a>

        <span class="label label-default">Category</span>
        <a href="http://blog.siphos.be/category/hadoop.html">Hadoop</a>


<span class="label label-default">Tags</span>
	<a href="http://blog.siphos.be/tag/hadoop/">hadoop</a>
        /
	<a href="http://blog.siphos.be/tag/mainframe/">mainframe</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>At my work, I have the pleasure of being involved in a big data project that
uses Hadoop as the primary platform for several services. As an architect, I
try to get to know the platform's capabilities, its potential use cases, its
surrounding ecosystem, etc. And although the implementation at work is not in
its final form (yay agile infrastructure releases) I do start to get a grasp of
where we might be going.</p>
<p>For many analysts and architects, this Hadoop platform is a new kid on the block
so I have some work explaining what it is and what it is capable of. Not for the
fun of it, but to help the company make the right decisions, to support management
and operations, to lift the fear of new environments. One thing I've once said is
that "Hadoop is the poor man's mainframe", because I notice some high-level
similarities between the two.</p>


<p>Somehow, it stuck, and I was asked to elaborate. So why not bring these points
into a nice blog post :)</p>
<p><strong>The big fat disclaimer</strong></p>
<p>Now, before embarking on this comparison, I would like to state that I am <strong>not</strong>
saying that Hadoop offers the same services, or even quality and functionality
of what can be found in mainframe environments. Considering how much time, effort
and experience was already put in the mainframe platform, it would be strange if
Hadoop could match the same. This post is to seek some similarities and, who knows,
learn a few more tricks from one or another.</p>
<p>Second, I am not an avid mainframe knowledgeable person. I've been involved as
an IT architect in database and workload automation technical domains, which also
spanned the mainframe parts of it, but most of the effort was within the distributed
world. Mainframes remain somewhat opaque to me. Still, that shouldn't prevent me
from making any comparisons for those areas that I do have some grasp on.</p>
<p>And if my current understanding is just wrong, I'm sure that I'll learn from the
comments that you can leave behind!</p>
<p>With that being said, here it goes...</p>
<p><strong>Reliability, Availability, Serviceability</strong></p>
<p>Let's start with some of the promises that both platforms make - and generally are
also able to deliver. Those promises are of reliability, availability and serviceability.</p>
<p>For the mainframe platform, these quality attributes are shown as the <a href="https://www.ibm.com/support/knowledgecenter/zosbasics/com.ibm.zos.zmainframe/zconc_RAS.htm">mainframe strengths</a>.
The platform's hardware has extensive self-checking and self-recovery
capabilities, the systems can recover from failed components without service
interruption, and failures can be quickly determined and resolved. On the mainframes,
this is done through a good balance and alignment of hardware and software, design
decisions and - in my opinion - tight control over the various components and
services.</p>
<p>I notice the same promises on Hadoop. Various components are checking the state
of the hardware and other components, and when something fails, it is often 
automatically recovered without impacting services. Instead of tight control
over the components and services, Hadoop uses a service architecture and APIs
with Java virtual machine abstractions.</p>
<p>Let's consider hardware changes. </p>
<p>For <strong>hardware failure and component substitutions</strong>, both platforms are capable
of dealing with those without service disruption.</p>
<ul>
<li>Mainframe probably has a better reputation in this matter, as its components
  have a very high Mean Time Between Failure (MTBF), and many - if not all - of
  the components are set up in a redundant fashion. Lots of error detection and
  failure detection processes try to detect if a component is close to failure,
  and ensure proper transitioning of any workload towards the other components
  without impact.</li>
<li>Hadoop uses redundancy on a server level. If a complete server fails, Hadoop
  is usually able to deal with this without impact. Either the sensor-like
  services disable a node before it goes haywire, or the workload and data that
  was running on the failed node is restarted on a different node. </li>
</ul>
<p>Hardware (component) failures on the mainframe side will not impact the services
and running transactions. Component failures on Hadoop might have a noticeable
impact (especially if it is OLTP-like workload), but will be quickly recovered.</p>
<p>Failures are more likely to happen on Hadoop clusters though, as it was designed
to work with many systems that have a worse MTBF design than a mainframe. The
focus within Hadoop is on resiliency and fast recoverability. Depending on the
service that is being used, active redundancy can be in use (so disruptions are
not visible to the user).</p>
<p>If the Hadoop workload includes anything that resembles online transactional
processing, you're still better off with enterprise-grade hardware such as ECC
memory to at least allow improved hardware failure detection (and perform
proactive workload management). CPU failures are not that common (at least not
those without any upfront Machine Check Exception - MCE), and disk/controller
failures are handled through the abstraction of HDFS anyway.</p>
<p>For <strong>system substitutions</strong>, I think both platforms can deal with this in a
dynamic fashion as well:</p>
<ul>
<li>For the mainframe side (and I'm guessing here) it is possible to switch
  machines with no service impact <em>if</em> the services are running on LPARs
  that are joined together in a Parallel Sysplex setup (sort-of clustering
  through the use of the Coupling Facilities of mainframe, which is supported
  through high-speed data links and services for handling data sharing and
  IPC across LPARs). My company 
  <a href="https://www-03.ibm.com/press/us/en/pressrelease/47812.wss">switched to the z13 mainframe</a>
  last year, and was able to keep core services available during the migration.</li>
<li>For Hadoop systems, the redundancy on system level is part of its design.
  Extending clusters, removing nodes, moving services, ... can be done with
  no impact. For instance, switching the active HiveServer2 instance means
  de-registering it in the ZooKeeper service. New client connects are then no
  longer served by that HiveServer2 instance, while active client connections
  remain until finished.
  There are also in-memory data grid solutions such as through the Ignite
  project, allowing for data sharing and IPC across nodes, as well as
  building up memory-based services with Arrow, allowing for efficient
  memory transfers.</li>
</ul>
<p>Of course, also <strong>application level code failures</strong> tend to only disrupt that
application, and not the other users. Be it because of different address
spaces and tight runtime control (mainframe) or the use of different
containers / JVMs for the applications (Hadoop), this is a good feat to have
(even though it is not something that differentiates these platforms from
other platforms or operating systems).</p>
<p><strong>Let's talk workloads</strong></p>
<p>When we look at a mainframe setup, we generally look at different workload
patterns as well. There are basically two main workload approaches for the
mainframe: batch, and On-Line Transactional Processing (OLTP) workload. In
the OLTP type, there is often an additional distinction between synchronous
OLTP and asynchronous OLTP (usually message-based). </p>
<p>Well, we have the same on Hadoop. It was once a pure batch-driven platform
(and many of its components are still using batches or micro-batches in their
underlying designs) but now also provides OLTP workload capabilities. Most of
the OLTP workload on Hadoop is in the form of SQL-like or NoSQL database
management systems with transaction manager support though.</p>
<p>To manage these (different) workloads, and to deal with prioritization of
the workload, both platforms offer the necessary services to make things both
managed as well as business (or "fit for purpose") focused.</p>
<ul>
<li>Using the Workload Manager (WLM) on the mainframe, policies can be set on
  the workload classes so that an over-demand of resources (cross-LPARs) results
  in the "right" amount of allocations for the "right" workload. To actually 
  manage jobs themselves, the Job Entry Subsystem (JES) to receive jobs and
  schedule then for processing on z/OS. For transactional workload, WLM
  provides the right resources to for instance the involved IMS regions.</li>
<li>On Hadoop, workload management is done through Yet Another Resource 
  Negotiator (YARN), which uses (logical) queues for the different workloads.
  Workload (Application Containers) running through these queues can be, 
  resource-wise, controlled both on the queue level (high-level resource
  control) as well as process level (low-level resource control) through
  the use of Linux Control Groups (CGroups - when using Linux based systems
  course).</li>
</ul>
<p>If I would try to compare both against each other, one might say that the
YARN queues are like WLMs service classes, and for batch applications, the
initiators on mainframe are like the Application Containers within YARN
queues. The latter can also be somewhat compared to IMS regions in case
of long-running Application Containers.</p>
<p>The comparison will not hold completely though. WLM can be tuned based on
goals and will do dynamic decision making on the workloads depending on its
parameters, and even do live adjustments on the resources (through the 
System Resources Manager - SRM). Heavy focus on workload management on
mainframe environments is feasible because extending the available resources
on mainframes is usually expensive (additional Million Service Units - MSU).
On Hadoop, large cluster users who notice resource contention just tend
to extend the cluster further. It's a different approach.</p>
<p><strong>Files and file access</strong></p>
<p>Another thing that tends to confuse some new users on Hadoop is its approach
to files. But when you know some things about the mainframe, this does remain
understandable.</p>
<p>Both platforms have a sort-of master repository where data sets (mainframe)
or files (Hadoop) are registered in. </p>
<ul>
<li>On the mainframe, the catalog translates data set names into the right
  location (or points to other catalogs that do the same)</li>
<li>On Hadoop, the Hadoop Distributed File System (HDFS) NameNode is
  responsible for tracking where files (well, blocks) are located across
  the various systems</li>
</ul>
<p>Considering the use of the repository, both platforms thus require the
allocation of files and offer the necessary APIs to work with them. But
this small comparison does not end here.</p>
<p>Depending on what you want to store (or access), the file format you use
is important as well.
- On mainframe, Virtual Storage Access Method (VSAM) provides both the
  methods (think of it as API) as well as format for a particular data
  organization. Inside a VSAM, multiple data entries can be stored in a
  structured way. Besides VSAM, there is also Partitioned Data Set/Extended
  (PDSE), which is more like a directory of sorts. Regular files are Physical
  Sequential (PS) data sets.
- On Hadoop, a number of file formats are supported which optimize the use of
  the files across the services. One is Avro, which holds both methods and
  format (not unlike VSAM), another is Optimized Row Columnar (ORC).  HDFS also
  has a number of options that can be enabled or set on certain locations (HDFS
  uses a folder-like structure) such as encryption, or on files themselves,
  such as replication factor.</p>
<p>Although I don't say VSAM versus Avro are very similar (Hadoop focuses more on
the concept of files and then the file structure, whereas mainframe focuses on
the organization and allocation aspect if I'm not mistaken) they seem to be
sufficiently similar to get people's attention back on the table.</p>
<p><strong>Services all around</strong></p>
<p>What makes a platform tick is its multitude of supported services. And even
here can we find similarities between the two platforms.</p>
<p>On mainframe, DBMS services can be offered my a multitude of softwares.
Relational DBMS services can be provided by IBM DB2, CA Datacom/DB, NOMAD, ...
while other database types are rendered by titles such as CA IDMS and ADABAS.
All these titles build upon the capabilities of the underlying components and
services to extend the platform's abilities.</p>
<p>On Hadoop, several database technologies exist as well. Hive offers a SQL layer
on top of Hadoop managed data (so does Drill btw), HBase is a non-relational
database (mainly columnar store), Kylin provides distributed analytics, MapR-DB
offers a column-store NoSQL database, etc.</p>
<p>When we look at transaction processing, the mainframe platform shows its decades
of experience with solutions such as CICS and IMS. Hadoop is still very much
at its infancy here, but with projects such as Omid or commercial software solutions
such as Splice Machine, transactional processing is coming here as well. Most
of these are based on underlying database management systems which are extended with
transactional properties.</p>
<p>And services that offer messaging and queueing are also available on both
platforms: mainframe can enjoy Tibco Rendezvous and IBM WebSphere MQ, while
Hadoop is hitting the news with projects such as Kafka and Ignite.</p>
<p>Services extend even beyond the ones that are directly user facing. For instance,
both platforms can easily be orchestrated using workload automation tooling.
Mainframe has a number of popular schedulers up its sleeve (such as IBM TWS,
BMC Control-M or CA Workload Automation) whereas Hadoop is generally easily
extended with the scheduling and workload automation software of the distributed
world (which, given its market, is dominated by the same vendors, although many
smaller ones exist as well). Hadoop also has its "own" little scheduling
infrastructure called Oozie.</p>
<p><strong>Programming for the platforms</strong></p>
<p>Platforms however are more than just the sum of the services and the properties
that it provides. Platforms are used to build solutions on, and that is true for
both mainframe as well as Hadoop.</p>
<p>Let's first look at scripting - using interpreted languages. On mainframe, you
can use the Restructed Extended Executor (REXX) or CLIST (Command LIST). Hadoop
gives you Tez and Pig, as well as Python and R (through PySpark and SparkR).</p>
<p>If you want to directly interact with the systems, mainframe offers the Time
Sharing Option/Extensions (TSO/E) and Interactive System Productivity Facility
(ISPF). For Hadoop, regular shells can be used, as well as service-specific
ones such as Spark shell. However, for end users, web-based services such 
as Ambari UI (Ambari Views) are generally better suited.</p>
<p>If you're more fond of compiled code, mainframe supports you with COBOL, Java
(okay, it's "a bit" interpreted, but also compiled - don't shoot me here), C/C++
and all the other popular programming languages. Hadoop builds on top of Java,
but supports other languages such as Scala and allows you to run native
applications as well - it's all about using the right APIs.</p>
<p>To support development efforts, Integrated Development Environments (IDEs) are
provided for both platforms as well. You can use Cobos, Micro Focus Enterprise
Developer, Rational Developer for System z, Topaz Workbench and more for
mainframe development. Hadoop has you covered with web-based notebook solutions
such as Zeppelin and JupyterHub, as well as client-level IDEs such as Eclipse
(with the Hadoop Development Tools plugins) and IntelliJ.</p>
<p><strong>Governing and managing the platforms</strong></p>
<p>Finally, there is also the aspect of managing the platforms.</p>
<p>When working on the mainframe, management tooling such as the Hardware
Management Console (HMC) and z/OS Management Facility (z/OSMF) cover operations
for both hardware and system resources. On Hadoop, central management
software such as Ambari, Cloudera Manager or Zettaset Orchestrator try to
cover the same needs - although most of these focus more on the software
side than on the hardware level.</p>
<p>Both platforms also have a reasonable use for multiple roles: application
developers, end users, system engineers, database adminstrators, operators, 
system administrators, production control, etc. who all need some kind of access
to the platform to support their day-to-day duties. And when you talk roles,
you talk authorizations.</p>
<p>On the mainframe, the Resource Access Control Facility (RACF) provides
access control and auditing facilities, and supports a multitude of services on
the mainframe (such as DB2, MQ, JES, ...). Many major Hadoop services, such
as HDFS, YARN, Hive and HBase support Ranger, providing a single pane for
security controls on the Hadoop platform.</p>
<p>Both platforms also offer the necessary APIs or hooks through which system
developers can fine-tune the platform to fit the needs of the business, or
develop new integrated solutions - including security oriented ones. 
Hadoop's extensive plugin-based design (not explicitly
named) or mainframe's Security Access Facility (SAF) are just examples of this.</p>
<p><strong>Playing around</strong></p>
<p>Going for a mainframe or a Hadoop platform will always be a management decision.
Both platforms have specific roles and need particular profiles in order to
support them. They are both, in my opinion, also difficult to migrate away from
once you are really using them actively (lock-in) although it is more digestible
for Hadoop given its financial implications.</p>
<p>Once you want to start meddling with it, getting access to a full platform used
to be hard (the coming age of cloud services makes that this is no longer the
case though), and both therefore had some potential "small deployment" uses.
Mainframe experience could be gained through the Hercules 390 emulator, whereas
most Hadoop distributions have a single-VM sandbox available for download.</p>
<p>To do a full scale roll-out however is much harder to do by your own. You'll
need to have quite some experience or even expertise on so many levels that 
you will soon see that you need teams (plural) to get things done.</p>
<p>This concludes my (apparently longer than expected) write-down of this matter.
If you don't agree, or are interested in some insights, be sure to comment!</p>
            </div>
            <!-- /.entry-content -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <p>
        To comment as a guest, use "Or sign up with disqus" and then select the "I'd rather post as guest" option.
        </p>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'simplicityisaformofart'; // required: replace example with your forum shortname

                    var disqus_identifier = 'comparing-hadoop-with-mainframe';
                var disqus_url = 'http://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>

<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="https://github.com/sjvermeu"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
                <li class="list-group-item"><a href="https://twitter.com/sjvermeu"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
              </ul>
            </li>





    <li class="list-group-item"><h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
      <ul class="list-group" id="links">
        <li class="list-group-item">
            <a href="http://www.gentoo.org" target="_blank">
                Gentoo Linux
            </a>
        </li>
      </ul>
    </li>
    </ul>
</section>
            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Sven Vermeulen
            &middot; Powered by <a href="https://github.com/DandyDev/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://blog.siphos.be/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="http://blog.siphos.be/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="http://blog.siphos.be/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'simplicityisaformofart'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->

</body>
</html>