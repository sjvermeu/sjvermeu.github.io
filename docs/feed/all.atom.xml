<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art...</title><link href="https://blog.siphos.be/" rel="alternate"></link><link href="https://blog.siphos.be/feed/all.atom.xml" rel="self"></link><id>https://blog.siphos.be/</id><updated>2022-05-21T13:00:00+02:00</updated><entry><title>Containers are the new IaaS</title><link href="https://blog.siphos.be/2022/05/containers-are-the-new-iaas/" rel="alternate"></link><published>2022-05-21T13:00:00+02:00</published><updated>2022-05-21T13:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-05-21:/2022/05/containers-are-the-new-iaas/</id><content type="html">&lt;p&gt;At work, as with many other companies, we're actively investing in new
platforms, including container platforms and public cloud. We use Kubernetes
based container platforms both on-premise and in the cloud, but are also very
adamant that the container platforms should only be used for application
workload that is correctly designed for cloud-native deployments: we do not
want to see vendors packaging full operating systems in a container and
then shouting they are now container-ready.&lt;/p&gt;
&lt;p&gt;Sadly, we notice more and more vendors abusing containerization to wrap their
products in and selling it as 'cloud-ready' or 'container-ready'. For many
vendors, containers allow them to bundle everything as if it were an
appliance, but without calling it an appliance - in our organization, we
have specific requirements on appliances to make sure they aren't just
pre-build systems that lack the integration, security, maintainability and
supportability capabilities that we would expect from an appliance.&lt;/p&gt;
&lt;p&gt;Even developers are occasionally tempted to enlarge container images with a
whole slew of middleware and other services, making it more monolithic
solutions than micro-services, just running inside a container because they
can. I don't feel that this evolution is beneficial (or at least not yet),
because the maintainability and supportability of these images can be very
troublesome.&lt;/p&gt;
&lt;p&gt;This evolution is similar to the initial infrastructure-as-a-service
offerings, where the focus was on virtual machines: you get a platform on top
of which your virtual machines run, but you remain responsible for the virtual
machine and its content. But unlike virtual machines, where many organizations
have standardized management and support services deployed for, containers are
often shielded away or ignored. But the same requirements should be applied to
containers just as to virtual machines.&lt;/p&gt;
&lt;p&gt;Let me highlight a few of these, based on my &lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;Process view of
infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost and licensing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Be it on a virtual machine or in a container, the costs and licensing of the
products involved must be accounted for. For virtual machines, this is often
done through license management tooling that facilitates tracking of software
deployments and consumption. These tools often use agents running on the
virtual machines (and a few run at the hypervisor level so no in-VM agents are
needed).&lt;/p&gt;
&lt;p&gt;Most software products also use licensing metrics that are tailored to
(virtual) hardware (like processors) or deployments (aka nodes, i.e. a
per-operating-system count). Software vendors often have the right to audit
software usage, to make sure companies do not abuse their terms and
conditions. &lt;/p&gt;
&lt;p&gt;Now let's tailor that to a container environment, where platforms like
Kubernetes can dynamically scale up the number of deployments based on the
needs. Unlike more static virtual machine-based deployments, we now have a
more dynamic environment. How do you measure software usage here? Running
software license agents inside containers isn't a good practice. Instead, we
should do license scanning in the images up-front, and tag resources
accordingly. But not many license management tooling is already
container-aware, let alone aligned with a different way of working.&lt;/p&gt;
&lt;p&gt;But "our software license management tooling is not container-ready yet" is
not an adequate answer to software license audits, nor will the people in the
organization that are responsible for license management be happy with such
situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Product lifecycle&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the licensing part, companies also want to track which software
versions are being used: not just for vulnerability management purposes, but
also to make sure the software remains supported and fit for purpose.&lt;/p&gt;
&lt;p&gt;On virtual machines, regular software scanning and inventory setup can be done
to report on the software usage. And while on container environments this can
be easily done at the image level (which software and versions are available in
which containers) this often provides a pre-deployment view, and doesn't tell
us if a certain container is being used or not, nor if additional deployments
have been triggered since the container is launched.&lt;/p&gt;
&lt;p&gt;Again, deploying in-container scanning capabilities seems to be
contra-productive here. Having an end-to-end solution that detects and
registers software titles and products based on the container images, and then
provides insights into runtime deployments (and history) seems to be a better match.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authorization management (and access control)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When support teams need to gain access to the runtime environment (be it for
incident handling, problem management, or other operational tasks) most
companies will already have a somewhat finer-grained authorization system in
place: you don't want to grant full system administrator rights if they aren't
needed.&lt;/p&gt;
&lt;p&gt;For containers, this is often not that easy to accomplish: the design of
container platforms is tailored to situations where you don't want to
standardize on in-container access: runtimes are ephemeral, and support is
handled through logging and metric, with adaptation to the container images
and rolling out new versions. If containers are starting to get used for more
classical workloads, authorization management will become a more active field
to work out.&lt;/p&gt;
&lt;p&gt;Consider a database management system within the container alongside the
vendor software. Managing this database might become a nightmare, especially
if it is only locally accessible (within the container or pod). And before you
yell how horrible such a setup would be for a container platform... yes, but
it is still a reality for some.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Auditing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Auditing is a core part of any security strategy, logging who did what, when,
from where, on what, etc. For classical environments, audit logging, reporting
and analysis are based upon static environment details: IP addresses,
usernames, process names, etc.&lt;/p&gt;
&lt;p&gt;In a container environment, especially when using container orchestration,
these classical details are not as useful. Sure, they will point to the
container platform, but IP addresses are often shared or dynamically assigned.
Usernames are dynamically generated or are pooled resources. Process
identifiers are not unique either.&lt;/p&gt;
&lt;p&gt;Auditing for container platforms needs to consider the container-specific
details, like namespaces. But that means that all the components involved in
the auditing processes (including the analysis frameworks, AI models, etc.)
need to be aware of these new information types.&lt;/p&gt;
&lt;p&gt;In the case of monolithic container usage, this can become troublesome as the
in-container logging often has no knowledge of the container-specific nature,
which can cause problems when trying to correlate information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I only touched upon a few processes here. Areas such as quality assurance and
vulnerability management are also challenges for instance, as is data
governance. None of the mentioned processes are impossible to solve, but
require new approaches and supporting services, which make the total cost of
ownership of these environments higher than your business or management might
expect.&lt;/p&gt;
&lt;p&gt;The rise of monolithic container usage is something to carefully consider. In
the company I work for, we are strongly against this evolution as the enablers
we would need to put in place are not there yet, and would require significant
investments. It is much more beneficial to stick to container platforms for
the more cloud-native setups, and even in those situations dealing with ISV
products can be more challenging than when it is only for internally developed
products.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1527975405730336768"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;
</content><category term="Architecture"></category><category term="kubernetes"></category><category term="container"></category><category term="iaas"></category><category term="infrastructure"></category><category term="virtual-machine"></category></entry><entry><title>Defining what an IT asset is</title><link href="https://blog.siphos.be/2022/02/defining-what-an-it-asset-is/" rel="alternate"></link><published>2022-02-13T13:00:00+01:00</published><updated>2022-02-13T13:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-02-13:/2022/02/defining-what-an-it-asset-is/</id><summary type="html">&lt;p&gt;One of the main IT processes that a company should strive to have in place
is a decent IT asset management system. It facilitates knowing what assets
you own, where they are, who the owner is, and provides a foundation for
numerous other IT processes.&lt;/p&gt;
&lt;p&gt;However, when asking "what is an IT asset", it gets kind off fuzzy...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;One of the main IT processes that a company should strive to have in place
is a decent IT asset management system. It facilitates knowing what assets
you own, where they are, who the owner is, and provides a foundation for
numerous other IT processes.&lt;/p&gt;
&lt;p&gt;However, when asking "what is an IT asset", it gets kind off fuzzy...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Searching for a definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I went on to search for a definition of an "IT asset", my first
thought was to check on the more common reference frameworks out there.
Surely, if they tout the benefits of a well-functioning IT asset management
system and process, they will declare what an IT asset is, right?&lt;/p&gt;
&lt;p&gt;Many of these frameworks do not have their resources publicly available. They do have
enough material online to provide me with the necessary insights.&lt;/p&gt;
&lt;p&gt;The first focus I had was on ITIL, formerly known as the Information
Technology Infrastructure Library. There are quite a few articles on the &lt;a href="https://www.itil-docs.com/blogs/asset-management"&gt;ITIL
Docs&lt;/a&gt; site on IT asset
management, but while they do have some breadcrumbs, they do not offer a full
definition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;as financial management of IT assets is an important sub-process,
  I can derive that assets are either expected to have a financial
  value, or that such economical assets are a large subset of the
  IT assets in general&lt;/li&gt;
&lt;li&gt;assets always have a lifecycle to follow (although I am uncertain
  if this will help in deriving a definition)&lt;/li&gt;
&lt;li&gt;assets support the IT in the strategic and financial decision-making&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I found a &lt;a href="https://itil.it.utah.edu/downloads/ITILv2_Terms_and_Definitions_r2.0_0808.pdf"&gt;Terms and Definitions&lt;/a&gt;
extract that has an asset defined as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"[An IT asset is a] component of a business process. Assets can include
people, accommodation (facilities), computer systems, networks, paper records,
fax machines, etc."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, that definition is too broad for IT asset management.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-5.pdf"&gt;NIST 1800-5 IT Asset Management publication&lt;/a&gt;
focuses on technologies that ensure a smooth business operation.
It includes computers, mobile devices, operating systems, applications,
data, and network resources. It later mentions that IT assets include items
such as servers, desktops, laptops, and network appliances. And when it
touches upon the value of IT asset management, it too mentions that it is
about informed business-driven decision-making.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.isaca.org/resources/cobit"&gt;ISACA's CObIT&lt;/a&gt;, IT assets are
frequently mentioned. Here as well, there is a strong indication that they
relate to a financial part and a value delivery part. For instance, in the
objective "Ensured Benefits Delivery", we find:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Create and maintain portfolios of I&amp;amp;T-enabled investment programs, IT
services and IT assets, which form the basis for the current IT budget and
support the I&amp;amp;T tactical and strategic plans"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is an entire objective called "Managed Assets" which covers the bulk of
the IT asset management practice. It again refers to the assets, but focuses
less on the definition of what an IT asset is, and more on what it supports.
For instance:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Manage I&amp;amp;T assets [...] to make sure that their use delivers value at optimal
cost, they remain operational (fit for purpose), and they are accounted for
and physically protected."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the practice of managing an inventory, the references are towards IT assets
that are "required to deliver services and that are owned or controlled by the
organization with an expectation of future benefit".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.techopedia.com/definition/16946/it-asset"&gt;Techopedia&lt;/a&gt; has it
defined as "a piece of software or hardware within an information technology
environment."&lt;/p&gt;
&lt;p&gt;On &lt;a href="https://en.wikipedia.org/wiki/Asset_(computer_security)"&gt;Wikipedia's "Asset (computer
security)"&lt;/a&gt; page, 
the definition refers to the ISO/IEC 13335-1:2004 publication, and summarizes it
as "any data, device, or other component of the environment that supports
information-related activities." It also refers to an older ENISA page
(archived), and looking at the current &lt;a href="https://www.enisa.europa.eu/topics/threat-risk-management/risk-management/current-risk/risk-management-inventory/glossary"&gt;ENISA
Glossary&lt;/a&gt;
it covers the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"[An IT asset is] anything that has value to the organization, its business
operations and their continuity, including Information resources that support
the organization's mission."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ENISA appears to quote ISO/IEC PDTR 13335-1 as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deriving a definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From the search, I think I have some grasp of what an IT asset is. Or at
least, how I look at it. Two main attributes exist that define if something in
an IT environment is an asset or not.&lt;/p&gt;
&lt;p&gt;First, it has to offer or realize an IT value to the environment. Often, 
IT value is about automating (a part of a) business process.&lt;/p&gt;
&lt;p&gt;Second, the asset has to have a value of its own. That value is typically 
economical, but can also be intangible, like intellectual property.&lt;/p&gt;
&lt;p&gt;Through these two attributes, I would create the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An IT asset is anything that provides visible IT value towards the
organization, and has an intrinsic value on its own. A visible IT value is
generally in support of IT automation. An intrinsic value can be economical,
but also a capital asset like intellectual property tied to the IT asset
itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some processes make a distinction between regular IT assets and more critical
(or important) assets. In most cases, I read that it focuses on the impact that
a defect or unavailability of the asset has on the organization. So, a critical
IT asset gets the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A critical (or important) IT asset is an IT asset that directly realizes a
(business or IT) service, where the business impact (in case of exploited risk
or unavailability) is non-trivial.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This distinction is visible in CObIT for instance, which is the framework that
I'm most used to working with. CObIT has different activities related to
critical/important IT assets compared to general IT assets. A computer mouse
might be an IT asset, but it is not a critical/important asset. Or, at least, I
hope your company isn't going to feel it if one mouse is malfunctioning.&lt;/p&gt;
&lt;p&gt;While having a definition to work with is essential to confirm the scope for
the activities a company needs to attend to, the next step isn't to run off
and tag everything that seems to fit the definition. Instead, I recommend
first starting by classifying the IT assets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifying the assets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next you want to classify for your organization what you consider to be IT
assets. This clarifies which types you want to follow the IT asset management
processes for. We've had examples mentioned earlier, like software, operating
systems, servers, but there are plenty more assets out there.&lt;/p&gt;
&lt;p&gt;For instance, a leased line you have between yourself and a third party is an IT
asset. Leased lines are network lines with a certain quality associated with
them and which you lease from a network provider. Their quality attributes
include line performance, expected availability, recovery capabilities, etc.
Such lines have economical value, and they offer support to the IT processes
that use that network.&lt;/p&gt;
&lt;p&gt;To create a classification, you might want to start with the conceptual
data model that I've &lt;a href="https://blog.siphos.be/2022/01/an-it-conceptual-data-model/"&gt;shared
previously&lt;/a&gt;. You can, of
course, also use an existing classification model. The &lt;a href="https://www.ungm.org/Public/UNSPSC"&gt;United Nations Standard
Products and
Services Code (UNSPSC)&lt;/a&gt; has a class list
published which you can use to tailor your classification.&lt;/p&gt;
&lt;p&gt;The classification shouldn't limit itself to IT asset management.
It is often better to work out this model for the company in general first
(including requirements from other processes like solution build and
configuration management) and then derive what classes should be part of the IT
asset management process.&lt;/p&gt;
&lt;p&gt;A well-maintained classification will also make it easier for the company
to work out how to improve the asset management processes. It enables the search
for more tangible solutions, a more structured analysis of the current
situation, and easier discussions with the stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While there does not appear to be a commonly accepted definition of what an IT
asset is, most frameworks do have similar concepts attributed to IT assets.
Common concepts are the value the assets deliver to the organization, and the
value the assets hold themselves. Trying to create a good definition helps in
trying to grasp at what level an IT asset should exist.&lt;/p&gt;
&lt;p&gt;Yet, working out a definition is only a first step. After the definition,
building out a good classification system helps to make IT asset management
more tangible for the organization. Most online resources use such classes
(like servers, desktops, operating systems, ...) to clarify what they consider
as IT assets, rather than attempting to derive a workable definition.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1492833029907173380"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="asset-management"></category><category term="cobit"></category><category term="itil"></category></entry><entry><title>An IT conceptual data model</title><link href="https://blog.siphos.be/2022/01/an-it-conceptual-data-model/" rel="alternate"></link><published>2022-01-17T10:00:00+01:00</published><updated>2022-01-17T10:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-01-17:/2022/01/an-it-conceptual-data-model/</id><summary type="html">&lt;p&gt;This time a much shorter post, as I've been asked to share this information
recently and found that it, by itself, is already useful enough to publish. It
is a conceptual data model for IT services.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;This time a much shorter post, as I've been asked to share this information
recently and found that it, by itself, is already useful enough to publish. It
is a conceptual data model for IT services.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The IT model, and why it is useful&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A conceptual data model for IT services supports several IT processes, with a
strong focus on &lt;em&gt;asset management&lt;/em&gt; and &lt;em&gt;configuration management&lt;/em&gt;. Many IT
vendors that have solutions active within those processes will have their own
data model in place, but I often feel that their models have room for
improvement.&lt;/p&gt;
&lt;p&gt;Some of these models are too fine-grained, others are limited to server
infrastructure. And while most applications allow for further customization, I
feel that an IT architect should have a conceptual model in mind for their
actions and projects.&lt;/p&gt;
&lt;p&gt;The conceptual data model that I'm currently working on looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="An IT CDM, first version" src="https://blog.siphos.be/images/202201/it-cdm-v1.png"&gt;&lt;/p&gt;
&lt;p&gt;My intention is to update this CDM with new insights when I capture those. It is
not my intention to further develop the data model into a physical data model,
but perhaps in the long term I could make it a conceptual one (explaining what
the attributes are of each concept).&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1483002656478093315"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="cdm"></category><category term="asset-management"></category><category term="configuration-management"></category></entry><entry><title>Ownership and responsibilities for infrastructure services</title><link href="https://blog.siphos.be/2022/01/ownership-and-responsibilities-for-infrastructure-services/" rel="alternate"></link><published>2022-01-13T09:00:00+01:00</published><updated>2022-01-13T09:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-01-13:/2022/01/ownership-and-responsibilities-for-infrastructure-services/</id><summary type="html">&lt;p&gt;In a perfect world, using infrastructure or technology services would be
seamless, without impact, without risks. It would auto-update, tailor to
the user needs, detect when new features are necessary, adapt, etc. But
while this is undoubtedly what vendors are saying their product delivers,
the truth is way, waaaay different.&lt;/p&gt;
&lt;p&gt;Managing infrastructure services implies that the company or organization
needs to organize itself to deal with all aspects of supporting a service.
What are these aspects? Well, let's go through those that are top-of-mind
for me...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In a perfect world, using infrastructure or technology services would be
seamless, without impact, without risks. It would auto-update, tailor to
the user needs, detect when new features are necessary, adapt, etc. But
while this is undoubtedly what vendors are saying their product delivers,
the truth is way, waaaay different.&lt;/p&gt;
&lt;p&gt;Managing infrastructure services implies that the company or organization
needs to organize itself to deal with all aspects of supporting a service.
What are these aspects? Well, let's go through those that are top-of-mind
for me...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Operational support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you have a service running, then you need to ensure operational
support is in place in case there are issues. Be it to resolve a
malfunction, a security issue, or a performance degradation - you need
(human) resources to ensure that the service remains running adequately.&lt;/p&gt;
&lt;p&gt;In many organizations, this is handled in a three- or sometimes 
even four-level support structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;First line&lt;/em&gt; is generally a call center that procedurally validates if
  an issue is service-bound, or if they can assist the user to correctly
  or better use the service. &lt;em&gt;First line&lt;/em&gt; often does not require any
  knowledge of the customer base nor target infrastructure, and is strongly
  procedure-oriented. They do not have operational duties on the services
  themselves, and are an important part to weed out unstructured or invalid
  service requests. They then escalate the issues to &lt;em&gt;second line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Second line&lt;/em&gt; is an organization that has knowledge on the customer
  base and the services themselves. They are also often the last line
  that has a wide view of all services within the company, as subsequent
  support levels are more specialized. &lt;em&gt;Second line&lt;/em&gt; has the ability to
  take actions on the services themselves (like restarting services)
  if this is agreed upon in the past with the main stakeholders, and when
  this is executed in a controlled manner. If &lt;em&gt;second line&lt;/em&gt; isn't able to
  resolve an issue, it moves to &lt;em&gt;third line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Third line&lt;/em&gt; support is generally the team that is operationally involved
  in the service itself. If the problem lays with a customer portal for
  instance, then &lt;em&gt;third line support&lt;/em&gt; is often the team that maintains the
  customer portal. They know the service and its usage in detail, and are
  in many organizations the last line of support.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some organizations even continue this layering, with a fourth line of
support being a technical expert in a particular component used by the
service. If the customer portal has problems, and it is within the database,
and the DBAs of the customer portal team do not have the knowledge to
resolve it, they might escalate further to a dedicated DBA team (which
is not assigned to any particular business service but specifically
organized to be experts in database and database administration).&lt;/p&gt;
&lt;p&gt;Not all companies have a technology-oriented support team though, and
many companies will consider this as part of 3rd line support as well,
if not just to be more aligned with market terminology on support structures.
Still, organizing and optimizing this third line of support is often
something that infrastructure service support is heavily involved in.&lt;/p&gt;
&lt;p&gt;Regardless of the structure approached by the organization, these teams
will need the knowledge and supporting tools and procedures to do their job.
You need people that can develop support procedures, create simplified
automation (for second line to execute), and continuously update that
information. And if a vendor is involved, then the support line will need to
have knowledge on how to approach the vendor: what are the procedures and
processes for raising incidents, what is the priority queue like? Does
the vendor have certain SLAs that the team should know about?&lt;/p&gt;
&lt;p&gt;The several layers of support will need continuous training, even if it
is just refreshing past information. It is also wise to involve these
support lines in information sharing, like when you know there is a growth
in database reliance in the business services, or when you know many
databases are being migrated from one technology to another. Second line
for instance might be able to use that information, together with their
cross-organizational knowledge, to better triage issues.&lt;/p&gt;
&lt;p&gt;Finally, this support structure is often tied to service level agreements that
the company makes either internally, or with its customers. Hence, the support
must be guaranteed within those SLAs. That implies that the support must be
driven through multiple people, as the organization has to provide support
during off-hours, during holidays, during absence... heck, even during global
pandemics. I use the "3 FTE per technology" principle: you need three people
with enough knowledge to support the technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintenance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As mentioned in the support part, teams exist that are responsible for
the service itself, including patching and updates. This is part of the
maintenance requirement on services, and infrastructure services are not
different. Perhaps even more so than business services, infrastructure
services have a wider impact if they are hit with a bug or with
performance degradation, as many business services rely on the infrastructure
to be up and running, highly available, well-performing, and secure.&lt;/p&gt;
&lt;p&gt;Maintenance tasks for services include, alongside the participation in the
operational support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Executing security- and stability patch validation and roll-out (updates)&lt;/li&gt;
&lt;li&gt;Proactively assessing the state of the service to see if improvements
  or mitigations are needed, before these result in actual issues&lt;/li&gt;
&lt;li&gt;Ensuring sufficient capacity is available for now and in the immediate
  future&lt;/li&gt;
&lt;li&gt;Resolving performance issues, be it by increasing resources, moving
  services to different locations or hosts, fine-tuning configuration
  parameters, offloading workload to different services, etc.&lt;/li&gt;
&lt;li&gt;Executing planned maintenance activities, which can include upgrades
  to higher versions. When the service cannot be transparently upgraded,
  this will involve a thorough alignment with all stakeholders as part of
  the change management processes.&lt;/li&gt;
&lt;li&gt;In case of larger, wide-spanning incidents (or even disasters), the teams
  play an active role in the orchestrated recovery together with all other
  teams.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Designing and architecting the integrations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To ensure that the services are well supported and can be maintained in
an efficient manner, there is often a strong focus on the proper design of
the infrastructure services and their role in the architecture. This
design does not just include pointing out which components exist where,
but also how the service integrates within the larger landscape:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are administration tasks executed? How do administrators authenticate
  themselves?&lt;/li&gt;
&lt;li&gt;Where are the monitoring metrics found and stored? Do you use trend analysis,
  and in which system?&lt;/li&gt;
&lt;li&gt;How are the assets related to the service tracked? What is the lifecycle for
  the individual assets, and how does that impact the maintainability of the
  service?&lt;/li&gt;
&lt;li&gt;What is the best way to use the service, and what will not be allowed?&lt;/li&gt;
&lt;li&gt;How are backups taken, and what does a restore procedure look like? Do you
  support both in-place restores, side restores, etc.&lt;/li&gt;
&lt;li&gt;What underlying resource requirements exist? Do you require certain
  tiered storage, or network performance requirements? If you allow your
  service to be instantiated by others (rather than maintain a platform
  yourself), what are the target resources that you allow? Are there
  minimal resource requirements?&lt;/li&gt;
&lt;li&gt;How do users interact with the service? Do they access it directly,
  or do you require intermediate gateways (like reverse proxies)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, design and architecture go beyond integrations. I focus strongly on
integrations here as it is a part of design and architecture that has strong
dependencies and relations with other teams and technologies. To work out
the integration side of a service, you can't do this autonomously without
assistance from the other service teams. Of course, if those teams have
everything well documented and architected, and easy to consume, then the
team responsible for the design and architecture can do a large part of the
work independently, but having a final validation or confirmation is always
beneficial.&lt;/p&gt;
&lt;p&gt;One of the values of a proper integration design is a higher quality of the
service delivery. If integrations are missed, you'll find that a service 
might not be ready to be activated in production, and then suddenly there
is a stronger focus on timely delivery than on quality. Situations where
firewall rules need to be quickly pushed and opened up because a project
failed to assess their integrations, resulting in security risks, is
sadly enough all too common.&lt;/p&gt;
&lt;p&gt;Larger organizations will often have architects and designers within the teams
or directorates to support this endeavor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secure setup and deployment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that some integrations might result in heightened risks, and
infrastructure services are often widely used or consumed, the organization
will need to make sure that the services are designed to be secure.&lt;/p&gt;
&lt;p&gt;Security of a service is more than just ensuring it is up-to-date. You
will also need to make sure it is configured correctly (as misconfigurations
are a frequent occurrence of security incidents), that the authentication
and authorizations are properly designed (and where needed or possible,
using multi-factor authentication), that the deployment considers the
placement and interactions (firewalls, zoning, etc.), that the service
provides functional (or perhaps even physical) segregation, that the
data governance is appropriate and aligned with regulatory and company
requirements, that the service is continuously validated by the security
tooling available (patch indications, vulnerability management, ...), etc.&lt;/p&gt;
&lt;p&gt;As services also evolve when they are alive, secure setup and deployment
is not a one-off (but the initial thoughts and designs are not to be
underestimated): the teams will need to assess the impact of new
insights (like security notifications, vulnerabilities, global changes
by the organization, new threats in the wider IT world) which implies
that the team has a continuous security and risk focus.&lt;/p&gt;
&lt;p&gt;In many cases, there is even a coding and development component with
a service. Not many services can be installed and deployed without requiring
any integration scripting or even in-depth coding (such as custom plugins).
And when there is coding, there is the need for secure coding: following
a Secure Development LifeCycle (SDLC) approach to get assurance about
the secure state of the developed code.&lt;/p&gt;
&lt;p&gt;When the deployment uses infrastructure-as-code methods, follows a
GitOps approach, or similar, then there should also be sufficient attention
to the secure setup of these pipelines and the platforms on which they
run. The code (or configurations) hosted should also follow appropriate
security guidelines&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robust and reliable services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Designing for a trustworthy, secure environment is one thing. The other
major focus area for infrastructure services is the availability,
robustness, and resilience of the service. While not all services
require to be up and running 24/7, nowadays it is hard to imagine
many services to still have significant downtimes.&lt;/p&gt;
&lt;p&gt;So, you often need to architecture and design for a resilient service,
which can take a beating if it has to. Organize load- and stress testing.
Organize disruptive testing if needed, and learn from the results to build
a better service. Design for a service that you can do technical maintenance
on without disrupting the customers themselves as much as possible - but
don't overshoot.&lt;/p&gt;
&lt;p&gt;If the service is set up in multiple locations, make sure that there is
independence between these locations (often across different regions) so
that failures in one region do not affect the other regions. Consider a setup
as used in many public cloud environments: high availability across availability
zones in the same region, and regional independence while allowing for
cross-region usage scenarios for the customers.&lt;/p&gt;
&lt;p&gt;Assess what can go wrong, and either try to update the architectures and
designs to be more resilient against these failures, or establish procedures
and processes to quickly recover. A common focus area here is to recover
from disasters (using so-called Disaster Recovery Procedures), and there 
are plenty of disasters to assess: data center failures, large Internet
outages, ransomware or other cyberattacks, worldwide epidemic outbreaks,
etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quality assurance at all stages&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The organization has to be able to provide fixes for defects and problems that
were raised, or to preemptively fix issues before they reach the customers. That
doesn't mean that the organization needs to be able to develop the fixes itself:
especially with off-the-shelf products the development is done by the
independent software vendor (ISV). However, the organization does have the
responsibility to track and put their weight on this so that the issues are
indeed properly resolved (and in case of a third-party product, preferably
through a fix that is applied to the main product, and not a one-off for that
particular company). Of course, if the service is developed in-house, then the
development of the fixes has to be guaranteed by the organization as well.&lt;/p&gt;
&lt;p&gt;To be able to provide secure and reliable services, it is vital to have
good change management processes and tools in place so that you can
approach the various stages of quality assurance before reaching production.
In &lt;a href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/"&gt;The pleasures of having
DTAP&lt;/a&gt; I mention
the benefits of having four environments for the various stages of a development
lifecycle (development, testing, acceptance, and production) and that is
perfectly applicable to infrastructure services as well, even when the
environments for infrastructure services might be isolated from those of the
more business-oriented development stages: you want to make sure that the
business-oriented development has production-grade services for its processes,
and not the intermediate and possibly less reliable in-development infrastructure
services.&lt;/p&gt;
&lt;p&gt;Throughout these environments, testing can (should) be introduced to provide
guidance to the engineers and developers to improve on the service. Testing can
take several forms: unit testing within the frameworks, integration testing of
code in larger environments, integration testing of new products in those
environments, sanity testing with simple use cases to ensure nothing blows
up (figuratively... hopefully), regression tests to ensure no fixed defects
creep back in, acceptance tests for specific features or use cases, load
testing to ensure the product stays up under the expected loads (and the
individual components or services have the right performance profile), stress testing
to ensure the product is resilient against higher loads or bursts, destructive
tests to see the product behaves as expected when unauthorized usage is
performed, security testing and penetration testing to provide assurance
on the secure state of the product deployment, etc.&lt;/p&gt;
&lt;p&gt;In many cases, the QA part is driven by organizational requirements, and not all
products require the same intensity on the tests and assurance levels. But, by
introducing the right automation, it is far easier to include multiple test
scenarios and test types in your pipeline.&lt;/p&gt;
&lt;p&gt;Products then pass through the various stages according to the change
management principles and processes that are in effect in the organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Strategy and roadmap&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The team responsible for an infrastructure service will also need to consider
the service in the long term: is the current technology (or set of products)
still state-of-the-art, mature, and following market practices? Or does the team
consider the technology to be relatively outdated and in need of an update? When
would the right time be to address this update?&lt;/p&gt;
&lt;p&gt;Perhaps the currently used technology is nearing its end-of-sale, end-of-support
(EOS), or even end-of-life (EOL). In that case, the team has to be ready to
address these lifecycle stages accordingly, be it through migrations, or
refactoring of current usages. Perhaps the teams find that it is more sensible
to get an extended support contract in place, or that they have the ability to
take the support (including code development) completely internally. Whatever
the choice is, it has to be made clear for the wider organization, and the
support team has to be ready to take on its commitments.&lt;/p&gt;
&lt;p&gt;There is a distinction between the service (or better yet, "capability") that is
offered, versus the products and technologies that support and realize it.
Capabilities will be needed by the organization for a long, long time, whereas
the products that realize these capabilities can have much shorter timeframes.&lt;/p&gt;
&lt;p&gt;The team will need to consider alternatives for the products, and see when those
alternatives make sense to address and implement. Perhaps the organization might
benefit from multiple implementations using different technologies because they
serve different usage scenarios, and the costs of keeping multiple technologies
in the air is less than the value it provides. Or perhaps an alternative needs
to be quickly realizable in case of a sudden exit scenario of the current
technology (or vendor). In that case, while the alternative doesn't need to be
up and running, the team must be able to address the change in due time.&lt;/p&gt;
&lt;p&gt;The need to have a proper roadmap on the capability and products that are being
used also reflects in the relationship that that team has with the vendor. For
strategically important products, an organization might even want to participate
in that vendor's Customer Advisory Board (CAB) or equivalent programs. The
team should have the time and resources to collaborate with that vendor to build
a partnership, participate in the conferences and other events, as that provides
input to the team on the progress and future of that product. Those insights are
primordial to properly design and organize an internal roadmap.&lt;/p&gt;
&lt;p&gt;Once the internal roadmap has taken shape, then the responsible teams are
involved in supporting the organization through updates and upgrades, by
communicating the need for these upfront (so that the internal customers can
plan around it), and to track the progress so there are no lingering risks for
the wide organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More elaborate (internal) customer support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The support for the infrastructure service often has to extend beyond the
operational support that I started with in this article. Teams that provide
certain capabilities within the organization don't do this as a vendor, but as
part of that organization. So when an internal customer needs help in migrating
away from a service, the team is still involved in this process.&lt;/p&gt;
&lt;p&gt;Hence, more elaborate support such as migrations (both towards a new technology,
service or capability, as well as migration away from it), finding
suitable alternatives that are not properly handled by the capability (and thus
might be best provided by a different team), ... helps in building out the trust
that the wider organization has in IT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost, licenses, and contractual obligations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many services have certain contractual obligations associated with them, often
known as the Terms and Conditions of the contract and product usage.
Infrastructure teams will need to make sure that these T&amp;amp;Cs are known and that
the organization adheres to them.&lt;/p&gt;
&lt;p&gt;The cost of an infrastructure service usage also needs to be correctly devised
and accounted for. Teams have to make sure the product usage remains within the
allocated licenses (or, if there is no capping in place, that the usage is
sufficiently constrained that the organization does not get any surprises), and
is often involved in defining a chargeback towards the rest of the organization.&lt;/p&gt;
&lt;p&gt;I tend to make a distinction between showback (show the organization how much a
service costs to the company or usage group) and chargeback (a governance-driven
or tax-driven requirement for charging usage to the organization), as the latter
is more a company decision on how to approach this, whereas the showback is the
actual, factual cost. Showback is needed to support conscious decisions on next
steps or consumption patterns, whereas chargeback might be necessary for
tax reasons in larger corporations where IT is considered as part of a different
legal entity.&lt;/p&gt;
&lt;p&gt;Addressing cost, licenses, and T&amp;amp;Cs is not to be underestimated. Many vendors
make this very difficult, as that allows for many interpretations during license
audits that can give a nice bonus to the vendor if he can show that his
interpretation is more appropriate than how you thought that the contract or
license was structured.&lt;/p&gt;
&lt;p&gt;The cost of a product or technology is not just the cost of the purchase, and
even that cost is still somewhat variable: many things are open for negotiation,
and you also don't need to tackle this directly with the vendor, as there is a
large market of middle parties that facilitate purchasing technologies. These
can often provide better rates as they can bundle purchases of multiple
customers and thus negotiate better deals with the main vendor.&lt;/p&gt;
&lt;p&gt;The cost also depends on the support contract associated with it, as well as
the costs of other depending technologies (such as capacity requirements) that come
from its implementation. This is often neglected in SaaS purchases: even though
you have correctly negotiated a good price for the SaaS service, you might be
jeopardizing your internet connectivity and need to upgrade the bandwidth, the
anti-DDoS service, your firewall capabilities, etc. because the consumption of
that SaaS service is significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The responsibilities for managing and tracking infrastructure services are large
and not to be underestimated. It is not a matter of deploying a new service and
assuming everybody can deal with it, nor are all responsibilities equally
visible to the end-user.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1481535279861280769"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="RACI"></category><category term="responsibilities"></category></entry><entry><title>The pleasures of having DTAP</title><link href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/" rel="alternate"></link><published>2021-12-30T12:00:00+01:00</published><updated>2021-12-30T12:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-12-30:/2021/12/the-pleasures-of-having-DTAP/</id><summary type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What are these four environments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's go over the four environments one by one with a small introduction
to their purpose. I'll cover more specific use cases further down in this
post.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Development&lt;/em&gt; environment is a functionally complete environment on
which the development of products or code is done. It should have the same
technologies in place and very similar setups (deployments) so that
developers are not facing a too different environment. Too much difference
might imply different behavior, which is contra-productive. The environment
is accessible by developers and testers, and is mainly development-oriented.&lt;/p&gt;
&lt;p&gt;Products or code that are being developed will be visible and used in this
environment. Developers and engineers hardly have any threshold to reach for
deploying or modifying code here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Testing&lt;/em&gt; environment is used when the development has reached a phase
where the product or code has passed a minimum of quality. Unit tests
succeed, the code builds fine, and the developer has indicated that the
code or product is ready for wider testing (hence the name).&lt;/p&gt;
&lt;p&gt;A testing environment generally applies a multitude of tests, most of
them (hopefully) automated, but with a strong dependency on testers
(also known as Quality Assurance engineers or QA engineers) to find issues.&lt;/p&gt;
&lt;p&gt;The automated tests focus on integrations, regression testing, security
testing, etc., and provide insights into the new builds or products which the
development team can take up and iterate over to improve the product.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Acceptance&lt;/em&gt; environment is a production-like environment, not
just for one product, but for the entire business or business unit: the same
setup, the same infrastructure, the same foundations, the same application
portfolio, the same integrations, etc. This environment intends to 
validate that the product is fully ready to be released. It is often
also abbreviated as the &lt;em&gt;User Acceptance Testing (UAT)&lt;/em&gt; environment.&lt;/p&gt;
&lt;p&gt;While the testing environment is generally approached by QA engineers, the
acceptance environment should be more tailored to business testers. When
developers or engineers introduce a feature, the stakeholders that
requested that feature use the acceptance environment to accept if the
coming release fulfills their request or not.&lt;/p&gt;
&lt;p&gt;As the environment is production-like in its entirety (and not just for
a single product), it is a prime target for executing performance tests
as well. Cross-product dependencies and processes (like migrations) are
validated here too.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Production&lt;/em&gt; environment is the environment in which the product or
code "goes live", where the customers use it. That does not mean that
a product put in production is immediately accessible for the customers:
there is still a difference between deployment (bring to production),
activation (enable usage), and release (use by customers).&lt;/p&gt;
&lt;p&gt;While DTAP is well-known in larger organizations, there are some challenges
or misconceptions that I would like to point out, and which I discuss
further down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environments are more than just the systems where the code is deployed to.
  Each environment has production services associated with it.&lt;/li&gt;
&lt;li&gt;A prime challenge to implementing DTAP is the cost associated with it. But
  it does not need to be as expensive as you think, and DTAP implementations
  often have a positive business case.&lt;/li&gt;
&lt;li&gt;Agile methodologists might find DTAP to be old-style. They are correct
  that many implementations are prohibitive towards fast deployment and
  release strategies, but that isn't because DTAP is conceptually wrong.&lt;/li&gt;
&lt;li&gt;Not all environments need the same data. On the contrary, a proper DTAP
  design likely uses separate datasets in each environment to deal with 
  the security and regulatory requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conceptual environments still require production services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The purpose of each environment is strongly tied to the 'phase' in which
the code or the product resides in the development lifecycle. That means
that these environments have a strong focus on that product or code, and
not on the services that are needed.&lt;/p&gt;
&lt;p&gt;Indeed, a development environment also entails services that are already
'in production', like a usable workstation, development services like
code repositories and build systems, ticketing services, and more. A testing
environment requires test automation engines, regression test frameworks,
security tools, and more. All these services are production-ready - and they
often have their own DTAP environments as well.&lt;/p&gt;
&lt;p&gt;It is a common misconception that a development-oriented system or service
has a lower SLA or lower risk profile than production, and infrastructure
architecture should make clear that there is a difference between the
systems that host the products or code under review and the systems that
facilitate the functionality needed within the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Static cost is a major inhibitor for implementing DTAP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaller companies or organizations might be hesitant to introduce DTAP
environments as it might be cost-prohibitive. While it is true
that, from a 'static' view, a DTAP environment costs more than a
production-only environment, you need to consider the impact of implementing
DTAP in the processes.&lt;/p&gt;
&lt;p&gt;The main purpose of DTAP is not to make your CFO angry, but to improve the
quality of your production environment (and usage). Ask yourself: how costly
is it when your production systems go down, or when your customers complain
and you need to fix things... Do you update code directly on the server(s)?
What if a security patch is rolled out and suddenly prevents your
customer-facing application from working?&lt;/p&gt;
&lt;p&gt;While DTAP mentions four environments, some companies settle with three, and
others with five or more. Perhaps your testing and acceptance are done by the
same people, and you do not have many automated testing facilities at hand.
Splitting your pre-production environments into multiple environments doesn't
make sense yet, and you might first want to focus on improving your testing
maturity in general.&lt;/p&gt;
&lt;p&gt;If you want to make the case for DTAP, consider the use cases or scenarios that
have visibly disrupted your business and how/where the environments would have
helped. In many cases, you'll notice that there is a positive business case
for a step-wise move towards DTAP.&lt;/p&gt;
&lt;p&gt;Furthermore, a proper design of these environments will facilitate an
economical view towards DTAP. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can use pay-as-you-use environments (as is commonly the case in the 
  public cloud) which you only activate when you do your testing. If you have
  a 24/7 customer-facing service in production, that doesn't mean that your
  acceptance environment has to be 24/7. Yes, it should be as much
  production-like as possible, but if it isn't doing anything outside
  business hours, then you don't need it running outside business hours.&lt;/li&gt;
&lt;li&gt;Commercial products often have distinct terms and conditions for
  non-production usage. You can have databases in production with a
  premium, gold service level agreement, while having the same database in
  the other environments with a low, bronze service level agreement
  towards that vendor: cheaper, but technically the same.&lt;/li&gt;
&lt;li&gt;Abstraction and virtualization technologies allow for better control
  of the resources that are being used. For instance, you can have
  an acceptance environment that is only at 20% of the resources
  of production for day-to-day validation, and then increase its resources
  to 100% for load testing periods. If these environments are not in a
  pay-as-you-use model, shifting resources from one environment to
  another allows for controlling the costs.&lt;/li&gt;
&lt;li&gt;Security controls in these environments might be different as well,
  assuming that these environments have different data needs: if you
  use fictitious data in development and testing, and anonymized data
  in acceptance, then the investments on, say, data leakage controls
  might be different for these environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, while DTAP is at first glance a costly approach, the actual case is
that it is positive for the company.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DTAP is not inefficient, but some implementations are&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a world where "Release fast, release often" is the norm, having a rigid
DTAP strategy might be contra-productive. However, this is more an
implementation concern than a conceptual one. If I consider the downsides
that &lt;a href="https://medium.com/the-liberators/want-to-be-agile-drop-your-dtap-pipeline-7dcb496fe9e3"&gt;Christiaan Verwijs&lt;/a&gt;
mentions in his post, I feel that I can safely state that this is not due
to the lifecycle of the code or product, but rather the choices that a
company has made while assessing and implementing a release strategy.&lt;/p&gt;
&lt;p&gt;There is no need to bundle and make bulk releases with DTAP. You can
perfectly design an environment where DevOps teams can release easily
to production. More so, the bulk release strategy is frequently the result
of an application design constraint, not a deployment constraint.&lt;/p&gt;
&lt;p&gt;Development methodologies and DTAP environments do need to be tailored
to each other. The purpose of DTAP is to facilitate the quality of products
and code, and thus should be tailored towards efficient and qualitative
development processes. In many environments, DTAP is synonymous with
"infrastructure operations" and that's a wrong approach. 
Operations-oriented teams (like &lt;em&gt;Site Reliability Engineers (SREs)&lt;/em&gt;),
development-oriented teams, and DevOps teams all have the same benefits from
DTAP.&lt;/p&gt;
&lt;p&gt;Some might state that an acceptance environment is no longer suitable in
the modern age, as they can deploy products and code to production without
losing the benefits of the acceptance environment. With blue/green
deployments or canary releases, you can enable business testers and
stakeholders to validate new features or code before releasing it
to the wider public.&lt;/p&gt;
&lt;p&gt;To accomplish this properly, however, the platform that is used will
balance resources accordingly, and you're conceptually implementing an
(albeit temporary) acceptance environment in an automated way. This is
an implementation choice and has to be balanced against the requirements
that the organization has.&lt;/p&gt;
&lt;p&gt;For instance, if you work with sensitive data, you might not be allowed
to use this data during testing. In Europe, the &lt;em&gt;General Data Protection
Regulation (GDPR)&lt;/em&gt; is a strong regulatory requirement for dealing with
sensitive data. It isn't a playbook though: companies need to evaluate
how and where data is used, and perhaps the balance made by the company
allows, if not with explicit consent, to use data unmodified for
acceptance testing. But if that isn't the case and your acceptance tests
need to use sanitized data, then having separate environments is likely
more sensible (although different implementations exist that allow
for anonymization in production as well - they're, however, not as easy
to implement).&lt;/p&gt;
&lt;p&gt;Plus, DTAP does not imply that production is doing everything in a single
unit of work: Deploy, Activate and Release. You can still perfectly position
those tasks in production while having an explicit acceptance environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Separate datasets in each environment make sense&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For regulated companies and organizations, security officers might want
to use the DTAP distinction to focus on data minimization strategies as well.
As mentioned before, the GDPR is a strong regulatory requirement whose
alignment can be facilitated by a well-designed DTAP approach.&lt;/p&gt;
&lt;p&gt;You can use fictitious data in development and testing, with development
using datasets that developers use for validating the specific functionality
they are working on (and preferably share and put alongside the code
and products), whereas testing uses a coherent but still fictitious
dataset. I use "coherent" here as an indication that the data should
be functionally correct and integer: a (fictitious) person record in the
customer database in the testing environment should be mapped to the
(fictitious) calls or other interactions that are stored in the support
database (also in the testing environment) and the (fictitious) portfolio
that this (fictitious) person has in the product database (in the testing
environment).&lt;/p&gt;
&lt;p&gt;Don't underestimate how powerful, but also how challenging a good fictitious
dataset is.&lt;/p&gt;
&lt;p&gt;For acceptance testing, perhaps the company decided that anonymized data
is to be used. Or it uses pseudonymized data (which is a weaker form)
with additional technical controls to prevent leakage and attacks (including
inference) that try to deduce the origin of the data.&lt;/p&gt;
&lt;p&gt;Again, these are choices by the company or organization, which need to be
taken with the risk and business stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DTAP is a sensible approach for improving quality in products and code.
While it isn't the holy grail for quality assurance, it has solid
foundations that many larger companies and organizations feel comfortable
with. The implementation details make or break how well-adjusted the DTAP
approach is for modern development processes and regulatory requirements.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1476505537047109635"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="DTAP"></category><category term="environments"></category><category term="zoning"></category><category term="development"></category><category term="test"></category><category term="acceptance"></category><category term="production"></category></entry><entry><title>Creating an enterprise open source policy</title><link href="https://blog.siphos.be/2021/11/creating-an-enterprise-open-source-policy/" rel="alternate"></link><published>2021-11-20T15:00:00+01:00</published><updated>2021-11-20T15:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-20:/2021/11/creating-an-enterprise-open-source-policy/</id><summary type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Assess the current situation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When drafting a policy, make sure you know what the current situation already
is. Especially when the policy might be very restrictive, you might be facing a
huge backlash from the organization if the policy is not reflecting the reality.
If that is the case, and the policy still needs to go through, proper
communication and grooming will be needed (and of course, the "upper management
hammer" can help out as well).&lt;/p&gt;
&lt;p&gt;Often, higher management is not aware of the current situation either. They
might think that open source is hardly in use. Presenting them with facts and
figures not only makes it more understandable, it will also support the need for
a decent open source policy.&lt;/p&gt;
&lt;p&gt;When you have a good view on the current usage, you can use that to track where
you want to go to. For instance, if your company wants to adopt open source more
actively, and pursue open source contributions, you might want to report on the
currently detected contributions, and use that for follow-up later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get HR and compliance involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before you embark on the journey of developing a decent open source policy, make
sure you have management support on this, as well as people from HR and your
compliance department (unless your policy will be extremely restrictive, but
let's hope that is not the case).&lt;/p&gt;
&lt;p&gt;You will need (legal &amp;amp;) compliance involved in order to draft and assess the
impact of internal developers and engineers working on open source projects, as
well as the same people working on open source projects in their free time. Both
are different use cases but have to be assessed regardless.&lt;/p&gt;
&lt;p&gt;HR is generally involved at a later stage, so they know how the company wants to
deal with open source development. This could be useful for recruitment, but
also for HR to understand what the policy is about in case of issues.&lt;/p&gt;
&lt;p&gt;An important consideration to assess is how the company, and the contractual
obligations that the employees have, deals with intellectual property. In some
companies, the contract allows for the employees to retain the intellectual
property rights for their creations outside of company projects. However, that
is not always the case, and in certain sectors intellectual property might be
assumed to be owned by the company whenever the creation is something in which
the company is active. And that might be considered very broadly (such as
anything IT related for employees of an IT company).&lt;/p&gt;
&lt;p&gt;The open source policy that you develop should know what the contractual
stipulations say, and clarify for engineers and developers how the company
considers the intellectual property ownership. This is important, as it defines
who can decide to contribute something to open source.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Understand and simplify license requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many of the decisions that the open source policy has to clarify will be related
to the open source licenses in use. Moreover, it might even be relevant to
define what open source is to begin with.&lt;/p&gt;
&lt;p&gt;A good source to use is the &lt;a href="https://opensource.org/osd"&gt;Open Source Definition&lt;/a&gt;
as published and maintained by the &lt;a href="https://opensource.org/"&gt;Open Source Initiative
(OSI)&lt;/a&gt;. Another definition is the one by the &lt;a href="https://www.fsf.org/"&gt;Free
Software Foundation&lt;/a&gt; titled "&lt;a href="https://www.fsf.org/about/what-is-free-software"&gt;What is free software and
why is it so important for society&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;The license is the agreement that the owner of the software puts out that
declares how users can use that software. Most, if not all software that a
company uses, will have a license - open source or not. But most commercial
software titles have specific licenses that you need to go through for each
specific product, as the licenses are not reused. In the open source world,
licenses are reused so that end users do not need to go through product-specific
terms.&lt;/p&gt;
&lt;p&gt;The OSI organization has a list of &lt;a href="https://opensource.org/licenses"&gt;approved
licenses&lt;/a&gt;. However, even amongst these
licenses, you will find different types of licenses out there. While they are
commonly grouped into &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;copyleft&lt;/a&gt; and
&lt;a href="https://fossa.com/blog/all-about-permissive-licenses/"&gt;permissive&lt;/a&gt; open source
licenses, there are two main categories within the copyleft licenses that you
need to understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strong copyleft licenses that require making all source code available upon
  distribution, or sometimes even disclosure of the application base&lt;/li&gt;
&lt;li&gt;"scoped" copyleft licenses that require making only the source code available of the
  modules or libraries that use the open source license (especially if you
  modified them) without impacting the entire application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the term "strong copyleft" is something that I think is somewhat generally
accepted (such as in the Snyk article "&lt;a href="https://snyk.io/learn/open-source-licenses/"&gt;Open Source Licenses: Types and
Comparison&lt;/a&gt;" or in &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;Wikipedia's
article&lt;/a&gt;), I do not like to use its
opposite "weak" term, as the licenses themselves do not reduce the open source
identity from the code. Instead, they make sure the scope of the license is
towards a particular base (such as a library) and not the complete application
that uses the license.&lt;/p&gt;
&lt;p&gt;Hence, open source policies might want to focus on those three license types for
each of the use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;permissive licenses, like Apache License 2.0 or MIT&lt;/li&gt;
&lt;li&gt;scoped copyleft licenses, like LGPL or EPL-2.0&lt;/li&gt;
&lt;li&gt;strong copyleft licenses, like GPL or AGPL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Differentiate on the different open source use cases&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several use cases that the policy will need to tackle. These are, in
no particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using off-the-shelf, ready-to-use open source products&lt;/li&gt;
&lt;li&gt;Using off-the-shelf libraries and modules for development&lt;/li&gt;
&lt;li&gt;Using open source code&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for company purposes&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for personal/private purposes&lt;/li&gt;
&lt;li&gt;Launching and maintaining open source projects from the company&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these use cases might have their specific focuses. Combine that with the
license categories listed earlier, and you can start assessing how to deal with
these situations.&lt;/p&gt;
&lt;p&gt;For instance, you might want to have a policy that generally boils down to the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When using off-the-shelf, ready-to-use open source products, all types of
  products are allowed, assuming the organization remains able to support the
  technologies adopted. Furthermore, the products have to be known by the
  inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using libraries or modules in development projects, only open source
  products with permissive or scoped copyleft licenses can be used. Furthermore,
  the libraries or modules have to be well managed (kept up-to-date) and known
  by the inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using open source code, only code that is published with a permissive
  license can be used. At all times, a reference towards the original author
  has to be retained.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for company purposes, approval has
  to be given by the hierarchical manager of the team. Contributions have to be
  tagged appropriately as originating from the company (e.g. using the company
  e-mail address as author). Furthermore, employees are not allowed to
  contribute code or intellectual property that is deemed a competitive
  advantage for the company.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for personal/private purposes,
  employees are prohibited to use code from the company or to do contributions
  using their company's e-mail address. However, the company does not claim
  ownership on the contributions an employee does outside the company's projects
  and hours.&lt;/li&gt;
&lt;li&gt;When creating new projects or publishing internal projects as open source,
  sufficient support for the project has to be granted from the company, and the
  publications are preferentially done within the same development services
  (like version control) under management of the company. This ensures
  consistency and control over the company's assets and liability. Projects have
  to use a permissive license (and perhaps even a single, particular license).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, if the company actively pursues an open source first strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Off-the-shelf, ready-to-use open source products are preferred over
  propriatary products. Internal support teams must be able to deal with
  general maintenance and updates. The use of commercially backed products is
  not mandatory, but might help when there is a need for acquiring short-term
  support (such as through independent consultants).&lt;/li&gt;
&lt;li&gt;Development projects must use projects that use permissive or scoped copyleft
  licenses for the libraries and dependencies of that project. Only when the
  development project itself uses a strong copyleft license are dependencies
  with (the same) strong copyleft license allowed. Approval to use a strong
  copyleft license is left to the management board.&lt;/li&gt;
&lt;li&gt;Engineers and developers retain full intellectual property rights to their
  contributions. However, a Contributor License Agreement (CLA) is used to grant
  the company the rights to use and distribute the contributions under the
  license mentioned, as well as initiate or participate in legal actions related
  to the contributed code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Clarify what is allowed to be contributed and what not&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example I already indicated a "do not contribute code that is
deemed a competitive advantage" statement. While it would be common sense,
companies will need to clarify this (if they follow this principle) in their
policies so they are not liable for problems later on.&lt;/p&gt;
&lt;p&gt;A competitive advantage primarily focuses on a company's crown jewels, but can
be extended with code or other intellectual property (like architectural
information, documentation, etc.) that refers to indirect advantageous
solutions. If a company is a strong data-driven company that gains massive
insights from data, it might refuse to share its artificial intelligence related
code.&lt;/p&gt;
&lt;p&gt;There are other principles that might decide if code is contributed or not. For
instance, the company might only want to contribute code that has received all
the checks and controls to ensure it is secure, it is effective and efficient,
and is understandable and well-written. After all, when such contributions are
made in name of the company, the quality of that code reflects upon the company
as well.&lt;/p&gt;
&lt;p&gt;I greatly suggest to include examples in the open source policy to clarify or
support certain statements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assess the maturity of an open source product&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When supporting the use of open source products, the policy will also have to
decide which open source products can be used and which ones can't. Now, it is
it possible to create an exhaustive list (as that would defeat the purpose of an
open source policy). Instead, I recommend to clarify how stakeholders can assess
if an open source product can be used or not.&lt;/p&gt;
&lt;p&gt;Personally, I consider this from a "maturity" point of view. Open source
products that are mature are less likely to become a liability within a larger
company, whereas products that only have a single maintained (like my own
&lt;a href="https://github.com/sjvermeu/cvechecker"&gt;cvechecker&lt;/a&gt; project) are not to be used
without understanding the consequences.&lt;/p&gt;
&lt;p&gt;So, what is a mature open source project? There are online resources that could
help you out (like the Qualipso-originated &lt;a href="https://en.wikipedia.org/wiki/OpenSource_Maturity_Model"&gt;Open Source Maturity Model
(OSMM)&lt;/a&gt;), but
personally I tend to look at the following principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The project has an active development, with more than 5 active contributors in
  the last three months.&lt;/li&gt;
&lt;li&gt;The project is visibly used by several other projects or products.&lt;/li&gt;
&lt;li&gt;The project has well-maintained documentation, both for developers and for
  users. This can very well be a decent wiki site.&lt;/li&gt;
&lt;li&gt;The project has an active support community, with not only an issue system,
  but also interactive services like forums, IRC, Slack, Discord, etc.&lt;/li&gt;
&lt;li&gt;The project supports more than one major version in parallel, and has a clear
  lifecycle for its support (such as "major version is supported up to at least
  1 year after the next major version is released").&lt;/li&gt;
&lt;li&gt;The project publishes its artefacts in a controlled and secure manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A policy is just the beginning, not the end&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As always, there will be situations where a company wants to allow a one-off
case to deviate from the policy. Hence, make clear how deviations can be
targeted.&lt;/p&gt;
&lt;p&gt;For instance, you might want to position an architecture review board to support
deviations from the license usage. When you do, make sure that this governance
body knows how to deal with such deviations - understanding what licenses are,
what the impact might be towards the organization, etc.&lt;/p&gt;
&lt;p&gt;Furthermore, once the policy is ready to be made available, make sure you have
support for that policy in the organization, as well as supporting tools and
processes.&lt;/p&gt;
&lt;p&gt;You might want to include an internal community to support open source/free
software endeavors. This community can help other stakeholders with the
assessment of a product's maturity, or with the license identification.&lt;/p&gt;
&lt;p&gt;You might want to make sure you can track license usage in projects and
deployments. For software development projects, there are plenty of commercial
and free services that scan and present license usage (and other details) for a
project. Inventory and asset management utilities often also include
identification of detected software. Validate that you can report on open source
usage if the demand comes up, and that you can support development and
engineering teams in ensuring open source usage is in line with the company's
expectations.&lt;/p&gt;
&lt;p&gt;The company might want to dedicate resources in additional leakage detection and
prevention measures for the open source contributions. While the company might
already have code scanning techniques in place in their on-premise version
control system, it might be interesting to extend this service to the public
services (like &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt; and
&lt;a href="https://about.gitlab.com/"&gt;GitLab&lt;/a&gt;). And with that, I do not want to imply
using the same tools and integrations, but more on a functional level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing off&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few companies, and most governmental organizations, publish their open source
policies online. The &lt;a href="https://todogroup.org/"&gt;TODO Group&lt;/a&gt; has graceously drafted
a &lt;a href="https://github.com/todogroup/policies"&gt;list of examples and templates&lt;/a&gt; to
use. They might be a good resource to use when drafting up your own.&lt;/p&gt;
&lt;p&gt;Having a clear and understandable open source policy simplifies discussions, and
with the appropriate support within the organization it might jumpstart
initiatives even further. Assuming the policy is sufficiently supportive of open
source, having it published might eliminate the fear of engineers and developers
to suggest certain open source projects.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1462043477835976705"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="opensource"></category><category term="enterprise"></category><category term="legal"></category><category term="compliance"></category></entry><entry><title>Hybrid cloud can be very complex</title><link href="https://blog.siphos.be/2021/11/hybrid-cloud-can-be-very-complex/" rel="alternate"></link><published>2021-11-08T20:00:00+01:00</published><updated>2021-11-08T20:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-08:/2021/11/hybrid-cloud-can-be-very-complex/</id><summary type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Hybrid cloud complexity starts with the definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing that I've already learned is not to use "hybrid cloud" without
defining what I mean. And if somebody else uses it (or a research article), I
will frantically try to get a definition on what the person or article implies
with the term.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://azure.microsoft.com/en-us/overview/what-is-hybrid-cloud-computing/"&gt;Microsoft&lt;/a&gt;
for instance defines hybrid cloud as "a computing environment that combines an
on-premises datacenter with a public cloud, allowing data and applications to
be shared between them."&lt;/p&gt;
&lt;p&gt;This definition isn't unambiguous. What does Microsoft mean with "sharing"? If
I expose an application and/or its data through APIs that are shielded and
independently managed in each environment, and allow for interaction between
those APIs, does that still entail a hybrid cloud architecture? Because if that
is the case, then I want to know what other cloud interaction architectures
Microsoft thinks exist besides hybrid cloud.&lt;/p&gt;
&lt;p&gt;I do think that the intention of Microsoft's definition is that the cloud
hosting environments are considered as "similar enough" to the on premise
environment, and managed in the same way (some cloud specifics
notwithstanding), as inspired by their claim that hybrid cloud allow
"businesses [to] use the cloud to instantly scale capacity up or down to handle
excess capacity" and that organizations using hybrid cloud architectures "are
able to use many of the same security mreasures that they use in their existing
on-premises infrastructure".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gartner.com/en/information-technology/glossary/hybrid-cloud-computing"&gt;Gartner&lt;/a&gt;
defines hybrid cloud computing as "policy-based and coordinated service
provisioning, use and management across a mixture of internal and external
cloud services." That doesn't narrow things down, and in &lt;a href="https://www.gartner.com/document/3956442"&gt;(paywalled) research
articles&lt;/a&gt;, Gartner does express that
"hybrid cloud is a vague term that does not allow enough granularity for
implementation planning by cloud and infrastructure professionals".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://go.forrester.com/blogs/13-08-02-cloud_management_in_a_hybrid_cloud_world/"&gt;Forrester&lt;/a&gt;
follows a definition that is very generic, as "a cloud service connected to any
other corporate resource" makes it a hybrid cloud service. Regardless of how it
is infrastructurally or application-wise integrated.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ibm.com/cloud/learn/hybrid-cloud"&gt;IBM&lt;/a&gt; declares hybrid cloud as
"integrates public cloud services, private cloud services and on-premises
infrastructure and provides orchestration, management and application
portability across all three. The result is a single, unified and flexible
distributed computing environment [...]"&lt;/p&gt;
&lt;p&gt;This definition does seem to imply an architecture that considers all hosting
environments as infrastructurally equal (as they see the sum of all
environments as a single, unified environment).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud"&gt;Wikipedia&lt;/a&gt;
mentions that hybrid cloud "is a composition of a public cloud and a private
environment [...] that remain distinct entities but are bound together,
offering the benefits of multiple deployment models." Such a definition leaves
the implementation details open, as it boils down to any architecture where you
mix such hosting environments.&lt;/p&gt;
&lt;p&gt;For me, a &lt;strong&gt;hybrid cloud on infrastructure level&lt;/strong&gt; implies that the different
environments are similarly or equally managed, using the same processes,
principles and most often even tooling, and that application teams have little
impact on where their application (or application components) are hosted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The promise of hybrid cloud towards business&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When business decision makers hear (or are confronted with) hybrid cloud, they
are often told that it a perfect way to deal with capacity management. Whereas
a pure on-premise deployment model requires you to purchase and deploy enough
capacity to deal with your maximum workload (and even more, if you need to
consider disaster recovery situations), a hybrid cloud could simply "add more
resources as needed, without requiring the application to be refactored for
cloud-native or cloud hosting".&lt;/p&gt;
&lt;p&gt;Let's make this more tangible with an example: a simple ticket sales service,
which consists out of a website (frontend) and an API (which is backend-alike).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales service overview" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-overview.png"&gt;&lt;/p&gt;
&lt;p&gt;The company that manages this ticket sales application is currently fully
on-premise, with a simple deployment model where the front and backend are
hosted on a web application hosting cluster (which could be a Kubernetes
cluster), and the backend also uses a database.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level infrastructure" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-hlinfra.png"&gt;&lt;/p&gt;
&lt;p&gt;Ticket sales are seasonally bound, and often ticket sales platforms are
services offered to the specific events. Suppose a major event wants to sell
its tickets through this ticket sale application, and you are afraid that the
website part will not be able to deal with the load, then you could use a
hybrid cloud setup to enable bursting on the front-end side.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level bursting" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-burstfrontend.png"&gt;&lt;/p&gt;
&lt;p&gt;Of course, this is just one of many target architectures that might solve the
capacity challenge, and there is no reason to believe the API itself wouldn't
be overloaded as well. But let's stick to this simple example.&lt;/p&gt;
&lt;p&gt;From a business perspective, this all sounds very fun and promising. There
seems to be no initial investment needed, and the capacity of the cloud is
limitless, not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network investments needed for such hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, as always, the devil is in the details. If we were to pursue this
architecture, we need to have the public cloud and the on premise environment
properly connected. You don't want to use regular Internet access, because the
intention is to see these environments as a single, unified environment, and
you don't open up your internal systems directly to the Internet either do you?&lt;/p&gt;
&lt;p&gt;So you need to architect the public cloud usage to be as private as possible,
and then connect that environment with your on premise network, preferably
through a high speed private link. Sure, you can use VPN over internet, but
with a private link you have more guarantees on the latency for instance, and
for many cloud environments such private link interactions are also bringing
benefits for data ingress/egress (cheaper for you). They also generally have
better SLAs (although larger environments will have very high internet-related
SLAs) and do not require the same security protection measures (like anti-DDoS
protection).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales network connectivity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-connectivity.png"&gt;&lt;/p&gt;
&lt;p&gt;In the design, we assume that the end users still first go through your on
premise environment, as the perimeter protections you have in place for
instance still need to apply. Perimeter protections are not just simple
firewall capabilities. It includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anti-DDoS measures&lt;/li&gt;
&lt;li&gt;context gathering for, and applying coarse-grained access controls&lt;/li&gt;
&lt;li&gt;intrusion detection and prevention&lt;/li&gt;
&lt;li&gt;anti-malware protection&lt;/li&gt;
&lt;li&gt;application attack prevention&lt;/li&gt;
&lt;li&gt;network traffic filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A perimeter is meant to act as a first line of defense. However, when
integrating networks from external sites, you still need some protection
measures applied (shown as external site protection in the diagram), as you are
handing off some ownership to other parties and thus want to have some
safeguards in place.&lt;/p&gt;
&lt;p&gt;Because we still have all traffic through the perimeter, burst loads can still
jeopardize the service offering itself. If the perimeter, internet line, or the
load balancer that spreads the load across the frontends is saturated, then
your service will go down. The hybrid cloud setup used in this example wont
help out here.&lt;/p&gt;
&lt;p&gt;Second, the high speed private link will need to be able to deal with not only
the load of the user to the frontend (and back), but also between the frontend
and backend. And if you were to support bursting the backend application to the
public cloud as well, then it needs to deal with the load between this
application and the database.&lt;/p&gt;
&lt;p&gt;The link is also often not something you set up yourself between you and the
public cloud. You will need intermediate parties to support this, as often this
first requires you to have private links to certain larger networks, and then
have this larger network set up a private link to the point of presence where
you want to 'attach' to that public cloud environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Management complexity rises with hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The investments do not stop just at the network connectivity. You will need to
look into managing the web application (such as deployment and releases),
servers (bootstrapping, updating, maintaining), and other network areas.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hybrid cloud management complexity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-management.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's start with the web application management. Your existing management
systems will now need to deal with the public cloud as well. Your application
needs to be deployed on multiple clusters, and you will need to reconfigure the
load balancers in front to know where these clusters are. Unlike the
pre-installed environments on premise, public cloud is more dynamic (you want
to use it for bursting after all), so the target IP addresses might change (or
you set up fixed IP addresses, but that costs money even when you don't use
them).&lt;/p&gt;
&lt;p&gt;You will need to deal with deployments that can succeed left and fail right, or
vice-versa. While this is not impossible to deal with, the current way of
working might not support that yet because, you know, you never had to deal
with it.&lt;/p&gt;
&lt;p&gt;What about tracking performance and user experience? Your application
management suite might not know about public cloud setups yet, and once
included, it might find that there is latency impact. But can you just use this
APM suite in the public cloud? Perhaps your company has a site license which
does not include other locations. Or it requires per-node licenses, and require
each node to be assigned a license for 30 days at least. In case of burst
situations, you might only have these systems up for a few hours, and with the
next action, these will be new nodes with new licenses.&lt;/p&gt;
&lt;p&gt;Also on the server management level you might find many obstacles. Your on
premise system might use a certain hypervisor integration (e.g. using VMware
vCenter API) which you don't have in the public cloud. So you need to adapt the
management system anyway, which means you need to develop your management
systems to create a hybrid cloud, rather than reap the benefits of hybrid cloud
directly.&lt;/p&gt;
&lt;p&gt;Your servers might use many control systems that have the same licensing issues
as mentioned earlier. Or they are latency-bound, causing either reachability
issues, or requiring you to adapt the infrastructure architecture of these
management systems to be aware of the public cloud.&lt;/p&gt;
&lt;p&gt;Also on network management level it isn't just about connectivity. Your
firewall management might not see the public cloud firewalls automatically (or
doesn't support it), or your current network design doesn't allow for the
bursting of network environments (subnets) in a sufficient dynamic manner.&lt;/p&gt;
&lt;p&gt;The more you consider this hybrid cloud situation, the more you find out that
you will need to revisit all management, support and control systems for this
setup. And you know what is hard to do in an IT environment? Reassessing &lt;em&gt;all&lt;/em&gt;
management, support and control systems. You are effectively redesigning your
entire IT environment, and that is exactly what the promise of "hybrid cloud"
wanted to take away. Or at least, the promise that is done to certain decision
makers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendors know that it is complex (and are happy because of it)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Most of the IT vendors that are related to the management, support and control
of your infrastructure, will say that management in hybrid cloud is hard: &lt;a href="https://www.redhat.com/en/blog/operating-hybrid-architecture-and-managing-complexity"&gt;Red
Hat&lt;/a&gt;
for instance mentions in its container-related article that "Saying that
Kubernetes makes it possible to build a cross-environment management layer
doesn't mean it's easy". &lt;a href="https://blogs.arubanetworks.com/solutions/easing-the-complexity-of-hybrid-cloud/"&gt;Aruba
Networks&lt;/a&gt;
(part of PHE) mentions that "the major drawback is complex management of these
platforms". Consulting firms concur as well, with for instance David Linthicum,
Chief Cloud Strategy Officer of Deloitte Consulting, mentioning in &lt;a href="https://techbeacon.com/enterprise-it/4-things-you-need-know-about-managing-complex-hybrid-clouds"&gt;4 things
you need to know about managing complex hybrid
clouds&lt;/a&gt;
that "hybrid clouds are usually complex, hard to build, and hard to manage".&lt;/p&gt;
&lt;p&gt;Of course, IT vendors will be happy to tell you that it is hard (and for once I
concur), because they will then follow up with how their shiny tool supports
hybrid cloud much better than your existing ones. IT vendors know that an
infrastructural hybrid cloud means a redesign of (many parts of) your IT
architecture, so there is big money involved.&lt;/p&gt;
&lt;p&gt;Hence, it is important that hybrid cloud endeavours are assessed completely,
and that your business decision makers hold off with the decision until the
full scope of this exercise is known (or at least, you have a decent ballpark
estimate on the impact, not just financially, but also time-to-market). &lt;/p&gt;
&lt;p&gt;So, what are all the areas you need to consider? That's difficult to state, but
hopefully the list below can help you out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are your servers bootstrapped (initial deployment)? Can this interact
  with the cloud APIs to do the same in the cloud, or will you need to adapt
  your processes?&lt;/li&gt;
&lt;li&gt;Once your server is bootstrapped, how do you add software, libraries and
  other artefacts to it?&lt;/li&gt;
&lt;li&gt;What security services do you need on your systems? Anti-malware? Behavior
  analytics? Intrusion detection and prevention? Privileged access management
  utilities? &lt;/li&gt;
&lt;li&gt;How do your engineers and administrators follow-up on the systems? Monitoring
  approaches? Application performance management? Trace capabilities? Logging?&lt;/li&gt;
&lt;li&gt;Are there any control systems in place that manage the infrastructure? Are
  these control systems latency-sensitive?&lt;/li&gt;
&lt;li&gt;How do you deal with applicative releases? If you have CI/CD infrastructure
  in place, how would it deal with a burst environment? Does it have any
  dynamical detection capabilities?&lt;/li&gt;
&lt;li&gt;Can your support systems deal with more ephemeral infrastructure (capacity that
  is added for a few hours and then removed again)?&lt;/li&gt;
&lt;li&gt;Can your processes deal with ephemeral infrastructure?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Perhaps your current environment is already capable of dealing with such hybrid
clouds. While the situations I am confronted with at work support my view that
we need to apply a different 'hybrid' approach than the 'seamless
infrastructure' one (I like to see the environments progress more
independently, gradually move towards a &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;zero
trust&lt;/a&gt; model), I do
believe that such hybrid cloud setups can work in certain situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions and feedback&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do come across an intention declaration to move to a hybrid cloud (which
follows the infrastructural 'seamless' setup as implied in this post), make
sure you inform the stakeholders of the consequences. Show that a majority of
management, support and control systels are not designed nor capable of dealing
with this bursting out-of-the-box, and that this will require a significant IT
investment which might not be visible to the decision maker currently.&lt;/p&gt;
&lt;p&gt;If you have the time and resources, try to already build up the arguments for
it by focusing on your management, support and control systems, validating how
ready they would be, and what types of investments would be needed. Compare
this with a setup where the infrastructure side of the hybrid cloud still uses
separate environments, and where you manage each environment using the
strengths of that environment.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1457745672304840709"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="hybrid"></category><category term="cloud"></category></entry><entry><title>Transparent encryption is not a silver bullet</title><link href="https://blog.siphos.be/2021/10/transparent-encryption-is-not-a-silver-bullet/" rel="alternate"></link><published>2021-10-19T08:20:00+02:00</published><updated>2021-10-19T08:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-19:/2021/10/transparent-encryption-is-not-a-silver-bullet/</id><summary type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Listing the threats to protect against&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's first list the threats you want to protect against. It is beneficial that
these threats are also scored in the organization for their likelihood of
occurrence and effect, so that you can optimize and prioritize the measures
appropriately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data leakage through theft or loss of storage media&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (OS level)&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (middleware/database level)&lt;/li&gt;
&lt;li&gt;Data leakage through application vulnerability (including injection attacks)&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through data-in-transit interception&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through local privilege escalation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all the "data leakage" threats are also about loss of confidentiality,
and any loss of confidentiality can also result in data leakage, I made the
distinction in name as the data intercepted through that threat is generally
not as 'bulky' as the others.&lt;/p&gt;
&lt;p&gt;To visualize the threats, consider the situation of an application that has a
database as its backend. The application is hosted on a different system than
the database. In the diagram, the blue color indicates an application-specific
focus. This does not mean it isn't infrastructure oriented anymore, but more
that it can't be transparently implemented without the application supporting
it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application and database interaction" src="https://blog.siphos.be/images/202110/te-accesspatterns.png"&gt;&lt;/p&gt;
&lt;p&gt;There are eight roles listed (well, technically seven roles but let's keep it simple and make "physical access" also a role), ranging from the application user to the physical access:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;application user&lt;/em&gt; interacts with the application itself, for instance
  from a browser to the web application.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;application administrator&lt;/em&gt; also interacts with the application, but has more privileges. The user might also have access to the system on which the application itself resides (but that isn't further modelled here).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;network poweruser&lt;/em&gt; is a user that has access to the network traffic
  between the client and application, as well as to the network traffic between
  the application and the database. Depending on the privileges of the users,
  these powerusers can be administrators on systems that reside in the same
  network.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware user&lt;/em&gt; is a role that has access to the application
  data in the database directly (so not (only) through the application). This
  can commonly be a supporting function in the organization.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware administrator&lt;/em&gt; is the administrator of the
  database engine (or other middleware component that is used).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system administrator&lt;/em&gt; is the administrator for the server on which the
  database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system user&lt;/em&gt; is an unprivileged user that has access to the server on
  which the database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;physical access&lt;/em&gt; is a role that has physical access to the server and
  storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, while the example is easiest to understand with a database system, be
aware that there exist many other middleware services that manage data (like
queueing systems) and the same threats and measurements apply to them as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent encryption is a physical medium data protection measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Transparent encryption, such as through LUKS (with DM-Crypt) on Linux, will
encrypt the data on the disks, while still presenting the data unencrypted to
the users. All users. Its purpose hence is not to prevent unauthorized users
from accessing the data directly, but to prevent the storage media to expose
the data if the media is leaked or lost.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Disk Encryption" src="https://blog.siphos.be/images/202110/te-tde.png"&gt;&lt;/p&gt;
&lt;p&gt;In the diagram, you notice that the transparent disk encryption only takes
effect between the server and its storage. Hence, the only 'inappropriate'
access that it is mitigating is the physical access to the server storage. Note
that physical access to the server itself is still an important attack vector
that isn't completely mitigated here - attackers with physical access to
servers will not have a too hard time to find an entrypoint to the system.
Advanced attackers might even be able to capture the data from memory without
being detected.&lt;/p&gt;
&lt;p&gt;Transparent disk encryption is very sensible when dealing with removable media
(like USB sticks), especially if they contain any (possible) confidential data
and the method for transparent encryption is supported on all systems where you
are going to use the removable media. In larger enterprises, it also makes sense
to apply as well when multiple teams or even companies have physical access and
could attempt to maliciously access the systems.&lt;/p&gt;
&lt;p&gt;For server disks or SAN storage for instance, this has to be balanced against
the downsides of the encryption. You can do disk encryption from the storage
array for instance, but this might impact the array's capability for
deduplication and compression. If your data centers are highly secured, and you
do not allow the storage media to leave the premises without being properly
wiped or destroyed, then such transparent encryption imo has little value.&lt;/p&gt;
&lt;p&gt;Of course, when you have systems hosted in third party locations, then you do
have a higher risk that the media are being removed or stolen, especially if
those locations are accessed by many others, and your own space isn't
physically further protected. So while a company-controlled data center with
tight access requirements, policies and controls that no media leaves the
premises and what not could easily evaluate to not apply transparent disk
encryption, using a public cloud service or a non-private colocation facility
should assess encryption capabilities on disk (and higher).&lt;/p&gt;
&lt;p&gt;Furthermore, a properly configured database system will not expose its data to
unauthorized users to start with, so the &lt;em&gt;system user&lt;/em&gt; role should not have
access to the data. But once you have local access to a system, there is always
the threat that a privilege escalation bug is triggered that allows the
(previously lower privileged) user to access protected files.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent database encryption isn't that much better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies (or more general middleware) offer transparent
encryption themselves. In this case, the actual database files on the system
are encrypted by the database engine, but the database users still see the data
as it is unencrypted.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Database Encryption" src="https://blog.siphos.be/images/202110/te-tdbe.png"&gt;&lt;/p&gt;
&lt;p&gt;Here again, it is important to know what you are protecting yourself from.
Transparent database/middleware encryption does prevent the non-middleware
administrators from directly viewing the data through the files. However,
system administrators generally have the means to become the database (or
middleware) administrator, so while the threat is not direct, it is still
indirectly there.&lt;/p&gt;
&lt;p&gt;The threat of privilege escalation on the system level is partially mitigated.
While a full system compromise will lead to the system user getting system
administrator privileges, partial compromise (such as receiving access to the
data files, but not to the encryption key itself, or not being able to
impersonate users but just access data) will be mitigated by the transparent
database encryption.&lt;/p&gt;
&lt;p&gt;Important to see here is that the threats related to the physical access are
also mostly mitigated by the transparent database encryption, with the
exception that database-only encryption might result in the encryption key
being leaked if it is situated on the system storage.&lt;/p&gt;
&lt;p&gt;Most of the threats however are still not mitigated: network interception (if
it doesn't use a properly configured TLS channel), admin access, database user
access, application admin and application users (through application
vulnerability) can still get access to all that data. The only focus these
measures have is data loss through physical access.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Database or middleware supported, application-driven encryption is somewhat better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies support out-of-the-box data encryption through the
appropriate stored procedures or similar. In this case, the application itself
is designed to use these encryption methods from the database (or middleware),
and often holds the main encryption key itself (rather than in the database).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Database or middleware supported data encryption" src="https://blog.siphos.be/images/202110/te-dmsde.png"&gt;&lt;/p&gt;
&lt;p&gt;While this prevents some of the attack vectors (for instance, some attacks
against the application will not result in getting a context that is able to
decrypt the data) and mitigates the attack vectors related to direct database
user access, there are still plenty of issues here.&lt;/p&gt;
&lt;p&gt;System administrators and database administrators are still able to control the
encryption/decryption process. Sure, it becomes harder and requires more
thought and expertise (like modifying the stored procedures to also store the
key or the data in a different table for them to access), but it remains possible.&lt;/p&gt;
&lt;p&gt;Because of the attack complexity, this measure is one that starts to meet
certain expectations. And because the database or middleware is still
responsible for the encryption/decryption part, it can still use its knowledge
of the data for things like performance tuning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application-managed data encryption is a highly appreciated measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With application-managed data encryption, the application itself will encrypt
and decrypt the data even before it is sent over to the database or middleware.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application-managed data encryption" src="https://blog.siphos.be/images/202110/te-amde.png"&gt;&lt;/p&gt;
&lt;p&gt;With this measure, many of the threats are mitigated. Even network interception
is partially prevented, as the network interception now is only still possible
to obtain data between the client and the application, and not between the
application and database. Also, all roles that are not application-related will
no longer be able to get to the data.&lt;/p&gt;
&lt;p&gt;Personally, I think that application-managed data encryption is always
preferred over the database- or middleware supported encryption methods.
Not only does it remove many threats, it is also much more portable, as you do
not need a database or middleware that supports it (and thus have to include
logic for that in the application).&lt;/p&gt;
&lt;p&gt;Of course, applications will need to ensure that they can still use the
functionalities of the database and middleware appropriately. If you store
names in the database in an encrypted fashion, it is no longer possible to do a
select on its content appropriately.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Client-managed data encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest level of protection against the threats listed, but of course also
the most impactful and challenging to implement, is to use client-managed data
encryption.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Client-managed data encryption" src="https://blog.siphos.be/images/202110/te-cmde.png"&gt;&lt;/p&gt;
&lt;p&gt;A web application might for instance have a (properly designed) encryption
method brought to the browser (e.g. using javascript), allowing the end user to
have sensitive data be encrypted even before it is transmitted over the
network.&lt;/p&gt;
&lt;p&gt;In that case, none of the attack vectors will be able to obtain the data. Of
course, there are plenty of other attack vectors (protecting web applications
is an art by itself), but for those we covered, client-managed encryption does
tick many of the boxes.&lt;/p&gt;
&lt;p&gt;However, client-managed data encryption is also very complex to do securely
while still being able to fully support the users. Most applications that
employ this focus already on sensitive material (like password managers) and
use end user provided information to generate the encryption keys. You need to
be able to deal with stale versions (old javascript libraries), multitude of
browsers (if it is browser-based), vulnerabilities within browsers themselves
and the web application, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Network encryption (as in the use of TLS encrypted communications) only focuses
on the confidentiality and integrity of the communication, in our example
towards the network poweruser that might be using network interception.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Network encryption" src="https://blog.siphos.be/images/202110/te-ne.png"&gt;&lt;/p&gt;
&lt;p&gt;While the majority of other threats are still applicable, I do want to point
out that network encryption is an important measure against other threats. For
instance, with network encryption, attackers cannot easily inject code or data
in existing flows. In case of the client-managed data encryption approach for
instance, the use of network encryption is paramount, as otherwise an 'in the
middle' attacker can just remove the client-side encryption part of the code
that is transmitted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I hope that this article provides better insights in when transparent
encryption is sensible, and when not. With the above assessment, it should be
obvious that transparent (and thus without any application support) encryption
methods do not cover all the threats out there, and it is likely that your
company already has other means to cover the threats that it does handle.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Full overview" src="https://blog.siphos.be/images/202110/te-full.png"&gt;&lt;/p&gt;
&lt;p&gt;The above image shows all the different encryption levels and where in the
application, database and system interactions they are situated.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1450345580778110980"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="encryption"></category><category term="transparent"></category><category term="luks"></category><category term="dm-crypt"></category></entry><entry><title>Evaluating the zero trust hype</title><link href="https://blog.siphos.be/2021/10/evaluating-the-zero-trust-hype/" rel="alternate"></link><published>2021-10-05T00:00:00+02:00</published><updated>2021-10-05T00:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-05:/2021/10/evaluating-the-zero-trust-hype/</id><summary type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the zero trust hype, two sources are driving (or aggregating) most of the
content that exists for zero trust: &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;NIST's Zero Trust Architecture
publication&lt;/a&gt; (report
800-207) and &lt;a href="https://cloud.google.com/beyondcorp/"&gt;Google's BeyondCorp Zero Trust Enterprise
Security&lt;/a&gt; resources.&lt;/p&gt;
&lt;p&gt;The NIST publication is a "dry" consolidation of what zero trust entails, and
focuses on the architecture and design principles for a zero trust environment.
It defines a zero trust architecture as an architecture that "assumes there is
no implicit trust granted to assets or users accounts based solely on their
physical or network location". &lt;/p&gt;
&lt;p&gt;The principles that it applies are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All data sources and computing services are considered resources&lt;/li&gt;
&lt;li&gt;All communication is secured regardless of network location&lt;/li&gt;
&lt;li&gt;Access to individual enterprise resources is granted on a per-session basis&lt;/li&gt;
&lt;li&gt;Access to resources is determined by dynamic policy [...] and may include
  other behavioral and environmental attributes&lt;/li&gt;
&lt;li&gt;The enterprise monitors and measures the integrity and security posture of all
  owned and associated assets&lt;/li&gt;
&lt;li&gt;All resource authentication and authorization are dynamic and strictly
  enforced before access is allowed&lt;/li&gt;
&lt;li&gt;The enterprise collects as much information as possible about the current
  state of assets, network infrastructure, and communications, and uses it to
  improve its security posture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within the publication, a common view is used to explain zero trust and the
components that take an active role within the architecture. This view is
happily shared by vendors to show where in the zero trust architecture their
component(s) are positioned.&lt;/p&gt;
&lt;p&gt;&lt;img alt="NIST core view on zero trust" src="https://blog.siphos.be/images/202110/zerotrust-core.png"&gt;&lt;/p&gt;
&lt;p&gt;The publication further evaluates a few possible architectural approaches (or
patterns if you will) for zero trust, with specific focus on the network side.
It ends with a chapter on migrating to a zero trust architecture.&lt;/p&gt;
&lt;p&gt;The Google resources through its BeyondCorp publication are more loosely written
and have a stronger focus on the cultural and principle aspects of zero trust.
One could see these publications more as an introduction to the value that zero
trust provides to a company and its users, with the focus on exposing services
everywhere, providing dynamic access controls through proxy services, and
eliminating classical patterns like using Virtual Private Networks (VPN) to bind
everything together.&lt;/p&gt;
&lt;p&gt;The main motivation beyond the zero trust principles in Google's publication is
to eliminate the perimeter-style protection where all controls are on the
perimeter, after which users have nearly free rein across the internally
exposed infrastructure.&lt;/p&gt;
&lt;p&gt;The principles it applies are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Access to services must not be determined by the network from which you
  connect&lt;/li&gt;
&lt;li&gt;Access to services is granted based on contextual factors from the user and
  their device&lt;/li&gt;
&lt;li&gt;Access to services must be authenticated, authorized, and encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While these two main resources embody the bulk of what zero trust is, it does
not determine it completely. Many vendors and consultancy firms have
their view of zero trust, which largely coincides with the above, but often
has specific attention points or even foundations that are not part of the
previously mentioned resources.&lt;/p&gt;
&lt;p&gt;The term "zero trust" implies a "trust nothing and nobody" approach to
architecture and design, which you can fill in and apply everywhere. Of course,
you eventually will need to apply some level of trust somewhere, and how this is
done can depend on so many factors that it is unlikely that we will ever settle
down in the zero trust hype on what is and isn't proper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Focus areas in zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While evaluating zero trust, I read through many other resources out there.
Besides the paywalled analyst resources from Gartner and Forrester, it also
included resources from vendors to learn how they see zero trust evolve.&lt;/p&gt;
&lt;p&gt;In most of these resources, there are commonalities that everybody seems to
agree on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approach authentication and authorization at all layers in the stack: device,
  operating system, network, communication path (next-hop), communication
  session, application, etc.&lt;/li&gt;
&lt;li&gt;Enforce high maturity in asset management and inventory management. Asset
  management is more than just devices (it also entails applications, cloud
  services, etc.) and you should not only focus on those you own, but also those
  that are associated with your architecture (such as Bring Your Own Device
  (BYOD) assets)&lt;/li&gt;
&lt;li&gt;Ensure data classification and data management are applied and continuously
  evaluated and updated.&lt;/li&gt;
&lt;li&gt;Contain workloads within sufficiently small logical bounds. This could be
  through micro-segmentation (but that is not the sole method out there).&lt;/li&gt;
&lt;li&gt;Expose services globally (as in, globally reachable), but that does not
  imply that all services are accessible by each and every one.&lt;/li&gt;
&lt;li&gt;Use dynamic access policies and policy enforcement. Dynamic includes
  context-based accesses (access decisions are taken by more than just the
  authentication side of things) as well as authorizations that can change as
  new insights are passed on (such as threat intelligence).&lt;/li&gt;
&lt;li&gt;Perform continuous monitoring, including behavioral assessments.&lt;/li&gt;
&lt;li&gt;Encrypt everything (or more soundly put, cryptographically protect resources
  at all layers of the stack).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="https://www.cisa.gov"&gt;Cybersecurity and Infrastructure Security Agency&lt;/a&gt; has
recently also released the first draft of its &lt;a href="https://www.cisa.gov/publication/zero-trust-maturity-model"&gt;Zero Trust Maturity
Model&lt;/a&gt; that
companies can use to evaluate their posture against the zero trust principles.
It is strongly based upon the NIST explanation of zero trust, with attention to
five pillars (identity, device, network/environment, application workload, and
data) and three foundations (visibility and analytics, automation and
orchestration, and governance). Again, we observe some interpretation of what
zero trust could entail, in this particular case how the US government would
like to approach this towards its agencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why zero trust isn't exactly new&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attentive readers will already understand that most of the principles or focus
areas in zero trust are not new. Let's take a few of the core components and
principles and see how novel these are.&lt;/p&gt;
&lt;p&gt;One of the core components in the zero trust architecture is a policy
enforcement methodology, one that detaches enforcement from declaration.
Separating the mechanism from a policy isn't new. &lt;a href="https://ieeexplore.ieee.org/document/502679"&gt;Decentralized trust
management&lt;/a&gt;, published in 1996,
attempted to implement the necessary abstractions for it. The &lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xacml"&gt;Extensible Access
Control Markup
Language&lt;/a&gt;,
published by OASIS in 2003, is an open standard for integrating the different policy
components.&lt;/p&gt;
&lt;p&gt;The ability to perform authentication at all levels of a stack is also not new.
We can execute device authentication using the &lt;a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module"&gt;Trusted Platform
Module&lt;/a&gt; for instance,
whose first publication was in 2009. The use of certificates for authenticating
websites is common since &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security"&gt;SSL v3 came
about&lt;/a&gt; in 1996.
Authenticating end users through passwords is as old as IT itself, and
multi-factor authentication has had plenty of research since 2005. It is very
popular nowadays since the introduction of the &lt;a href="https://datatracker.ietf.org/doc/html/rfc6238"&gt;Time-based One-time Password
(T-OTP)&lt;/a&gt; as published in 2011.&lt;/p&gt;
&lt;p&gt;Even the use of user profiling for security analytics isn't novel. In 2004, the
paper on &lt;a href="https://ieeexplore.ieee.org/abstract/document/1386699"&gt;User profiling for computer
security&lt;/a&gt; was the start
of what became a very active market in cybersecurity nowadays: User Entity and
Behavior Analytics (UEBA).&lt;/p&gt;
&lt;p&gt;The dismissal of the perimeter-only security architecture seems to be the most
specific 'new' principle, although the foundations for security have long been
to not just consider security from a network point of view: starting with the
layered architecture and requirement tracking by Peter G. Neumann's &lt;a href="http://www.csl.sri.com/users/neumann/survivability.pdf"&gt;Practical
Architectures for Survivable Systems and Networks&lt;/a&gt;
published in 2000, we have seen the market take up more and more traction on
securing the different layers and assessing security not just based on the
perimeter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Personal observations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Zero trust is energizing the cybersecurity ecosystem, allowing both active
research and commercial evolutions/improvements. With the further
digitization of our environment, the significant increase in exposed services (think
IoT), and users that are always online, companies should indeed ensure that their
services (both external-facing and internal ones) are secure. The
increase in attention through the "zero trust" hype is positive, but should not
be considered completely new. Instead, it is an aggregation of already existing
best practices and designs.&lt;/p&gt;
&lt;p&gt;The lack of a common architecture (despite NISTs efforts) is to be expected, as
each company, organization or government has a different architecture and
vision. This, of course, means that decision-makers will need to understand that
"zero trust" is not a pattern to apply blindly. Vendors will attempt to
influence businesses, but without a good understanding of the current
environment and understanding the direction a company wants to go, these will
just be tools. And as the saying goes, "A fool with a tool is still a fool".&lt;/p&gt;
&lt;p&gt;Many companies will already have started on their journey to "zero trust"
without having it named as such. Layered security, security in depth, and other
statements already contribute to the zero trust approach. If you want to
approach zero trust, it is wise to consider where you are at already, and what
main principles you want to address next. You can call it "zero trust" or your
"zero trust strategy" to get attention, but beware of external influences that
might want to inject complexity because you called it "zero trust". The benefit
is not in attaining a zero trust compliant architecture, but in ensuring the
company has a good security posture, including the flexibility to adjust as the
environment evolves.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1445380710706073613"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="zero-trust"></category><category term="security"></category><category term="enterprise"></category><category term="network-security"></category></entry><entry><title>Scale is a cloud threat</title><link href="https://blog.siphos.be/2021/09/scale-is-a-cloud-threat/" rel="alternate"></link><published>2021-09-28T17:00:00+02:00</published><updated>2021-09-28T17:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-28:/2021/09/scale-is-a-cloud-threat/</id><summary type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Perimeter controls and isolation domains&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before tackling the scale of a cloud service, let's consider an on premise
service. Services that run on premise for a company are often built up
specifically for that company, and have no relationship with other customers of
the same service. Taking the NoSQL example, companies can perfectly run NoSQL
database services on premise that have no internet presence. Moreover, these
services are also often not directly exposed to the internet.&lt;/p&gt;
&lt;p&gt;Running services within your own premises reduces the likelihood that attackers
exploit vulnerabilities of that service. Attackers that are not particularly
eyeballing your company might not know you have that service on premise. Even if
they do know, having proper protections in place should prevent direct access to
those services.&lt;/p&gt;
&lt;p&gt;&lt;img alt="On premise services" src="https://blog.siphos.be/images/202109/cloud-scale-on-premise.png"&gt;&lt;/p&gt;
&lt;p&gt;Some situations do require services to expose themselves to the internet. This
exposure increases the &lt;em&gt;attack surface&lt;/em&gt; for the service significantly.
However, these services are still part of a rather isolated deployment that I
call an &lt;strong&gt;isolation domain&lt;/strong&gt;: a logical aggregation of services that share
one or more integrations and interactions, broadening the scope of
potential vulnerabilities and misconfigurations.&lt;/p&gt;
&lt;p&gt;Separate isolation domains imply that vulnerabilities or misconfigurations
that rely on information from the domain cannot spread. This is not the same as
separate deployments or environments, as those often do share certain
integrations. For instance, all NoSQL databases within a company might use that
company's identity provider for federated authentication. But NoSQL databases
exposed to the internet from two completely different companies are often in
separate isolation domains.&lt;/p&gt;
&lt;p&gt;The Cosmos DB vulnerability exploited the fact that all Cosmos DB deployments
are part of the same isolation domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perimeter and isolation domain challenges for public cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Public cloud platform services, like Cosmos DB, are often lacking these two
attributes: they have different perimeter protections in place, and share the
same isolation domain.&lt;/p&gt;
&lt;p&gt;I do not want to imply that public cloud providers do not provide perimeter
protections against their services. They most definitely do, but the scope of
the perimeter is different from what a company would apply. Whereas a company
gains some measurable security by hiding services or ensuring those services are
not reachable from unauthorized contexts, public cloud platform services need to
be easily accessible for the public cloud to become successful. Security
paradigms like &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;Zero
Trust&lt;/a&gt; are needed to
raise the security posture of these services. Companies that are building
solutions within the public cloud will find that this requires a different
mindset, and that these environments are not comparable with the traditional
on-premise designs.&lt;/p&gt;
&lt;p&gt;For the Cosmos DB vulnerability, the FAQ mentions that instances that are not
internet facing are still somewhat impacted (as the credentials could have been
leaked) but accessing the database (by using the credentials) will not be
possible without additional vulnerabilities or misconfigurations being
addressed. This is comparable to an administrator password leakage for your
properly isolated on-premise database: while your database might not be
immediately accessible by attackers, you're still going to change the password
as soon as possible to prevent it from being used in later attacks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Public cloud platform services" src="https://blog.siphos.be/images/202109/cloud-scale-public-cloud.png"&gt;&lt;/p&gt;
&lt;p&gt;The isolation domain is a bigger hurdle to take though, as this is almost always
by design. Platform services always share interactions or integrations across
all the customers of the public cloud. Even though you have your own logical
deployment (or even ask for your own physical deployment), the main interface to
access your service is shared. The service you use to authenticate users or
systems is shared (even when it will eventually use federated authentication to
your own identity provider, the initial service is still the same).&lt;/p&gt;
&lt;p&gt;This shared isolation domain makes each public cloud service a fantastic target
for attackers (and luckily also security researchers). Exploits might not just
reveal data or insights from one customer, but from thousands of customers all
over the world. And the bigger the cloud provider, the bigger the impact.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shared control planes also imply sharing the isolation domain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This problem of using a shared isolation domain is not restricted to public
cloud platform services only. Even on premise deployments that use a public
control plane are taking part in the same isolation domain as all other
customers of the same service.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Control plane also implies sharing isolation domains" src="https://blog.siphos.be/images/202109/cloud-scale-control-plane.png"&gt;&lt;/p&gt;
&lt;p&gt;Suppose you set up a big data platform on premise, but use your vendor's SaaS
service as a control plane to manage this big data platform. This SaaS service
is also used by the other customers of that vendor, so your deployment is part
of the same isolation domain.&lt;/p&gt;
&lt;p&gt;While such setups have benefits (such as using the same control plane for
multiple deployments across different environments and even hosting setups, and
not having to manage and maintain the control services yourself) they do
increase the risk exposure in a not dissimilar fashion from the pure public
cloud services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to tackle these concerns&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Knowing about these increased risks (reachability/exposure, and the shared
isolation domain) is the foremost important part that this article wants to
address. Once these risks are considered, companies can start taking
precautions. I've mentioned the zero trust model as a way to address the
reachability/exposure risk. To address the shared isolation domain, reducing the
impact of a successful exploit can be done through proper architecture and
design that uses the "it is not if, but when" principle for cyberattacks.&lt;/p&gt;
&lt;p&gt;For instance, the data within the databases can use application-level encryption
(meaning the encryption is not done by or through the database, but by the
front-end application that interacts with the database) to reduce the impact of
data leakage through such vulnerabilities. Proper data governance processes
should also be in place to remove any data that is no longer needed on that
database. Active security validations on log data should exist to detect
deviating access patterns, and access controls should be in place to prevent
unauthorized access even from succesfully authenticated users or systems.&lt;/p&gt;
&lt;p&gt;In the Cosmos DB case, the vulnerability was possible through a selectable
feature: deployments that do not have the Jupyter Notebook feature active would
not leak the credentials. Hence, proper configuration management of services and
disabling features that are not going to be used is paramount for cloud
services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If architects are sufficiently aware of the added risks of public cloud
services, they can properly balance these risks against the benefits of the
public cloud, and make appropriate adjustments to the architecture and design of
the solutions. The main challenge here is to make sure this awareness is raised,
and that this awareness is not only reaching the architects, but also the
engineers and other stakeholders. If not, architects risk that they will be seen
as "innovation inhibitors" if they would recommend changes and improvements to
tackle these risks.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1442867880639401989"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="cloud"></category><category term="vulnerability"></category></entry><entry><title>Naming conventions</title><link href="https://blog.siphos.be/2021/09/naming-conventions/" rel="alternate"></link><published>2021-09-15T19:00:00+02:00</published><updated>2021-09-15T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-15:/2021/09/naming-conventions/</id><summary type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Naming conventions imply standardization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you settle on a naming convention, you're effectively putting some
standardization in place which you expect everybody to follow, and which should
also cover 100% of the cases. So, when assessing a possible naming convention,
first identify what standards you need to enforce and are future proof.&lt;/p&gt;
&lt;p&gt;Say you are addressing database object naming conventions. Are you able to
enforce this at all times? You might want to start tables with &lt;code&gt;tbl_&lt;/code&gt; and views
with &lt;code&gt;vw_&lt;/code&gt;, but when you are dealing with ISV software, they generally do not
allow such freedom on 'their' database definitions. Your DBAs thus will learn
to deal with setups that are more flexible anyway.&lt;/p&gt;
&lt;p&gt;Using a naming convention for internal development is of course still a
possible path to pursue. But in that case, you will need to look at the
requirements from the development teams (and related stakeholders).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standardization does not imply naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The inverse isn't true: even though you might have certain standards in place,
it doesn't mean that the object names need to reflect the standards. If your
company standardizes on two operating systems (like Red Hat Enterprise Linux
and Microsoft Windows), it doesn't mean that server names have to include an
identifier that maps to Linux or Windows.&lt;/p&gt;
&lt;p&gt;I personally often fall into this trap - I see standards, so I want to see them
fixed in the naming convention because that allows better control over
following the standards. But naming conventions aren't about control, they are
about exposing identifiable information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure it for readability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trying to add too much information in a naming convention makes it
more complex for users to deal with. You might be able to read and understand
the naming convention immediately upon seeing it, but are all the other
stakeholders equally invested in understanding the naming conventions? &lt;/p&gt;
&lt;p&gt;Say that you have a hostname that looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppchypkc05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While I can tell you that this name comes from the following convention, it
might be overdoing things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s&lt;/strong&gt; to identify it is a server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is a physical server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is hosted in a production environment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c&lt;/strong&gt; to identify it is a cattle-alike managed server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hypk&lt;/strong&gt; to identify the ownership (in this case, hypervisor usage, KVM)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c05&lt;/strong&gt; to identify it is the fifth cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;m01&lt;/strong&gt; to identify it is the first master node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reg1&lt;/strong&gt; to identify the first region (location)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if you still want to include this information, using separators might make
this more obvious. For instance, for the given name, I would suggest splitting
this as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppc-hypk-c05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first two parts are then global naming convention requirements, with the
first set being about the type of system whereas the second is about ownership,
and the third is then a naming convention specific to that owner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assets that follow a certain naming convention provide information about that
asset that a reader can immediately assume, without having to do additional
lookups. The intention here is that you want to define important information
that many stakeholders will need immediately to support their work (and thus
their efficiency). Insights that are useful for a select set of stakeholders
might not be suitable for a naming convention (or at least not a global one).&lt;/p&gt;
&lt;p&gt;You should consider every stakeholder that comes in contact with the name of
the asset, and how that stakeholder would obtain the information they need. If
you have a central, easily accessible configuration management system, it might
be possible to have many structured insights exposed through that interface,
but is that useful when you are dealing with lists of assets?&lt;/p&gt;
&lt;p&gt;Suppose you do not include the host class for hostnames, with the host class
being what class of system the host is (server, workstation, router, firewall,
appliance, ...). Does your SOC team need this insight every time they are going
through events? Does your helpdesk need that information? What about the
resource managers?&lt;/p&gt;
&lt;p&gt;If all these stakeholders do need that information over and over again, it
might be sensible to include it in the naming convention. If, however, only a
few stakeholders need that information, you might want to expose that easily
through different means. For instance, resource managers might be able to easily
join that information with the asset management system information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information NOT to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes, you want to have some information about objects easily available,
but not for everybody. It might be information that can be abused for nefarious
purposes. In that case, you want this information to be shielded and only
offered to authenticated and authorized users. For instance, if you use separate
accounts for administering systems, you might not want to add information about
what type of admin account it is, as account enumeration might reveal too much
immediately and provide attackers with better insights.&lt;/p&gt;
&lt;p&gt;So, rather than having &lt;code&gt;ken_adadmin&lt;/code&gt; for Ken's Active Directory administration
account, stick to a nonsensible account identification like &lt;code&gt;ua1503&lt;/code&gt; (user
account 1503). Stakeholders that need information about accounts, in this case,
can still notice it is a user account rather than a system or machine account
and will need to query the central repositories for more information (such as
AD to get information about the user - and don't forget to add sensitive users
to, for instance, the &lt;code&gt;Protected Users&lt;/code&gt; group in AD).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use layered naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With "global naming convention" I am suggesting the ability to add naming
conventions for specific purposes, but leave that open in general. A server
name could, for instance, require an indication of the environment (production or
not) and the fact that it is a server (and not a workstation), but leave a part
of the name open for the administrators. The administrators can then add their
local naming convention to it.&lt;/p&gt;
&lt;p&gt;An active directory group, for instance, might have a standard global naming
convention (usually the start of the group name) and leave the second part
open, whereas specific teams can then use that part to add in their local naming
convention. Groups that are used for NAS access might then use a naming
convention to identify which NAS share and which privileges are assigned,
whereas a group that is used for remote access support can use VPN naming
conventions.&lt;/p&gt;
&lt;p&gt;The University of Wisconsin has their &lt;a href="https://kb.wisc.edu/iam/page.php?id=30600"&gt;Campus Active Directory - Naming
Convention&lt;/a&gt; published online, and
the workstation and server object part is a good example of this: while the
objects in AD have to follow a global naming convention (because Active
Directory is often core to an organization) it leaves some room for local
department policies to assign their own requirements:
&lt;code&gt;&amp;lt;department&amp;gt;&amp;lt;objectfunction&amp;gt;-&amp;lt;suffix&amp;gt;&lt;/code&gt; only has the first two fields
standardized globally, with the &lt;code&gt;&amp;lt;suffix&amp;gt;&lt;/code&gt; field left open (but within certain
length constraints).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consider the full name for your naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do want to add information in a naming convention, do not consider
this purely on a single object type, but at the full name. A hostname by itself
is just a hostname, but when you consider the fully qualified hostname (thus
including domain names) you know that certain information points can be put in
the domain name rather than the hostname. The people over at &lt;a href="https://www.serverdensity.com/"&gt;Server
Density&lt;/a&gt; have a post titled "&lt;a href="https://blog.serverdensity.com/server-naming-conventions-and-best-practices/"&gt;Server Naming
Conventions and Best
Practices&lt;/a&gt;"
where they describe that the data center location (for the server) is a
subdomain.&lt;/p&gt;
&lt;p&gt;Another example is for databases, where you not only have a table, but also the
database in which the table is located. Hence, ownership of that table can
easily be considered on the database level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learn from mistakes or missing conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you approach naming conventions, you will make mistakes. But before making
mistakes yourself, try looking out for public failures that might have been due
to (bad or missing) naming conventions. Now, most public root cause analysis
reports do not go in-depth on the matter completely, but they do provide some
insights we might want to learn from.&lt;/p&gt;
&lt;p&gt;For instance, the incident that AWS had on February 28th, 2017, has a &lt;a href="https://aws.amazon.com/message/41926/"&gt;Summary of
the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1)
Region&lt;/a&gt;. While there is no immediate
indication about the naming conventions used (mainly that a wrong command input
impacted more servers than it should), we could ask ourselves if the functional
purpose of the servers was included in the name (or, if not in the name, if it
was added in other labeling information that the playbook should use). The
analysis does reveal that AWS moved on to implement partitions (which they call
cells), and the cell name will likely become part of the naming convention (or
other identifiers).&lt;/p&gt;
&lt;p&gt;Also internally, it is important to go over the major incidents and their
root causes, and see if the naming conventions of the company are appropriate
or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Still need examples?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While most commercial companies will not expose their own naming conventions
(as there is no value for them to receive, and it exposes information that
malicious users might abuse), many governmental agencies and educational
institutions do have this information publicly available, given their
organization public nature. Hence, searching for "naming convention" on &lt;code&gt;*.gov&lt;/code&gt;
and &lt;code&gt;*.edu&lt;/code&gt; already reveals many examples.&lt;/p&gt;
&lt;p&gt;Personally, I am still a stickler for naming conventions, but I am slowly
accepting that some information might be better exposed elsewhere.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1438175688444596227"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="naming"></category></entry><entry><title>Location view of infrastructure</title><link href="https://blog.siphos.be/2021/09/location-view-of-infrastructure/" rel="alternate"></link><published>2021-09-07T18:00:00+02:00</published><updated>2021-09-07T18:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-07:/2021/09/location-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;


&lt;p&gt;&lt;img alt="Location view representation" src="https://blog.siphos.be/images/202109/location-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through the layers from bottom to top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Easiest to represent: geographic location and facilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The geographic location is the least IT-specific view out there, as it
represents where everything is in the world. These views are popular not only to
scope projects better (like data center locations) but also to support getting
important infrastructural metrics.&lt;/p&gt;
&lt;p&gt;WAN latency, for instance, is limited by the distance (you can't outsmart
physics), and by knowing the path between two points, you can calculate the
throughput and latency (such as through the &lt;a href="https://wintelguy.com/wanlat.html"&gt;Wintelguy WAN Latency
Estimator&lt;/a&gt;). When designing redundant network
connections between separate locations, you might depend on multiple line
providers. To ensure there are no chokepoints where both providers have their
lines go through the same location, you can ask for the fiber path details to
validate this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A10's DDoS Threat Intelligence
map" src="https://blog.siphos.be/images/202109/a10-ddos-threat-intelligence.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://www.a10networks.com/"&gt;A10 Networks&lt;/a&gt;, through their &lt;a href="https://www.a10networks.com/products/network-security-services/threat-intelligence-service/"&gt;DDoS
Threat Intelligence
Service&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Using geographic locations also facilitates understanding by other stakeholders,
even if it has a less technological impact on the case at hand. For instance,
while undergoing active DDoS attacks, a geographic representation of where they
come from helps to get more understanding from management, even though on
network-level you're more interested in the Autonomous System (AS) networks that
are involved. Those are very large groups of networks that cover the main
global-wide routing of data.&lt;/p&gt;
&lt;p&gt;If we drill down from a geographic location, the next view is the
facility-related one. Here, the view focuses on building design and
infrastructure (such as HVAC and power distribution in data rooms or data
centers), as well as the location of individual devices (floor plans, rack
spaces). Facility views help not just with initial network designs where you
want to onboard a new headquarter location, but also with capacity management
within data centers (identifying hotspots), dealing with wireless networks and
their impact on the surroundings, cable management for networks, ensuring the
resilience of infrastructure services and more.&lt;/p&gt;
&lt;p&gt;A decent facility view is very helpful when dealing with operational technology
environments (IoT), and can be dynamically generated. A while ago, I was at a
conference where they showed people's movement based on the data received from
their smartphones. They used the view to see which areas were too crowded (it
was pre-COVID times), as well as to see if there are sudden movements that might
indicate problems or threats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Foundations for networks: connectivity, underlay, and virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next three layers in the location view focus on the foundations for a
company's network. They are strongly IT oriented with the main stakeholders
being the telco- and infrastructure related teams and roles. Unlike the higher
level viewpoints, the foundations require more thought in their design as errors
are harder to correct.&lt;/p&gt;
&lt;p&gt;The connectivity focuses on the cabling and other connections made between
devices. This includes backplane-related connectivity, something that is
relevant when using enclosures or pre-engineered systems. Connectivity and
cabling seem rudimentary, but are critical for the proper functioning of a
network. Remember the science report about possible faster-than-light
neutrino's? Well, a &lt;a href="http://blogs.nature.com/news/2012/02/faster-than-light-neutrino-measurement-has-two-possible-errors.html"&gt;faulty connection was partially to
blame&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The network underlay is the network view that network engineers have on their
network. For small environments, the network view from the engineering point of
view might be the same as the view from the application side, but for larger
environments, I often see a distinction between the two. And when that occurs,
the underlay view is less of a concern for application engineers and business
stakeholders (unless of course there are major issues with the underlay design),
but that does not make it less important. People are often not aware of how our
electricity net works, but if it fails, we're all affected. Similarly, if the
network underlay is badly designed, the higher networks will see troubles too.&lt;/p&gt;
&lt;p&gt;The network virtualization stack is a technology component that supports
building virtual networks on top of the underlay environment. So while the
underlay is like the foundation on top of which all networks are hosted, the
virtualization makes this possible. In that sense, it is similar to the
hypervisor level on the component view (and perhaps is less of a location view
than a component one, although network virtualization technologies do require a
common understanding of the full network to function properly).&lt;/p&gt;
&lt;p&gt;Companies use different virtualization technologies and concepts. A simple
network virtualization technology is the VLAN (Virtual LAN), which presents
itself as a single broadcast domain to the participating systems, even though
these systems might not be physically connected to the same switch or switch
environment. It is even possible to stretch VLANs across wide areas.&lt;/p&gt;
&lt;p&gt;But virtualization can go further. Technologies such as Cisco ACI or VMware NSX
don't just focus on the LAN level, but also virtualize the network on the
addressing and routing part. And with Network Functions Virtualization (NFV), we
also include firewalls and traffic control. However, do not consider NFV to just
be the next phase beyond Software Defined Networks (SDN), as NFV and SDN are
different beasts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;View from application and system side: topology and protocols&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest levels in the location view focus on the network as it is seen by
the business applications and systems that a company hosts.&lt;/p&gt;
&lt;p&gt;The network topology is the view on segregation, segments/subnets, and the
network functions that take part in the overall environment (such as DNS/DHCP/IP
address management, firewall functionality, proxies, and other gateways). This
is the view that is probably going to change the most, as it constantly evolves
based on business demand and IT evolutions. Topology views are not just
one-of-a-kind: depending on the scope you want to address, multiple views might
be needed to convey the message you want to tell.&lt;/p&gt;
&lt;p&gt;One type of view within the topology is the &lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning
view&lt;/a&gt; which I've
covered before, and which is very expressive towards the other stakeholders: it
covers the entire company's environment while abstracting enough of the details
so that it remains understandable.&lt;/p&gt;
&lt;p&gt;If we were to zoom in further on the network topology, you get into the specific
interactions that are made between systems, which are standardized in protocols.
But while the network (and application) protocols are often very standardized,
they are also very challenging to understand.&lt;/p&gt;
&lt;p&gt;The main challenge is that there are so many protocols out there, with so many
options and implementation choices, that you need to be an expert to
troubleshoot issues if they arise. Web applications aren't just disclosed over
the HTTP protocol: you have channel encryption (TLS), might be using WebSockets,
or even the QUIC protocol. And if you think you understand HTTP, do you
understand HTTP/2?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The past few posts (with a few historical ones) make up what I consider being 
the infrastructure domain, and how to structurally approach changes within. Of
course, these are not the only views out there, and based on the project ahead,
different viewpoints might come up. But for a holistic view of the
infrastructure domain, I think these five cover it well.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1435271642507264000"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="location"></category><category term="network"></category><category term="virtualization"></category><category term="protocol"></category></entry><entry><title>Process view of infrastructure</title><link href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/" rel="alternate"></link><published>2021-09-01T11:20:00+02:00</published><updated>2021-09-01T11:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-01:/2021/09/process-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Six process groups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of process frameworks out there. I've covered many of these
in a previous article (&lt;a href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/"&gt;What is the infrastructure domain?&lt;/a&gt;), with &lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL&lt;/a&gt;
and &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT&lt;/a&gt; being my main resources.&lt;/p&gt;
&lt;p&gt;Companies often select a mature framework to align their IT on. After all, why
invent everything over and over again if a popular framework with all its
resources exists. But just like how companies like to use commercially available
resources, they also like to adjust and nudge it left and right to fit
the organization better. And that's fine.&lt;/p&gt;
&lt;p&gt;I'm going to give a spin to it and combine processes with non-functionals, as
infrastructure is often about non-functionals. That doesn't mean that frameworks
like ITIL or CObIT are not good, but explaining them easily is sometimes a
challenge, and I'm not versed enough in the intricacies of these frameworks
to use them as a starting point, rather as a reference.&lt;/p&gt;
&lt;p&gt;The six groups that I feel cover the infrastructure domain sufficiently are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt;, which is about the company and the organization
  used within the company.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Consumers &amp;amp; Suppliers&lt;/em&gt;, which is about the interaction (and supporting needs)
  with the consumers of our services, as well as with the providers.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Research &amp;amp; Development&lt;/em&gt;, which is about preparing updates on the services
  and architecture&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Risk &amp;amp; Security&lt;/em&gt;, which enables risk reduction strategies and facilitates
  secure integrations of services&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Custodianship&lt;/em&gt;, which is about facilitating maintenance and support
  improvements that are more environment-wide&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Engineering &amp;amp; Operations&lt;/em&gt;, which focuses on the operational control and
  support of the services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In some ugly visualization, these groups (and the processes or non-functionals
that are within) can be represented as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Six process groups for infrastructure" src="https://blog.siphos.be/images/202109/six-process-groups.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through each of the groups in a bit more detail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Governance and Organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first group is titled &lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt; and covers the company
specifics. It has five processes or non-functionals in it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strategy and Innovation&lt;/li&gt;
&lt;li&gt;Enterprise Architecture&lt;/li&gt;
&lt;li&gt;Organizational Efficiency&lt;/li&gt;
&lt;li&gt;Separation of Concerns&lt;/li&gt;
&lt;li&gt;Formalization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;strategy&lt;/em&gt; of a company is an important starting point for larger changes,
as architects need to make sure that the changes they want to guide, coach, or
support are aligned with the company strategy. Most companies will have a
hierarchy of strategies, with the main strategy being translated to specific
strategies or strategic objectives, which are then further formalized down.&lt;/p&gt;
&lt;p&gt;Alongside supporting the company strategy in general, domain architects might
need to plan the strategy for their domain as well (as I had to do for the
infrastructure domain). This, again, is a translation of the overall strategy,
showing how infrastructure will support the strategic objectives.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;innovation&lt;/em&gt; part, the infrastructure domain needs to make clear how to
support innovative ideas and suggestions in the organization. Often,
the infrastructure and operations field is considered to be protective, and
might be perceived as obstructing innovative ideas. That's not a correct view -
while the operations field often has a more conservative view of the
infrastructure to make sure the production environment is available and secure,
it also has a viewpoint on how to deal with innovation. For instance, the
delivery of sandbox environments in which innovations can play a role, or
prototype environments that have limited, secured integration possibilities
toward the rest of the environment.&lt;/p&gt;
&lt;p&gt;Innovation is not only limited to technological innovation. Innovative ideas on
governance and organization (such as when agile development practices were
being formulated) are also important to track, as they influence many
other processes and non-functional attributes.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;enterprise architecture&lt;/em&gt; part covers abstracting and guiding the
organization, the business, the product development, etc. in a coherent and
well-documented manner. It lays the foundations for an effective and efficient,
collaborative, large organization. Domain architects ensure that their domain
architecture is related to the enterprise architecture (and contributes to the
enterprise architecture directly).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Organizational efficiency&lt;/em&gt; focuses more on how the organization functions.
Does the organization use DevOps teams, for instance, or is it more traditional
in the development versus operations part? Does the organization have vertical
technology-oriented teams, or more horizontal, solution-driven teams, or a
mixture? Knowing how the company organizes its IT is important to properly
structure and present changes. Domain architects also provide input towards
reorganizations, as they can easily define how such shifts in responsibilities
will affect the organization and its efficiency.&lt;/p&gt;
&lt;p&gt;With the &lt;em&gt;segregation of duties&lt;/em&gt;, I focus on which roles can be shared and which
ones can't. Knowing which segregation applies in the organization (such as
the different security officer focus, the segregation between risk officers and
audit officers, the exclusivity between domain administrators (in Active
Directory terms) and regular server administrators) is an important driver
for architecting. Architects can suggest introducing additional segregations or
suggest methods for removing the need to segregate these duties (as often those
are inspired by historical events and assessed on older capabilities). I've
mentioned DevOps before, and those who supported a DevOps transition will know
about the historical segregation between development and operations.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;formalization&lt;/em&gt; is about how to have formal evidence of decisions.
While this is often just part of a company's 'governance', I focus on it
specifically, as it is always balancing efficiency and effectiveness. When
formal evidence is expected by the organization, it is wise to keep track of why
this is. Often, formalization can be optimized further without reducing the
benefits or impacting the requirements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consumers and Suppliers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whereas the first group focused on the company itself, the group on consumers
and suppliers focuses on the users of the infrastructure services that are
offered (which often, but not always, are internal customers) and the suppliers
for the various services the organization consumes.&lt;/p&gt;
&lt;p&gt;It covers the following four processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost &amp;amp; Licensing&lt;/li&gt;
&lt;li&gt;Portfolio&lt;/li&gt;
&lt;li&gt;Agreements &amp;amp; Support&lt;/li&gt;
&lt;li&gt;Incidents &amp;amp; Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;cost and licensing&lt;/em&gt; part is often a large time-consumer. I include
chargeback and showback here as well, although I must be very clear that actual
costs and chargeback-reported costs are not the same. For the services
infrastructure offers internally, knowing the costs (showback) and charging the
costs through to the internal customers (chargeback) are hard processes to
tackle, requiring intensive thoughts on what is and isn't allowed, how the
company looks at the services, etc.&lt;/p&gt;
&lt;p&gt;While looking at the suppliers, an important part is to understand and optimize
the licensing. Each product and service that you consume costs money one way or
another. I've had the "pleasure" of being an Oracle license manager (as in,
responsible within my company for tracking, reporting, and optimizing the costs
associated with all Oracle products we consumed) for a while, and being part of
the Microsoft license management team (with responsibility for data center
oriented products). Knowing the license requirements, the terms and conditions,
the contractual obligations (and deviations that a company negotiated), etc. is
very useful.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;portfolio&lt;/em&gt; is about knowing what you consume (from vendors) and offer
internally, and how you intend to track and evolve it. For infrastructure
services, for instance, you want to make sure you have a decent catalog (which
is part of the portfolio) that your internal customers can consume. Designing
the catalog is an important first step in assessing and deriving the domain
architecture if it isn't available yet.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;agreements and support&lt;/em&gt; I look not just at the contractual agreements
related to service consumption (as in, the terms and conditions related to the
licensing), but also towards agreements on areas such as support (Can we call
the vendor any time of the day? How much time does the vendor contractually
have before they 'pick up the phone'? Is the service agreement conforming to
market expectations?). The same is true for the service agreements offered
internally - something that is best aligned with the portfolio.&lt;/p&gt;
&lt;p&gt;Deriving decent service level agreements (SLA), and ensuring they can be
tracked and asserted, can be important if the vendor isn't all that
trustworthy, as well as to show your internal customers that you care about
reaching and keeping the SLAs.&lt;/p&gt;
&lt;p&gt;Support is also about how to reach and interact with the support organization.
From an infrastructure point of view, that isn't always as easy as it sounds.
Some vendors require specific applications to interact with their support
organization, and your company might not allow those applications. Or, certain
metrics need to be sent out to a cloud service, but that cloud service isn't
easily identified as being secure and compliant enough with the regulations you
have to cover.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;incidents and problems&lt;/em&gt; item covers the standard incident and problem
resolution processes, and how these are handled in the organization. It is
about standardizing what incidents are, how to react to them, how to derive
problems from the observations, prioritizing work related to incidents and
problems, and more. A decent incident and problem tracking solution is a must,
but the solution itself is just part of the setup. A good tool does not give
you an efficient and effective organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research &amp;amp; Development&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next group is about evolving the service offerings. I consider the following
four processes and non-functionals as part of this group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Design&lt;/li&gt;
&lt;li&gt;Product Lifecycle&lt;/li&gt;
&lt;li&gt;Quality Control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With &lt;em&gt;architecture&lt;/em&gt;, I focus on the solution architecture, and how the solutions
interact with the domain architecture. The domain architecture is generally
part of the enterprise architecture, whereas the solution architecture is
something that regularly needs updates based on the changes that are being
planned. Most of this architecture is done by the system architects with
the support or coaching of the domain architect.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;design&lt;/em&gt; is the next phase of a development cycle and is more detailed
than the solution architecture. Designs are often handled by the
engineering teams themselves, with the support of the system architects. For
domain architects, knowing where the designs are and to read them/understand
them is vital for a good collaboration between architects and engineering teams.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;product life-cycle&lt;/em&gt; focuses on the entire life-cycle of a product, starting
with innovative ideas and research, prototypes, towards supporting the
development of the product, and even after deployment towards end-of-life
support/tracking, or in case of bought products the end-of-sale,
end-of-premier-support, extended support, custom support, and whatnot.&lt;/p&gt;
&lt;p&gt;Balancing the product life-cycles against each other is a common occurrence for
architects and product owners, as it is always a puzzle about when to make which
changes, release what versions, etc. If you don't track the life-cycle of a
product continuously, you might face situations where you need to
purchase older products because your reinvestment wasn't planned yet, but
capacity limits require an increase anyway.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;quality control&lt;/em&gt; is about ensuring the quality of the products is 
according to expectations. This includes support for different environments
(pre-production), specific quality testing (which I'll discuss later as well),
supporting QA teams (if you have those), and the processes for reporting
defects, etc. It also includes quality assurance on products purchased from
third parties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Risk &amp;amp; Security&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A major part of my work is to assess the risk exposure and ensuring a secure
and reliable infrastructure. Hence, it shouldn't come as a surprise that it is
an entire group by itself.&lt;/p&gt;
&lt;p&gt;While security is a large domain (with lots of focus on processes and
assurance), the following processes and non-functionals are strongly
represented in the infrastructure domain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crypto&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorization&lt;/li&gt;
&lt;li&gt;Privacy&lt;/li&gt;
&lt;li&gt;Access Control&lt;/li&gt;
&lt;li&gt;Audit &amp;amp; Compliance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;em&gt;crypto&lt;/em&gt;, a major challenge is not only to ensure cryptographic services or
protocols are used where it makes sense (or better said, not used where it
makes sense) but also to understand the intricate details of the cryptographic
services, knowing what service is used for which purpose, etc. I could make an
entire article purely to discuss the sense and nonsense of transparent
encryption on file systems or databases...&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;authentication&lt;/em&gt; processes (and the closely related identity processes) are
to support assurance on the identity of a user, process, system, device, or
any other type of subject. Knowing how the authentication is handled, which
authentication protocols are used, the landscape in case of federated
authentication, etc. can take up several days to know. Authentication is no
longer based on user IDs and passwords. We have OpenID Connect, SAML, Kerberos,
TACACS, RADIUS, NTLM, and more, which all have their quirks. And those are just
the protocols to handle authentication: they don't talk about user management,
the processes you will need to support in case of account abuse, etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Authorization&lt;/em&gt;, while often combined with the authentication phase, is about
knowing what a (freshly authenticated) identity may do (authorized). Here, we
have the challenges of coarse-grained authorization versus fine-grained
authorizations, dynamic authorizations, transient authorizations, or
authorizations that are inherited from others. Often, architects will need to
design the authorization granularity and approach based on the organizational
and security requirements.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;privacy&lt;/em&gt; controls are about ensuring confidential or strictly confidential
data (if those are the terms used by the organization) are properly protected.
Data can be anonymized, pseudonymized, redacted, tokenized, encrypted, and/or
de-identified. Architects should know which control is possible where, which
services can be used, what the impact is of the controls, as well as what the
organizational data requirements are.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;access control&lt;/em&gt; part is closely related to the authorization part. In
effect, it should be what enforces the authorizations. Access control is a wide
domain with many, many products and services working with (or against
apparently) each other. Especially in more modern architectures where zero
trust plays a role, you'll notice that access control is a challenging beast,
with dynamic and contextual controls becoming primary services rather than the
standard, relatively static (role-based) access controls.&lt;/p&gt;
&lt;p&gt;The last part is the &lt;em&gt;audit and compliance&lt;/em&gt;, where audit focuses on obtaining
traceability of all events (what has happened where, when, by whom), whereas
compliance looks at assuring current state and processes are according to
expectations. Compliance can be about assuring adherence to the organizational
processes and standards, but also that a system's configuration is accurate and
still in effect. So, yes, it is more than what a "compliance" department would
focus on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custodianship&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In custodianship, I group processes and non-functionals that often play a more
active role after having a successful deployment, or after a project is
finished. While these do not imply that they are to be implemented by different
teams (that's the organizational efficiency which has to decide on this), I
notice that they are challenging to keep up properly for a large organization.&lt;/p&gt;
&lt;p&gt;In it, I cover the following processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Governance&lt;/li&gt;
&lt;li&gt;Rationalization&lt;/li&gt;
&lt;li&gt;Reporting &amp;amp; Insights&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Data governance&lt;/em&gt; is about defining and tracking data, data flows, data
definitions, as well as their purpose. You need proper data governance to know
which privacy measures to apply, as one of its measures is the retention of the
data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Rationalization&lt;/em&gt; is the effort to rationalize existing infrastructure usage
and services. While major rationalization exercises are company-wide
initiatives, there are many benefits to achieve with small, incremental
rationalization exercises. Most of the time, rationalization exercises are
about cost reduction, but that doesn't always need to be the case. Of course,
eventually, everything is about finances nowadays.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;reporting and insights&lt;/em&gt;, I consider the means to report on various areas
(such as capacity, cost, performance, and SLA breaches), as well as gain
insights from the data at hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Engineering &amp;amp; Operations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final group covers the disciplines that I see on the engineering and
operations side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Orchestration&lt;/li&gt;
&lt;li&gt;Testing&lt;/li&gt;
&lt;li&gt;Change Management&lt;/li&gt;
&lt;li&gt;Operational Control&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;configuration&lt;/em&gt; part is to ensure that the systems and services are
properly configured and that the life-cycle of the configuration items is
guaranteed as well. &lt;/p&gt;
&lt;p&gt;With &lt;em&gt;orchestration&lt;/em&gt;, the focus is on ensuring larger environments are optimally
used and the appropriate abstractions are in place. Kubernetes is a good
example of an orchestration that requires close attention and support. But
others exist as well, such as those in the Hadoop ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt; focuses on the various testing strategies that enable us to trust the
final product and services and help in ensuring no regressions are
creeping in. Testing on the infrastructure side is about load and performance
testing, smoke testing, regression testing, destructive testing, etc.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;change management&lt;/em&gt; process is about properly staging the changes,
communicating the changes, following up on the changes, etc. It is not just
about preparing a deployment and then hitting a button: you want to validate
that the change is successful, track the performance to ensure nothing is acting
strangely, etc.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;operational control&lt;/em&gt;, I consider the systems that drive the operational
systems autonomously. Self-driving and self-healing are the two non-functionals
I embed under this. Many cluster management systems are part of this, and
designing for self-driving and self-healing infrastructure comes up more and
more with modern systems.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;monitoring&lt;/em&gt; covers tracking the telemetry of the systems, the logs
that are generated, and derive the right insights from it. I initially wanted
to call it "Observation" as that seems to be the term that comes up more and
more (monitoring too much resembles watching telemetry for thresholds, whereas
observation goes much beyond that) but monitoring seems to resound most amongst
users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A huge list that covers most of the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This list of processes and non-functional attributes covers most, if not all,
requirements that are related to the infrastructure domain. The component
view that I mentioned in a previous post, for instance, is part of the
architecture and design processes in the Research &amp;amp; Development group.&lt;/p&gt;
&lt;p&gt;However, because they are still processes and non-functionals, they can often
seem to be less tangible. Sure, you have to manage the costs, but how do you do
that? What processes do you have in place to manage cost insights? How do you
deliver these insights? What tooling is used to support the organization? What
tooling is mandatory to use (like license management tools from larger vendors)?
Detailing this and making the right choices is part of being an architect.&lt;/p&gt;
&lt;p&gt;In the next post, I will look at the location view. Unlike the process view,
which is often shared with other IT domains, the location view is something
that is often more exclusive for the infrastructure domain.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1432996739846397957"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="process"></category></entry><entry><title>Component view of infrastructure</title><link href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/" rel="alternate"></link><published>2021-08-27T21:10:00+02:00</published><updated>2021-08-27T21:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-08-27:/2021/08/component-view-of-infrastructure/</id><summary type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Infrastructure domain in viewpoints&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While searching for a way to describe the infrastructure domain,
I tend to align with certain viewpoints as well, as it allows architects
to decompose a complex situation into more manageable parts. So the question
is no longer "how do I show what the infrastructure domain is", but rather
"what different viewpoints do I need to cover the scope of (and
explanation on) the infrastructure domain".&lt;/p&gt;
&lt;p&gt;I currently settle on five views:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;component view&lt;/em&gt;, which covers the vertical stack of an IT infrastructure
  component.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;location view&lt;/em&gt;, which is the horizontal stack for IT infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;process view&lt;/em&gt;, which covers the general enterprise requirements for IT
  infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;service view&lt;/em&gt;, which provides insights into what functional offerings are
  provided (and for which I posted a current view a short while ago, titled "&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An
  IT services overview&lt;/a&gt;")&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;zoning view&lt;/em&gt;, which represents the IT environment landscape. A few years
  ago, I covered this as well in "&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;Structuring infrastructural
  deployments&lt;/a&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these views are related to each other, but represent insights that are
particularly useful for certain discussions or representations. Some viewpoints
are even details for another. For instance, the &lt;em&gt;zoning view&lt;/em&gt; is a view that
provides more detail on a particular layer in the &lt;em&gt;location view&lt;/em&gt;. A simple
relationship between the above five views is the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between the five infrastructure views" src="https://blog.siphos.be/images/202108/five-infra-views.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, this isn't a proper meta-model, just a representation. It starts
with what the infrastructure domain has to accomplish (process view),
which defines the services the domain has to support. These services
comprise several components, and these are deployed in various
zones across the organization. The zone overview is part of the more
elaborate location views.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Components are a good introduction to infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While a good coverage of the infrastructure domain would start with the
process view, I think it is not always the easiest. Not all stakeholders
are fully acquainted with processes and what they entail, and I feel it
might be easier to start with a more tangible view, i.e. a component
view.&lt;/p&gt;
&lt;p&gt;For instance, when explaining what IT infrastructure is to an outsider
(say, a family member that isn't active in the IT world), I often start with
a component view (often using a cellphone as a starting example), then going
about the massive amount of components that need to be managed, hence the need
for proper processes. After elaborating a bit on the various processes involved,
we can then go to a service overview, to then move on to the hosting of all
those services in a structured and reliable environment (zoning), with the
various challenges related to locations.&lt;/p&gt;
&lt;p&gt;So, what is the component view that I reuse a lot? It is basically
the vertical stack that most hosting-related services use to explain where
their product is situated:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layered view on a component level" src="https://blog.siphos.be/images/202108/component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;If you start with a cellphone view, then you can easily describe the hardware,
operating system, application, and data layers in the view. You can mention that
the hardware is an expensive one-time investment that the user hopes to use for
a few years (so you can explain &lt;em&gt;capital expenditures (CapEx)&lt;/em&gt; and &lt;em&gt;operational
expenditures (OpEx)&lt;/em&gt;. The latter can be a cloud service that the
user synchronizes its data to, like Apple iCloud or Google Drive).&lt;/p&gt;
&lt;p&gt;The distinction between operating system and application, and its impact on
the users, can also be explained easily: operating system upgrades are
heavier, and users often want to choose when this occurs, as operating system
upgrades are not always fully backward compatible. Or, the user's hardware isn't
supported on the next operating system (e.g. upgrading Apple iOS 12 to iOS 13,
or Android 10 to Android 11). Applications, on the other hand, are often
automatically updated and are less intrusive. However, because there are
many applications, managing the application landscape can be more daunting than
the operating system one.&lt;/p&gt;
&lt;p&gt;Then we can move on to the scaling challenges that an organization has to
face, which will gradually build up more insights into the component layers. For
instance, if a company is developing and maintaining a mobile application, it
wants to test its new releases on different operating system
versions.  But it would not be sensible to have each developer walk around with
six phones because they need to test the application on iOS 12, iOS 13, iOS 14,
Android 9, Android 10, and Android 11. Instead, testing could be done on
emulators (which can be considered hypervisors, albeit often not that exhaustive
in features).&lt;/p&gt;
&lt;p&gt;This introduces concepts of optimizing resources for cost, but also the
benefits of having these services available 'at distance' (remote access
to the emulation environments) as well as first steps in virtualization.
You can state that this emulation is something the user can do on their
laptops, but that in enterprise environments this is done with either
cloud services or on the enterprise servers, as that facilitates collaboration
with team members, and simplifies managing these assets when the teams get
larger or smaller. And these servers, well, they too are virtualized for
resource optimization.&lt;/p&gt;
&lt;p&gt;We can also discuss the data layer, and the challenge that a regular user
has when their phone is near its limits (e.g. storage is full), the options the
user has (add SD card if the phone supports it, or use cloud storage services),
and compare that with larger enterprises where data hosting is often either
centralized or abstracted, so that systems are not bound to the limits of their
device's storage capacity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views enable scalability and cost insights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The layered view on components, of course, is a meta-view rather than an actual
one: it shows how a stack can be built up, but the actual benefit is when
you look at the component view of a solution.&lt;/p&gt;
&lt;p&gt;For instance, if we were to assess a Kubernetes cluster, it could be represented
as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kubernetes component view" src="https://blog.siphos.be/images/202108/k8s-component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Going bottom-up on this view, we can identify (and thus elaborate on) the
various layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the hardware level, we see four physical servers (named sppc01 to sppc04).
  These servers are of a particular brand and have 32 Gb of memory each (which
  isn't a lot, the cluster is rather small).&lt;/li&gt;
&lt;li&gt;KVM is used as the hypervisor. The hypervisor combines the four physical
  servers in a single cluster.&lt;/li&gt;
&lt;li&gt;KVM then provides eight virtual systems (named svpc01 to svpc08) from the
  cluster. The first three are used for the Kubernetes control plane, the others
  are the worker nodes. Note that it is recommended to host the nodes of the
  control plane on different physical machines so that a failure on one physical
  machine doesn't jeopardize the cluster availability. This can be configured on
  the hypervisor, but that is outside the scope of this article.&lt;/li&gt;
&lt;li&gt;The physical servers use a hardened Gentoo Linux operating system using the
  musl C library, whereas the virtual servers use a regular Gentoo Linux
  installation as their operating system.&lt;/li&gt;
&lt;li&gt;The orchestration layer is Kubernetes itself, using the CRI-O container
  runtime as middleware.&lt;/li&gt;
&lt;li&gt;The applications depicted are those of the Kubernetes ecosystem, with the main
  control plane applications and worker node applications listed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we were to host an application inside the Kubernetes cluster, it would
be deployed on the worker nodes. The logical design of a Kubernetes cluster
is not something to be represented in a component view (that's more for
the location view, as there we will talk about the topology of services).&lt;/p&gt;
&lt;p&gt;With such component views, we can have some insights into the costs. Of course,
this is just a simple Kubernetes cluster, and built with pure open-source
software, so the costs are going to be on the hardware side (and the resources
they consume). In larger enterprises, however, the hypervisor is often a
commercially backed one like Hyper-V (Microsoft) and vSphere (VMware), which
have their specific licensing terms (which could be the number of machines or
even CPUs).  Also, enterprises often use a commercially backed Kubernetes, like
Rancher or OpenShift (Red Hat, part of IBM), which often have per-node licensing
terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views are just the beginning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I use a component view to explain what infrastructure is about,
it is merely the beginning. It provides a rudimentary layered view, which most
people can easily relate to. Content-wise, it is reasonably understandable (or
easy enough to explain) for people that aren't IT savvy, and is something that
you can easily find a lot of material for online.&lt;/p&gt;
&lt;p&gt;If we delve into the processes of (or related to) infrastructure, it becomes
more challenging to keep the readers/listeners with you. Processes can (will)
often be very abstract, and going into the details of each process is a lengthy
endeavor. I'll cover that in a later post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback? Comments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few days ago I've dropped Disqus as comment engine from my blog site, mainly
for concerns about my visitor's security, as well as the advertisements that it
embedded. I want my blog to be simple and straightforward, so I decided to not
have any other third-party services with it for now.&lt;/p&gt;
&lt;p&gt;So, if you have feedback or comments, don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1431332634370711552"&gt;discussion on
Twitter&lt;/a&gt;&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="component"></category><category term="viewpoint"></category></entry><entry><title>Disaster recovery in the public cloud</title><link href="https://blog.siphos.be/2021/07/disaster-recovery-in-the-public-cloud/" rel="alternate"></link><published>2021-07-30T20:00:00+02:00</published><updated>2021-07-30T20:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-30:/2021/07/disaster-recovery-in-the-public-cloud/</id><summary type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is a disaster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters are major incidents that have wide-ranging consequences to the
regular operations of the business. What entails a disaster can be different
between organizations, although they are commonly tied to the size of the
infrastructure and the organizational and infrastructural maturity. I'll get
back to the size dependency later when covering public cloud.&lt;/p&gt;
&lt;p&gt;A small organization that only has a few systems can declare a
disaster when all those systems are unreachable because their network
provider's line is interrupted. A larger organization probably has
redundancy of the network in place to mitigate that threat. And even
without the redundancy, organizations might just not depend that much
on those services.&lt;/p&gt;
&lt;p&gt;The larger the environment becomes though, the more the business depends
on the well-being of the services. And while I can only hope that high
availability, resiliency and appropriate redundancy are taken into account
as well, there are always threats that could jeopardize the availability
of services.&lt;/p&gt;
&lt;p&gt;When the problem at hand is specific to one or a manageable set of services,
then taking appropriate action to remediate that threat is generally not a
disaster. It can be a severe incident, but in general it is taken up
by the organization as an incident with a sufficiently small yet
efficient and well organized coordination: the teams involved are 
low in numbers, and the coordination can be done accurately.&lt;/p&gt;
&lt;p&gt;However, when the problem is significant or has a very wide scope, then
depending on the standard incident coordination will be insufficient. You
need to coordinate across too many teams, make sure communication is done
correctly, business is continuously involved/consulted, and most of all - 
you want to make sure that the organization doesn't independently try
to resolve issues when they don't have a full view on the situation
themselves.&lt;/p&gt;
&lt;p&gt;The latter is a major reason in my opinion why a DRP is so important
to have (the plan/procedure, not an actual disaster). If there is no
proper, well-aligned plan of action, teams will try to get in touch
with other teams, polluting communication and only getting incomplete
information. They might take action that other teams should know about
(but won't) or are heavily impacted by (e.g. because they are at that
time trying to do activities themselves). It can make the situation
much worse.&lt;/p&gt;
&lt;p&gt;Because we have to make a distinction between incident management
and disaster management, an organization has to formally declare
a problem as a disaster, and communicate that singular fact ("we
are now in disaster mode") so that all teams know how to respond: 
according to the Disaster Recovery Plan (DRP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disasters are not just 'force majeure'&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters aren't extraordinary events or circumstances beyond the
control of the organization. Depending on the business needs, you
might very well take precautionary actions against situations you've
never encountered before and will never encounter. We've recently had
a disastrous weather in Belgium (and other countries in Western Europe)
with floods happening in large areas. But that doesn't mean that
for an organization a flood event will trigger a disaster declaration
within a company (the disastrous weather was a disaster from a
human side, with several dozen deaths and millions of damage, so it
feels somewhat incorrect to consider the threat from a theoretical
standpoint here).&lt;/p&gt;
&lt;p&gt;If you're located in a flood-sensitive environment, you can still take
precautionary actions and assess what to do in case of a flood event. 
Depending on the actions taken, a flood event (threat) will not manifest
into availability concerns, data and infrastructure destruction, people
unavailability, etc. It is only when the threat itself turns into an
unforeseen (or non-remediated) situation that we speak of a disaster.&lt;/p&gt;
&lt;p&gt;This is why disasters depend on organizations, and how risk averse
the organization (and business) is. Some businesses might not want to
take precautionary actions against situations that in the past only
occur once every 100 years, especially if the investment they would
have to do is too significant compared to the losses they might have.&lt;/p&gt;
&lt;p&gt;Common disaster threats (sometimes also called catastrophic events)
that I'm used to evaluate from an infrastructure point of view, with a
company that has four strategic data centers, multiple headquarter
locations and a high risk averse setting (considering the financial
market it plays in) are cyberattacks, local but significant infrastructure
disruptions (data center failures or destruction), people-oriented
threats (targetting key personnel), critical provider outages,
disgruntled employees, and so forth. Searching for risk matrices
online can give you some interesting pointers, such as the European
Commission's &lt;a href="https://ec.europa.eu/echo/sites/default/files/swd_2017_176_overview_of_risks_2.pdf"&gt;Overview of Natural and Man-made Disaster Risks the
European Union may
face&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Public cloud related events&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In case of public cloud, the catastrophic events that might occur are
different, and it would be wrong to just consider the same events and
with the same action plan. A prime example, and the one I really want
people to focus on, is regional outages.&lt;/p&gt;
&lt;p&gt;If your current company considers region-wide failures (for
instance because you have two data centers but within the same
region) more from a reactive point of view rather than preventive
(e.g. the DRP in case of region-wide failures is to approach
the reconstruction within the region whenever possible, rather
than fail over to a different region), it might feel the same about
public cloud region failures.&lt;/p&gt;
&lt;p&gt;That would be wrong though. Whereas it is likely that a region-wide
failure for a company is not going to happen in its lifetime, a public
cloud provider is so much more massive in size, that the likelihood
of region-wide failures is higher. If you do a quick search for
region-wide failures in AWS or Azure, you'll find plenty of examples.
And while the failures themselves might be considered 'incidents' from
the public cloud provider point of view, they might be disasters for
the companies/customers that rely on them.&lt;/p&gt;
&lt;p&gt;For me, tackling disaster recovery when dealing with public cloud strongly
focuses on region failures and (coordinated) recovery from region failures.
Beyond region failures, I also strongly recommend to look into the dependencies
that the public cloud consumption has with services outside of the public cloud.
Some of these dependencies might also play a role in certain catastrophic
events. Say that you depend on Azure AD for your authentication and
authorization, and Microsoft is suddenly facing not just a world-wide
Azure AD outage, but also has to explain to you that they cannot restore its
data.&lt;/p&gt;
&lt;p&gt;Preparing for disasters is about preparing for multiple possible catastrophic
events, and in case of public cloud, you're required to think at massive scales.
And that includes designing for region-wide failures as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Impact of public cloud disasters to the organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Generally, if your organization has a decent maturity in dealing with disaster
recovery planning, they will be using Service Level Agreements with the
business to help decide what to do in case of disasters. Important
non-functionals that are included here are RTO (Recovery Time Objective), RPO
(Recovery Point Objective), and MTD (Maximum Tolerable Downtime). There are
others possibly listed in the SLA as well, but let me focus on these three.
If you want to learn more about contigency planning in general, I recommend
to go through the &lt;a href="https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf"&gt;NIST Special Publication 800-34 Rev.1, "Contingency Planning
Guide for Federal Informatino
Systems"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the RTO, we represent the time it is allowed to take to recover a service
to the point that it functions again. This might include with reduced capacity
or performance degradation. The RTO can be expressed in hours, days, weeks
or other arbitrary value. It is a disaster-oriented value, not availability!
As long as no disaster is declared, the RTO is not considered.&lt;/p&gt;
&lt;p&gt;The RPO identifies how much data loss is acceptable by the business in case of
a disaster. Again, this is disaster-oriented: the business can very well take
extra-ordinary steps to ensure full transactional consistency outside of 
disaster situations, yet allow for a "previous day" RPO in case of a disaster.&lt;/p&gt;
&lt;p&gt;The MTD is most often declared not on a single service, but at business service
level, and explains how long unavailability of that service is tolerated before
it seriously impacts the survivability of the business. It is related to the
RTO, as most services will have an RTO that is more strict/lower value than the
overall MTD, whereas the MTD is near non-negotiable.&lt;/p&gt;
&lt;p&gt;Now, what does public cloud disasters have to do with this? Well, in theory
nothing, as this model and process of capturing business requirements is quite
sensible and maps out perfectly as well. However (and here's the culprit),
an organization that sets up new services on a frequent basis might get
accustomed to certain values, and these values might not be as easy to approach
in a public cloud environment. Furthermore, the organization might not be
accustomed to different disaster scenario's for the SLA: having different sets
of RTO/RPO depending on the category of disaster.&lt;/p&gt;
&lt;p&gt;Let's get back to the region-wide disasters. A company might have decided not
to have region-wide proactive measures in place, and fixate their SLA
non-functionals on local disasters: a data center failure is considered a threat
that still requires proactive measures, whereas regional failures are treated
differently.  The organization decides to only have one SLA set defined, and
includes RTO and RPO values based on the current, local threat matrix. They
might decide that a majority of applications or services has a RPO of "last
transaction", meaning the data has to be fully restored at the latest situation
in case of a disaster.&lt;/p&gt;
&lt;p&gt;This generally implies synchronous replication as an infrastructural
solution. If the organization is used to having this method available (for
instance through SAN replication, cluster technologies, database recovery,
etc.) then they won't sweat at all if the next dozen services all require
the same RPO.&lt;/p&gt;
&lt;p&gt;But now comes the public cloud, and a strong point of attention is region-wide
failures. Doing synchronous replication across regions is not a proper tactic
(as it implies significant performance degradation) and especially not sensible
to do at the same scale as with local replication (e.g. between availability
zones in the same region). Now you have to tell your business that this RPO
value is not easily attainable in the public cloud. The public cloud, which
solves all the wonders in the world. The public cloud, which has more maturity
on operations than your own company. Yet you can't deliver the same SLA?&lt;/p&gt;
&lt;p&gt;Apples and pears. The disasters are different, so your offering might be
different. Of course, you should explain that your 'on premise' disaster
scenarios do not include region-wide failures, and that if you include
the same scenarios for 'on premise' that that RPO value would not be
attainable on premise either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The public cloud provides many capabilities, and has to deal with a
significantly larger environment than companies are used to. This also means
that disasters that are considered 'extremely unlikely' are now 'likely' (given
the massive scale of the public cloud), and that the threats you have to
consider while dealing with disaster recovery have to be re-visited for public
cloud enabled scenarios.&lt;/p&gt;
&lt;p&gt;My recommendation is to tackle the disaster-oriented non-functional requirements
by categorizing the disasters and having different requirements based on the
disaster at hand. Mature your cloud endeavours so that regional outages
are not a problem anymore (moving them away from the 'disaster' board), and 
properly map all dependencies you have through the public cloud exercises so
that you can build up a good view on what possible threats exist that would
require a well-coordinated approach to tackle the event.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="DRP"></category></entry><entry><title>What is the infrastructure domain?</title><link href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/" rel="alternate"></link><published>2021-07-19T15:20:00+02:00</published><updated>2021-07-19T15:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-19:/2021/07/what-is-the-infrastructure-domain/</id><summary type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;


&lt;p&gt;Of course, I do not want to approach this solely from my own viewpoint. There
are plenty of reference architectures and frameworks out there that could assist
in this. However, I still have the feeling that they are either too complex to
use for non-architect stakeholders, too simple to expose the domain properly, or
just don't fit the situation that I am currently faced with. And that's OK,
many of these frameworks are intended for architects, and from those frameworks
I can borrow insights left and right to use for a simple visualization, a
landscaping of some sort.&lt;/p&gt;
&lt;p&gt;So, let's first look at the frameworks and references out there. Some remarks
though that might be important to understand the post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When I use "the infrastructure domain", I reflect on how I see it. Within the
  company that I work for, there is some guidance on what the scope is of the
  infratructure domain, and that of course strongly influences how I look at
  "the infrastructure domain". But keep in mind that this is still somewhat
  organization- or company oriented. YMMV.&lt;/li&gt;
&lt;li&gt;While I am lucky enough to have received the time and opportunity to learn
  about enterprise architecture and architecture frameworks (and even got a
  masters degree for it), I also learned that I know nothing. Enterprises are
  complex, enterprise architecture is not a single framework or approach, and
  the enterprise architecture landscape is continuously evolving. So it is very
  well possible that I am missing something (and I will gladly learn from
  feedback on this).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Open Group Architecture Framework (TOGAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you would ask for one common framework out there for enterprise architecture,
then &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/"&gt;TOGAF&lt;/a&gt; is
probably it. It is a very exhaustive framework that focuses on various aspects
of enterprise architecture: the architecture development methodology, techniques
to use, the content of an architecture view (like metamodel descriptions), and the
capabilities that a mature organization should pursue.&lt;/p&gt;
&lt;p&gt;A core part of TOGAF is the &lt;em&gt;Architecture Development Method&lt;/em&gt; cycle, which has
several phases, including phases that are close to the infrastructure domain:
"Technology Architecture (D)" as well as areas of "Opportunities and Solutions
(E)" and "Information Systems Architectures (C)". Infrastructure is more than
'just' technology, but the core of it does fit within the technology part.&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF Cycle" src="https://blog.siphos.be/images/202107/togaf-adm-cycle.png"&gt;
&lt;em&gt;The ADM cycle, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap04.html"&gt;The Open Group, TOGAF 9.2, Chapter 4&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With TOGAF, you can support a full enterprise architecture view from the
strategy and vision up to the smallest change and its governance. However, the
key word here is &lt;em&gt;support&lt;/em&gt;, as TOGAF will not really give you much food for
simply representing the scope of infrastructure.&lt;/p&gt;
&lt;p&gt;I'm not saying it isn't a good framework, on the contrary. Especially with
&lt;a href="http://www.opengroup.org/archimate-forum"&gt;ArchiMate&lt;/a&gt; as modeling language (also
from The Open Group), using TOGAF and its meta model is a good way to facilitate
a mature architecture practice and enterprise-wide view within the
organization. But just like how application architecture and design requires a
bit more muscle than &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap30.html"&gt;TOGAF's
metamodel&lt;/a&gt;
supports, the same is true for infrastructure.&lt;/p&gt;
&lt;p&gt;There are also plenty of other enterprise frameworks out there that can easily
be mapped to TOGAF. Most of these focus mainly on the layering (business,
information, application, technology), processes (requirement management and the
like) and viewpoints (how to visualize certain insights) and, if you're fluent
in TOGAF, you can easily understand these other frameworks as well. I will not
be going through those in detail, but I also do not want to insinuate that they
are not valid anymore if you compare them with TOGAF: TOGAF is very extensive
and has a huge market adoption, but sometimes an extensive and exhaustive
framework isn't what a company needs...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TOGAF Technical Reference Model / Integrated Information Infrastructure
Reference Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As TOGAF is extremely extensive, it has parts that can be used to reference or
visualize infrastructure a bit better. In TOGAF 9.1, we had the &lt;em&gt;TOGAF Technical
Reference Model (TRM)&lt;/em&gt; and &lt;em&gt;TOGAF Integrated Information Infrastructure
Reference Model (III-RM)&lt;/em&gt; where you might feel that this is closer to what I
am seeking (for instance,
&lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;III-RM&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF III-RM" src="https://blog.siphos.be/images/202107/togaf-iii-rm.png"&gt;
&lt;em&gt;Focus of the III-RM, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;The Open Group, TOGAF 9.1, Chapter 44&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While it does become a bit more tangible, TOGAF does not expand much on this
reference model. Instead, it is more meant as a starting point for organizations
to develop their own reference models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Information Technology Infrastructure Library (ITIL)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL 4&lt;/a&gt; is another very
common framework, this time owned by AXELOS Limited. The focus of ITIL is on
process support, with many processes (sorry,
'&lt;a href="https://wiki.en.it-processmaps.com/index.php/ITIL_4"&gt;practices&lt;/a&gt;' as they are
called now) being very much close to what I consider to be part of the
infrastructure domain. The practices give a good overview of 'what things to
think about' when dealing with the infrastructure domain. Now, ITIL is not
exclusive to the infrastructure domain, and the company that I work for
currently considers many of these practices as processes that need to be tackled
across all domains.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ITIL Practices" src="https://blog.siphos.be/images/202107/itil-practices.jpg"&gt;
&lt;em&gt;ITIL 4 Practices, taken from &lt;a href="https://valueinsights.ch/-the-itil-4-practices-overview/"&gt;Value Insights, The ITIL 4 Practices Overview&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Still, I do want to see some of the ITIL practices reflected in the generic
infrastructure view as they are often influencing the infrastructure domain and
the projects within. The ITIL practices make it possible to explain that it
isn't just about downloading and installing software or quickly buying an
appliance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference Model of Open Distributed Processing (RM-ODP)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.rm-odp.net/"&gt;RM-ODP standard&lt;/a&gt; has a strong focus on distributed
processing (hence the name), which is a big part of what the infrastructure
domain is about. If we ignore the workplace environment for a bit, and focus on
hosting of applications, the majority of today's hosting initiatives are on
distributed platforms.&lt;/p&gt;
&lt;p&gt;&lt;img alt="RM-ODP Five Viewpoints" src="https://blog.siphos.be/images/202107/rm-odp.png"&gt;
&lt;em&gt;Five viewpoints of RM-ODP, taken from &lt;a href="https://sparxsystems.com/products/3rdparty/odp/index.html"&gt;MDG Technology for ODP - UML for ODP, SparxSystems&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within RM-ODP guidance is given on how to handle requirement management, how to
document the processing semantics, how to identify the components, integrations
and limitations of the systems, and how to select the most appropriate
technology. The intention of RM-ODP is to be as precise and formal as possible,
and leave no room for interpretation. To accomplish that, RM-ODP uses an object
model approach.&lt;/p&gt;
&lt;p&gt;Unlike the more business and information system oriented frameworks, RM-ODP has
a strong focus on infrastructure. Its viewpoints include engineering,
computational and technology for instance. The challenge that rises here
however is that it sticks to the more engineering oriented abstractions, which
make perfect sense for people and stakeholders involved in the infrastructure
domain, but is often Chinese for others.&lt;/p&gt;
&lt;p&gt;Personally, I consider RM-ODP to be a domain-specific standard strongly
associated with the infrastructure domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Department of Defense Architecture Framework (DoDAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dodcio.defense.gov/library/dod-architecture-framework/"&gt;DoDAF&lt;/a&gt;
 is an architecture framework that has a strong focus on the definition
and visualization of the different viewpoints. It is less tangible than RM-ODP,
instead focusing on view definitions: what and how should something be
presented to the stakeholders. The intention of DoDAF is that organizations
develop and use an enterprise architecture that supports and uses the
viewpoints that DoDAF prescribes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="DoDAF viewpoints" src="https://blog.siphos.be/images/202107/dodaf-viewpoints.png"&gt;
&lt;em&gt;DoDAF viewpoints, taken from &lt;a href="https://www.visual-paradigm.com/guide/enterprise-architecture/what-is-dodaf-framework/"&gt;"What is DoDAF Framework", Visual Paradigm&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unlike broad scale architecture frameworks that look at an enterprise in its
entirety, my impression is that DoDAF is more towards product architecture.
That makes DoDAF more interesting for solution architectures, which often
require to be more detailed and thus hit closer to home when looking at the
infrastructure domain. However, it is not something I can 'borrow' from to
easily explain what infrastructure is about.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nato.int/cps/en/natohq/topics_157575.htm"&gt;NATO's Architecture Framework (NAF)&lt;/a&gt;
seems to play witin the same realm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sherwood Applied Business Security Architecture (SABSA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SABSA framework and methodology has a strong security focus, but covers the
requirements from full enterprise view up to the solutions themselves. One of
the benefits of SABSA is inherent to this security-orientation: you really need
to know and understand how things work before you can show that they are
secure. Hence, SABSA is a quite complete framework and methodology.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SABSA Metamodel" src="https://blog.siphos.be/images/202107/sabsa-metamodel.png"&gt;
&lt;em&gt;SABSA metamodel, taken from &lt;a href="https://sabsa.org/the-chief-architects-blog-a-brief-history-of-sabsa-21-years-old-this-year/"&gt;"A Brief History of SABSA: Part 1", sabsa.org&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important focus area in SABSA is the integration between services, which is
something I am frequently confronted with at work. Yet unlike the more solution
driven frameworks, SABSA retains its business-oriented top-down approach, which
places it alongside the TOGAF one in my opinion. Moreover, we can apply TOGAF's
development method while using SABSA to receive more direct requirements and
design focus.&lt;/p&gt;
&lt;p&gt;Its risk and enabler orientation offers a help to not only explain how things
are set up, but also why. Especially in the sector I am currently active in
(financial sector) having a risk-based, security-conscious approach is a good
fit. The supporting list of attributes, metrics, security services, etc. allow
for defining more complete architectures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Control Objectives for IT (CObIT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a similar area as ITIL, the &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT
framework&lt;/a&gt; focuses less on a complete
enterprise architecture framework and more on processes, process maturity, and
alignment of the processes within the organization. I am personally a fan of
CObIT as it is a more tangible framework, with more clear deliverables and
requirements, compared to others. But like with most frameworks, it has
received numerous updates to fit in continuously growing environments and
cultures which makes it heavy to use.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CObIT Core Model" src="https://blog.siphos.be/images/202107/cobit-core-model.jpg"&gt;
&lt;em&gt;The CObIT 2019 Core Model, taken from "&lt;a href="https://www.isaca.org/resources/news-and-trends/industry-news/2020/using-cobit-2019-to-plan-and-execute-an-organization-transformation-strategy"&gt;Using CObIT 2019 to plan and execute
an organization transformation strategy,
ISACA.org&lt;/a&gt;"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The framework is less about the content of infrastructure and technology, and
more about how to assess, develop, build, maintain, operate and control
whatever service is taken up. However, there are references to infrastructure
(especially when dealing with non-functionals) or controls that are actively
situated in infrastructure (such as backup/restore, disaster recovery, etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IT for IT (IT4IT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open Group has a similar framework like CObIT, called
&lt;a href="https://www.opengroup.org/it4it"&gt;IT4IT&lt;/a&gt;. It does have a reference architecture
view that attempts to group certain deliverables/services together to create a
holistic view on what IT should offer. But unlike the larger enterprise
frameworks it focuses strongly on service delivery and its dependencies.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IT4IT Reference Architecture" src="https://blog.siphos.be/images/202107/it4it-reference-architecture.png"&gt;
&lt;em&gt;IT4IT Reference Architecture, taken from &lt;a href="https://www.opengroup.org/it4it-forum"&gt;The Open Group IT4IT
Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within the IT4IT reference architecture, a grouping is attempted that maps on a
value stream, starting from a strategy over the deployment up to the detection
and correction facilities. This value stream orientation is common across
frameworks, but often feels like the value is always to "add more", "deliver
more". In my opinion, rationalization exercises, decommissioning and
custodianship is too much hidden. Sure, it is part of the change management
processes and operational maintenance, but those are extremely valuable areas
that are not expressively visible in these frameworks. Compare that to the
attention that risk and security receives: while security consciousness should
be included at all phases of the value stream, security is always explicitly
mentioned.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendor-specific visualizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Several vendors have their own visualization methodology that represents more
specific requirements from the domain(s) in which those vendors are active.
These are generally domain-specific visualizations, even with a vendor-specific
view. Such methodologies are nice to use when dealing with specific viewpoints,
but I do not believe these should be considered "architecture" frameworks. They
don't deal with requirement management, strategy alignment, and often lack
functional and non-functional insights. Still, they are a must to know in the
infrastructure areas.&lt;/p&gt;
&lt;p&gt;If you are active in Amazon AWS for instance, then you've undoubtedly come
across drawings like the one visible in "&lt;a href="https://aws.amazon.com/blogs/architecture/wordpress-best-practices-on-aws/"&gt;Wordpress: Best Practices on
AWS&lt;/a&gt;".
These drawings provide a deployment viewpoint that lists the main interactions
between AWS services.&lt;/p&gt;
&lt;p&gt;When you are more network oriented, then you've been immersed in Cisco's network
diagrams, like the one visible in "&lt;a href="https://www.cisco.com/c/en/us/td/docs/solutions/Verticals/PCI_Retail/roc.html"&gt;Verizon Business Assessment for: Cisco PCI
Solution for
Retail&lt;/a&gt;".
These network diagrams again focus on the deployment viewpoint of the network
devices and their main interactions.&lt;/p&gt;
&lt;p&gt;There are probably plenty more of these specific visualizations, but the two
mentioned above are most visible to me currently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of frameworks out there to learn from, and some of these can be
used to find ways of explaining what the infrastructure domain is about.
However, they are often all very complete and require an architectural mindset
to start from, which is not obvious when trying to convey something to outside
or indirect stakeholders.&lt;/p&gt;
&lt;p&gt;Few frameworks have a reference that is directly consumable by non-architect
stakeholders. The most tangible ones seem to be related to the IT processes, but
those still require an IT mindset to interpret.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="pattern"></category></entry><entry><title>Organizing service documentation</title><link href="https://blog.siphos.be/2021/07/organizing-service-documentation/" rel="alternate"></link><published>2021-07-08T09:20:00+02:00</published><updated>2021-07-08T09:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-08:/2021/07/organizing-service-documentation/</id><summary type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Why I tend to document some of it myself&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within the company I currently work for, not all architecture and designs are
handled by a central repository, but that doesn't mean there is no architecture
and design available. They are more commonly handled through separate documents,
online project sites and the like. If we had a common way of documenting
everything in the same tool using the same processes and the same taxonomy, it
wouldn't make sense to document things myself... unless even then I would find
that I am missing some information.&lt;/p&gt;
&lt;p&gt;It all started when I tried to keep track of past decisions for a service or
solution. Decisions on architecture boards, on risk forums, on department
steering committees and what not. &lt;em&gt;Historical insights&lt;/em&gt; I call it, and it often
provides a good sense of why a solution or service came up, what the challenges
were, which principles were used, etc.&lt;/p&gt;
&lt;p&gt;Once I started tracking the historical decisions and topics, I quickly moved on
to a common structure: an entry page with the most common information about the
service or solution, and then subpages for the following categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Administration (processes, authorizations, procedures)&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorizationn&lt;/li&gt;
&lt;li&gt;Auditing and logging&lt;/li&gt;
&lt;li&gt;Configuration management&lt;/li&gt;
&lt;li&gt;Cost management&lt;/li&gt;
&lt;li&gt;Cryptography and privacy&lt;/li&gt;
&lt;li&gt;Data management (data handling, definitions, governance, lineage,
  backup/restore, ...)&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;Design and development (incl. naming convention)&lt;/li&gt;
&lt;li&gt;Figures&lt;/li&gt;
&lt;li&gt;High availability and disaster recovery&lt;/li&gt;
&lt;li&gt;History&lt;/li&gt;
&lt;li&gt;Operations (actor groups, source systems &amp;amp; interactions/external interfacing)&lt;/li&gt;
&lt;li&gt;Organization (management, organisational structure within the company, etc.)&lt;/li&gt;
&lt;li&gt;Performance management&lt;/li&gt;
&lt;li&gt;Patterns&lt;/li&gt;
&lt;li&gt;Processes&lt;/li&gt;
&lt;li&gt;Quality assurance &amp;amp; reporting&lt;/li&gt;
&lt;li&gt;Roadmap and restrictions (incl. lifecycle)&lt;/li&gt;
&lt;li&gt;Risks and technical debt&lt;/li&gt;
&lt;li&gt;Runtime information&lt;/li&gt;
&lt;li&gt;Terminology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, I won't go in depth about all the different categories listed. Perhaps some
areas warrant closer scrutiny in later posts, but for now the names of the
categories should be sufficiently self-explanatory.&lt;/p&gt;
&lt;p&gt;If there is an internal service (website or similar) that covers the
details properly, then I will of course not duplicate that information. Instead,
I will add a link to that resource and perhaps just document how to interpret
the information on that resource.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The entry page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The entry page of a service or solution always looks the same. It starts off
with a few attributes associated with the service:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The taxonomy used within the company&lt;/li&gt;
&lt;li&gt;The main point of contact(s)&lt;/li&gt;
&lt;li&gt;The backlog where the responsible team tracks its progress and evolution&lt;/li&gt;
&lt;li&gt;A link to the main documentation resources&lt;/li&gt;
&lt;li&gt;The internal working name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;taxonomy&lt;/em&gt; is something I strongly appreciate in my current company. It is
a uniform identifier associated with the product, service or solution, and is
used for several of the operational processes the company has. This taxonomy
comes up in things like chargeback, service level agreements, responsibility
overviews, data classifications, enterprise architectural overviews, etc.&lt;/p&gt;
&lt;p&gt;For instance, a managed macbook (asset) might have a taxonomy identifier of
&lt;code&gt;mmac&lt;/code&gt;, or we have a service for exchanging internal company data identified as
&lt;code&gt;cd70&lt;/code&gt; (it doesn't need to have an identifier that "reads" properly). Of course,
people don't just run around shouting the identifiers, but when we go through
the information available at the company, this identifier is often the primary
key so to speak to find the information.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;main points of contacts&lt;/em&gt;, I usually just document the person that is
my go-to person to get some quick insights. The full list of all contacts (like
product owner, product manager, system architect, business analyst, release
manager, etc.) is managed in a central tool (and uses the taxonomy to quickly
find the right person), so I just have the few names that I quickly need listed
here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;backlog&lt;/em&gt; is something I recently added to support any questions on "when
will we have feature XYZ". In the past, I had to contact the people to get this
information, but that often became cumbersome, especially when the team uses a
proper tool for tracking the work on the service.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;main documentation&lt;/em&gt; is often the most important part. It is a link to the
documentation that the team provides for end users, architects or other roles.
Some teams still have their information on a NAS, others in a document library
on SharePoint, others use a wiki, and there are teams that use a structured
document management system. Working for a big company has its consequences...&lt;/p&gt;
&lt;p&gt;Finally, the &lt;em&gt;internal working name&lt;/em&gt; is the name that a service or solution
receives the most. For infrastructure services, this is often the name of the
product from the time the product entered the organization. While the vendor has
switched the name of the product numerous times since, the old name sticks. For
instance, while I will document IBM's cloud offering as "IBMCloud" (its current
name) I will list its working name as "Bluemix" because that's how the company
internally often refers to it.&lt;/p&gt;
&lt;p&gt;After the basic attributes, I have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a few paragraphs for &lt;em&gt;description&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;a diagram or architecture view to give a &lt;em&gt;high level design&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;most important questions&lt;/em&gt; surrounding the service or solution&lt;/li&gt;
&lt;li&gt;some &lt;em&gt;tips and titbits&lt;/em&gt; for myself&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The high level design is often a view that I maintain myself, which uses the
&lt;a href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/"&gt;abstraction&lt;/a&gt; that I
mentioned earlier in my blog. It is not covering everything, but to a sufficient
level that I can quickly understand the context of the service or solution and
how it is generally implemented.&lt;/p&gt;
&lt;p&gt;The most important questions are mostly a refresher for questions that pop up
during discussions. For instance, for an API gateway, common questions might be
"What are the security controls that it enforces" or "Does the API GW perform
schema validation on JSON structures?".&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The history of a service&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below the entry page, the various categories come up. As I mentioned, it all
started with the historical insights on the service. By having a chronological
overview of all decisions and related material per service, I can quickly find
the most important information in many cases.&lt;/p&gt;
&lt;p&gt;Want to know the latest architecture for a service? Let's look in the history
when the last architectural review was, and at which decision body or board it
came up. Once found, I just need to go to the meeting minutes or case details to
find it.&lt;/p&gt;
&lt;p&gt;Want to know why the decision was taken to allow a non-standard integration?
Take a look at the history where this decision was taken, and consult its
meeting minutes.&lt;/p&gt;
&lt;p&gt;Need to ask for a continuance for something but you're just new in the team and
don't know why or how it was approved in the past? No worries, I'll share with
you the history of the service, where you can find the information, and coach
you a bit through our organization.&lt;/p&gt;
&lt;p&gt;Having the history for services and solutions available has been a massive
efficiency gain for myself and my immediate stakeholders. Of course, I would
have loved if the organization tracked this themselves, but as long as they
don't (especially since organization changes more often than technology) I will
put time and effort to track it myself (at least for the services and solutions
that I am interested in).&lt;/p&gt;
&lt;p&gt;The historical information I track myself is not a copy/paste of the meeting
minutes of those entries. I try to use a single paragraph explaination, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ARB  2020/12/05     &amp;quot;Switch of PostgreSQL authentication provider to PAM+sssd&amp;quot;
    Approval to switch the authentication provider of the PostgreSQL
    database assets from the internal authentication to a PAM-supported
    method, and use the RHEL sssd solution as a facilitator. Link with
    Active Directory.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ARB&lt;/code&gt; is the name of the decision body (here it could be the &lt;em&gt;Architecture
Review Board&lt;/em&gt;) and tells me where I can find more details if needed. I don't
really bother with trying to add actual links, because that takes time and often
the links become invalid after we switch from one solution to another.&lt;/p&gt;
&lt;p&gt;Since then, I also started adding information related to the service that isn't
just decision body oriented:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Incident    2021/06/08  &amp;quot;Fastly major outage&amp;quot;
    A major outage occurred on Fastly, a widely used cloud edge provider,
    between 09:47 UTC and 12:35 UTC. This impacted service ABC and DEF.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Incidents can be internal or external, and if they are internal I'll document
the incident and root cause analysis numbers associated with the incident as
well.&lt;/p&gt;
&lt;p&gt;It also doesn't need to be about problems. It can be announcements from the
vendor as well, as long as the announcement is or can be impactful for my work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How complete is this overview&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My overview is far, far, far from complete. It is also not my intention to make
it a complete overview, but instead use it as a quick reference when needed.
Services that are commonly discussed (for instance because they have wide
implications on other domain's architectures) are documented more in depth than
services that are barely influential to my meetings and projects. And that
doesn't mean that the services themselves are not important.&lt;/p&gt;
&lt;p&gt;Furthermore, the only areas that I truly want to have up-to-date, is the entry
page and the history. For all the other information I always hope to be able to
link to existing documentation that is kept up-to-date by the responsible teams.&lt;/p&gt;
&lt;p&gt;But in case the information isn't available, using the same structure for noting
down what insights that I gather helps out tremendously.&lt;/p&gt;
&lt;p&gt;I also don't want my overview to become a critical asset. It is protected from
prying eyes (as there is occasionally confidential information inside) and I am
coaching the organization to take up a lot of the information gathering and
documentation in a structured way. For instance, for managing EOL information,
we are publishing this in a standard way for all internal stakeholders to see
(and report on). The roadmap and strategy for the services within the domain are
now being standardized within the backlog tool as well, so that everybody can
clearly document when they expect to work on something, when certain investments
are needed, etc.&lt;/p&gt;
&lt;p&gt;In the past, architects often had to scramble all that information together
(hence one of my categories on &lt;em&gt;Roadmap&lt;/em&gt;) whereas we can now use the backlog
tools of the teams themselves to report on it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which tool to use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Personally, I use a wiki-alike service for this, so that I can search through
the pages, move and merge information, use tagging and labels and what not. I
also think that, unless the company already has a central tool for this, a well
maintained wiki with good practices and agreements on how to use it would do
wonders.&lt;/p&gt;
&lt;p&gt;I've been playing around in my spare time with several wiki technologies.
&lt;a href="https://www.dokuwiki.org/dokuwiki"&gt;Dokuwiki&lt;/a&gt; is still one of my favorites due
to its simplicity, whereas &lt;a href="https://www.mediawiki.org/wiki/MediaWiki"&gt;MediaWiki&lt;/a&gt;
is one of my go-to's for when the organization really wants to pursue a scalable
and flexible organization-wide wiki. However, considering that I try to
structure the information in a hierarchical way, I am planning to play around
with &lt;a href="https://www.bookstackapp.com/"&gt;BookStack&lt;/a&gt; a bit more.&lt;/p&gt;
&lt;p&gt;But while having a good tool is important, it isn't the critical part of
documenting information. Good documentation, in my opinion, comes from a good
structure and a coherent way of working. If you do it yourself, then of course
it is coherent, but it takes time and effort to maintain it. If you collaborate
on it, you have to make sure everybody follows the same practices and agreements
- so don't make them too complex.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="documentation"></category><category term="structure"></category><category term="wiki"></category></entry><entry><title>Not sure if TOSCA will grow further</title><link href="https://blog.siphos.be/2021/06/not-sure-if-TOSCA-will-grow-further/" rel="alternate"></link><published>2021-06-30T14:30:00+02:00</published><updated>2021-06-30T14:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-30:/2021/06/not-sure-if-TOSCA-will-grow-further/</id><summary type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;


&lt;p&gt;But while I do see some adoption of TOSCA, I get the feeling that it is
struggling with its position against the various infrastructure-as-code
(IaC) frameworks that are out there. While many of these frameworks do
not inherently support the abstraction that TOSCA has, it is not all that
hard to apply similar principles and use those frameworks to facilitate
multi-cloud deployments.&lt;/p&gt;
&lt;p&gt;Before considering the infrastructural value of TOSCA further, let's
see what TOSCA is about first.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplifying and abstracting cloud deployments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TOSCA offers a model where you can declare how an application should be
hosted in the cloud, or in a cloud-native environment (like a Kubernetes
cluster). For instance, you might want to describe a certain document
management system, which has a web application front-end deployed on 
a farm of web application servers with a load balancer in front of it,
a backend processing system, a database system and a document storage
system. With TOSCA, you can define these structural elements with their
resource requirements.&lt;/p&gt;
&lt;p&gt;For instance, for the database system, we could declare that it has to
be a PostgreSQL database system with a certain administration password,
and within the database system we define two databases with their
own user roles:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;topology_template:
  ...
  node_templates:
    db_server:
      type: tosca.nodes.Compute
      ...
    postgresql:
      type: tosca.nodes.DBMS.PostgreSQL
      properties:
        root_password: &amp;quot;...&amp;quot;
      requirements:
        host: db_server
    db_filemeta:
      type: tosca.nodes.Database.PostgreSQL
      properties:
        name: db_filemeta
        user: fmusr
        password: &amp;quot;...&amp;quot;
      artifacts:
        db_content:
          file: files/file_server_metadata.txt
          type: tosca.artifacts.File
      requirements:
        - host: postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The parameters can, and should be further parameterized. TOSCA supports
declaring inputs that are provided upon deployment so you can safely
publicize the TOSCA structure without putting passwords in there
for instance. Furthermore, TOSCA allows you to add scripts to execute
when a resource is created, which is a common requirement for database
systems.&lt;/p&gt;
&lt;p&gt;But that's not all. Within TOSCA, you then further define the relationship
between the different systems (nodes), including connectivity requirements.
Connections can then be further aligned with virtual networks to model
the network design of the application.&lt;/p&gt;
&lt;p&gt;One of the major benefits of TOSCA is that it also provides abstraction on
the requirements. While the above example explicitly pushes for a PostgreSQL
database hosted on a specific compute server, we could also declare that we
need a database management system, or for the network part we need firewall
capabilities. The TOSCA interpreter, when mapping the model to the target
cloud environment, can then either suggest or pick the technology service
itself. The TOSCA model can then have different actions depending on the
selected technology. For the database for instance, it would have different
deployment scripts.&lt;/p&gt;
&lt;p&gt;The last major benefit that I would like to point out are the workflow
and policy capabilities of TOSCA. You can declare how for instance a 
backup process should look like, or how to cleanly stop and start the
application. You can even model how a rolling upgrade of the application
or database could be handled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is not just theoretical&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Standards can often be purely theoretical, with one or a few reference
implementations out there. That is not the case with TOSCA. While reading
up on TOSCA, it became clear that it has a strong focus on Network
Functions Virtualization (NFV), a term used to denote the shift
from appliance-driven networking capabilities towards an environment
that has a multitude of solutions running in virtual environments, and
where the infrastructure adopts to this virtualized situation with
(for network) virtual routers, firewalls, etc. Another standards body,
namely the European Telecommunications Standards Institute (ETSI), seems
to be the driving force behind the NFV architecture.&lt;/p&gt;
&lt;p&gt;TOSCA has a simple profile for NFV, which aligns with ETSI's NFV and 
ensures that TOSCA parsers that support this profile can easily be used
to set up and manage solutions in virtualized environments (and thus
also cloud environments). The amount of online information about TOSCA
with respect to the NFV side is large, although still in my opinion
strongly vendor-oriented (products that support TOSCA) and standards-oriented
(talks about how well it fits). On TOSCA's &lt;a href="https://www.oasis-open.org/tosca-implementation-stories/"&gt;implementation stories&lt;/a&gt;
page, two of the three non-vendor stories are within the telco industry.&lt;/p&gt;
&lt;p&gt;There are a few vendors that heavily promote
TOSCA: &lt;a href="https://cloudify.co/"&gt;Cloudify&lt;/a&gt; and &lt;a href="https://designer.otc-service.com"&gt;Ubicity&lt;/a&gt;
offer multi-cloud orchestrators that are TOSCA-based. Many vendors, 
including the incumbent network technology vendors like Cisco and Nokia,
embrace TOSCA NFV. But most information from practical TOSCA usage out
there is in open source solutions. The list of &lt;a href="https://github.com/oasis-open/tosca-community-contributions/wiki/Known-TOSCA-Implementations"&gt;known TOSCA implementations&lt;/a&gt;
mentions plenty of open source products. One of the solutions that I
am considering of playing around with is &lt;a href="https://turandot.puccini.cloud/"&gt;turandot&lt;/a&gt;,
which uses TOSCA to compose and orchestrate Kubernetes workloads.&lt;/p&gt;
&lt;p&gt;As an infrastructure architect, TOSCA could be a nice way of putting
initial designs into practice: after designing solutions in a language
like ArchiMate, which is in general not 'executable', the next step could
be to move the deployment specifications into TOSCA and have the next
phases of the project use and enhance the TOSCA definition. But that
easily brings me to what I consider to be shortcomings of the current
situation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inhibitors for growth potential&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are a number of issues I have with the current state of TOSCA.&lt;/p&gt;
&lt;p&gt;TOSCA's ecosystem &lt;em&gt;seems&lt;/em&gt; to be lacking sufficient visualization support.
I did come across &lt;a href="https://projects.eclipse.org/projects/soa.winery"&gt;Winery&lt;/a&gt;
but that seems to be it. I would really like to see a solution that reads
TOSCA and generates an architectural overview. For instance, for the
example I started this blog with, something like the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visualization of a deployment" src="https://blog.siphos.be/images/202106/tosca-archimate.png"&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, my impression is that TOSCA is strongly and mostly Infrastructure
as a Service (IaaS) oriented. The company I currently work for strongly
focuses on platform services, managed cloud services rather than the
more traditional infrastructure services where we would still have to do
the blunt of the management ourselves. Can TOSCA still play a role
in solutions that are fully using platform services? Perhaps the answer
here is "no", as those managed services are often very cloud-vendor specific,
but that isn't always the case, and can often also be tackled using the
abstraction and implementation specifics that TOSCA supports.&lt;/p&gt;
&lt;p&gt;I also have to rely too much on impressions. While the TOSCA ecosystem
has plenty of open source solutions out there, I find it hard to get
tangible examples: TOSCA definitions of larger-scale definitions that
not only show an initial setup, but are actively maintained to show
maintenance evolution of the solution. If TOSCA is so great for vendors
to have a cloud-independent approach, why do I find it hard to find
vendors that expose TOSCA files? If the adoption of TOSCA stops
at the standards bodies and too few vendors, then it is not likely
to grow much further.&lt;/p&gt;
&lt;p&gt;TOSCA orchestration engines often are in direct competition with
general IaC orchestration like Terraform. Cloudify has a post that
&lt;a href="https://cloudify.co/blog/terraform-vs-cloudify/"&gt;compares Terraform with their solution&lt;/a&gt;
but doesn't look into how Terraform is generally used in CI/CD
processes that join Terraform with the other services that create a
decent end-to-end orchestration for cloud deployments. For Kubernetes,
it competes with Helm and the likes - not fully, as TOSCA has other 
benefits of course, but if you compare how quickly Helm is taking
the lead in Kubernetes you can understand the struggle that TOSCA in
my opinion has.&lt;/p&gt;
&lt;p&gt;Another inhibitor is TOSCA's name. If you search for information on
TOSCA, you need to exclude &lt;a href="https://www.tricentis.com/resources/tricentis-tosca-overview/"&gt;Tricentis'&lt;/a&gt;
continuous testing platform, the &lt;a href="https://en.wikipedia.org/wiki/Tosca"&gt;1900's Opera&lt;/a&gt;,
and several other projects, films, and other entities that use the same
name. You'll need to explicitly mention OASIS and/or cloud as well if
you want to find decent articles about TOSCA, knowing well that there
can be pages out there that are missed because of it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I appreciate the value TOSCA brings, I feel that it might not grow
to its fullest potential. I hope to be wrong of course, and I would
like to see big vendors publish their reference architecture TOSCA material
so that large-scale solutions are shown to be manageable using TOSCA and
that solution architects do not need to reinvent the wheel over and
over again, as well as link architecture with the more operations
side of things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To learn more about TOSCA, there are a few resources that I would recommend
here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=tosca"&gt;OASIS TOSCA Technical Committee&lt;/a&gt;
  has a number of resources linked. The &lt;a href="https://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.3/os/TOSCA-Simple-Profile-YAML-v1.3-os.pdf"&gt;TOSCA Simple Profile in YAML Version 1.3&lt;/a&gt;
  PDF is a good read which gradually explains the structure of a TOSCA YAML
  file with plenty of examples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.etsi.org/technologies/nfv"&gt;Network Functions Virtualisation (NFV)&lt;/a&gt;
  is the ETSI site on NFV. Given the focus on NFV that I find around when
  looking at TOSCA (and is even referenced on this page) understanding what
  NFV is about clarifies a bit more how valuable TOSCA is/can be in this
  environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NHYRESmE6uA"&gt;OCB: AMA on TOSCA the Topology and Orchestration Specification for Cloud Applications - Tal Liron&lt;/a&gt;
  is an hour-long briefing that covers TOSCA not only in theory but also applies
  it in practice, and covers some of the new features that are coming up.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="TOSCA"></category><category term="OASIS"></category><category term="topology"></category><category term="orchestration"></category><category term="infrastructure"></category><category term="IaC"></category><category term="NFV"></category></entry><entry><title>Integrating or customizing SaaS within your own cloud environment</title><link href="https://blog.siphos.be/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/" rel="alternate"></link><published>2021-06-23T15:10:00+02:00</published><updated>2021-06-23T15:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-23:/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/</id><summary type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Integrations versus customizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of ways to integrate solutions in a larger ecosystem. Most
integrations focus on data integration (file transfers mostly) and service
integration (using APIs or Enterprise Service Bus solutions), although many
other creative methods are used to facilitate the integration of a SaaS
within the organization’s portfolio.&lt;/p&gt;
&lt;p&gt;This creativity, in my opinion, often transforms into customization of a SaaS
solution rather than an integration approach. SaaS services are being extended
with new, customized functionality, but in a way that we’re no longer thinking
about integrating this customization with the SaaS, but injecting the SaaS
with closely tied services. And SaaS providers are often happy to support
this, as it binds the customer to their solution.&lt;/p&gt;
&lt;p&gt;Now, customizations are not integrations, and integrations are not
customizations. If you customize a SaaS offering, then you still need an
integration between the custom development and the SaaS offering. Sometimes
this integration is as simple as uploading the customization into the SaaS
and the SaaS does the rest. Or the customization is a completely separate
application service, which integrates over managed APIs with the SaaS. Or you
use an intermediate solution that bridges the two solutions.&lt;/p&gt;
&lt;p&gt;While many integrations are possible, I feel that there are a few integration
approaches that are in most cases just wrong. One of these is linking the
SaaS within your own private solution (network or cloud).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don’t just extend a SaaS environment into your own&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I don’t believe that it is wise to just extend a SaaS environment into your
own, even when the SaaS provider enables this. Services like VPC peering,
which can be used to link your VPC with the SaaS provider’s VPC, are easy
ways to do so, but applying it without adjusting the architecture for it
makes your long-term maintainability and security more challenging. How
do you ensure that the SaaS does not abuse this link? How do you ensure
that you don’t accidentally leak information to the SaaS? How do you ensure
high availability and resiliency is retained?&lt;/p&gt;
&lt;p&gt;In traditional architectures, we would have these provider’s locations
considered as a separate network, and introduce the appropriate controls
(like those offered by application firewalls, reverse proxies, etc.) in
between the business application and the third party. This would often be
facilitated by the network operations teams, and governed through more
centralized environments.&lt;/p&gt;
&lt;p&gt;In cloud environments, architectures can be completely different, and
individual applications might pick integration architectures that are more
fruitful for them, without considering the larger environment. If the DevOps
teams that manage the solution architecture are mature, then they too will
consider the various non-functionals that play a role with such integrations.
But if the experience is lacking, setting up direct extensions towards the
SaaS might seem to be a quick and valid solution.&lt;/p&gt;
&lt;p&gt;Some areas that I would focus on when such integrations are requested, are
(in no particular order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impact of the integration to the deployment architecture of the solution,
  considering the availability zones and region concepts used within the
  solution&lt;/li&gt;
&lt;li&gt;Authentication of the integration at various levels (not just
  authentication based on the identity being used)&lt;/li&gt;
&lt;li&gt;Isolation of the integration, ensuring that no other parties are impacted
  by the integration&lt;/li&gt;
&lt;li&gt;Filtering capabilities on network and application level&lt;/li&gt;
&lt;li&gt;Logging and metrics to get visibility on the integration, its usage, the
  volumes sent over, etc.&lt;/li&gt;
&lt;li&gt;Resilience in case of temporary failures (be it through buffering
  mechanisms, or asynchronous integrations)&lt;/li&gt;
&lt;li&gt;Registration of the integration in the enterprise architecture, so that
  assessments, vendor relationship, and other processes are aware of the
  integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When properly tackled, then services like AWS PrivateLink of course do
have a role to play. But it isn’t to just link one solution with another
and be done with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granting SaaS providers limited access to your resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another approach that I see happening is to grant a SaaS provider
administrative access to your own resources. Just like with extending SaaS
environments, I feel that this is not something to apply by default and has
to be carefully assessed. For some SaaS solutions, this is part of their
selling proposal, and is something you know up front. But not all SaaS
solutions are equally obvious in this requirement.&lt;/p&gt;
&lt;p&gt;Some organizations might not have their cloud architecture, account structure
and the like designed to enable SaaS providers to get administrative access
to (some) resources. If the current architectures focus on highly integrated
accounts and solutions, then granting a SaaS administrative access might
jeopardize the security and stability of your overall architecture.&lt;/p&gt;
&lt;p&gt;Furthermore, granting a third party access to your resources also has
implications on accountability. If a SaaS has access to storage within your
account, it could accidentally manipulate data it shouldn’t have access to,
upload another customer’s data on your storage (or vice-versa), or due to a
cyber incident upload toxic data to your account. The provider might also
inadvertently access the data in an economically unfriendly way.&lt;/p&gt;
&lt;p&gt;Using such access patterns should be carefully designed. While you can
often not implement specific IT or security measures on the solution design,
it might be possible to use separate accounts for instance, focusing on
the integration between your ‘core’ solutions and this intermediate one
to ensure a secure and resilient setup, while optimizing cost management
for this intermediate account. You can even consider putting such accounts
under a different tree in the organization structure and apply restrictive
policies such as through AWS’ Service Control Policies (SCP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Creating solutions that link with third parties requires thought and
design. Cloud providers make it a lot easier to change and apply connections
and integrations, but that does not make the architectural work that
precedes it less obvious - on the contrary.&lt;/p&gt;
&lt;p&gt;Customizations with SaaS providers still need to be carefully assessed
and integrated, with attention on the non-functionals such as resilience,
availability, security and the like.&lt;/p&gt;
&lt;p&gt;If the SaaS provider needs access to your own resources, carefully assess
how fine-grained this can be implemented and how the accountability is
assigned. See if an intermediate account can be used where both you and
the SaaS provider have administrative access to, while keeping the rest
of the organization’s data and solutions elsewhere.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="SaaS"></category><category term="integration"></category><category term="customization"></category></entry><entry><title>An IT services overview</title><link href="https://blog.siphos.be/2021/06/an-it-services-overview/" rel="alternate"></link><published>2021-06-14T17:30:00+02:00</published><updated>2021-06-14T17:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-14:/2021/06/an-it-services-overview/</id><summary type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;A single picture doesn’t say it all&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To start off with my overview, I had a need to structure the hundreds of
technology services that I want to keep an eye on in a way that I can 
quickly find it back, as well as present to other stakeholders what 
infrastructure services are about. This structure, while not perfect, 
currently looks like in the figure below. Occasionally, I move one or
more service groups left or right, but the main intention is just to
have a structure available.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overview of the IT services" src="https://blog.siphos.be/images/202106/it_service_overview.png"&gt;&lt;/p&gt;
&lt;p&gt;Figures like these often come about in mapping exercises, or capability models.
A capability model that I recently skimmed through is the
&lt;a href="https://www.if4it.com/SYNTHESIZED/MODELS/ENTERPRISE/enterprise_capability_model.html"&gt;IF4IT Enterprise Capability Model&lt;/a&gt;.
I stumbled upon this model after searching for some reference architectures
on approaching IT services, including a paper titled
&lt;a href="https://www.researchgate.net/publication/238620971_IT_Services_Reference_Catalog"&gt;IT Services Reference Catalog&lt;/a&gt;
by Nelson Gama, Maria do Mar Rosa, and Miguel Mira da Silva.&lt;/p&gt;
&lt;p&gt;Capability models, or service overviews like the one I presented, do not fit
each and every organization well. When comparing the view I maintain with
others (or the different capability and service references out there), I
notice two main distinctions: grouping, and granularity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Certain capabilities might be grouped one way in one reference, and use a
  totally different grouping in another. A database system might be part of
  a “Databases” group in one, a “Data Management” group in another, or even
  “Information Management” in a third. Often, this grouping also reveals the
  granularity that the author wants to pursue.&lt;br&gt;
  Grouping allows for keeping things organized and easier to explain, but has
  no structural importance. Of course, a well-chosen grouping also allows you
  to tie principles and other architectural concerts to the groups themselves,
  and not services in particular. But that still falls under the explainability
  part.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The granularity is more about how specific a grouping is. In the example
  above, “Information Management” is the most coarse-grained grouping, whereas
  “Databases” might be a very specific one. Granularity can convey more insights
  in the importance of services, although it can also be due to historical
  reasons, or because an overview started from one specific service and expanded
  later. In that case, it is very likely that the specific service and its
  dependencies are more elaborately documented.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the figure I maintain, the grouping is often based both on the extensiveness 
of a group (if a group contains far too many services, I might want to see if I
can split up the group) as well as historical and organizational choices. For
instance, if the organization has a clear split between network oriented
teams and server oriented teams, then the overview will most likely convey
the same message, as we want to have the overview interpretable by many
stakeholders - and those stakeholders are often aware of the organizational
distinctions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services versus solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I try to keep track of the evolutions of &lt;em&gt;services&lt;/em&gt; and &lt;em&gt;solutions&lt;/em&gt; within this
overview. Now, the definition of a “service” versus “solution” does warrant
a bit more explanation, as it can have multiple meanings. I even use “service”
for different purposes depending on the context.&lt;/p&gt;
&lt;p&gt;For domain architecture, I consider an “&lt;em&gt;infrastructure service&lt;/em&gt;” as a product
that realizes or supports an IT capability. It is strongly product oriented
(even when it is internally developed, or a cloud service, or an appliance)
and makes a distinction between products that are often very closely related.
For instance, Oracle DB is an infrastructure service, as is the Oracle
Enterprise Manager. The Oracle DB is a product that realizes a “relational
database” capability, whereas OEM is a “central infrastructure management”
capability.&lt;/p&gt;
&lt;p&gt;The reason I create distinct notes for these is because they have different
life cycles, might have different organizational responsible teams, different
setups, etc. Hence, components (parts of products) I generally do not consider
as separate, although there are definitely examples where it makes sense to
consider certain components separate from the products in which they are
provided.&lt;/p&gt;
&lt;p&gt;The several hundred infrastructure services that the company is rich in are
all documented under this overview.&lt;/p&gt;
&lt;p&gt;Alongside these infrastructure services I also maintain a solution overview.
The grouping is exactly the same as the infrastructure services, but the
intention of solutions is more from a full internal offering point of view.&lt;/p&gt;
&lt;p&gt;Within &lt;em&gt;solution architectures&lt;/em&gt;, I tend to focus on the choices that the
company made and the design that follows it. Many solutions are considered
‘platforms’ on top of which internal development, third party hosting or
other capabilities are provided. Within the solution, I describe how the
various infrastructure services interact and work together to make the
solution reality.&lt;/p&gt;
&lt;p&gt;A good example here is the mainframe platform. Whereas the mainframe itself
is definitely an infrastructure service, how we internally organize the
workload and the runtimes (such as the LPARs), how it integrates with the
security services, database services, enterprise job scheduling, etc. is
documented in the solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Not all my domain though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not all services and solutions that I track are part of ‘my’ domain though.
For instance, at my company, we make a distinction between the
infrastructure-that-is-mostly-hosting, and
infrastructure-that-is-mostly-workplace. My focus is on the ‘mostly hosting’
orientation, whereas I have a colleague domain architect responsible for
the ‘mostly workplace’ side of things.&lt;/p&gt;
&lt;p&gt;But that’s just about accountability. Not knowing how the other solutions
and services function, how they are set up, etc. would make my job harder,
so tracking their progress and architecture is effort that pays off.&lt;/p&gt;
&lt;p&gt;In a later post I’ll explain what I document about services and solutions
and how I do it when I have some time to spend.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="overview"></category><category term="service"></category><category term="landscape"></category><category term="catalog"></category><category term="capability"></category></entry><entry><title>The three additional layers in the OSI model</title><link href="https://blog.siphos.be/2021/06/the-three-additional-layers-in-the-OSI-model/" rel="alternate"></link><published>2021-06-09T11:10:00+02:00</published><updated>2021-06-09T11:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-09:/2021/06/the-three-additional-layers-in-the-OSI-model/</id><summary type="html">&lt;p&gt;At my workplace, I jokingly refer to the three extra layers on top of the
OSI network model as a way to describe the difficulties of discussions or
cases. These three additional layers are Financial Layer, Politics Layer
and Religion Layer, and the idea is that the higher up you go, the more
challenging discussions will be.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At my workplace, I jokingly refer to the three extra layers on top of the
OSI network model as a way to describe the difficulties of discussions or
cases. These three additional layers are Financial Layer, Politics Layer
and Religion Layer, and the idea is that the higher up you go, the more
challenging discussions will be.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Recap on the OSI model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open Systems Interconnection (OSI) model is a conceptual model for
network-oriented communications, which has 7 layers - each with their own
specific peculiarities and service offerings.&lt;/p&gt;
&lt;p&gt;The seven layers are, from top to bottom:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Application Layer which provide the high-level interfaces to the
   application&lt;/li&gt;
&lt;li&gt;The Presentation Layer which maps data as seen / used by an application
   to the network &lt;/li&gt;
&lt;li&gt;The Session Layer which enables continuous / conversational communication
   between nodes&lt;/li&gt;
&lt;li&gt;The Transport Layer which looks at reliable transmissions between systems&lt;/li&gt;
&lt;li&gt;The Network Layer which handles functions like addressing and routing&lt;/li&gt;
&lt;li&gt;The Data Link Layer which looks at the interchange of frames (collection of
   bits and the interpretation of it) between two systems that are connected by
   a physical medium&lt;/li&gt;
&lt;li&gt;The Physical Layer which looks at how bits are sent over the physical medium&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Many network protocols can be mapped to one or more of these layers. The WiFi
standards focus on the physical and data link layers, while the IP standard
is network layer oriented. The TCP standard is strongly session and transport
layers oriented. The HTTP standard has a strong focus on the application and
presentation layer.&lt;/p&gt;
&lt;p&gt;While the OSI model isn’t 100% applied in standards and protocols, it is
conceptually a very common way of looking at communications and network
stacks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OSI layers as complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I jokingly refer to the OSI model for complexity and difficulties of
discussions, it is through the assumption that the challenges rise the higher
up in the stack you go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The increasing complexity of discussions" src="https://blog.siphos.be/images/202106/OSI_extended.png"&gt;&lt;/p&gt;
&lt;p&gt;Discussions related to the physical aspects are often the easiest: they are about
tangible objects, people understand or at least properly observe the physical
aspects. There is little misinterpretation of the information presented to the
discussion.&lt;/p&gt;
&lt;p&gt;Then, we start rising up the stack. Discussions that are close to the physical
aspects are still easy to moderate, but the higher up we go the more chance we
have of misinterpretation of the information at hand. People make more
assumptions to understand the discussion, which might lead to misunderstandings.&lt;/p&gt;
&lt;p&gt;Higher layers are also because we want to abstract away the complexity of
reality. Discussions are held on topics that have wider consequences, and thus we
abstract this complexity in models so that the discussions can move forward
quickly enough. If not all people understand the abstraction (and its
consequences), the discussions might quickly move to details and specifics that
do not provide much value to the discussion, but are considered as necessary by
some of the attendees.&lt;/p&gt;
&lt;p&gt;Even worse, the higher up we go, the more flexibility organizations expect. On
the lowest level we often have few and highly standardized choices, whereas on
higher levels we have different standards due to business units with other
requirements. The complexity rises tremendously, and we are often challenged by
stakeholders with different agendas.&lt;/p&gt;
&lt;p&gt;The OSI model’s highest layer is the application layer, which is the layer that
covers discussions on (business) applications, which is often the level of
discussions systems architects often have to deal with. A business unit wants
solution X, but that solution isn’t compatible with the standards and enablers
that the organization already has in place. Another business unit also wants
solution X, but in a different way. And while we have solution Y in place as
well that covers 80% of the functionality… Well, you know where this is going.&lt;/p&gt;
&lt;p&gt;Well, there are situations even worse, which make discussions and the attempts
to find a consensus even harder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Financial Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first one (layer 8 so to speak) is the Financial Layer. Here the
discussions are on the economic side of things, and if you think those are
the easier discussions out there (because financials are mathematically
sound, right?) I urge you to dive deeper into this subject, because
there is plenty of work out there to simplify these discussions.&lt;/p&gt;
&lt;p&gt;The only commonality I see with financial or economical discussions
is that they use a currency as a unit together with a time indication.
But that’s about it. Even the value of (or interpretation on) the
currency isn’t set in stone. &lt;/p&gt;
&lt;p&gt;Companies for instance often use internal charging for service usages.
This charging isn’t a cost that the company spends, it is mostly just
to attribute things to one business unit or the other. You might say it
is monopoly (the game) money. But that’s not always the case, as internal
charging in larger companies might also effectively reflect themselves in
the books / ledgers.&lt;/p&gt;
&lt;p&gt;And when you are charged - regardless if it is monopoly money or not - you
tend to really try to get the best deal… even though we are talking about
chargeback and thus not actual “profits and loss” of the company. If you
force your charging to be lower, you’re forcing other’s charging to get
higher. And they don’t like that, but rather than discussing this amongst
themselves, the discussions are often pointed towards the owner of the
service and its chargeback method.&lt;/p&gt;
&lt;p&gt;Or when project leaders make a business case (to show if a project is
beneficial from the financial side for the company) using chargeback
information. Chargeback doesn’t always (and in my opinion, mostly never)
reflect the true costs of a service, so using chargeback-currency for a
profit &amp;amp; loss currency is a big no-no. Yet this is oh so common still.&lt;/p&gt;
&lt;p&gt;Even when we have the right mindset and focus on actual costs… Well,
they are hard to obtain, because actual costs are complex beasts. You’ll
find plenty of resources online about the Total Cost of Ownership (TCO),
but none of them are truly the right resource to never look at others.
TCOs are hard, you need to consider things you don’t even knew you have
to consider.&lt;/p&gt;
&lt;p&gt;Or when you want to know just how something incurs on license costs.
Well, good luck in understanding how the vendor measures it (looking
at you, Oracle and Microsoft), or what deals your company has made with
the vendors under the umbrella of “enterprise agreements” which make all
the online resources you find useless. I had the role of license manager
for all our Oracle products for a few years, and was involved with the
Microsoft license management within my company, and it took me a while
to streamline it and communicate it properly within the organization
so all stakeholders were sufficiently aware of what it meant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Politics Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But financial discussions are peanuts compared with discussions that
reflect internal politics. I’m explicitly not calling this layer
strategy, although most of the discussions related to a companies’
strategy (or the strategy of the business units) are situated here
as well. It is also not governmental politics here, but the internal
domains where internal politics are reflected.&lt;/p&gt;
&lt;p&gt;These are the discussions where you, as an architect or engineer,
come in with a presentation that covers all the ends, handles the
financial side correctly and with agreements from other architects
or engineers that the figures are sound, the solution alternatives
correctly evaluated, and the preferred solution a good balance of
all the requirements… only to find that the temperature in the room
says otherwise.&lt;/p&gt;
&lt;p&gt;Discussions on the politics layer cover everything except what is
truly at the heart of what the discussion is about. They are about
hidden agendas that you will not be presented with at the meeting.
Many senior profiles (or, perhaps better, expert profiles) in an
organization tend to know what these hidden agendas are about, and
know how to ‘play’ and use these politics further. While their
seniority of course also focuses on the efficiency side (for instance,
a senior project leader can tackle larger projects, structure projects
better, are fluent in reporting, etc.) it is truly their organizational
experience (and sometimes ‘reading people’ skills) that makes them
quickly grow in their expertise.&lt;/p&gt;
&lt;p&gt;The challenge is to obtain the hidden agendas and work on disentangling
the complexity of it. Hidden agendas are often based on wrong assumptions,
past emotional stress, or missing information. While you often cannot
try to attack these agendas head-first, understanding their nature can
often help in presenting cases in a way that facilitates the discussions.
You can disarm the emotional stress, or start with tackling the assumptions
that might float around.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Religion Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sadly, even seasoned expert profiles however come across discussions that
are not just about internal politics or people’s hidden agendas. Sometimes
discussions just take on a religious nature. This is when people are adamant
that something is true, regardless of the facts or other material that you
bring in.&lt;/p&gt;
&lt;p&gt;Religious discussion can be found all over the place, not just on management
level. They can be about tooling, operating systems, designs, hosting choices,
etc. You all know there are people who can discuss Linux versus Windows for
ages and ages. Discussions on cloud versus on premise also often are religious
in nature. And the less time the organization allows for finding facts and
figures, the more these kinds of discussions pop up, and nobody wins on these
in the long term.&lt;/p&gt;
&lt;p&gt;Knowing when a discussion is internal politics (and thus can still be tackled)
versus religion is hard. But once I know a discussion is no longer going to be
settled with facts, figures, and emotional/psychological approaches, then I
will be likely to try and evade those meetings. I’ll try to facilitate a
management decision (if it isn’t religious on management level either) or
just deal with the onslaught.&lt;/p&gt;
&lt;p&gt;Challenges that are to be settled on the religion layer are no longer about
finding the optimal solution, but finding a solution or decision that is
“not in the wrong direction”. And while this is a much lower bar, it is hard
enough to reach it.&lt;/p&gt;</content><category term="Misc"></category><category term="OSI"></category><category term="meeting"></category><category term="humor"></category></entry><entry><title>Virtualization vs abstraction</title><link href="https://blog.siphos.be/2021/06/virtualization-vs-abstraction/" rel="alternate"></link><published>2021-06-03T10:10:00+02:00</published><updated>2021-06-03T10:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-03:/2021/06/virtualization-vs-abstraction/</id><summary type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Abstraction: common and simplified interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The term "abstraction" has slightly different connotations based on the
context in which the term is used. Generally speaking, an abstraction
provides a less detailed view on an object and shows the intrinsic qualities
upon which one looks at that object. Let's say we have a PostgreSQL database
and a MariaDB database. An abstract view on it could find that it has a lot
of commonalities, such as tabular representation of data, a network-facing
interface through which their database clients can interact with the
database, etc.&lt;/p&gt;
&lt;p&gt;We then further generalize this abstraction to come to the generalized
"relational database management system" concept. Furthermore, rather than
focusing on the database-specific languages of the PostgreSQL database and
the MariaDB database (i.e. the commands that database clients send to the
database), we abstract the details that are not shared across the two, and
create a more common set of commands that both databases support.&lt;/p&gt;
&lt;p&gt;Once you standardize on this common set of commands, you get more freedom in
exchanging one database technology for the other. This is exactly what
happened several dozen years ago, and resulted in the SQL standard
(ISO/IEC 9075). This standard is a language that, if all your relational
database technologies support it, allows you - as an organization - to work
with a multitude of database technologies while still having a more efficient
and standardized way of dealing with it.&lt;/p&gt;
&lt;p&gt;Now, the SQL language standard is one example. IT is filled with many other
examples, some more formally defined as standards than others. Let's look at
a more recent example, within the area of application containerization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRI and OCI are abstraction implementations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the Docker project, now supported through the Docker company, started
with enabling application containerization in a more efficient way, it leaned
upon the capabilities that the Linux kernel offered on hiding information and
isolating resources (namespaces and control groups) and expanded on it to
make it user friendly. It was an immediate hit, and has since then resulted
in a very competitive market.&lt;/p&gt;
&lt;p&gt;With Docker, applications could be put in more isolated environments and run
in parallel on the same system, without these applications seeing the other
ones. Each application has its own, private view on the system. With these
containers, the most important service that is still shared is the kernel,
with the kernel offering only those services to the containers that it can
keep isolated.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Container runtime abstraction" src="https://blog.siphos.be/images/202106/container-runtimes.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/"&gt;https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, while Docker can be easily attributed to bringing this to the wider
public, other initiatives quickly followed suit. Multiple container
technologies were coming to life, and started to bid for a place in the
containerization market. To be able to compete here, many of these attempted
to use the same interfaces (be it system calls, commands or other) as Docker
used, so the users can more easily switch. But while trying to copy and
implement the same interfaces is a possible venue, it is still strongly
controlled by the evolution that Docker is taking.&lt;/p&gt;
&lt;p&gt;Since then, larger projects like Kubernetes have started introducing an
abstraction between the container runtime (which implements the actual
containerization) and the container definitions and management (which uses
the containerization). Within Kubernetes for instance, this is through the
Common Runtime Interface (CRI), and the Open Container Interface (OCI) is
used to link the container runtime management with the underlying container
technologies.&lt;/p&gt;
&lt;p&gt;Introducing such an abstraction is a common way to establish a bit more
foothold in the market. Rather than trying to copy the market leader
verbatim, you create an intermediate layer, with immediate implementation
for the market leader as well, but with the promise that anyone that uses
the intermediate layer will be less tied to a single vendor or project: it
abstracts that vendor or project specifics away and shows mainly the
intrinsic qualities needed.&lt;/p&gt;
&lt;p&gt;If that abstraction is successful, other implementations for this abstraction
layer can easily come in and replace the previous technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstraction is not virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The introduction of abstraction layers, abstract technologies or abstract
languages should not be misunderstood for virtualization. Abstraction does
not hide or differently represent the resources beneath. It does not represent
itself as something else, but merely leaves out details that might make
interactions with the different technologies more complex.&lt;/p&gt;
&lt;p&gt;Virtualization on the other hand takes a different view. Rather than removing
the specific details, it represents a resource as something that it isn't
(or isn't completely). Hypervisors like KVM create a virtual hardware view,
and translates whatever calls towards the virtual hardware into calls to the
actual hardware - sometimes to the same type of hardware, but often towards
the CPU or resources that simulate the virtualized hardware further.&lt;/p&gt;
&lt;p&gt;Abstraction is a bit like classification, and defining how to work with a
resource through the agreed upon interfaces for that class. If you plug in
a USB device like a USB stick or USB keyboard or mouse, operating systems
will be able to interact with it regardless of its vendor and product,
because it uses the abstraction offered by the device classes: the USB mass
storage device class for the USB stick, or the USB human interface device
class for the keyboard and mouse. It abstracts away the complexity of
dealing with multiple implementations, but the devices themselves still
need to be classified as such.&lt;/p&gt;
&lt;p&gt;On hypervisors, you can create a virtual USB stick which in reality is just
a file on the hypervisor host or on a network file share. The hypervisor
virtualizes the view towards this file as if it is a USB stick, but in reality
there is no USB involved at all. Again, this doesn't have to be the case,
the hypervisor might as well enable virtualization of the USB device and
still eventually interact with an actual USB device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLANs are virtualized networks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another example of a virtualization is network virtualization through the
use of VLANs. In a virtual local area network (VLAN), all systems that
interact with this VLAN will see each other on the network as if they are
part of the same broadcast domain. Well, they are part of the same broadcast
domain. But if you look at the physical network implementation, this does not
imply that all these systems are attached to the same switch, and that no
routers are put in between to facilitate the communication.&lt;/p&gt;
&lt;p&gt;In larger enterprises, the use of VLANs is ubiquitous. Network virtualization
enables the telco teams and departments to optimize the actual physical
network without continuously impacting the configurations and deployments of
the services that use the network. Teams can create hundreds or thousands of
such VLANs while keeping the actual hardware investments under control, and
even be able to change and manage the network without impacting services.&lt;/p&gt;
&lt;p&gt;This benefit is strongly tied to virtualization, as we see the same in
hardware virtualization for server and workstation resources. By offering
virtualized systems, the underlying hardware can be changed, replaced or
switched without impact on the actual software that is running within the
virtualized environment. Well, mostly without impact, because not all
virtualization technologies or implementations are creating a full
virtualized view - sometimes shortcuts are created to improve efficiency
and performance. But in general, it works Just Fine (tm).&lt;/p&gt;
&lt;p&gt;Resource optimization and consolidation is easily accomplished when using
virtualization. You need far fewer switches in a virtualized network, and
you need far fewer servers for a virtualized server farm. But, it does come
at a cost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Virtualization introduces different complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you introduce a virtualization layer, be it for network, storage,
hardware or application runtimes, you introduce a layer that needs to be
actively managed. Abstraction is often much less resource intensive, as it
is a way to simplify the view on the actual resources while still being 100%
aligned with those underlying resources. Virtualization means that you need
to manage the virtualized resources, and keep track of how these resources
map to the actual underlying resources.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vSphere services" src="https://blog.siphos.be/images/202106/vsphere.png"&gt;
&lt;em&gt;Source: &lt;a href="https://virtualgyaan.com/vmkernel-components-and-functionality/"&gt;https://virtualgyaan.com/vmkernel-components-and-functionality/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's look at virtualized hardware for servers. On top of it, you have to
run and maintain the hypervisor, which represents the virtual hardware to
the operating systems. Within those (virtually running) operating systems,
you have a virtual view on resources: CPU, memory, etc. The sum of all
(virtual) CPUs is often not the same as the sum of all (actual) CPUs
(depending on configuration of course), and in larger environments the
virtual operating systems might not even be running on the same hardware
as they did a few hours ago, even though the system has not been restarted.&lt;/p&gt;
&lt;p&gt;Doing performance analysis implies looking at the resources within (virtual)
as well as mapped on the actual resources, which might not be of the same
type. A virtual GPU representation might be mapped to an actual GPU (and if
you want performance, I hope it is) but doesn't have to be. I've done
investigations on a virtual Trusted Platform Module (TPM) within a virtual
system running on a server that didn't have a TPM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assessing which standardization to approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I'm confronted with an increase in IT complexity, I will often be
looking at a certain degree of standardization to facilitate this in the
organization. But what type of standardization to approach depends strongly
on the situation.&lt;/p&gt;
&lt;p&gt;Standardization by rationalization is often triggered by cost optimization
or knowledge optimization. An organization that has ten different relational
database technologies in use could benefit of a rationalization in the number
of technologies to support. However, unless there is also sufficient
abstraction put in place, this rationalization can be intensive. Another
rationalization example could be on public cloud, where an organization
chooses to only focus on a single or two cloud providers but not more.&lt;/p&gt;
&lt;p&gt;While rationalization is easy to understand and explain, it does have adverse
consequences: you miss the benefits of whatever you're rationalized away,
and unless another type of standardization is put in place, it will be hard
to switch later on if the rationalization was ill-advised or picked the
wrong targets to rationalize towards.&lt;/p&gt;
&lt;p&gt;Standardization by abstraction focuses more on simplification. You are
introducing technologies that might have better interoperability through
this abstraction, but this can only be successful if the abstraction is
comprehensive enough to still use the underlying resources in an optimal
manner.&lt;/p&gt;
&lt;p&gt;My own observation on abstraction is that it is not commonly accepted by
engineers and developers at face value. It requires much more communication
and explanation than rationalization, which is often easy to put under "cost
pressure". Abstraction focuses on efficiency in a different way, and thus
requires different communication. At the company I currently work for,
we've introduced the Open Service Broker (OSB) API as an abstraction for
service instantiation and service catalogs, and after even more than a
year, including management support, it is still a common endeavor to
explain and motivate why we chose this.&lt;/p&gt;
&lt;p&gt;Virtualization creates a highly efficient environment and supports resource
optimizations that aren't possible in other ways. Its benefits are much easier
to explain to the organization (and to management), but has a downside that
is often neglected: it introduces complexity. Hence, virtualization should
only be pursued if you can manage the complexity, and that it isn't much
worse than the complexity you want to remove. Virtualization requires
organizational support which is more platform-oriented (and thus might be
further away from the immediate 'business value' IT often has to explain),
in effect creating a new type of technology within the ever increasing
catalog of IT services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software-defined infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While virtualization has been going around in IT for quite some time (before
I was born), a new kid on the block is becoming very popular: software-defined
infrastructure. The likes of Software Defined Network (SDN), Compute (SDC) and
Storage (SDS) are already common or becoming common. Other implementations,
like the Software Defined Perimeter, are getting jabbed by vendors as well.&lt;/p&gt;
&lt;p&gt;Now, SDI is not its own type of standardization. It is a way of managing
resources through code, and thus is a way of abstracting the infrastructure.
But unlike using a technology-agnostic abstraction, it pulls you into a
vendor-defined abstraction. That has its pros and cons, and as an architect
it is important to consider how to approach infrastructure-as-code, as SDI
implementations are not the only way to accomplish this.&lt;/p&gt;
&lt;p&gt;Furthermore, SDI does not imply virtualization. Certainly, if a technology
is virtualized, then SDI will also easily interact with it, and help you
define and manage the virtualized infrastructure as well as its underlay
infrastructure. But virtualization isn't a prerequisite for SDI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you're confronted with chaos and complexity, don't immediately start
removing technologies under the purview of "rationalization". Consider your
options on abstraction and virtualization, but be aware of the pros and cons
of each.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="virtualization"></category><category term="abstraction"></category></entry><entry><title>SELinux System Administration 3rd Edition</title><link href="https://blog.siphos.be/2021/01/selinux-system-administration-3rd-edition/" rel="alternate"></link><published>2021-01-06T20:00:00+01:00</published><updated>2021-01-06T20:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-01-06:/2021/01/selinux-system-administration-3rd-edition/</id><summary type="html">&lt;p&gt;As I mentioned previously, recently my latest installment of "SELinux System
Administration" has been released by Packt Publishing. This is already the
third edition of the book, after the first (2013) and second (2016) editions
have gotten reasonable success given the technical and often hard nature of
full SELinux administration.&lt;/p&gt;
&lt;p&gt;Like with the previous editions, this book remains true to the public of
system administrators, rather than SELinux policy developers. Of course,
SELinux policy development is not ignored in the book.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As I mentioned previously, recently my latest installment of "SELinux System
Administration" has been released by Packt Publishing. This is already the
third edition of the book, after the first (2013) and second (2016) editions
have gotten reasonable success given the technical and often hard nature of
full SELinux administration.&lt;/p&gt;
&lt;p&gt;Like with the previous editions, this book remains true to the public of
system administrators, rather than SELinux policy developers. Of course,
SELinux policy development is not ignored in the book.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What has changed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First and foremost, it of course updates the content of the previous edition
to be up to date with the latest evolutions within SELinux. There are no earth
shattering changes, so the second edition isn't suddenly deprecated. The examples
are brought up to date with a recent distribution setup, for which I used
Gentoo Linux and CentOS.&lt;/p&gt;
&lt;p&gt;The latter is, given the recent announcement of CentOS stopping support for
CentOS version 8 in general, a bit confrontational, although it doesn't
really matter that much for the scope of the book. I hope that &lt;a href="https://rockylinux.org/"&gt;Rocky
Linux&lt;/a&gt; will get the focus and support it deserves.&lt;/p&gt;
&lt;p&gt;Anyway, I digress. A significant part of the updates on the existing content
is on SELinux-enabled applications, applications that act as a so-called object
manager themselves. While quite a few were already covered in the past, these
applications continue to enhance their SELinux support, and in the third edition
a few of these receive a full dedicated chapter.&lt;/p&gt;
&lt;p&gt;There are also a small set of SELinux behavioral changes, like SELinux' NNP
support, as well as SELinux userspace changes like specific extended attributes
for restorecon.&lt;/p&gt;
&lt;p&gt;Most of the book though isn't about changes, but about new content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What has been added&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As administrators face SELinux-aware applications more and more, the book
goes into much more detail on how to tune SELinux with those SELinux-aware
applications. If we look at the book's structure, you'll find that it has
roughly three parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using SELinux, which covers the fundamentals of using SELinux and
   understanding what SELinux is.&lt;/li&gt;
&lt;li&gt;SELinux-aware platforms, which dives into the SELinux-aware application
   suites that administrators might come in contact with&lt;/li&gt;
&lt;li&gt;Policy management, which focuses on managing, analyzing and even
   developing SELinux policies.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By including additional content on SEPostgreSQL, libvirt, container
platforms like Kubernetes, and even Xen Security Modules (which is not
SELinux itself, but strongly influenced and aligned to it to the level
that it even uses the SELinux userspace utilities) the book is showing how
wide SELinux is being used.&lt;/p&gt;
&lt;p&gt;Even on policy development, the book now includes more support than before.
While another book of mine, SELinux Cookbook, is more applicable to policy
development, I did not want to keep administrators out of the loop on how
to develop SELinux policies at all. Especially not since there are more
tools available nowadays that support policy creation, like udica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux CIL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the changes I also introduced in the book is to include SELinux
Common Intermediate Language (CIL) information and support. When we need
to add in a small SELinux policy change, the book will suggest CIL based
changes as well.&lt;/p&gt;
&lt;p&gt;SELinux CIL is not commonly used in large-scale policy development. Or at
least, not directly. The most significant policy development out there,
the &lt;a href="https://github.com/SELinuxProject/refpolicy/wiki"&gt;SELinux Reference Policy&lt;/a&gt;,
does not use CIL directly itself, and the level of support you find for
the current development approach is very much the default way of working. So
I do not ignore this more traditional approach.&lt;/p&gt;
&lt;p&gt;The reason I did include more CIL focus is because CIL has a few advantages
up its sleeve that is harder to get with the traditional language. Nothing
major perhaps, but enough that I feel it should be more actively promoted
anyway. And this book is hopefully a nice start to it.&lt;/p&gt;
&lt;p&gt;I hope the book is a good read for administrators or even architects that
would like to know more about the technology.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="packt"></category><category term="book"></category></entry><entry><title>Abstracting infrastructure complexity</title><link href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/" rel="alternate"></link><published>2020-12-25T23:00:00+01:00</published><updated>2020-12-25T23:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-12-25:/2020/12/abstracting-infrastructure-complexity/</id><summary type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Complexity is a challenging beast&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If someone were to attempt drawing out how the IT infrastructure of a larger
IT environment looks like in reality, it would soon become very, very large and
challenging to explain. Perhaps not chaotic, but definitely complicated.&lt;/p&gt;
&lt;p&gt;One of the challenges is the amount of "something" that is out there. That can
be the amount of devices you have, the amount of servers in the network, the
amount of flows going through firewalls or gateways, the amount of processes
running on a server, the amount of workstations and end user devices in use,
the amount of containers running in the container platform, the amount of cloud
platform instances that are active... &lt;/p&gt;
&lt;p&gt;The "something" can even be less tangible than the previous examples such as
the amount of projects that are being worked on in parallel or the amount of
changes that are being prepared. However, that complexity is not one I'll deal
with in this post.&lt;/p&gt;
&lt;p&gt;Another challenge is the virtualized nature of IT infrastructure, which has
a huge benefit for the organization and simplifies infrastructure services
for its own consumers, but does make it more, well, complicated to deal with.&lt;/p&gt;
&lt;p&gt;Virtual networks (vlans), virtual systems (hypervisors), virtual firewalls,
virtual applications (with support for streaming desktop applications to the
end user device without having the applications installed on that device),
virtual storage environments, etc. are all wonderful technologies which allow
for much more optimized resource usage, but does introduce a higher complexity
of the infastructure at large.&lt;/p&gt;
&lt;p&gt;To make sense of such larger structures, we start making abstractions of what
we see, structuring it in a way that we can more easily explain, assess or analyze
the environment and support changes properly. These abstract views do reflect
reality, but only to a certain extend. Not every question that can be asked can
be answered satisfactory with the same abstract view, but when it can, it is very
effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstracting service complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my day-to-day job, I'm responsible for the infrastructure of a reasonably
large environment. With "responsible" I don't want to imply that I'm the one
and only party involved of course - responsibilities are across a range of
people and roles. I am accountable for the long-term strategy on
infrastructure and the high-level infrastructure architecture and its offerings,
but how that plays out is a collaborative aspect.&lt;/p&gt;
&lt;p&gt;Because of this role, I do want to keep a close eye on all the services that
we offer from infrastructure side of things. And hence, I am often confronted
with the complexity mentioned earlier. To resolve this, I try to look at all
infastructure services in an abstract way, and document it in the same way so
that services are more easily explained.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An Archimate based view on the abstractions listed" src="https://blog.siphos.be/images/202012/abstracting-infrastructure-complexity-kvm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 - A possible visualization of the abstraction model, here in Archimate&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The abstraction I apply is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with &lt;em&gt;components&lt;/em&gt;, building blocks that are used and which refer
  to a single product or technology out there. A specific Java product can
  be considered such a component, because by itself it hardly has any value.&lt;/li&gt;
&lt;li&gt;Components are put together to create a &lt;em&gt;solution&lt;/em&gt;. This is something that
  is intended to provide value to the organization at large, and is the level
  at which something is documented, has an organizational entity responsible
  for it, etc. Solutions are not yet instantiated though. An example of a
  solution could be a Kafka-based pub/sub solution, or an OpenLDAP-based
  directory solution.&lt;/li&gt;
&lt;li&gt;Solutions are used to create &lt;em&gt;services&lt;/em&gt;. A service is something that has
  an SLA attached to it. In most cases, the same solution is used to create
  multiple services. We can think of the Kafka-based pub/sub solution that
  has three services in the organization: a regular non-production one,
  a regular production one, and a highly available production service.&lt;/li&gt;
&lt;li&gt;Services are supported through one or more &lt;em&gt;clusters&lt;/em&gt;. These are a
  way for teams to organize resources in support of a service. Some services
  might be supported by multiple clusters, for instance spread across
  different data centers. An OpenLDAP-based service might be supported by
  a single OpenLDAP cluster with native synchronization support spread across
  two data centers, or by two OpenLDAP clusters with a different
  synchronization mechanism between the two clusters.&lt;/li&gt;
&lt;li&gt;Clusters exist out of one or more &lt;em&gt;instances&lt;/em&gt;. These are the actual deployed
  technology processes that enable the cluster. In an OpenLDAP cluster, you
  could have two master processes (&lt;code&gt;slapd&lt;/code&gt; processes) running, which are the
  instances within the cluster.&lt;/li&gt;
&lt;li&gt;On top of the clusters, we enable &lt;em&gt;containers&lt;/em&gt; (I call those containers, but
  they don't have anything to do with container technology like Docker containers).
  The containers are what the consumers are actually interested in. That could
  be an organization unit in an LDAP structure, a database within an RDBMS, 
  a set of topics within a Kafka cluster, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are the basic abstractions I apply for most of the technologies, allowing
me to easily make a good view on the environment. Let's look at a few examples
here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: Virtualization of Wintel systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a large, virtualized environment, you generally have a specific hypervisor
software being used: be it RHV (Red Hat Virtualization) based upon
KVM, Microsoft HyperV, VMWare vSphere or something else - the technology used
is generally well known. That's one of the components being used, but that is
far from the only component.&lt;/p&gt;
&lt;p&gt;To better manage the virtualized environment the administration teams might
use an orchestration engine like Ansible, Puppet or Saltstack. They might also
have a component in use for automatically managing certificates and what not.&lt;/p&gt;
&lt;p&gt;All these components are needed to build a full virtualization solution. For
me, as an architect, knowing which components are used is useful for things
like lifecycle management (which components are EOL, which components can be
easily replaced with a different one versus components that are more lock-in
oriented, etc.) or inventory management (which component is deployed where,
which version is used), which supports things like vulnerability management
(if we can map components to their Common Platform Enumeration (CPE) then
we can easily see which vulnerabilities are reported through the Common
Vulnerabilities and Exposure (CVE) reports).&lt;/p&gt;
&lt;p&gt;The interaction between all these components creates a sensible solution,
which is the virtualization solution. At this level, I'm mostly interested
in the solution roadmap, the responsibilities and documentation associated
with it, the costs, maturity of the offering within the organization, etc.
It is also on the solution level that most architectural designs are made,
and the best practices (and malpractices) are documented.&lt;/p&gt;
&lt;p&gt;The virtualization solution itself is then instantiated within the
organization to create one or more services. These could be different
services based on the environment (a lab/sandbox virtualization service
with low to no SLA, a non-production one with standard SLA, a non-production
one with specific disaster recovery requirements, a production one with
standard SLA (and standard disaster recovery requirements), a high-performance
production one, etc.&lt;/p&gt;
&lt;p&gt;These services are mostly important for other architects, project leads
or other stakeholders that are going to make active use of the virtualization
services - the different services (which one could document as "service
plans") make it more obvious on what the offering is, and what differentiation
is supported.&lt;/p&gt;
&lt;p&gt;Let's consider a production, standard SLA virtualization service. The
system administrators of the virtualization environment might enable this
service across multiple clusters. This could be for several reasons: this
could be due to limits (maximum number of hosts per cluster), or because
of particular resource requirements (different CPU architecture requirements
- yes even with virtualization this is still a thing), or to make
things manageable for the administrators in general.&lt;/p&gt;
&lt;p&gt;While knowing which cluster an application is on is, in general, not
that important, it can be very important when there are problems, or when
limits are being reached. As an architect, I'm definitely interested in
knowing why multiple clusters are made (what is the reasoning behind it) as
it gives a good view on what the administrators are generally dealing with.&lt;/p&gt;
&lt;p&gt;Within a cluster (to support the virtualization) you'll find multiple hosts.
Often, a cluster is sized to be able to deal with one or two host fall-outs
so that the virtual machines (which are hosted on the cluster - these are
the "containers" that I spoke of) can be migrated to another host with only
a short downtime as a consequence (if their main host crashed) or no downtime
at all (if it is scheduled maintenance of the host). These hosts are the
instances of the cluster.&lt;/p&gt;
&lt;p&gt;By using this abstraction, I can "map" the virtualization environment in
a way that I have a good enough view, without proclaiming to be anything
more than an informed architect, on this setup to support my own work,
and to be able to advice management on major investment requirements,
challenges, strategic evolutions and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More than just documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the above method is used for documenting the environment in which
I work (and which works well for the size of the environment I have to deal
with), it can be used for simplifying management of the technologies as
well. This level of abstraction can easily be used in environments that
push self-servicing forward.&lt;/p&gt;
&lt;p&gt;Let's take the &lt;a href="https://www.openservicebrokerapi.org/"&gt;Open Service Broker API&lt;/a&gt;
as an example. This is an API that defines how to expose (infrastructure)
services to consumers that can easily create (provision) and destroy 
(deprovision) their own services. Brokers that support the API will
then automatically handle the service management. This model can easily
be put in to support the previous abstraction.&lt;/p&gt;
&lt;p&gt;Take the virtualization environment again. If we want to enable self-servicing
on a virtualized environment, we can think of an offering where internal customers
can create new virtual machines (provision) either based on a company-vetted
template, or through an image (like with virtual appliances). The team that
manages the virtualization environment has a number of services, which they
describe in the service plans exposed by the API. An internal customer, when
privisioning a virtual machine, is thus creating a "container" for the right
service (based on their selected service plan) and on the right cluster
(based upon the parameters that the internal customer passes along with the
creation of its machine).&lt;/p&gt;
&lt;p&gt;We can do the same with databases: a certain database solution (say PostgreSQL)
has its own offerings (exposed through service plans linked to the service), and
internal customers can create their own database ("container") on the right
cluster through this API.&lt;/p&gt;
&lt;p&gt;I personally have a few scripts that I use at home myself to quickly set
up a certain technology, using the above abstraction level as the foundation.
Rather than having to try and remember how to set up a multi-master OpenLDAP
service, or a replicated Kafka setup, I have scripts that create this based
upon this abstraction: the script parameters always use the service, cluster,
instance and container terminology and underlyingly map this to the
technology-specific approach.&lt;/p&gt;
&lt;p&gt;It is my intention to also promote this abstraction usage within my
work environment, as I believe it allows us to more easily explain what
all the infrastructure is used for, but also to more easily get new employees
known to our environment. But even if that isn't reached, the abstraction is
a huge help for me to assess and understand the multitude of technologies
that are out there, be it our mainframe setup, the SAN offerings, the
network switching setup, the databases, messaging services, cloud
landing zones, firewall setups, container platforms and more.&lt;/p&gt;</content><category term="Architecture"></category><category term="infrastructure"></category><category term="archimate"></category></entry><entry><title>Working on infra strategy</title><link href="https://blog.siphos.be/2020/10/working-on-infra-strategy/" rel="alternate"></link><published>2020-10-04T13:20:00+02:00</published><updated>2020-10-04T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-10-04:/2020/10/working-on-infra-strategy/</id><summary type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Transitioning to domain architect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I have been an infrastructure architect for quite some time already, my
focus then was always on specific areas within infrastructure (databases,
scheduling, big data), or on general infrastructure projects or challenges
(infrastructure zoning concept, region-wide disaster recovery analysis and
design). As one of my ex-colleagues and mentors put it, as infrastructure
architects you are allowed to piss on each other's area: you can (and perhaps
should) challenge the vision and projects of others, of course in a
professional way.&lt;/p&gt;
&lt;p&gt;I heeded the advice of this person, and was able to get a much better grip on
all our infrastructure services, their designs and challenges. I mentioned
earlier on that my day job became more intensive: it was not just the direct
responsibilities that I had that became more challenging, my principle to learn
and keep track of all infrastructure evolutions were a large part of it as
well. This pays off, as feedback and advice within the architecture review
boards is more to the point, more tied to the situation.&lt;/p&gt;
&lt;p&gt;Furthermore, as an architect, I still try to get my hands dirty on everything
bouncing around. When I was focusing on big data, I learned Spark and
pySpark, I revisited my Python knowledge, and used this for specific cases
(like using Python to create reports rather than Excel) to make sure I get a
general feel of what engineers and developers have to work with. When my focus
was on databases, I tried to get acquainted with DBA tasks. When we were
launching our container initiative, I set up and used Kubernetes myself (back
then this was also to see if SELinux is working properly with Kubernetes and
during the installation).&lt;/p&gt;
&lt;p&gt;While this does not make me anything near what our engineers and experts are
doing, I feel it gives me enough knowledge to be able to talk and discuss
topics with these colleagues without being that "ivory tower" architect,
and better understand (to a certain level) what they are going through when
new initiatives or solutions are thrown at them.&lt;/p&gt;
&lt;p&gt;End of 2019, the company decided that a reorganization was due, not only on
department and directorate level, but also on the IT and Enterprise
Architecture level. One of the areas that improved was to make sure the
infrastructure in general was also covered and supported by the EA team.
Part of that move, two of my infrastructure architect colleagues and
myself joined the EA team. One colleague is appointed to tackle a strategic
theme, another is now domain architect for workplace/workforce,
and I got the task of covering the infrastructure domain. Well, it is called
infrastructure, but focus on the infrastructure related to hosting of
applications and services: cloud hosting, data center, network, compute,
private cloud, container platform, mainframe, integration services,
middleware, etc. Another large part of what I consider "infrastructure" is
part of the workplace domain, which my colleague is pushing forward.&lt;/p&gt;
&lt;p&gt;While I was still handing over my previous workload, coaching the new colleague
that got thrown in to make sure both him and the teams involved are not left
with a gap, the various domain enterprise architects got a first task: draft
up the strategy for the domain… and don't wait too long ;-)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tackling a domain strategy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, I've drafted infrastructural strategies quite a few times already,
although those have most focus on the technology side. The domain view
goes beyond just the technological means: to be able to have a well-founded
strategy, I also have to tackle the resources and human side of things, the
ability of the organization to deal with (yet another?) transformation, the
processes involved, etc.&lt;/p&gt;
&lt;p&gt;Unlike the more specific area focus I had in the past, where the number of
direct stakeholders is limited, the infrastructure domain I now support has
many more direct stakeholders involved. If I count the product managers, system
architects, product owners (yes we are trying the Scaled Agile approach in our
organization) and the managerial roles within the domain, I have 148 people to
involve, spread across 7 Agile Release Trains with different directorate
steering. The consumers of the infrastructure services (which are more part of
business delivery services rather than on IT level) are even much larger than
that, and are the most important ones (but also more difficult) to get in touch
with.&lt;/p&gt;
&lt;p&gt;Rather than just asking what the main evolutions are in the several areas of
the domains, I approached this more according to practices I read in books like
"Good Strategy, Bad Strategy" by Richard Rumelt. I started with interviews of
all the stakeholders to get to learn what their challenges and problems are.
I wanted our strategy to tackle the issues at hand, not focus on technological
choices. Based on these interviews, I grouped the issues and challenges to see
what are the primary causes of these issues.&lt;/p&gt;
&lt;p&gt;Then I devised what action domains I need to focus on in the strategy. An
action domain was an area that more clearly describes the challenges ahead:
while I had close to 200 challenges observed from the interviews, I can assign
the huge majority of them to one of two action domains: if we tackle these
domains then we are helping the organization in most of their challenges.
After validating that these action domains are indeed covering the needs of
the organization, I started working on the principles how to tackle these
issues.&lt;/p&gt;
&lt;p&gt;Within the principles I want to steer the evolution within the infrastructure
domain, without already focusing on the tangible projects to accomplish that.
The principles should map to both larger projects (which I wanted to describe
in the strategy as well) as well as smaller or more continuity-related
projects. I eventually settled with four principles:
  - one principle covering how to transform the environment,
  - one principle covering what we offer (and thus also what we won't be
    offering anymore),
  - one principle which extends our scope with a major area that our internal
    customers are demanding, and
  - one principle describing how we will design our services&lt;/p&gt;
&lt;p&gt;Four principles are easy enough to remember for all involved, and if they are
described well, they are steering enough for the organization to take up in
their solutions. But with principles alone the strategy is not tangible enough
for everyone, and many choices to be made are not codified within those
principles. The next step was to draw out the vision for  infrastructure, based
upon current knowledge and the principles above, and show the major areas of
work that lays ahead, as well as give guidance on what these areas should
evolve to.&lt;/p&gt;
&lt;p&gt;I settled for eight vision statements, each worked out further with high level
guidance, as well as impact information: how will this impact the organization?
Do we need specific knowledge or other profiles that we miss? Is this a vision
that instills a cultural change (which often implies a slower adoption and the
need for more support)? What are the financial consequences? What will happen
if we do not pursue this vision?&lt;/p&gt;
&lt;p&gt;Within each vision, I collaborated with the various system architects and other
stakeholders to draft out epics, changes that support the vision and are ready
to be taken up in the Scaled Agile approach of the organization. The epics that
would be due soon were fully expanded, with a lean business case (attempt) and
phasing. Epics that are scheduled later (the strategy is a 5-year plan) are
mainly paraphrased as expanding those right now provides little value.&lt;/p&gt;
&lt;p&gt;While the epics themselves are not fully described in the strategy (the visions
give the rough approach), drafting these out is a way to verify if the vision
statements are feasible and correct, and is a way to check if the organization
understands and supports the vision.&lt;/p&gt;
&lt;p&gt;From the moment I got the request to the final draft of the strategy note,
around 2 months have passed. The first draft was slideware and showed the
intentions towards management (who wanted feedback within a few weeks after
the request), after which the strategy was codified in a large document, and
brought for approval on the appropriate boards.&lt;/p&gt;
&lt;p&gt;That was only the first hurdle though. Next was to communicate this strategy
further…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication and involvement are key&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The strategic document was almost finalized when COVID-19 struck. The company
moved to working at home, and the way of working changed a lot. This also
impacted how to approach the communication of the strategy and trying to get
involvement of people. Rather than physically explaining the strategy, watching
the body language of the people to see if they understand and support it or
not, I was facing digital meetings where we did not yet have video.
Furthermore, the organization was moving towards a more distributed approach
with smaller teams (higher agility) with fewer means of bringing out
information to larger groups.&lt;/p&gt;
&lt;p&gt;I selected a few larger meetings (such as those where all product managers and
system architects are present) to present and discuss the strategy, but also
started making webinars on this so that interested people could get informed
about it. I even decided to have two webinars: a very short one (3 minutes)
which focuses on the principles alone (and quickly summarizes the vision
statements), and an average one (20-ish minutes) which covers the principles
and vision statements.&lt;/p&gt;
&lt;p&gt;I also made recordings of the full explanations (e.g. those to management
team), which take 1 hour, but did not move those towards a webinar (due to
time pressure). Of course, I also published the strategy document itself for
everyone, as well as the slides that accompany it.&lt;/p&gt;
&lt;p&gt;One of the next steps is to translate the strategy further towards the
specific agile release trains, drafting up specific roadmaps, etc. This will
also allow me to communicate and explain the strategy further. Right now, this
is where we are at - and while I am happy with the strategy content, I do feel
that the communication part received too little attention from myself, and is
something I need to continue to put focus on.&lt;/p&gt;
&lt;p&gt;If a strategy is not absorbed by the organization, it fails as a strategy. And
if you do not have sufficient collaboration on the strategy after it was
'finalized' (not just communication but collaboration) then the organization
cannot absorb it. I also understand that the infrastructure strategy isn't
the only one guiding the organization: each domain has a strategy, and
while the domain architects do try to get the strategies aligned (or at least
not contradictory to each other), it is still not a single, company-wide
strategy.&lt;/p&gt;
&lt;p&gt;Right now, colleagues are working on consolidating the various strategies on
architectural level, while the agile organization is using the strategies to
formulate their specific solution visions (and for a handful of solutions I'm
also directly involved).&lt;/p&gt;
&lt;p&gt;We'll see how it pans out.&lt;/p&gt;
&lt;p&gt;So, do you think this is a sensible approach I took? How did you tackle
communication and collaboration of such initiatives during COVID-19 measures? &lt;/p&gt;</content><category term="Architecture"></category></entry><entry><title>cvechecker 3.9 released</title><link href="https://blog.siphos.be/2018/09/cvechecker-3.9-released/" rel="alternate"></link><published>2018-09-09T13:20:00+02:00</published><updated>2018-09-09T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-09-09:/2018/09/cvechecker-3.9-released/</id><content type="html">&lt;p&gt;Thanks to updates from Vignesh Jayaraman, Anton Hillebrand and Rolf Eike Beer,
a new release of &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; is
now made available.&lt;/p&gt;
&lt;p&gt;This new release (v3.9) is a bugfix release.&lt;/p&gt;
</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Automating compliance checks</title><link href="https://blog.siphos.be/2018/03/automating-compliance-checks/" rel="alternate"></link><published>2018-03-03T13:20:00+01:00</published><updated>2018-03-03T13:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-03-03:/2018/03/automating-compliance-checks/</id><summary type="html">&lt;p&gt;With the configuration baseline for a technical service being described fully (see the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first&lt;/a&gt;, &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second&lt;/a&gt; and &lt;a href="https://blog.siphos.be/2018/01/documenting-a-rule/"&gt;third&lt;/a&gt; post in this series), it is time to consider the validation of the settings in an automated manner. The preferred method for this is to use &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, which is nowadays managed by the &lt;a href="https://oval.cisecurity.org/"&gt;Center for Internet Security&lt;/a&gt;, abbreviated as CISecurity. Previously, OVAL was maintained and managed by Mitre under NIST supervision, and Google searches will often still point to the old sites. However, documentation is now maintained on CISecurity's &lt;a href="https://github.com/OVALProject/Language/tree/5.11.2/docs"&gt;github repositories&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But I digress...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the configuration baseline for a technical service being described fully (see the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first&lt;/a&gt;, &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second&lt;/a&gt; and &lt;a href="https://blog.siphos.be/2018/01/documenting-a-rule/"&gt;third&lt;/a&gt; post in this series), it is time to consider the validation of the settings in an automated manner. The preferred method for this is to use &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, which is nowadays managed by the &lt;a href="https://oval.cisecurity.org/"&gt;Center for Internet Security&lt;/a&gt;, abbreviated as CISecurity. Previously, OVAL was maintained and managed by Mitre under NIST supervision, and Google searches will often still point to the old sites. However, documentation is now maintained on CISecurity's &lt;a href="https://github.com/OVALProject/Language/tree/5.11.2/docs"&gt;github repositories&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But I digress...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Read-only compliance validation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the main ideas with OVAL is to have a language (XML-based) that represents state information (what something should be) which can be verified in a read-only fashion. Even more, from an operational perspective, it is very important that compliance checks &lt;em&gt;do not alter&lt;/em&gt; anything, but only report.&lt;/p&gt;
&lt;p&gt;Within its design, OVAL engineering has considered how to properly manage huge sets of assessment rules, and how to document this in an unambiguous manner. In the previous blog posts, ambiguity was resolved through writing style, and not much through actual, enforced definitions.&lt;/p&gt;
&lt;p&gt;OVAL enforces this. You can't write a generic or ambiguous rule in OVAL. It is very specific, but that also means that it is daunting to implement the first few times. I've written many OVAL sets, and I still struggle with it (although that's because I don't do it enough in a short time-frame, and need to reread my own documentation regularly).&lt;/p&gt;
&lt;p&gt;The capability to perform read-only validation with OVAL leads to a number of possible use cases. In the &lt;a href="http://oval.mitre.org/language/version5.10/OVAL_Language_Specification_09-14-2011.pdf"&gt;5.10 specification&lt;/a&gt; a number of use cases are provided. Basically, it boils down to vulnerability discovery (is a system vulnerable or not), patch management (is the system patched accordingly or not), configuration management (are the settings according to the rules or not), inventory management (detect what is installed on the system or what the systems' assets are), malware and threat indicator (detect if a system has been compromised or particular malware is active), policy enforcement (verify if a client system adheres to particular rules before it is granted access to a network), change tracking (regularly validating the state of a system and keeping track of changes), and security information management (centralizing results of an entire organization or environment and doing standard analytics on it).&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm focusing on configuration management.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OVAL structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although the OVAL standard (just like the XCCDF standard actually) entails a number of major components, I'm going to focus on the OVAL definitions. Be aware though that the results of an OVAL scan are also standardized format, as are results of XCCDF scans for instance.&lt;/p&gt;
&lt;p&gt;OVAL definitions have 4 to 5 blocks in them:
- the &lt;strong&gt;definition&lt;/strong&gt; itself, which describes what is being validated and how. It refers to one or more tests that are to be executed or validated for the definition result to be calculated
- the &lt;strong&gt;test&lt;/strong&gt; or tests, which are referred to by the definition. In each test, there is at least a reference to an object (what is being tested) and optionally to a state (what should the object look like)
- the &lt;strong&gt;object&lt;/strong&gt;, which is a unique representation of a resource or resources on the system (a file, a process, a mount point, a kernel parameter, etc.). Object definitions can refer to multiple resources, depending on the definition.
- the &lt;strong&gt;state&lt;/strong&gt;, which is a sort-of value mapping or validation that needs to be applied to an object to see if it is configured correctly
- the &lt;strong&gt;variable&lt;/strong&gt;, an optional definition which is what it sounds like, a variable that substitutes an abstract definition with an actual definition,  allowing to write more reusable tests.&lt;/p&gt;
&lt;p&gt;Let's get an example going, but without the XML structure, so in human language. We want to define that the Kerberos definition on a Linux system should allow forwardable tickets by default. This is accomplished by ensuring that, inside the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file (which is an INI-style configuration file), the value of the &lt;code&gt;forwardable&lt;/code&gt; key inside the &lt;code&gt;[libdefaults]&lt;/code&gt; section is set to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In OVAL, the definition itself will document the above in human readable text, assign it a unique ID (like &lt;code&gt;oval:com.example.oval:def:1&lt;/code&gt;) and mark it as being a definition for configuration validation (&lt;code&gt;compliance&lt;/code&gt;). Then, it defines the criteria that need to be checked in order to properly validate if the rule is applicable or not. These criteria include validation if the OVAL statement is actually being run on a Linux system (as it makes no sense to run it against a Cisco router) which is Kerberos enabled, and then the criteria of the file check itself. Each criteria links to a test.&lt;/p&gt;
&lt;p&gt;The test of the file itself links to an object and a state. There are a number of ways how we can check for this specific case. One is that the object is the &lt;code&gt;forwardable&lt;/code&gt; key in the &lt;code&gt;[libdefaults]&lt;/code&gt; section of the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file, and the state is the value &lt;code&gt;true&lt;/code&gt;. In this case, the state will point to those two entries (through their unique IDs) and define that the object must exist, and all matches must have a matching state. The "all matches" here is not that important, because there will generally only be one such definition in the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Note however that a different approach to the test can be declared as well. We could state that the object is the &lt;code&gt;[libdefaults]&lt;/code&gt; section inside the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file, and the state is the value &lt;code&gt;true&lt;/code&gt; for the &lt;code&gt;forwardable&lt;/code&gt; key. In this case, the test declares that multiple objects must exist, and (at least) one must match the state.&lt;/p&gt;
&lt;p&gt;As you can see, the OVAL language tries to map definitions to unambiguous definitions. So, how does this look like in OVAL XML?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The OVAL XML structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://blog.siphos.be/static/2018/oval.xml"&gt;full example&lt;/a&gt; contains a few more entries than those we declare next, in order to be complete. The most important definitions though are documented below.&lt;/p&gt;
&lt;p&gt;Let's start with the definition. As stated, it will refer to tests that need to match for the definition to be valid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;definitions&amp;gt;
  &amp;lt;definition id=&amp;quot;oval:com.example.oval:def:1&amp;quot; version=&amp;quot;1&amp;quot; class=&amp;quot;compliance&amp;quot;&amp;gt;
    &amp;lt;metadata&amp;gt;
      &amp;lt;title&amp;gt;libdefaults.forwardable in /etc/krb5.conf must be set to true&amp;lt;/title&amp;gt;
      &amp;lt;affected family=&amp;quot;unix&amp;quot;&amp;gt;
        &amp;lt;platform&amp;gt;Red Hat Enterprise Linux 7&amp;lt;/platform&amp;gt;
      &amp;lt;/affected&amp;gt;
      &amp;lt;description&amp;gt;
        By default, tickets obtained from the Kerberos environment must be forwardable.
      &amp;lt;/description&amp;gt;
    &amp;lt;/metadata&amp;gt;
    &amp;lt;criteria operator=&amp;quot;AND&amp;quot;&amp;gt;
      &amp;lt;criterion test_ref=&amp;quot;oval:com.example.oval:tst:1&amp;quot; comment=&amp;quot;Red Hat Enterprise Linux is installed&amp;quot;/&amp;gt;
      &amp;lt;criterion test_ref=&amp;quot;oval:com.example.oval:tst:2&amp;quot; comment=&amp;quot;/etc/krb5.conf&amp;#39;s libdefaults.forwardable is set to true&amp;quot;/&amp;gt;
    &amp;lt;/criteria&amp;gt;
  &amp;lt;/definition&amp;gt;
&amp;lt;/definitions&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first thing to keep in mind is the (weird) identification structure. Just like with XCCDF, it is not sufficient to have your own id convention. You need to start an id with &lt;code&gt;oval:&lt;/code&gt; followed by the reverse domain definition (here &lt;code&gt;com.example.oval&lt;/code&gt;), followed by the type (&lt;code&gt;def&lt;/code&gt; for definition) and a sequence number.&lt;/p&gt;
&lt;p&gt;Also, take a look at the criteria. Here, two tests need to be compliant (hence the &lt;code&gt;AND&lt;/code&gt; operator). However, more complex operations can be done as well. It is even allowed to nest multiple criteria, and refer to previous definitions, like so (taken from the &lt;a href="https://raw.githubusercontent.com/GovReady/ubuntu-scap/master/ssg-rhel6-oval.xml"&gt;ssg-rhel6-oval.xml file&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;criteria comment=&amp;quot;package hal removed or service haldaemon is not configured to start&amp;quot; operator=&amp;quot;OR&amp;quot;&amp;gt;
  &amp;lt;extend_definition comment=&amp;quot;hal removed&amp;quot; definition_ref=&amp;quot;oval:ssg:def:211&amp;quot;/&amp;gt;
  &amp;lt;criteria operator=&amp;quot;AND&amp;quot; comment=&amp;quot;service haldaemon is not configured to start&amp;quot;&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 0&amp;quot; test_ref=&amp;quot;oval:ssg:tst:212&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 1&amp;quot; test_ref=&amp;quot;oval:ssg:tst:213&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 2&amp;quot; test_ref=&amp;quot;oval:ssg:tst:214&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 3&amp;quot; test_ref=&amp;quot;oval:ssg:tst:215&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 4&amp;quot; test_ref=&amp;quot;oval:ssg:tst:216&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 5&amp;quot; test_ref=&amp;quot;oval:ssg:tst:217&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 6&amp;quot; test_ref=&amp;quot;oval:ssg:tst:218&amp;quot;/&amp;gt;
  &amp;lt;/criteria&amp;gt;
&amp;lt;/criteria&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, let's look at the tests.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;tests&amp;gt;
  &amp;lt;unix:file_test id=&amp;quot;oval:com.example.oval:tst:1&amp;quot; version=&amp;quot;1&amp;quot; check_existence=&amp;quot;all_exist&amp;quot; check=&amp;quot;all&amp;quot; comment=&amp;quot;/etc/redhat-release exists&amp;quot;&amp;gt;
    &amp;lt;unix:object object_ref=&amp;quot;oval:com.example.oval:obj:1&amp;quot; /&amp;gt;
  &amp;lt;/unix:file_test&amp;gt;
  &amp;lt;ind:textfilecontent54_test id=&amp;quot;oval:com.example.oval:tst:2&amp;quot; check=&amp;quot;all&amp;quot; check_existence=&amp;quot;all_exist&amp;quot; version=&amp;quot;1&amp;quot; comment=&amp;quot;The value of forwardable in /etc/krb5.conf&amp;quot;&amp;gt;
    &amp;lt;ind:object object_ref=&amp;quot;oval:com.example.oval:obj:2&amp;quot; /&amp;gt;
    &amp;lt;ind:state state_ref=&amp;quot;oval:com.example.oval:ste:2&amp;quot; /&amp;gt;
  &amp;lt;/ind:textfilecontent54_test&amp;gt;
&amp;lt;/tests&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There are two tests defined here. The first test just checks if &lt;code&gt;/etc/redhat-release&lt;/code&gt; exists. If not, then the test will fail and the definition itself will result to false (as in, not compliant). This isn't actually a proper definition, because you want the test to not run when it is on a different platform, but for the sake of example and simplicity, let's keep it as is.&lt;/p&gt;
&lt;p&gt;The second test will check for the value of the &lt;code&gt;forwardable&lt;/code&gt; key in &lt;code&gt;/etc/krb5.conf&lt;/code&gt;. For it, it refers to an object and a state. The test states that all objects must exist (&lt;code&gt;check_existence="all_exist"&lt;/code&gt;) and that all objects must match the state (&lt;code&gt;check="all"&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The object definition looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;objects&amp;gt;
  &amp;lt;unix:file_object id=&amp;quot;oval:com.example.oval:obj:1&amp;quot; comment=&amp;quot;The /etc/redhat-release file&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;unix:filepath&amp;gt;/etc/redhat-release&amp;lt;/unix:filepath&amp;gt;
  &amp;lt;/unix:file_object&amp;gt;
  &amp;lt;ind:textfilecontent54_object id=&amp;quot;oval:com.example.oval:obj:2&amp;quot; comment=&amp;quot;The forwardable key&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;ind:filepath&amp;gt;/etc/krb5.conf&amp;lt;/ind:filepath&amp;gt;
    &amp;lt;ind:pattern operation=&amp;quot;pattern match&amp;quot;&amp;gt;^\s*forwardable\s*=\s*((true|false))\w*&amp;lt;/ind:pattern&amp;gt;
    &amp;lt;ind:instance datatype=&amp;quot;int&amp;quot; operation=&amp;quot;equals&amp;quot;&amp;gt;1&amp;lt;/ind:instance&amp;gt;
  &amp;lt;/ind:textfilecontent54_object&amp;gt;
&amp;lt;/objects&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first object is a simple file reference. The second is a text file content object. More specifically, it matches the line inside &lt;code&gt;/etc/krb5.conf&lt;/code&gt; which has &lt;code&gt;forwardable = true&lt;/code&gt; or &lt;code&gt;forwardable = false&lt;/code&gt; in it. An expression is made on it, so that we can refer to the subexpression as part of the test.&lt;/p&gt;
&lt;p&gt;This test looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;states&amp;gt;
  &amp;lt;ind:textfilecontent54_state id=&amp;quot;oval:com.example.oval:ste:2&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;ind:subexpression datatype=&amp;quot;string&amp;quot;&amp;gt;true&amp;lt;/ind:subexpression&amp;gt;
  &amp;lt;/ind:textfilecontent54_state&amp;gt;
&amp;lt;/states&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This test refers to a subexpression, and wants it to be &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing the checks with Open-SCAP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open-SCAP tool is able to test OVAL statements directly. For instance, with the above definition in a file called &lt;code&gt;oval.xml&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap oval eval --results oval-results.xml oval.xml
Definition oval:com.example.oval:def:1: true
Evaluation done.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output of the command shows that the definition was evaluated successfully. If you want more information, open up the &lt;code&gt;oval-results.xml&lt;/code&gt; file which contains all the details about the test. This results file is also very useful while developing OVAL as it shows the entire result of objects, tests and so forth.&lt;/p&gt;
&lt;p&gt;For instance, the &lt;code&gt;/etc/redhat-release&lt;/code&gt; file was only checked to see if it exists, but the results file shows what other parameters can be verified with it as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;unix-sys:file_item id=&amp;quot;1233781&amp;quot; status=&amp;quot;exists&amp;quot;&amp;gt;
  &amp;lt;unix-sys:filepath&amp;gt;/etc/redhat-release&amp;lt;/unix-sys:filepath&amp;gt;
  &amp;lt;unix-sys:path&amp;gt;/etc&amp;lt;/unix-sys:path&amp;gt;
  &amp;lt;unix-sys:filename&amp;gt;redhat-release&amp;lt;/unix-sys:filename&amp;gt;
  &amp;lt;unix-sys:type&amp;gt;regular&amp;lt;/unix-sys:type&amp;gt;
  &amp;lt;unix-sys:group_id datatype=&amp;quot;int&amp;quot;&amp;gt;0&amp;lt;/unix-sys:group_id&amp;gt;
  &amp;lt;unix-sys:user_id datatype=&amp;quot;int&amp;quot;&amp;gt;0&amp;lt;/unix-sys:user_id&amp;gt;
  &amp;lt;unix-sys:a_time datatype=&amp;quot;int&amp;quot;&amp;gt;1515186666&amp;lt;/unix-sys:a_time&amp;gt;
  &amp;lt;unix-sys:c_time datatype=&amp;quot;int&amp;quot;&amp;gt;1514927465&amp;lt;/unix-sys:c_time&amp;gt;
  &amp;lt;unix-sys:m_time datatype=&amp;quot;int&amp;quot;&amp;gt;1498674992&amp;lt;/unix-sys:m_time&amp;gt;
  &amp;lt;unix-sys:size datatype=&amp;quot;int&amp;quot;&amp;gt;52&amp;lt;/unix-sys:size&amp;gt;
  &amp;lt;unix-sys:suid datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:suid&amp;gt;
  &amp;lt;unix-sys:sgid datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:sgid&amp;gt;
  &amp;lt;unix-sys:sticky datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:sticky&amp;gt;
  &amp;lt;unix-sys:uread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:uread&amp;gt;
  &amp;lt;unix-sys:uwrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:uwrite&amp;gt;
  &amp;lt;unix-sys:uexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:uexec&amp;gt;
  &amp;lt;unix-sys:gread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:gread&amp;gt;
  &amp;lt;unix-sys:gwrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:gwrite&amp;gt;
  &amp;lt;unix-sys:gexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:gexec&amp;gt;
  &amp;lt;unix-sys:oread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:oread&amp;gt;
  &amp;lt;unix-sys:owrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:owrite&amp;gt;
  &amp;lt;unix-sys:oexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:oexec&amp;gt;
  &amp;lt;unix-sys:has_extended_acl datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:has_extended_acl&amp;gt;
&amp;lt;/unix-sys:file_item&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, this is just on OVAL level. The final step is to link it in the XCCDF file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referring to OVAL in XCCDF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The XCCDF Rule entry allows for a &lt;code&gt;check&lt;/code&gt; element, which refers to an automated check for compliance.&lt;/p&gt;
&lt;p&gt;For instance, the above rule could be referred to like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;Rule id=&amp;quot;xccdf_com.example_rule_krb5-forwardable-true&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Enable forwardable tickets on RHEL systems&amp;lt;/title&amp;gt;
  ...
  &amp;lt;check system=&amp;quot;http://oval.mitre.org/XMLSchema/oval-definitions-5&amp;quot;&amp;gt;
    &amp;lt;check-content-ref href=&amp;quot;oval.xml&amp;quot; name=&amp;quot;oval:com.example.oval:def:1&amp;quot; /&amp;gt;
  &amp;lt;/check&amp;gt;
&amp;lt;/Rule&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this set in the Rule, Open-SCAP can validate it while checking the configuration baseline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval --oval-results --results xccdf-results.xml xccdf.xml
...
Title   Enable forwardable kerberos tickets in krb5.conf libdefaults
Rule    xccdf_com.example_rule_krb5-forwardable-tickets
Ident   RHEL7-01007
Result  pass
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A huge advantage here is that, alongside the detailed results of the run, there is also better human readable output as it shows the title of the Rule being checked.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The detailed capabilities of OVAL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example I've used two examples: a file validation (against &lt;code&gt;/etc/redhat-release&lt;/code&gt;) and a file content one (against &lt;code&gt;/etc/krb5.conf&lt;/code&gt;). However, OVAL has many more checks and support for it, and also has constraints that you need to be aware of.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://github.com/OVALProject/Language/tree/master/docs"&gt;OVAL Project&lt;/a&gt; github account, the Language repository keeps track of the current documentation. By browsing through it, you'll notice that the OVAL capabilities are structured based on the target technology that you can check. Right now, this is AIX, Android, Apple iOS, Cisco ASA, Cisco CatOS, VMWare ESX, FreeBSD, HP-UX, Cisco iOS and iOS-XE, Juniper JunOS, Linux, MacOS, NETCONF, Cisco PIX, Microsoft SharePoint, Unix (generic), Microsoft Windows, and independent.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/independent-definitions-schema.md"&gt;independent&lt;/a&gt; one contains tests and support for resources that are often reusable toward different platforms (as long as your OVAL and XCCDF supporting tools can run it on those platforms). A few notable supporting tests are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filehash58_test&lt;/code&gt; which can check for a number of common hashes (such as SHA-512 and MD5). This is useful when you want to make sure that a particular (binary or otherwise) file is available on the system. In enterprises, this could be useful for license files, or specific library files.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;textfilecontent54_test&lt;/code&gt; which can check the content of a file, with support for regular expressions.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xmlfilecontent_test&lt;/code&gt; which is a specialized test toward XML files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keep in mind though that, as we have seen above, INI files specifically have no specialization available. It would be nice if CISecurity would develop support for common textual data formats, such as CSV (although that one is easily interpretable with the existing ones), JSON, YAML and INI.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/unix-definitions-schema.md"&gt;unix&lt;/a&gt; one contains tests specific to Unix and Unix-like operating systems (so yes, it is also useful for Linux), and together with the &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/linux-definitions-schema.md"&gt;linux&lt;/a&gt; one a wide range of configurations can be checked. This includes support for generic extended attributes (&lt;code&gt;fileextendedattribute_test&lt;/code&gt;) as well as SELinux specific rules (&lt;code&gt;selinuxboolean_test&lt;/code&gt; and &lt;code&gt;selinuxsecuritycontext_test&lt;/code&gt;), network interface settings (&lt;code&gt;interface_test&lt;/code&gt;), runtime processes (&lt;code&gt;process58_test&lt;/code&gt;), kernel parameters (&lt;code&gt;sysctl_test&lt;/code&gt;), installed software tests (such as &lt;code&gt;rpminfo_test&lt;/code&gt; for RHEL and other RPM enabled operating systems) and more.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="oval"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Documenting a rule</title><link href="https://blog.siphos.be/2018/01/documenting-a-rule/" rel="alternate"></link><published>2018-01-24T20:40:00+01:00</published><updated>2018-01-24T20:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-24:/2018/01/documenting-a-rule/</id><summary type="html">&lt;p&gt;In the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first post&lt;/a&gt; I talked about why configuration documentation is important. In the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second post&lt;/a&gt; I looked into a good structure for configuration documentation of a technological service, and ended with an XCCDF template in which this documentation can be structured.&lt;/p&gt;
&lt;p&gt;The next step is to document the rules themselves, i.e. the actual content of a configuration baseline.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first post&lt;/a&gt; I talked about why configuration documentation is important. In the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second post&lt;/a&gt; I looked into a good structure for configuration documentation of a technological service, and ended with an XCCDF template in which this documentation can be structured.&lt;/p&gt;
&lt;p&gt;The next step is to document the rules themselves, i.e. the actual content of a configuration baseline.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Fine-grained rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While from a high-level point of view, configuration items could be documented in a coarse-grained manner, a proper configuration baseline documents rules very fine-grained. Let's first consider a bad example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All application code files are root-owned, with read-write privileges for owner and group, and executable where it makes sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While such a rule could be interpreted correctly, it also leaves room for misinterpretation and ambiguity. Furthermore, it is not explicit. What are application code files? Where are they stored? What about group ownership? The executable permission, when does that make sense? Does the rule also imply that there is no privilege for world-wide access, or does it just ignore that?&lt;/p&gt;
&lt;p&gt;A better example (or set of examples) would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; is recursively user-owned by root&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; is recursively group-owned by root&lt;/li&gt;
&lt;li&gt;No files under &lt;code&gt;/opt/postgresql&lt;/code&gt; are executable except when specified further&lt;/li&gt;
&lt;li&gt;All files in &lt;code&gt;/opt/postgresql/bin&lt;/code&gt; are executable&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; has &lt;code&gt;system_u:object_r:usr_t:s0&lt;/code&gt; as SELinux context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql/bin/postgres&lt;/code&gt; has &lt;code&gt;system_u:object_r:postgresql_exec_t:s0&lt;/code&gt; as SELinux context&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And even that list is still not complete, but you get the gist. The focus here is to have fine-grained rules which are explicit and not ambiguous.&lt;/p&gt;
&lt;p&gt;Of course, the above configuration rule is still a "simple" permission set. Configuration baselines go further than that of course. They can act on file content ("no PAM configuration files can refer to pam_rootok.so except for runuser and su"), run-time processes ("The processes with /usr/sbin/sshd as command and with -D as option must run within the sshd_t SELinux domain"), database query results, etc.&lt;/p&gt;
&lt;p&gt;This granularity is especially useful later on when you want to automate compliance checks, because the more fine-grained a description is, the easier it is to develop and maintain checks on it. But before we look into remediation, let's document the rule a bit further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metadata on the rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's consider the following configuration rule:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;/opt/postgresql/bin/postgres&lt;/code&gt; has &lt;code&gt;system_u:object_r:postgresql_exec_t:s0&lt;/code&gt; as SELinux context&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the configuration baseline, we don't just want to state that this is the rule, and be finished. We need to describe the rule in more detail, as was described in the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;previous post&lt;/a&gt;. More specifically, we definitely want to
- know the rule's severity is, or how "bad" it would be if we detect a deviation from the rule
- have an indication if the rule is security-sensitive or more oriented to manageability
- a more elaborate description of the rule than just the title
- an indication why this rule is in place (what does it solve, fix or simplify)
- information on how to remediate if a deviation is found
- know if the rule is applicable to our environment or not&lt;/p&gt;
&lt;p&gt;The severity in the &lt;em&gt;Security Content Automation Protocol (SCAP)&lt;/em&gt; standard, which defines the XCCDF standard as well as OVAL and a few others like CVSS, uses the following possible values for severity: unknown, info, low, medium, high.&lt;/p&gt;
&lt;p&gt;To indicate if a rule is security-oriented or not, XCCDF's role attribute is best used. With the role attribute, you state if a rule is to be included in the final scoring (a weighted value given to the compliance of a system) or not. If it is, then it is security sensitive.&lt;/p&gt;
&lt;p&gt;The indication of a rule applicability in the environment might seem strange. If you document the configuration baseline, shouldn't it include only those settings you want? Well, yes and no. Personally, I like to include recommendations that we &lt;em&gt;do not follow&lt;/em&gt; in the baseline as well.&lt;/p&gt;
&lt;p&gt;Suppose for instance that an audit comes along and says you need to enable data encryption on the database. Let's put aside that an auditor should focus mainly/solely on the risks, and let the solutions be managed by the team (but be involved in accepting solutions of course), the team might do an assessment and find that data encryption on the database level (i.e. the database files are encrypted so non-DBA users with operating system interactive rights cannot read the data) is actually not going to remediate any risk, yet introduce more complexity.&lt;/p&gt;
&lt;p&gt;In that situation, and assuming that the auditor agrees with a different control, you might want to add a rule to the configuration baseline about this. Either you document the wanted state (database files do not need to be encrypted), or you document the suggestion (database files should be encrypted) but explicitly state that you do not require or implement it, and document the reasoning for it. The rule is then augmented with references to the audit recommendation for historical reasons and to facilitate future discussions.&lt;/p&gt;
&lt;p&gt;And yes, I know the rule "database files should be encrypted" is still ambiguous. The actual rule should be more specific to the technology).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documenting a rule in XCCDF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In XCCDF, a rule is defined through the &lt;code&gt;Rule&lt;/code&gt; XML entity, and is placed within a &lt;code&gt;Group&lt;/code&gt;. The Group entities are used to structure the document, while the &lt;code&gt;Rule&lt;/code&gt; entities document specific configuration directives.&lt;/p&gt;
&lt;p&gt;The postgres related rule of above could be written as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;Rule id=&amp;quot;xccdf_com.example_rule_pgsql-selinux-context&amp;quot;
      role=&amp;quot;full&amp;quot;
      selected=&amp;quot;1&amp;quot;
      weight=&amp;quot;5.1&amp;quot;
      severity=&amp;quot;high&amp;quot;
      cluster-id=&amp;quot;network&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;
    /opt/postgresql/bin/postgres has system_u:object_r:postgresql_exec_t:s0 as SELinux context
  &amp;lt;/title&amp;gt;
  &amp;lt;description&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The postgres binary is the main binary of the PostgreSQL database daemon. Once started, it launches the necessary workers. To ensure that PostgreSQL runs in the proper SELinux domain (postgresql_t) its binary must be labeled with postgresql_exec_t.
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The current state of the label can be obtained using stat, or even more simple, the -Z option to ls:
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:pre&amp;gt;~$ ls -Z /opt/postgresql/bin/postgres
-rwxr-xr-x. root root system_u:object_r:postgresql_exec_t:s0 /opt/postgresql/bin/postgres
    &amp;lt;/xhtml:pre&amp;gt;
  &amp;lt;/description&amp;gt;
  &amp;lt;rationale&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The domain in which a process runs defines the SELinux controls that are active on the process. Services such as PostgreSQL have an established policy set that controls what a database service can and cannot do on the system.
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      If the PostgreSQL daemon does not run in the postgresql_t domain, then SELinux might either block regular activities of the database (service availability impact), block behavior that impacts its effectiveness (integrity issue) or allow behavior that shouldn&amp;#39;t be allowed. The latter can have significant consequences once a vulnerability is exploited.
    &amp;lt;/xhtml:p&amp;gt;
  &amp;lt;/rationale&amp;gt;
  &amp;lt;fixtext&amp;gt;
    Restore the context of the file using restorecon or chcon.
  &amp;lt;/fixtext&amp;gt;
  &amp;lt;fix strategy=&amp;quot;restrict&amp;quot; system=&amp;quot;urn:xccdf:fix:script:sh&amp;quot;&amp;gt;restorecon /opt/postgresql/bin/postgres
  &amp;lt;/fix&amp;gt;
  &amp;lt;ident system=&amp;quot;http://example.com/configbaseline&amp;quot;&amp;gt;pgsql-01032&amp;lt;/ident&amp;gt;
&amp;lt;/Rule&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although this is lots of XML, it is easy to see what each element declares. The &lt;a href="https://csrc.nist.gov/CSRC/media/Publications/nistir/7275/rev-4/final/documents/nistir-7275r4_updated-march-2012_clean.pdf"&gt;NIST IR 7275 document&lt;/a&gt; is a very good resource to continuously consult in order to find the right elements and their interpretation.&lt;/p&gt;
&lt;p&gt;There is one element added that is "specific" to the content of this blog post series and not the XCCDF standard, namely the identification. As mentioned in an earlier post, organizations might have their own taxonomy for technical service identification, and requirements on how to number or identify rules. In the above example, the rule is identified as &lt;code&gt;pgsql-01032&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There is another attribute in use above that might need more clarification: the weight of the rule.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abusing CVSS for configuration weight scoring&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example, a weight is given to the rule scoring (weight of 5.1). This number is obtained through a &lt;a href="https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:N/AC:H/PR:N/UI:N/S:C/C:L/I:L/A:N/E:U/RL:O/CR:H/IR:H/AR:H/MAV:N/MAC:H/MPR:N/MUI:N/MS:U/MC:L/MI:N/MA:L"&gt;CVSS calculator&lt;/a&gt;, which is generally used to identify the risk of a security issue or vulnerability. CVSS stands for &lt;em&gt;Common Vulnerability Scoring System&lt;/em&gt; and is a popular way to weight security risks (which are then associated with vulnerability reports, &lt;em&gt;Common Vulnerabilities and Exposures (CVE)&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Misconfigurations can also be slightly interpreted as a security risk, although it requires some mental bridges. Rather than scoring the rule, you score the risk that it mitigates, and consider the worst thing that could happen if that rule is not implemented correctly. Now, worst-case thinking is subjective, so there will always be discussion on the weight of a rule. It is therefore important to have a consensus in the team (if the configuration baseline is team-owned) if this weight is actively used. Of course, an organization might choose to ignore the weight, or use a different scoring mechanism.&lt;/p&gt;
&lt;p&gt;In the above situation, I scored what would happen if a vulnerability in PostgreSQL was successfully exploited, and SELinux couldn't mitigate the risk as the label of the file was wrong. The result of a wrong label &lt;em&gt;could be&lt;/em&gt; that the PostgreSQL service runs in a higher privileged domain, or even in an unconfined domain (no SELinux restrictions active), so there is a heightened risk of confidentiality loss (beyond the database) and even integrity risk.&lt;/p&gt;
&lt;p&gt;However, the confidentiality risk is scored as low, and integrity even in between (base risk is low, but due to other constraints put in place integrity impact is reduced further) because PostgreSQL runs as a non-administrative user on the system, and perhaps because the organization uses dedicated systems for database hosting (so other services are not easily impacted).&lt;/p&gt;
&lt;p&gt;As mentioned, this is somewhat abusing the CVSS methodology, but is imo much more effective than trying to figure out your own scoring methodology. With CVSS, you start with scoring the risk regardless of context (CVSS Base), then adjust based on recent state or knowledge (CVSS Temporal), and finally adjust further with knowledge of the other settings or mitigating controls in place (CVSS Environmental).&lt;/p&gt;
&lt;p&gt;Personally, I prefer to only use the CVSS Base scoring for configuration baselines, because the other two are highly depending on time (which is, for documentation, challenging) and the other controls (which is more of a concern for service technical documentation). So in my preferred situation, the rule would be scored as 5.4 rather than 5.1. But that's just me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isn't this CCE?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People who use SCAP a bit more might already be thinking if I'm not reinventing the wheel here. After all, SCAP also has a standard called &lt;em&gt;Common Configuration Enumeration (CCE)&lt;/em&gt; which seems to be exactly what I'm doing here: enumerating the configuration of a technical service. And indeed, if you look at the &lt;a href="https://nvd.nist.gov/config/cce/index"&gt;CCE list&lt;/a&gt; you'll find a number of Excel sheets (sigh) that define common configurations.&lt;/p&gt;
&lt;p&gt;For instance, for Red Hat Enterprise Linux v5, there is an enumeration identified as CCE-4361-2, which states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;File permissions for /etc/pki/tls/ldap should be set correctly&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The CCE description then goes on stating that this is a permission setting (CCE Parameter), which can be rectified with &lt;code&gt;chmod&lt;/code&gt; (CCE Technical Mechanism), and refers to a source for the setting.&lt;/p&gt;
&lt;p&gt;However, CCE has a number of downsides.&lt;/p&gt;
&lt;p&gt;First of all, it isn't being maintained anymore. And although XCCDF itself is also a quite old standard, it is still being looked into (a draft new version is being prepared) and is actively used as a standard. Red Hat is investing time and resources into secure configurations and compliancy aligned with SCAP, and other vendors publish SCAP-specific resources as well. CCE however would be a list, and thus requires continuous management. That RHELv5 is the most recent RHEL CCE list is a bad thing.&lt;/p&gt;
&lt;p&gt;Second, CCE's structure is for me insufficient to use in configuration baselines. XCCDF has a much more mature and elaborate set of settings for this. What CCE does is actually what I use in the above example as the organization-specific identifier.&lt;/p&gt;
&lt;p&gt;Finally, there aren't many tools that actively use CCE, unlike CVSS, XCCDF, OVAL, CVSS and other standards under the SCAP umbrella, which are all still actively used and developed upon by tools such as Open-SCAP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before finishing this post, I want to talk about profiling.&lt;/p&gt;
&lt;p&gt;Within an XCCDF benchmark, several profiles can be defined. In the XCCDF template I defined a single profile that covers all rules, but this can be fine-tuned to the needs of the organization. In XCCDF profiles, you can select individual rules (which ones are active for a profile and which ones aren't) and even fine-tune values for rules. This is called tailoring in XCCDF.&lt;/p&gt;
&lt;p&gt;A first use case for profiles is to group different rules based on the selected setup. In case of Nginx for instance, one can consider Nginx being used as either a reverse proxy, a static website hosting or a dynamic web application hosting. In all three cases, some rules will be the same, but several rules will be different. Within XCCDF, you can document all rules, and then use profiles to group the rules related to a particular service use.&lt;/p&gt;
&lt;p&gt;XCCDF allows for profile inheritance. This means that you can define a base Profile (all the rules that need to be applied, regardless of the service use) and then extend the profiles with individual rule selections.&lt;/p&gt;
&lt;p&gt;With profiles, you can also fine-tune values. For instance, you could have a password policy in place that states that passwords on internal machines have to be at least 10 characters long, but on DMZ systems they need to be at least 15 characters long. Instead of defining two rules, the rule could refer to a particular variable (Value in XCCDF) which is then selected based on the Profile. The value for a password length is then by default 10, but the Profile for DMZ systems selects the other value (15).&lt;/p&gt;
&lt;p&gt;Now, value-based tailoring is imo already a more advanced use of XCCDF, and is best looked into when you also start using OVAL or other automated checks. The tailoring information is then passed on to the automated compliance check so that the right value is validated.&lt;/p&gt;
&lt;p&gt;Value-based tailoring also makes rules either more complex to write, or ambiguous to interpret without full profile awareness. Considering the password length requirement, the rule could become:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The /etc/pam.d/password-auth file must refer to pam_passwdqc.so for the password service with a minimal password length of 10 (default) or 15 (DMZ) for the N4 password category&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At least the rule is specific. Another approach would be to document it as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The /etc/pam.d/password-auth file must refer to pam_passwdqc.so with the proper organizational password controls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The documentation of the rule might document the proper controls further, but the rule is much less specific. Later checks might report that a system fails this check, referring to the title, which is insufficient for engineers or administrators to resolve.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generating the guide&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To close off this post, let's finish with how to generate the guide based on an XCCDF document. Personally, I use two approaches for this.&lt;/p&gt;
&lt;p&gt;The first one is to rely on Open-SCAP. With Open-SCAP, you can generate guides easily:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf generate guide xccdf.xml &amp;gt; ConfigBaseline.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The second one, which I use more often, is a custom XSL style sheet, which also introduces the knowledge and interpretations of what this blog post series brings up (including the organizational identification). The end result is similar (the same content) but uses a structure/organization that is more in line with expectations.&lt;/p&gt;
&lt;p&gt;For instance, in my company, the information security officers want to have a tabular overview of all the rules in a configuration baseline. So the XSL style sheet generates such a tabular overview, and uses in-documenting linking to the more elaborate descriptions of all the rules.&lt;/p&gt;
&lt;p&gt;An &lt;a href="https://blog.siphos.be/static/2018/xccdf.xsl"&gt;older version&lt;/a&gt; is online for those interested. It uses JavaScript as well (in case you are security sensitive you might want to look into it) to allow collapsing rule documentation for faster online viewing.&lt;/p&gt;
&lt;p&gt;The custom XSL has an additional advantage, namely that there is no dependency on Open-SCAP to generate the guides (even though it is perfectly possible to copy the XSL and continue). I can successfully generate the guide using &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=21714"&gt;Microsoft's msxml&lt;/a&gt; utility, using xsltproc, etc depending on the platform I'm on.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Structuring a configuration baseline</title><link href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/" rel="alternate"></link><published>2018-01-17T09:10:00+01:00</published><updated>2018-01-17T09:10:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-17:/2018/01/structuring-a-configuration-baseline/</id><summary type="html">&lt;p&gt;A good configuration baseline has a readable structure that allows all stakeholders to quickly see if the baseline is complete, as well as find a particular setting regardless of the technology. In this blog post, I'll cover a possible structure of the baseline which attempts to be sufficiently complete and technology agnostic.&lt;/p&gt;
&lt;p&gt;If you haven't read the blog post on &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;documenting configuration changes&lt;/a&gt;, it might be a good idea to do so as it declares the scope of configuration baselines and why I think XCCDF is a good match for this.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A good configuration baseline has a readable structure that allows all stakeholders to quickly see if the baseline is complete, as well as find a particular setting regardless of the technology. In this blog post, I'll cover a possible structure of the baseline which attempts to be sufficiently complete and technology agnostic.&lt;/p&gt;
&lt;p&gt;If you haven't read the blog post on &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;documenting configuration changes&lt;/a&gt;, it might be a good idea to do so as it declares the scope of configuration baselines and why I think XCCDF is a good match for this.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Chaptered documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As mentioned previously, a configuration baseline describes the configuration of a particular technological service (rather than a business service which is an integrated set of technologies and applications). To document and maintain the configuration state of the technology, I suggest the following eight chapters (to begin with):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Operating system and services&lt;/li&gt;
&lt;li&gt;Software deployment and file system&lt;/li&gt;
&lt;li&gt;Technical service settings&lt;/li&gt;
&lt;li&gt;Authentication, authorization, access control and auditing&lt;/li&gt;
&lt;li&gt;Service specific settings&lt;/li&gt;
&lt;li&gt;Cryptographic services&lt;/li&gt;
&lt;li&gt;Data and information handling&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Within each chapter, sections can be declared depending on how the technology works. For instance, for database technologies one can have a distinction between system-wide settings, instance-specific settings and even database-specific settings. Or, if the organization has specific standards on user definitions, a chapter on "User settings" can be used. The above is just a suggestion in an attempt to cover most aspects of a configuration baseline.&lt;/p&gt;
&lt;p&gt;With the sections of the chapter, rules are then defined which specify the actual configuration setting (or valid range) applicable to the technology. But the rule goes further than just a single-line configuration setting description.&lt;/p&gt;
&lt;p&gt;Each rule should have a &lt;em&gt;unique identifier&lt;/em&gt; so that other documents can reliably link to the rules in the document. Although XCCDF has a convention for this, I feel that the XCCDF way here is more useful for technical referencing while the organization is better off with a more human addressable approach. So while a rule in XCCDF has the identifier &lt;code&gt;xccdf_com.example.postgresql_rule_selinux-enforcing&lt;/code&gt; the human addressable identifier would be &lt;code&gt;postgresql_selinux-enforcing&lt;/code&gt; or even &lt;code&gt;postgresql-00001&lt;/code&gt;. In the company that I work for, we already have a taxonomy for services and a decision to use numerical identifiers on the configuration baseline rules.&lt;/p&gt;
&lt;p&gt;Each rule should be properly described, documenting what the rule is for. In case of a ranged value, it should also document how this range can be properly applied. For instance, if the number of worker threads is based on the number of cores available in the system, document the formula.&lt;/p&gt;
&lt;p&gt;Each rule should also document the risk that it wants to mitigate (be it a security risk, or a manageability aspect of the service, or a performance related tuning parameter). This aspect of the baseline is important whenever an implementation wants an exception to the rule (not follow it) or a deviation (different value). Personally, to make sure that the baseline is manageable, I don't expect engineers to immediately fill in the risk in great detail, but rather holistically. The actual risk determination is then only done when an implementation wants an exception or deviation, and then includes a list of potential mitigating actions to take. This way, a 300+ rule document does not require all 300+ rules to have a risk determination, especially if only a dozen or so rules have exceptions or deviations in the organization.&lt;/p&gt;
&lt;p&gt;Each rule should have sources linked to it. These sources help the reader understand what the rule is based on, such as a publicly available secure configuration baseline, an audit recommendation, a specific incident, etc. If the rule is also controversial, it might benefit from links to meeting minutes.&lt;/p&gt;
&lt;p&gt;Each rule might have consequences listed as well. These are known changes or behavior aspects that follow the implementation of the rule. For instance, a rule might state that TLS mutual authentication is mandatory, and the consequence is that all interacting clients must have a properly defined certificate (so proper PKI requirements) as well as client registration in the application.&lt;/p&gt;
&lt;p&gt;Finally, and importantly as well, each rule identifies the scope at which exceptions or deviations can be granted. For smaller groups and organizations, this might not matter that much, but for larger organizations, some configuration baseline rules can be "approved" by a small team or by the application owner, while others need formal advise of a security officer and approval on a decision body.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding a balanced approval hierarchy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The exception management for configuration baselines should not be underestimated. It is not viable to have all settings handled by top management decision bodies, but some configuration changes might result in such a huge impact that a formal decision needs to be taken somewhere, with proper accountability assigned (yes, this is the architect in me speaking).&lt;/p&gt;
&lt;p&gt;Rather than attempting to create a match for all rules, I again like to keep the decision here in the middle, just like I do with the risk determination. The maintainer of the configuration baseline can leave the "scope" of a rule open, and have an intermediate decision body as the main decision body. Whenever an exception or deviation is asked, the risk determination is made and filled in, and with this documented rule now complete a waiver is asked on the decision body. Together with the waiver request, the maintainer also asks this decision body if the rule in the future also needs to be granted on that decision body or elsewhere.&lt;/p&gt;
&lt;p&gt;The scope is most likely tied to the impact of the rule towards other services. A performance specific rule that only affects the application hosted on the technology can be easily scoped as being application-only. This means that the application or service owner can decide to deviate from the baseline. A waiver for a rule that influences system behavior might need to be granted by the system administrator (or team) as well as application or service owners that use this system. Following this logic, I generally use the following scope terminology:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tbd (to be determined), meaning that there is no assessment done yet&lt;/li&gt;
&lt;li&gt;application, meaning that the impact is only on a single application and thus can be taken by the application owner&lt;/li&gt;
&lt;li&gt;instance, meaning that the impact is on an instance and thus might be broader than a single application, but is otherwise contained to the technology. Waivers are granted by the responsible system administrator and application owner(s)&lt;/li&gt;
&lt;li&gt;system, meaning that the impact is on the entire system and thus goes beyond the technology. Waivers are granted by the responsible system administrator, application owner(s) and with advise from a security officer&lt;/li&gt;
&lt;li&gt;network, meaning that the impact can spread to other systems or influence behavior of other systems, but remains technical in nature. Waivers are granted by an infrastructure architecture board with advise from a security officer&lt;/li&gt;
&lt;li&gt;organization, meaning that the impact goes beyond technical influence but also impacts business processes. Waivers are granted by an architecture board with advise from a security officer and senior service owner, and might even be redirected to a higher management board.&lt;/li&gt;
&lt;li&gt;group, meaning that the impact influences multiple businesses. Waivers are granted by a specific management board&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each scope also has a "-pending" value, so "network-pending". This means that the owner of the configuration baseline suggests that this is the scope on which waivers can be established, but still needs to receive formal validation.&lt;/p&gt;
&lt;p&gt;The main decision body is then a particular infrastructure architecture board, which will redirect requests to other decision bodies if the scope goes beyond what that architecture board handles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Architectural settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first chapter in a baseline is perhaps the more controversial one, as it is not a technical setting and hard to validate. However, in my experience, tying architectural constraints in a configuration baseline is much more efficient than having a separate track for a number of reasons.&lt;/p&gt;
&lt;p&gt;For one, I strongly believe that architecture deviations are like configuration deviations. They should be documented similarly, and follow the same path as configuration baseline deviations. The scope off architectural rules are also all over the place, from application-level impact up to organization-wide.&lt;/p&gt;
&lt;p&gt;Furthermore, architectural positioning of services should not be solely an (infrastructure) architecture concern, but supported by the other stakeholders as well, and especially the responsible for the technology service.&lt;/p&gt;
&lt;p&gt;For instance, a rule could be that no databases should be positioned within an organizations &lt;em&gt;DeMilitarized Zone (DMZ)&lt;/em&gt;, which is a network design that shields off internally positioned services from the outside world. Although this is not a configuration setting, it makes sense to place it in the configuration baseline of the database technology. There are several ways to validate automatically if this rule is followed, depending for instance the organization IP plan.&lt;/p&gt;
&lt;p&gt;Another rule could be that web applications that host browser-based applications should only be linked through a reverse proxy, or that a load balancer must be put in front of an application server, etc. This might result in additional rules in the chapter that covers access control as well (such as having a particular IP filter in place), but these rules are the consequence of the architectural positioning of the service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operating system and services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The second chapter covers settings specific to the operating system on which the technology is deployed. Such settings can be system-wide settings like Linux' sysctl parameters, services which need to be enabled or disabled when the technology is deployed, and deviations from the configuration baseline of the operating system.&lt;/p&gt;
&lt;p&gt;An example of the latter depends of course on the configuration baseline of the operating system (assuming this is a baseline for a technology deployed on top of an operating system, it could very well be a different platform). Suppose for instance that the baseline has the &lt;code&gt;squashfs&lt;/code&gt; kernel module disabled, but the technology itself requires squashfs, then a waiver is needed. This is the level where this is documented.&lt;/p&gt;
&lt;p&gt;Another setting could be an extension of the SSH configuration (the term "services" in the chapter title here focuses on system services, such as OpenSSH), or the implementation of additional audit rules on OS-level (although auditing can also be covered in a different section).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software deployment and file system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The third chapter focuses on the installation of the technology itself, and the file system requirements related to the technology service.&lt;/p&gt;
&lt;p&gt;Rules here look into file ownership and permissions, mount settings, and file system declarations. Some baselines might even define rules about integrity of certain files (the &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt; supports checksum-based validations) although I think this is better tackled through a specific integrity process. Still, if such an integrity process does not exist and automated validation of baselines is implemented, then integrity validation of critical files could be in scope.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technical service settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the fourth chapter, settings are declared regarding the service without being service-specific. A service-specific setting is one that requires functional knowledge of the service, whereas technical service settings can be interpreted without functionally understanding the technology at hand.&lt;/p&gt;
&lt;p&gt;Let's take PostgreSQL as an example. A service-specific setting would be the maximum number of non-frozen transaction IDs before a VACUUM operation is triggered (the &lt;code&gt;autovacuum_freeze_max_age&lt;/code&gt; parameter). If you are not working with PostgreSQL much, then this makes as much sense as &lt;a href="https://en.wikipedia.org/wiki/Prisencolinensinainciusol"&gt;Prisencolinensinainciusol&lt;/a&gt;. It sounds like English, but that's about as far as you get.&lt;/p&gt;
&lt;p&gt;A technical service setting on PostgreSQL that is likely more understandable is the runtime account under which the database runs (you don't want it to run as root), or the TCP port on which it listens. Although both are technical in nature, they're much more understandable for others and, perhaps the most important reason of all, often more reusable in deployments across technologies.&lt;/p&gt;
&lt;p&gt;This reusability is key for larger organizations as they will have numerous technologies to support, and the technical service settings offer a good baseline for initial secure setup. They focus on the runtime account of the service, the privileges of the runtime account (be it capability-based on Linux or account rights on Windows), the interfaces on which the service is reachable, the protocol or protocols it supports, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authentication, authorization, access control and auditing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next chapter focuses on the &lt;em&gt;Authentication, Authorization and Accounting (AAA)&lt;/em&gt; services, but slightly worded differently (AAA is commonly used in networking related setups, I just borrow it and extend it). If the configuration baseline is extensive, then it might make sense to have separate sections for each of these security concepts.&lt;/p&gt;
&lt;p&gt;Some technologies have a strong focus on user management as well. In that case, it might make sense to first describe the various types of users that the technology supports (like regular users, machine users, internal service users, shared users, etc.) and then, per user type, document how these security services act on it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service specific settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next chapter covers settings that are very specific to the service. These are often the settings that are found in the best practices documentation, secure deployment instructions of the vendor, performance tuning parameters, etc.&lt;/p&gt;
&lt;p&gt;I tend to look first at the base configuration and administration guides for technologies, and see what the main structure is that those documents follow. Often, this can be borrowed for the configuration baseline. Next, consider performance related tuning, as that is often service specific and not related to the other chapters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cryptographic services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this chapter, the focus is on the cryptographic services and configuration.&lt;/p&gt;
&lt;p&gt;The most well-known example here is related to any TLS configuration and tuning. Whereas the location of the private key (used for TLS services) is generally mentioned in the third chapter (or at least the secure storage of the private key), this section will focus on using this properly. It looks at selecting proper TLS version, making a decent and manageable set of ciphers to support, enabling &lt;em&gt;Online Certificate Status Protocol (OCSP)&lt;/em&gt; on web servers, etc.&lt;/p&gt;
&lt;p&gt;But services often use cryptographic related algorithms in various other places as well. Databases can provide transparent data file encryption to ensure that offline access to the database files does not result in data leakage for instance. Or they implement column-level encryption.&lt;/p&gt;
&lt;p&gt;Application servers might support crypto related routines to the applications they host, and the configuration baseline can then identify which crypto modules are supported and which ones aren't.&lt;/p&gt;
&lt;p&gt;Services might be using cryptographic hashes which are configurable, or could be storing user passwords in a database using configurable settings. OpenLDAP for instance supports multiple hashing methods (and also supports storing in plain-text if you want this), so it makes sense to select a hashing method that is hard to brute-force (slow to compute for instance) and is salted (to make certain types of attacks more challenging).&lt;/p&gt;
&lt;p&gt;If the service makes use of stored credentials or keytabs, document how they are protected here as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data and information handling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Information handling covers both the regular data management activities (like backup/restore, data retention, archival, etc.) as well as sensitive information handling (to comply with privacy rules).&lt;/p&gt;
&lt;p&gt;The regular data management related settings look into both the end user data handling (as far as this is infrastructurally related - this isn't meant to become a secure development guide) as well as service-internal data handling. When the technology is meant to handle data (like a database or LDAP) then certain related settings could be both in the service specific settings chapter or in this one. Personally, I tend to prefer that technology-specific and non-reusable settings are in the former, while the data and information handling chapter covers the integration and technology-agnostic data handling.&lt;/p&gt;
&lt;p&gt;If the service handles sensitive information, it is very likely that additional constraints or requirements were put in place beyond the "traditional" cryptographic requirements. Although such requirements are often implemented on the application level (like tagging the data properly and then, based on the tags, handle specific fine-grained access controls, archival and data retention), more and more technologies provide out-of-the-box (or at least reusable) methods that can be configured.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An XCCDF template&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To support the above structure, I've made an &lt;a href="https://blog.siphos.be/static/2018/xccdf-template.xml"&gt;XCCDF template&lt;/a&gt; that might be a good start for documenting the configuration baseline of a technology. It also structures the chapters a bit more with various sections, but those are definitely not mandatory to use as it strongly depends on the technology being documented, the maturity of the organization, etc.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Documenting configuration changes</title><link href="https://blog.siphos.be/2018/01/documenting-configuration-changes/" rel="alternate"></link><published>2018-01-07T21:20:00+01:00</published><updated>2018-01-07T21:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-07:/2018/01/documenting-configuration-changes/</id><summary type="html">&lt;p&gt;IT teams are continuously under pressure to set up and maintain infrastructure services quickly, efficiently and securely. As an infrastructure architect, my main concerns are related to the manageability of these services and the secure setup. And within those realms, a properly documented configuration setup is in my opinion very crucial.&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm going to look into using the &lt;em&gt;Extensible Configuration Checklist Description Format (XCCDF)&lt;/em&gt; as the way to document these. This first post is an introduction to XCCDF functionally, and what I position it for.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT teams are continuously under pressure to set up and maintain infrastructure services quickly, efficiently and securely. As an infrastructure architect, my main concerns are related to the manageability of these services and the secure setup. And within those realms, a properly documented configuration setup is in my opinion very crucial.&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm going to look into using the &lt;em&gt;Extensible Configuration Checklist Description Format (XCCDF)&lt;/em&gt; as the way to document these. This first post is an introduction to XCCDF functionally, and what I position it for.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Documentation is a good thing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the ongoing struggle for time and resources, documenting configurations and architectures is often not top-of-mind. However, the lack of this information also leads to various problems: incidents due to misconfiguration, slow recovery timings due to incomprehensible setups, and not to forget: meetings. Yes, meetings, which are continuously discussing service aspects that influence one or more parameters, without any good traceability of past decisions.&lt;/p&gt;
&lt;p&gt;Some technologies allow to keep track of some metadata regarding to configurations. In configuration management tools like &lt;a href="https://puppet.com"&gt;Puppet&lt;/a&gt; or &lt;a href="https://saltstack.com"&gt;Saltstack&lt;/a&gt; engineers define the target state of their infrastructure, and the configuration management tool enforces this state on the service. Engineers can add in historical information as comments into these systems, and use version control on the files to have traceability of the settings.&lt;/p&gt;
&lt;p&gt;However, although in-line comments are very important, even for configuration sets, it is not a full documentation approach. In larger environments, where you are regularly audited for quality and security, or where multiple roles and stakeholders need to understand the settings and configuration of services, pointing to the code is not going to cut it.&lt;/p&gt;
&lt;p&gt;Configuration items need to be documented not solely with the documentation rule itself, but with the motivation related to it, and additional fields of interest depending on how the organization deals with it. This documentation can then be referred to from the configuration management infrastructure (so engineers and technical stakeholders can trace back settings) but also vice-versa: the documentation can refer to the configuration management implementation (so other stakeholders can deduce how the settings are implemented or even enforced).&lt;/p&gt;
&lt;p&gt;With a proper configuration document at hand, especially if it is supported through the configuration management tool(s) in the organization (regardless if it is one or multiple), it is much easier to have the necessary interviews with auditors, project leaders, functional and technical analysts, architects or even remote support teams.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Two-part documentation hierarchy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing to decide upon is at which level a team will document the settings. Is a single document possible for all infrastructure services? Most likely not. I believe that settings should be documented on the technology level (as it is specific to a particular technology) and on the 'business service' level (as it is specific to a particular implementation).&lt;/p&gt;
&lt;p&gt;On the technology level, we're talking about configuration documentation for "PostgreSQL", "Apache Knox" or "Nginx". At this level, the baseline is defined for a technology. The resulting document is then the &lt;em&gt;configuration baseline&lt;/em&gt; for that component.&lt;/p&gt;
&lt;p&gt;On the business service level, we're talking about configuration documentation for a particular service that is a combination of multiple implementations. For instance, a company intranet portal service is operationally implemented through a reverse proxy (HAProxy), an intelligent load balancer (Seesaw), next-gen firewall (pfSense), web server (Nginx), application server (Node.js), database (PostgreSQL), and operating systems (Linux). And more technologies come into play when we consider software deployment, monitoring, backup/restore, software-defined network, storage provisioning, archival solutions, license management services, etc.&lt;/p&gt;
&lt;p&gt;Hence, a configuration document should be available on this service level ("company intranet portal") which defines the usage profile of a service (more about that later) and the specific parameters related to this service, but only when they either deviate from the configuration baseline, or take a particular value within a range defined in the configuration baseline. This document is the &lt;em&gt;service technical configuration&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, as an example, on the Nginx configuration baseline, a rule might state that the maximum file size per upload is 12M (through the &lt;code&gt;client_max_body_size&lt;/code&gt; parameter). If the service has no problem with this rule, then it does not need to be documented on the service technical configuration. However, if this needs to be adapted (say that for the company portal the maximum file size is 64M) then it is documented.&lt;/p&gt;
&lt;p&gt;Another example is a ranged setting, where the baseline identifies a set of valid values and the service technical configuration makes a particular selection. For instance, the Nginx configuration baseline might mention that there must be between 5 and 50 worker processes (through the &lt;code&gt;worker_processes&lt;/code&gt; parameter). In the service technical configuration the particular value is then selected.&lt;/p&gt;
&lt;p&gt;From an architecture and security point of view, the first example is a deviation which must consider the risks and consequences that are applicable to the rule. These are (or should be) documented in the configuration baseline, including where this deviation can be approved (assuming the organization has a decision body for such things). The second example is not a deviation and, as such, is free to be chosen by the implementation team. The configuration baseline will generally inform the implementation teams about how to pick a proper value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service usage profiles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've talked about a &lt;em&gt;service usage profile&lt;/em&gt; earlier up, but didn't expand on it yet. So, what are these service usage profiles?&lt;/p&gt;
&lt;p&gt;Well, most technologies can be implemented for a number of targets and functional purposes. A database could be implemented as a dedicated service (one set of databases on a dedicated set of instances for a single business service) or a shared service (multiple databases, possibly on multiple instances for several business services). It can be tuned for online transactional purposes (OLTP) or online analytical processing (OLAP), often through data warehouse designs.&lt;/p&gt;
&lt;p&gt;A service usage profile is part of the configuration baseline, with settings specific to that particular usage. So for a database the engineering team responsible for the database technology setup might devise that the following usage profiles are applicable to their component:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dedicated OLTP&lt;/li&gt;
&lt;li&gt;Shared OLTP&lt;/li&gt;
&lt;li&gt;Dedicated DWH&lt;/li&gt;
&lt;li&gt;Shared DWH&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each usage profile has a number of configuration settings (of which many, if not most, are shared across other usage profiles) and a range of valid values (fine-tuning for a service). The service technical configuration for a particular business service then selects the particular usage profile. For instance, the company intranet portal might use a Dedicated OLTP usage profile for its PostgreSQL database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How XCCDF supports this structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Until now, I've only spoken about the values related to configuration documentation, and a high-level introduction to the hierarchy on the configurations. But how does the &lt;a href="https://scap.nist.gov/specifications/xccdf/"&gt;Extensible Configuration Checklist Description&lt;/a&gt; position itself in this?&lt;/p&gt;
&lt;p&gt;A number of reasons why XCCDF is a valid choice for configuration documentation are given next.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XCCDF allows technical writers to write the documentation in (basic) HTML while still linking the documentation to specific rules. Rather than having to use a tabular expression on all the valid configuration sets (like using a large spreadsheet table for all rules) and trying to force some documentation in it (Excel is not a text editor), XCCDF uses a hierarchical approach to structure documentation in logical sections (which it calls &lt;em&gt;Groups&lt;/em&gt;) and then refers to the rules applicable to that section (using the &lt;em&gt;Rule&lt;/em&gt; identifier).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF has out-of-the-box support for service profiles (through &lt;em&gt;Profile&lt;/em&gt; declarations). Fine-tuning and selecting profiles is called &lt;em&gt;tailoring&lt;/em&gt; in XCCDF. This also includes support for ranged values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is meant to (but does not have to) refer to the (automated or interview-based) validation of the rules as well. Automated validation of settings means that an engine can read the XCCDF document (and the referred statements) and check if an implementation adheres to the baseline. The standard for this is called &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, and a popular free software engine for this is &lt;a href="https://www.open-scap.org"&gt;OpenSCAP&lt;/a&gt;. The standard for interview-based validation is &lt;em&gt;Open Checklist Interactive Language (OCIL)&lt;/em&gt;. I have not played around with OCIL and supporting tooling, so comments on this are always welcome.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is an XML-based format, so its "source code" can easily be versioned in common version control systems like Git. This allows organizations to not only track changes on the documentation, but also have an active development lifecycle management on the configuration documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDFs schema implies a set of metadata to be defined during various declarations. It includes support for the &lt;a href="http://www.dublincore.org/specifications/"&gt;Dublin core metadata&lt;/a&gt; terms for content, references to link other resources structurally, and most importantly has a wide set of supporting entities for rules (which is the level on which configuration items are documented). This includes the rationale (why is the rule defined as is), fix text (human readable), fix (machine readable), rule role (is it security-sensitive and as such must be taken up in a security assessment report or not), severity (how bad is it if this rule is not followed), and many more. This both forces the user to consider the consequences of the rule, as well as guide the writer into properly structured documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF also suggests a number of classes for the documentation to standardize certain information types. This includes warnings, critical text, examples, and instructions. Such semantic declarations allow for a more uniform set of documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, a few constraints exist that you need to be aware of when approaching XCCDF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is an XML-based document format, and although NIST offers the necessary XML Schema definitions, writing proper XML has always been a challenge for many people. Also, no decent GUI or WYSIWYG tool that manages XCCDF files exists in my opinion. Yes, we have the &lt;a href="https://www.open-scap.org/tools/scap-workbench/"&gt;SCAP Workbench&lt;/a&gt; and the &lt;a href="https://www.g2-inc.com/scap.html"&gt;eSCAPe editor&lt;/a&gt;, but I feel that they are not as effective as they should be. As a result, the team or teams that write the baselines should either be XML-savvy, or you need to provide supporting infrastructure and services for it. However, YMMV.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the organization is not interested in compliance checks themselves (i.e. automated validation of adherence to the configuration baseline and service technical configuration) then XCCDF will entail too much overhead versus just having a template or approach (such as documenting items in a wiki). However, with some support (and perhaps automation) writing and maintaining XCCDF based configuration baselines becomes much easier.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past I've &lt;a href="http://blog.siphos.be/tag/xccdf/"&gt;blogged about XCCDF&lt;/a&gt; already, but that was with a previous blog technology and the migration wasn't as successful as I originally thought. XML snippets were all removed, and I'm too lazy to go back to my backups from 2013 and individually correct blogs.&lt;/p&gt;
&lt;p&gt;A good resource on XCCDF is the &lt;a href="https://csrc.nist.gov/CSRC/media/Publications/nistir/7275/rev-4/final/documents/nistir-7275r4_updated-march-2012_clean.pdf"&gt;NIST IR-7275 publication (PDF)&lt;/a&gt; which covers the XCCDF standard in much detail.&lt;/p&gt;
&lt;p&gt;The Center for Internet Security (CISecurity) maintains more than a hundred &lt;a href="https://www.cisecurity.org/cis-benchmarks/"&gt;CIS Benchmarks&lt;/a&gt;, all available for free as PDFs, and are often based on XCCDF (available to subscribed members).&lt;/p&gt;
&lt;p&gt;In the next blog post, I'll talk about the in-document structure of a good configuration baseline.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>SELinux and extended permissions</title><link href="https://blog.siphos.be/2017/11/selinux-and-extended-permissions/" rel="alternate"></link><published>2017-11-20T17:00:00+01:00</published><updated>2017-11-20T17:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-11-20:/2017/11/selinux-and-extended-permissions/</id><summary type="html">&lt;p&gt;One of the features present in the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;August release&lt;/a&gt; of the SELinux user space is its support for ioctl xperm rules in modular policies. In the past, this was only possible in monolithic ones (and CIL). Through this, allow rules can be extended to not only cover source (domain) and target (resource) identifiers, but also a specific number on which it applies. And ioctl's are the first (and currently only) permission on which this is implemented.&lt;/p&gt;
&lt;p&gt;Note that ioctl-level permission controls isn't a new feature by itself, but the fact that it can be used in modular policies is.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;One of the features present in the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;August release&lt;/a&gt; of the SELinux user space is its support for ioctl xperm rules in modular policies. In the past, this was only possible in monolithic ones (and CIL). Through this, allow rules can be extended to not only cover source (domain) and target (resource) identifiers, but also a specific number on which it applies. And ioctl's are the first (and currently only) permission on which this is implemented.&lt;/p&gt;
&lt;p&gt;Note that ioctl-level permission controls isn't a new feature by itself, but the fact that it can be used in modular policies is.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is ioctl?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many interactions on a Linux system are done through system calls. From a security perspective, most system calls can be properly categorized based on who is executing the call and what the target of the call is. For instance, the unlink() system call has the following prototype:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;int unlink(const char *pathname);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Considering that a process (source) is executing unlink (system call) against a target (path) is sufficient for most security implementations. Either the source has the permission to unlink that file or directory, or it hasn't. SELinux maps this to the unlink permission within the file or directory classes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : { file dir }  unlink;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, &lt;code&gt;ioctl()&lt;/code&gt; is somewhat different. It is a system call that allows device-specific operations which cannot be expressed by regular system calls. Devices can have multiple functions/capabilities, and with &lt;code&gt;ioctl()&lt;/code&gt; these capabilities can be interrogated or updated. It has the following interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;int ioctl(int fd, unsigned long request, ...);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The file descriptor is the target device on which an operation is launched. The second argument is the request, which is an integer whose value identifiers what kind of operation the &lt;code&gt;ioctl()&lt;/code&gt; call is trying to execute. So unlike regular system calls, where the operation itself is the system call, &lt;code&gt;ioctl()&lt;/code&gt; actually has a parameter that identifies this.&lt;/p&gt;
&lt;p&gt;A list of possible parameter values on a socket for instance is available in the Linux kernel source code, under &lt;a href="https://elixir.free-electrons.com/linux/latest/source/include/uapi/linux/sockios.h"&gt;include/uapi/linnux/sockios.h&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux allowxperm&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For SELinux, having the purpose of the call as part of a parameter means that a regular mapping isn't sufficient. Allowing &lt;code&gt;ioctl()&lt;/code&gt; commands for a domain against a resource is expressed as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This of course does not allow policy developers to differentiate between harmless or informative calls (like SIOCGIFHWADDR to obtain the hardware address associated with a network device) and impactful calls (like SIOCADDRT to add a routing table entry).&lt;/p&gt;
&lt;p&gt;To allow for a fine-grained policy approach, the SELinux developers introduced an extended allow permission, which is capable of differentiating based on an integer value.&lt;/p&gt;
&lt;p&gt;For instance, to allow a domain to get a hardware address (SIOCGIFHWADDR, which is 0x8927) from a TCP socket:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : tcp_socket ioctl 0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This additional parameter can also be ranged:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl 0x8910-0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And of course, it can also be used to complement (i.e. allow all ioctl parameters except a certain value):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl ~0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Small or negligible performance hit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;According to a &lt;a href="http://kernsec.org/files/lss2015/vanderstoep.pdf"&gt;presentation given by Jeff Vander Stoep&lt;/a&gt; on the Linux Security Summit in 2015, the performance impact of this addition in SELinux is well under control, which helped in the introduction of this capability in the Android SELinux implementation.&lt;/p&gt;
&lt;p&gt;As a result, interested readers can find examples of allowxperm invocations in the SELinux policy in Android, such as in the &lt;a href="https://android.googlesource.com/platform/system/sepolicy/+/master/private/app.te"&gt;app.te&lt;/a&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# only allow unprivileged socket ioctl commands
allowxperm { appdomain -bluetooth } self:{ rawip_socket tcp_socket udp_socket } ioctl { unpriv_sock_ioctls unpriv_tty_ioctls };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with that, we again show how fine-grained the SELinux access controls can be.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="ioctl"></category></entry><entry><title>SELinux Userspace 2.7</title><link href="https://blog.siphos.be/2017/09/selinux-userspace-2.7/" rel="alternate"></link><published>2017-09-26T14:50:00+02:00</published><updated>2017-09-26T14:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-09-26:/2017/09/selinux-userspace-2.7/</id><summary type="html">&lt;p&gt;A few days ago, &lt;a href="http://blog.perfinion.com/"&gt;Jason "perfinion" Zaman&lt;/a&gt; stabilized the 2.7 SELinux userspace on
Gentoo. This release has quite a &lt;a href="https://raw.githubusercontent.com/wiki/SELinuxProject/selinux/files/releases/20170804/RELEASE-20170804.txt"&gt;few new features&lt;/a&gt;, which I'll cover in later
posts, but for distribution packagers the main change is that the userspace
now has many more components to package. The project has split up the
policycoreutils package in separate packages so that deployments can be made
more specific.&lt;/p&gt;
&lt;p&gt;Let's take a look at all the various userspace packages again, learn what their
purpose is, so that you can decide if they're needed or not on a system. Also,
when I cover the contents of a package, be aware that it is based on the deployment
on my system, which might or might not be a complete installation (as with Gentoo,
different USE flags can trigger different package deployments).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago, &lt;a href="http://blog.perfinion.com/"&gt;Jason "perfinion" Zaman&lt;/a&gt; stabilized the 2.7 SELinux userspace on
Gentoo. This release has quite a &lt;a href="https://raw.githubusercontent.com/wiki/SELinuxProject/selinux/files/releases/20170804/RELEASE-20170804.txt"&gt;few new features&lt;/a&gt;, which I'll cover in later
posts, but for distribution packagers the main change is that the userspace
now has many more components to package. The project has split up the
policycoreutils package in separate packages so that deployments can be made
more specific.&lt;/p&gt;
&lt;p&gt;Let's take a look at all the various userspace packages again, learn what their
purpose is, so that you can decide if they're needed or not on a system. Also,
when I cover the contents of a package, be aware that it is based on the deployment
on my system, which might or might not be a complete installation (as with Gentoo,
different USE flags can trigger different package deployments).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;libsepol - manipulating SELinux binary policies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first package, known in Gentoo as &lt;code&gt;sys-libs/libsepol&lt;/code&gt;, is the library that
enables manipulating the SELinux binary policies. This is a core library, and is
the first SELinux userspace package that is installed on a system.&lt;/p&gt;
&lt;p&gt;It contains one command, &lt;code&gt;chkcon&lt;/code&gt;, which allows users to validate if a specific
security context exists within a binary policy file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ chkcon policy.29 user_u:user_r:mozilla_t:s0
user_u:user_r:mozilla_t:s0 is valid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The package does contain two manpages of old commands which are no longer available
(or I'm blind, either way, they're not installed and not found in the SELinux userspace
repository either) such as genpolusers and genpolbools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;libselinux - the main SELinux handling library&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The libselinux library, known in Gentoo as &lt;code&gt;sys-libs/libselinux&lt;/code&gt;, is the main SELinux
library. Almost all applications that are SELinux-aware (meaning they not only know SELinux
is a thing, but are actively modifying their behavior with SELinux-specific code) will
link to libselinux.&lt;/p&gt;
&lt;p&gt;Because it is so core, the package also provides the necessary bindings for different
scripting languages besides the standard shared objects approach, namely Python (as
many SELinux related tooling is written in Python) and Ruby.&lt;/p&gt;
&lt;p&gt;Next to the bindings and libraries, libselinux also offers quite a few executables
to query and manipulate SELinux settings on the system, which are shortly described
on the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Tools"&gt;SELinux userspace wiki&lt;/a&gt; but repeated here for convenience. Most of these
are meant for debugging purposes, as they are simple wrappers toward the libselinux
provided functions, but some of them are often used by administrations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;avcstat&lt;/code&gt; gives statistics about the in-kernel access vector cache, such as number
  of lookups, hits and misses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_create&lt;/code&gt; queries the kernel security server for a transition decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_av&lt;/code&gt; queries the kernel security server for an access vector decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_relabel&lt;/code&gt; queries the kernel security server for a relabel decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_member&lt;/code&gt; queries the kernel security server for a labeling decision on a
  polyinstantiated object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getconlist&lt;/code&gt; uses the &lt;code&gt;security\_compute\_user()&lt;/code&gt; function, and orders the resulting
  list based on the &lt;code&gt;default\_contexts&lt;/code&gt; file and per-user context files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getdefaultcon&lt;/code&gt; is like &lt;code&gt;getconlist&lt;/code&gt; but only returns the first context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_user&lt;/code&gt; queries the kernel security server fo a set of reachable user contexts
  from a source context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getfilecon&lt;/code&gt; gets the context of a file by path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getpidcon&lt;/code&gt; gets the context of a process by PID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getseuser&lt;/code&gt; queries the &lt;code&gt;seuser&lt;/code&gt; file for the resulting SELinux user and contxt for a
  particular linux login and login context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getsebool&lt;/code&gt; gets the current state of a SELinux boolean in the SELinux security server&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchpathcon&lt;/code&gt; queries the active filecontext file for how a particular path should
  be labeled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;policyvers&lt;/code&gt; queries the kernel security server for the maximum policy version supported&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getenforce&lt;/code&gt; gets the enforcing state of the kernel access vector cache&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sefcontext_compile&lt;/code&gt; generates binary filecontext files, optimized for fast querying&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_lookup&lt;/code&gt; looks up what the target default context is for various classes
  (supporting the X related SELinux types, database types, etc.)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_digest&lt;/code&gt; calculates the SHA1 digest of spec files, and returns a list
  of the specfiles used to calculate the digest. This is used by Android.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_partial_match&lt;/code&gt; determines if a direct or partial match is possible
  on a file path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_lookup_best_match&lt;/code&gt; obtains the best matching SELinux security context
  for file-based operations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinux_check_securetty_context&lt;/code&gt; checks whether a SELinux tty security context
  is defined as a securetty context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinux_check_access&lt;/code&gt; checks if the source context has the access permission
  for the specified class on the target context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxexeccon&lt;/code&gt; reports the SELinux context for an executable&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxenabled&lt;/code&gt; returns if SELinux is enabled or not&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setfilecon&lt;/code&gt; sets the context of a path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setenforce&lt;/code&gt; sets the enforcing state of the kernel access vector cache&lt;/li&gt;
&lt;li&gt;&lt;code&gt;togglesebool&lt;/code&gt; toggles a SELinux boolean, but only runtime (so it does not
  persist across reboots)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;checkpolicy - policy compiler&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The checkpolicy package, known in Gentoo as &lt;code&gt;sys-apps/checkpolicy&lt;/code&gt;, provides two
main applications, &lt;code&gt;checkpolicy&lt;/code&gt; and &lt;code&gt;checkmodule&lt;/code&gt;. Both applications are compilers
(unlike what the name implies) which build a binary SELinux policy. The main difference
between these two is that one builds a policy binary, whereas the other one builds a 
SELinux module binary.&lt;/p&gt;
&lt;p&gt;Developers don't often call these applications themselves, but use the build scripts.
For instance, the &lt;code&gt;semodule_package&lt;/code&gt; binary would be used to combine the binary policy
with additional files such as file contexts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;libsemanage - facilitating use of SELinux overall&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The libsemanage library, known in Gentoo as &lt;code&gt;sys-libs/libsemanage&lt;/code&gt;, contains SELinux
supporting functions that are needed for any regular SELinux use. Whereas libselinux
would be used everywhere, even for embedded systems, libsemanage is generally not for
embedded systems but is very important for Linux systems in overall.&lt;/p&gt;
&lt;p&gt;Most SELinux management applications that administrators come in contact with will be
linked with the libsemanage library. As can be expected, the &lt;code&gt;semanage&lt;/code&gt; application
as offered by the &lt;code&gt;selinux-python&lt;/code&gt; package is one of them.&lt;/p&gt;
&lt;p&gt;The only application that is provided by libsemanage is the &lt;code&gt;semanage_migrate_store&lt;/code&gt;,
used to migrate the policy store from the &lt;code&gt;/etc/selinux&lt;/code&gt; to the &lt;code&gt;/var/lib/selinux&lt;/code&gt;
location. This was done with the introduction of the 2.4 userspace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-python - Python-based command-line management utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-python package, known in Gentoo as &lt;code&gt;sys-apps/selinux-python&lt;/code&gt;, is one of
the split packages that originally where part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. It
contains the majority of management utilities that administrators use for handling
SELinux on their systems.&lt;/p&gt;
&lt;p&gt;The most known application here is &lt;code&gt;semanage&lt;/code&gt;, but it contains quite a few others
as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sepolgen&lt;/code&gt; generates an initial SELinux policy module template, and is short for
  the &lt;code&gt;sepolicy generate&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;&lt;code&gt;audit2why&lt;/code&gt; translates SELinux audit messages into a description of why the access
  was denied. It is short for the &lt;code&gt;audit2allow -w&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;audit2allow&lt;/code&gt; generates SELinux policy allow/dontaudit rules from logs of denied
  operations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolgen-ifgen&lt;/code&gt; generates an overview of available interfaces. This overview is used
  by &lt;code&gt;audit2allow&lt;/code&gt; to guess the right interface to use when allowing or dontauditing certain
  operations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolicy&lt;/code&gt; is the SELinux policy inspection tool, allowing to query various aspects of
  a SELinux configuration (namely booleans, communication flows, interfaces, network information
  and transition information). It also provides the ability to generate skeleton policies (as
  described with &lt;code&gt;sepolgen&lt;/code&gt;) and manual pages.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chcat&lt;/code&gt; changes a file's SELinux security category&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolgen-ifgen-attr-helper&lt;/code&gt; generates an overview of attributes and attribute mappings.
  This overview is used by &lt;code&gt;audit2allow&lt;/code&gt; to guess the right attribute to use when allowing
  or dontauditing certain operations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semanage&lt;/code&gt; is a SELinux policy management tool, allowing a multitude of operations
  against the SELinux policy and the configuration. This includes definition import/export,
  login mappings, user definitions, ports and interface management, module handling, 
  file contexts, booleans and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;semodule-utils - Developing SELinux modules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The semodule-utils package, known in Gentoo as &lt;code&gt;sys-apps/semodule-utils&lt;/code&gt;, is another split package
that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. In it, SELinux policy module
development utilities are provided. The package is not needed for basic operations such
as loading and unloading modules though.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;semodule_expand&lt;/code&gt; expands a SELinux base module package into a kernel binary policy file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_deps&lt;/code&gt; shows the dependencies between SELinux policy packages&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_link&lt;/code&gt; links SELinux policy module packages together into a single SELinux policy
  module&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_unpackage&lt;/code&gt; extracts a SELinux module into the binary policy and its associated
  files (such as file context definitions)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_package&lt;/code&gt; combines a modular binary policy file with its associated files (such
  as file context definitions) into a module package&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;mcstrans - Translate context info in human readable names&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The mcstrans package, known in Gentoo as &lt;code&gt;sys-apps/mcstrans&lt;/code&gt;, is another split package
that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. In it, the MCS translation
daemon is hosted. This daemon translates the SELinux-specific context ranges, like 
&lt;code&gt;s0-s0:c0.c1024&lt;/code&gt; to a human-readable set, like &lt;code&gt;SystemLow-SystemHigh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is a purely cosmetic approach (as SELinux internally always uses the sensitivity
and category numbers) but helps when dealing with a large number of separate categories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;restorecond - Automatically resetting file contexts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The restorecond package, known in Gentoo as &lt;code&gt;sys-apps/restorecond&lt;/code&gt;, is another split
package that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. It contains the
&lt;code&gt;restorecond&lt;/code&gt; daemon, which watches over files and directories and forces the right
SELinux label on it.&lt;/p&gt;
&lt;p&gt;This daemon was originally intended to resolve a missing feature in SELinux (having
more fine-grained rules for label naming) but with the named file transition support, the
need for this daemon has diminished a lot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;secilc - SELinux common intermediate language compiler&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The secilc package, known in Gentoo as &lt;code&gt;sys-apps/secilc&lt;/code&gt;, is the CIL compiler which
builds kernel binary policies based on the passed on CIL code. Although the majority
of policy development still uses the more traditional SELinux language (and supporting
macro's from the reference policy), developers can already use CIL code for policy generation.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;secilc&lt;/code&gt;, a final policy file can be generated through the CIL code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-dbus - SELinux DBus server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-dbus package (not packaged in Gentoo at this moment) provides a SELinux DBus
service which systems can use to query and interact with SELinux management utilities
on the system. If installed, the &lt;code&gt;org.selinux&lt;/code&gt; domain is used for various supported
operations (such as listing SELinux modules, through &lt;code&gt;org.selinux.semodule_list&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-gui - Graphical SELinux settings manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-gui package (not packaged in Gentoo at this moment) provides the
&lt;code&gt;system-config-selinux&lt;/code&gt; application which offers basic SELinux management support
in a graphical application. It supports boolean handling, file labeling, user mapping,
SELinux user management, network port definitions and module handling. As such, it can
be seen as the graphical helper utility for the &lt;code&gt;semanage&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-sandbox - Sandbox utility utilizing SELinux sandbox domains&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-sandbox package (not packaged in Gentoo at this moment) is a set of scripts
to facilitate the creation of SELinux sandboxes. With these utilities, which not only
use SELinux sandbox domains like &lt;code&gt;sandbox_t&lt;/code&gt; but also Linux namespaces, end users can
launch applications in a restricted environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;policycoreutils - Core SELinux management utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The policycoreutils package, known in Gentoo as &lt;code&gt;sys-apps/policycoreutils&lt;/code&gt;, contains 
basic SELinux tooling which is necessary to handle SELinux in a regular environment.
Supported utilities are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;newrole&lt;/code&gt; to switch a user session from one role to another&lt;/li&gt;
&lt;li&gt;&lt;code&gt;secon&lt;/code&gt; to query the SELinux context of a file, program or user input&lt;/li&gt;
&lt;li&gt;&lt;code&gt;genhomedircon&lt;/code&gt; to regenerate home directory context files, necessary when new users are
  defined on the system&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setfiles&lt;/code&gt; to set SELinux file security contexts on resources&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule&lt;/code&gt; to list, load and unload SELinux modules&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_init&lt;/code&gt; to launch an init script in the right domain&lt;/li&gt;
&lt;li&gt;&lt;code&gt;open_init_pty&lt;/code&gt; to run a program under a pseudo terminal with the right context set&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sestatus&lt;/code&gt; to query current policy status&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setsebool&lt;/code&gt; to set and, if wanted, persist a SELinux boolean value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxconfig&lt;/code&gt; to display the current active configuration paths&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restorecon&lt;/code&gt; to set SELinux file security contexts on resources&lt;/li&gt;
&lt;li&gt;&lt;code&gt;load_policy&lt;/code&gt; to load the SELinux policy, generally called from initramfs systems if the
  init system is not SELinux-aware&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restorecon_xattr&lt;/code&gt; manages the &lt;code&gt;security.restorecon_last&lt;/code&gt; extended attribute which is set
  by &lt;code&gt;setfiles&lt;/code&gt; or &lt;code&gt;restorecon&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gentoo also adds in two additional scripts:
* &lt;code&gt;rlpkg&lt;/code&gt; to reset file contexts on files provided by a Gentoo package
* &lt;code&gt;selocal&lt;/code&gt; to easily handle small SELinux rule additions to the active policy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There are even more&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attentive readers will notice that the &lt;code&gt;setools&lt;/code&gt; package is not discussed here. This package
is not provided by the SELinux userspace project, but is an important package for SELinux
policy developers as it contains the &lt;code&gt;sesearch&lt;/code&gt; command - an often used command to query
the active policy.&lt;/p&gt;
&lt;p&gt;The above list is thus a picture of the SELinux userspace utilities, which is becoming
quite a big application set now that some functionality is split off from the &lt;code&gt;policycoreutils&lt;/code&gt;
package.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Authenticating with U2F</title><link href="https://blog.siphos.be/2017/09/authenticating-with-u2f/" rel="alternate"></link><published>2017-09-11T18:25:00+02:00</published><updated>2017-09-11T18:25:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-09-11:/2017/09/authenticating-with-u2f/</id><summary type="html">&lt;p&gt;In order to further secure access to my workstation, after the &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switch to Gentoo
sources&lt;/a&gt;, I now enabled two-factor authentication through my Yubico U2F
USB device. Well, at least for local access - remote access through SSH requires
both userid/password as well as the correct SSH key, by &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining authentication
methods in OpenSSH&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enabling U2F on (Gentoo) Linux is fairly easy. The various guides online which talk
about the &lt;code&gt;pam_u2f&lt;/code&gt; setup are indeed correct that it is fairly simple. For completeness
sake, I've documented what I know on the Gentoo Wiki, as the &lt;a href="https://wiki.gentoo.org/wiki/Pam_u2f"&gt;pam_u2f article&lt;/a&gt;.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In order to further secure access to my workstation, after the &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switch to Gentoo
sources&lt;/a&gt;, I now enabled two-factor authentication through my Yubico U2F
USB device. Well, at least for local access - remote access through SSH requires
both userid/password as well as the correct SSH key, by &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining authentication
methods in OpenSSH&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enabling U2F on (Gentoo) Linux is fairly easy. The various guides online which talk
about the &lt;code&gt;pam_u2f&lt;/code&gt; setup are indeed correct that it is fairly simple. For completeness
sake, I've documented what I know on the Gentoo Wiki, as the &lt;a href="https://wiki.gentoo.org/wiki/Pam_u2f"&gt;pam_u2f article&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The setup, basically&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The setup of U2F is done in a number of steps:
1. Validate that the kernel is ready for the USB device
2. Install the PAM module and supporting tools
3. Generate the necessary data elements for each user (keys and such)
4. Configure PAM to require authentication through the U2F key&lt;/p&gt;
&lt;p&gt;For the kernel, the configuration item needed is the raw HID device support.
Now, in current kernels, two settings are available that both talk about
raw HID device support: &lt;code&gt;CONFIG_HIDRAW&lt;/code&gt; is the general raw HID device support,
while &lt;code&gt;CONFIG_USB_HIDDEV&lt;/code&gt; is the USB-specific raw HID device support.&lt;/p&gt;
&lt;p&gt;It is very well possible that only a single one is needed, but both where active
on my kernel configuration already, and Internet sources are not clear which one is
needed, so let's assume for now both are.&lt;/p&gt;
&lt;p&gt;Next, the PAM module needs to be installed. On Gentoo, this is a matter of installing
the &lt;code&gt;pam\_u2f&lt;/code&gt; package, as the necessary dependencies will be pulled in automatically:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge pam_u2f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, for each user, a registration has to be made. This registration is needed for the
U2F components to be able to correctly authenticate the use of a U2F key for a particular
user. This is done with &lt;code&gt;pamu2fcfg&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ pamu2fcfg -u&amp;lt;username&amp;gt; &amp;gt; ~/.config/Yubico/u2f_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The U2F USB key must be plugged in when the command is executed, as a succesful keypress (on the
U2F device) is needed to complete the operation.&lt;/p&gt;
&lt;p&gt;Finally, enable the use of the &lt;code&gt;pam\_u2f&lt;/code&gt; module in PAM. On my system, this is done
through the &lt;code&gt;/etc/pam.d/system-local-login&lt;/code&gt; PAM configuration file used by all
local logon services.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;auth     required     pam_u2f.so
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Consider the problems you might face&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When fiddling with PAM, it is important to keep in mind what could fail. During the setup, it
is recommended to have an open administrative session on the system so that you can validate if
the PAM configuration works, without locking yourself out of the system.&lt;/p&gt;
&lt;p&gt;But other issues need to be considered as well.&lt;/p&gt;
&lt;p&gt;My Yubico U2F USB key might have a high MTBF (Mean Time Between Failures) value, but once it fails,
it would lock me out of my workstation (and even remote services and servers that use it). For
that reason, I own a second one, safely stored, but is a valid key nonetheless for my workstation
and remote systems/services. Given the low cost of a simple U2F key, it is a simple solution for
this threat.&lt;/p&gt;
&lt;p&gt;Another issue that could come up is a malfunction in the PAM module itself. For me, this is handled
by having remote SSH access done without this PAM module (although other PAM modules are still involved,
so a generic PAM failure itself wouldn't resolve this). Of course, worst case, the system needs to be
rebooted in single user mode.&lt;/p&gt;
&lt;p&gt;One issue that I faced was the SELinux policy. Some applications that provide logon services don't have
the proper rights to handle U2F, and because PAM just works in the address space (and thus SELinux
domain) of the application, the necessary privileges need to be added to these services. My initial
investigation revealed the following necessary policy rules (refpolicy-style);&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;udev_search_pids(...)
udev_read_db(...)
dev_rw_generic_usb_dev(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first two rules are needed because the operation to trigger the USB key uses the udev tables
to find out where the key is located/attached, before it interacts with it. This interaction is then
controlled through the first rule.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simple yet effective&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Enabling the U2F authentication on the system is very simple, and gives a higher confidence that
malicious activities through regular accounts will have it somewhat more challenging to switch to
a more privileged session (one control is the SELinux policy of course, but for those domains that
are allowed to switch then the PAM-based authentication is another control), as even evesdropping on
my password (or extracting it from memory) won't suffice to perform a successful authentication.&lt;/p&gt;
&lt;p&gt;If you want to use a different two-factor authentication, check out the use of the &lt;a href="https://wiki.gentoo.org/wiki/Google_Authenticator"&gt;Google
authenticator&lt;/a&gt;, another nice article on the Gentoo wiki. It is also possible to use Yubico keys
for remote authentication, but that uses the OTP (One Time Password) functionality which isn't active
on the Yubico keys that I own.&lt;/p&gt;</content><category term="Security"></category><category term="gentoo"></category><category term="security"></category><category term="yubico"></category><category term="u2f"></category><category term="pam"></category></entry><entry><title>Using nVidia with SELinux</title><link href="https://blog.siphos.be/2017/08/using-nvidia-with-selinux/" rel="alternate"></link><published>2017-08-23T19:04:00+02:00</published><updated>2017-08-23T19:04:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-08-23:/2017/08/using-nvidia-with-selinux/</id><summary type="html">&lt;p&gt;Yesterday I've &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switched to the gentoo-sources kernel package&lt;/a&gt; on Gentoo Linux.
And with that, I also attempted (succesfully) to use the propriatary nvidia drivers
so that I can enjoy both a smoother 3D experience while playing minecraft, as well
as use the CUDA support so I don't need to use cloud-based services for small
exercises.&lt;/p&gt;
&lt;p&gt;The move to nvidia was quite simple, as the &lt;a href="https://wiki.gentoo.org/wiki/NVidia/nvidia-drivers"&gt;nvidia-drivers wiki article&lt;/a&gt; on
the Gentoo wiki was quite easy to follow.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Yesterday I've &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switched to the gentoo-sources kernel package&lt;/a&gt; on Gentoo Linux.
And with that, I also attempted (succesfully) to use the propriatary nvidia drivers
so that I can enjoy both a smoother 3D experience while playing minecraft, as well
as use the CUDA support so I don't need to use cloud-based services for small
exercises.&lt;/p&gt;
&lt;p&gt;The move to nvidia was quite simple, as the &lt;a href="https://wiki.gentoo.org/wiki/NVidia/nvidia-drivers"&gt;nvidia-drivers wiki article&lt;/a&gt; on
the Gentoo wiki was quite easy to follow.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Signing the modules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One difference I found with the article (which I've promply changed) is that
the signing command, necessary to sign the Linux kernel modules so that they
can be loaded (as unsigned or wrongly signed modules are not allowed on the
system), was different.&lt;/p&gt;
&lt;p&gt;It used to be as follows (example for a single module, it had to be repeated
for each affected kernel module):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# perl /usr/src/linux/scripts/sign-file sha512 \
      /usr/src/linux/signing_key.priv \
      /usr/src/linux/signing_key.x509 \
      /lib/modules/4.12.5-gentoo/video/nvidia-uvm.ko
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, from version 4.3.3 onward (as also explained by this excellent
&lt;a href="https://wiki.gentoo.org/wiki/Signed_kernel_module_support"&gt;Signed kernel module support article&lt;/a&gt; on the Gentoo wiki) this command
no longer uses a Perl script, but is an ELF binary. Also, the location
of the default signing key is moved into a &lt;code&gt;certs/&lt;/code&gt; subdirectory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling nvidia device files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the nvidia modules are loaded, additional device files are enabled.
One is the &lt;code&gt;nvidia0&lt;/code&gt; character device file, while the other is the
&lt;code&gt;nvidiactl&lt;/code&gt; character device file. And although I can imagine that the
&lt;code&gt;nvidiactl&lt;/code&gt; one is a control-related device file, I don't exactly know
for sure.&lt;/p&gt;
&lt;p&gt;However, attempts to use 3D applications showed (through SELinux denials)
that access to these device files is needed. Without that, applications just
crashed, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;org.lwjgl.LWJGLException: X Error - disp: 0x7fd164907b00 serial: 150 error: BadValue (integer parameter out of range for operation) request_code: 153 minor_code: 24
        at org.lwjgl.opengl.LinuxDisplay.globalErrorHandler(LinuxDisplay.java:320)
        at org.lwjgl.opengl.LinuxContextImplementation.nCreate(Native Method)
        at org.lwjgl.opengl.LinuxContextImplementation.create(LinuxContextImplementation.java:51)
        at org.lwjgl.opengl.ContextGL.&amp;lt;init&amp;gt;(ContextGL.java:132)
        at org.lwjgl.opengl.Display.create(Display.java:850)
        at org.lwjgl.opengl.Display.create(Display.java:757)
        at org.lwjgl.opengl.Display.create(Display.java:739)
        at bib.at(SourceFile:635)
        at bib.aq(SourceFile:458)
        at bib.a(SourceFile:404)
        at net.minecraft.client.main.Main.main(SourceFile:123)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Not really useful to debug for me, but the SELinux denials were a bit more obvious,
showing requests for read and write to the &lt;code&gt;nvidiactl&lt;/code&gt; character device.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;code&gt;matchpathcon&lt;/code&gt; I found out that the device files had to have the
&lt;code&gt;xserver_misc_device_t&lt;/code&gt; type (which they didn't have to begin with, as the device
files were added after the automated &lt;code&gt;restorecon&lt;/code&gt; was done on the &lt;code&gt;/dev&lt;/code&gt; location).&lt;/p&gt;
&lt;p&gt;So, adding the following command to my local init script fixed the context setting
at boot up:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;restorecon /dev/nvidiactl /dev/nvidia0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Also, the domains that needed to use nVidia had to receive the following
addition SELinux-policy-wise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dev_rw_xserver_misc(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Perhaps this can be made more fine-grained (as there are several other device
files marked as &lt;code&gt;xserver_misc_device_t&lt;/code&gt;) but for now this should suffice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optimus usage with X server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The other challenge I had was that my workstation uses an integrated Intel
device, and offloads calculations and rendering to nVidia. The detection by
X server did not work automatically though, and it took some fiddling to get
it to work.&lt;/p&gt;
&lt;p&gt;In the end, I had to add in an &lt;code&gt;nvidia.conf&lt;/code&gt; file inside &lt;code&gt;/etc/X11/xorg.conf.d&lt;/code&gt;
with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Section &amp;quot;ServerLayout&amp;quot;
        Identifier      &amp;quot;layout&amp;quot;
        Screen  0       &amp;quot;nvidia&amp;quot;
        Inactive        &amp;quot;intel&amp;quot;
EndSection

Section &amp;quot;Device&amp;quot;
        Identifier      &amp;quot;nvidia&amp;quot;
        Driver          &amp;quot;nvidia&amp;quot;
        BusID           &amp;quot;PCI:1:0:0&amp;quot;
EndSection

Section &amp;quot;Screen&amp;quot;
        Identifier      &amp;quot;nvidia&amp;quot;
        Device          &amp;quot;nvidia&amp;quot;
        Option          &amp;quot;AllowEmptyInitialConfiguration&amp;quot;
EndSection

Section &amp;quot;Device&amp;quot;
        Identifier      &amp;quot;intel&amp;quot;
        Driver          &amp;quot;modesetting&amp;quot;
EndSection

Section &amp;quot;Screen&amp;quot;
        Identifier      &amp;quot;intel&amp;quot;
        Device          &amp;quot;intel&amp;quot;
EndSection
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with a single &lt;code&gt;xrandr&lt;/code&gt; command I re-enabled split screen support (as by
default it now showed the same output on both screens):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ xrandr --output eDP-1-1 --left-of HDMI-1-2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I also had to set the output source to the nVidia device, by adding the following
lines to my &lt;code&gt;~/.xinitrc&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;xrandr --setprovideroutputsource modesetting NVIDIA-0
xrandr --auto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with that, another thing was crossed off from my TODO list. Which has become
quite large after my holidays (went to Kos, Greece) as I had many books and articles
on my ebook reader with me, which inspired a lot.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="nvidia"></category></entry><entry><title>Switch to Gentoo sources</title><link href="https://blog.siphos.be/2017/08/switch-to-gentoo-sources/" rel="alternate"></link><published>2017-08-22T19:04:00+02:00</published><updated>2017-08-22T19:04:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-08-22:/2017/08/switch-to-gentoo-sources/</id><summary type="html">&lt;p&gt;You've might already read it on the Gentoo news site, the &lt;a href="https://www.gentoo.org/news/2017/08/19/hardened-sources-removal.html"&gt;Hardened Linux kernel sources
are removed from the tree&lt;/a&gt; due to the &lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; change where the grsecurity
Linux kernel patches are no longer provided for free. The decision was made due to
supportability and maintainability reasons.&lt;/p&gt;
&lt;p&gt;That doesn't mean that users who want to stick with the grsecurity related hardening
features are left alone. &lt;a href="https://blogs.gentoo.org/ago/2017/08/21/sys-kernel-grsecurity-sources-available/#utm_source=feed&amp;amp;utm_medium=feed&amp;amp;utm_campaign=feed"&gt;Agostino Sarubbo has started providing sys-kernel/grsecurity-sources&lt;/a&gt;
for the users who want to stick with it, as it is based on &lt;a href="https://github.com/minipli/linux-unofficial_grsec"&gt;minipli's unofficial patchset&lt;/a&gt;.
I seriously hope that the patchset will continue to be maintained and, who knows, even evolve further.&lt;/p&gt;
&lt;p&gt;Personally though, I'm switching to the Gentoo sources, and stick with SELinux as one of the
protection measures. And with that, I might even start using my NVidia graphics card a bit more, 
as that one hasn't been touched in several years (I have an Optimus-capable setup with both an
Intel integrated graphics card and an NVidia one, but all attempts to use nouveau for the one game
I like to play - minecraft - didn't work out that well).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;You've might already read it on the Gentoo news site, the &lt;a href="https://www.gentoo.org/news/2017/08/19/hardened-sources-removal.html"&gt;Hardened Linux kernel sources
are removed from the tree&lt;/a&gt; due to the &lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; change where the grsecurity
Linux kernel patches are no longer provided for free. The decision was made due to
supportability and maintainability reasons.&lt;/p&gt;
&lt;p&gt;That doesn't mean that users who want to stick with the grsecurity related hardening
features are left alone. &lt;a href="https://blogs.gentoo.org/ago/2017/08/21/sys-kernel-grsecurity-sources-available/#utm_source=feed&amp;amp;utm_medium=feed&amp;amp;utm_campaign=feed"&gt;Agostino Sarubbo has started providing sys-kernel/grsecurity-sources&lt;/a&gt;
for the users who want to stick with it, as it is based on &lt;a href="https://github.com/minipli/linux-unofficial_grsec"&gt;minipli's unofficial patchset&lt;/a&gt;.
I seriously hope that the patchset will continue to be maintained and, who knows, even evolve further.&lt;/p&gt;
&lt;p&gt;Personally though, I'm switching to the Gentoo sources, and stick with SELinux as one of the
protection measures. And with that, I might even start using my NVidia graphics card a bit more, 
as that one hasn't been touched in several years (I have an Optimus-capable setup with both an
Intel integrated graphics card and an NVidia one, but all attempts to use nouveau for the one game
I like to play - minecraft - didn't work out that well).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;How secure is Gentoo sources with SELinux?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is hard to just say that one kernel tree or another is safe(r) or not. Security is not something
one can get with a simple check-list. It is a matter of properly configuring services and systems,
patching it when needed, limiting expoosure and what not.&lt;/p&gt;
&lt;p&gt;A huge advantage of grsecurity was that it had very insightful and advanced protection measures
(many of them focusing on memory-related attacks), and prevented unwanted behavior from applications
(and users) in a very fine-grained manner. With SELinux, I can still prevent some unwanted behavior,
but it is important to know that SELinux and grsecurity's kernel hardening features are orthogonal
to each other. It is only the grsecurity RBAC model that is somewhat in competition with SELinux.&lt;/p&gt;
&lt;p&gt;SELinux is able to define and manage behavior between types. However, within a single type, many
actions are not governed at all. SELinux can manage which types (domains) are able to invoke which
system calls, but once a call is allowed, SELinux doesn't do any additional controls anymore.&lt;/p&gt;
&lt;p&gt;Loosing protection controls from grsecurity, as a security activist, is not something I like. But
on the other hand, I need to consider the wide SELinux using audience in Gentoo, who is most likely
going to switch to the gentoo sources as well (at least the majority of them).&lt;/p&gt;
&lt;p&gt;Gentoo sources is not insecure by itself, as are many other kernel sources. A huge advantage is that
the gentoo sources are well maintained, so any kernel vulnerability that gets reported and fixed will
receive the proper fix in the Gentoo sources quickly as well (and if you think it can go even faster,
consider &lt;a href="https://wiki.gentoo.org/wiki/Project:Security/Padawan_Process"&gt;becoming a Gentoo security padawan&lt;/a&gt;. And with SELinux enabled, some additional security
controls can be implemented (the efficacy of it depends on the quality of the policy).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://kernsec.org/wiki/index.php/Kernel_Self_Protection_Project"&gt;Kernel Self Protection Project&lt;/a&gt; also aims to improve the Linux kernel security, and immediately
through upstreamed and accepted patches. This means that the protection measures, once in the kernel,
should remain inside (awkward regressions notwithstanding). I truly hope that the KSPP moves forward.
In the mean time, read up on the &lt;a href="https://www.kernel.org/doc/html/latest/security/self-protection.html"&gt;Kernel Self-Protection&lt;/a&gt; document to learn more about how to harden
the Linux kernel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So that's it, just one less security control?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For now, there is no immediate substitute. But that doesn't mean that there is nothing one can do
to increase the secure state of a Linux desktop, workstation or even IoT device. Although remotely
executable exploits do pop up and exist, many vulnerabilities in the Linux kernel are mainly exploitable
through a local access pattern.&lt;/p&gt;
&lt;p&gt;That means that vulnerabilities often can only be exploited through a local invocation (or through chaining
by using other vulnerabilities - often in completely different applications or services - in order to
execute the local malware). Hence, hardening of the entire system is extremely important.&lt;/p&gt;
&lt;p&gt;Previously, I had an account with multiple SELinux roles assigned to it. Depending on what I wanted to
do, I transitioned to the right role (either through the &lt;code&gt;newrole&lt;/code&gt; command, or through &lt;code&gt;sudo&lt;/code&gt; which
has integrated SELinux support). With the switch to the gentoo sources, I decided to make it a bit
harder for malware on my system to work: i start using separate Linux accounts depending on the purpose
(which I call persona).&lt;/p&gt;
&lt;p&gt;Developing SELinux policies is now done on a separate account, managing remote systems through another
account (although my servers use multi-factor authentication so there was already some additional safeguard
in place there), handling my side-work with another account, playing games with another account, etc.&lt;/p&gt;
&lt;p&gt;It isn't that I don't trust SELinux for this (as each domain is well isolated and controlled). But SELinux
cannot prevent vulnerabilities within applications if the action/result of a succesfully exploited
vulnerability does not change the expected behavior of the application versus the other resources
on the system (and even there, the fine-grained approach of policies might not even be sufficiently
fine-grained, as SELinux uses labels, and many resources have the same label assigned).&lt;/p&gt;
&lt;p&gt;Suppose some malware is able to capture me giving in my password, or is trying to phish for it. By
using separate accounts (with separate passphrazes of course) the impact is reduced a bit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other things on the plate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The change to different accounts was one thing I wanted to establish before switching to a new kernel
tree. There are other aspects that I want to investigate in the near future as well though.&lt;/p&gt;
&lt;p&gt;First of all, I'm probably going to enable &lt;a href="https://github.com/Yubico/pam-u2f"&gt;U2F authentication&lt;/a&gt; on my workstation as well for
all interactive accounts. It has been on my list for quite some time, and quickly going through the
publicly available fora doesn't reveal any major challenges to do so. Build the PAM module, update
the PAM service configurations and you're done. Hopefully. ;-)&lt;/p&gt;
&lt;p&gt;Next, I'm going to play around a bit with &lt;a href="https://wiki.gentoo.org/wiki/AddressSanitizer"&gt;AddressSanitizer&lt;/a&gt;. ASAN was incompatible with grsecurity,
but now that that's out of the way, there's no reason not to investigate it further. I am not going
to enable it for the kernel though (as some KSPP implemented measures are incompatible with ASAN as well),
and probably not for my complete workstation yet (even though it is sufficiently powerful to handle the major
performance impact).&lt;/p&gt;
&lt;p&gt;I'm going to put some more focus on &lt;a href="https://wiki.gentoo.org/wiki/Integrity_Measurement_Architecture"&gt;Integrity Measurement Architecture support&lt;/a&gt;, although my main protection
measure with IMA - the TPM or Trusted Platform Module - has been fried (don't ask) so I can't use it anymore.
Perhaps I'm going to buy a very lightweight/small system with a TPM on it to continue development. We'll see.&lt;/p&gt;
&lt;p&gt;My current knowledge of &lt;a href="https://en.wikipedia.org/wiki/Seccomp"&gt;seccomp&lt;/a&gt; is fairly theoretical (with a few hands-on tutorials, but that's it). It
has been on my TODO list for some time to look in more depth to it. Perhaps this is the right time.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="hardened"></category><category term="grsecurity"></category><category term="selinux"></category></entry><entry><title>Project prioritization</title><link href="https://blog.siphos.be/2017/07/project-prioritization/" rel="alternate"></link><published>2017-07-18T20:40:00+02:00</published><updated>2017-07-18T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-07-18:/2017/07/project-prioritization/</id><summary type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Structure your workload, the SAFe approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first exercise you want to implement is to structure the workload, ideas or
projects. Some changes are small, others are large. Some are disruptive, others
are evolutionary. Trying to prioritize all different types of ideas and changes
in the same way is not feasible.&lt;/p&gt;
&lt;p&gt;Structuring workload is a common approach. Changes are grouped in projects,
projects grouped in programs, programs grouped in strategic tracks. Lately,
with the rise in Agile projects, a similar layering approach is suggested in
the form of SAFe.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="http://www.scaledagileframework.com/"&gt;Scaled Agile Framework&lt;/a&gt; a structure is suggested that uses, as a
top-level approach, value streams. These are strategically aligned steps that
an organization wants to use to build solutions that provide a continuous flow
of value to a customer (which can be internal or external). For instance, for a
financial service organization, a value stream could focus on 'Risk Management
and Analytics'.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SAFe full framework" src="https://blog.siphos.be/images/201707/safe-full.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;SAFe full framework overview, picture courtesy of www.scaledagileframework.com&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;The value streams are supported through solution trains, which implement
particular solutions. This could be a final product for a customer (fitting in
a particular value stream) or a set of systems which enable capabilities for a
value stream. It is at this level, imo, that the benefits exercises from IT
portfolio management and benefits realization management research plays its
role (more about that later). For instance, a solution train could focus on an
'Advanced Analytics Platform'.&lt;/p&gt;
&lt;p&gt;Within a solution train, agile release trains provide continuous delivery for
the various components or services needed within one or more solutions. Here,
the necessary solutions are continuously delivered in support of the solution
trains. At this level, focus is given on the culture within the organization
(think DevOps), and the relatively short-lived delivery delivery periods. This
is the level where I see 'projects' come into play.&lt;/p&gt;
&lt;p&gt;Finally, you have the individual teams working on deliverables supporting a
particular project.&lt;/p&gt;
&lt;p&gt;SAFe is just one of the many methods for organization and development/delivery
management. It is a good blueprint to look into, although I fear that larger
organizations will find it challenging to dedicate resources in a manageable
way. For instance, how to deal with specific expertise across solutions which
you can't dedicate to a single solution at a time? What if your organization
only has two telco experts to support dozens of projects? Keep that in mind,
I'll come back to that later...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get non-content information about the value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the structuring of the workload, you need to obtain information about
the solutions that you want to implement (keeping with the SAFe terminology).
And bear in mind that seemingly dull things such as ensuring your firewalls are
up to date are also deliverables within a larger ecosystem. Now, with
information about the solutions, I don't mean the content-wise information, but
instead focus on other areas.&lt;/p&gt;
&lt;p&gt;Way back, in 1952, Harry Markowitz introduced &lt;a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory"&gt;Modern portfolio theory&lt;/a&gt; as a
mathematical framework for assembling a portfolio of assets such that the
expected return is maximized for a given level of risk (quoted from Wikipedia).
This was later used in an IT portfolio approach by McFarlan in his &lt;a href="https://hbr.org/1981/09/portfolio-approach-to-information-systems"&gt;Portfolio
Approach to Information Systems&lt;/a&gt; article, published in September 1981.&lt;/p&gt;
&lt;p&gt;There it was already introduced that risk and return shouldn't be looked at
from an individual project viewpoint, but how it contributes to the overall
risk and return. A balance, if you wish. His article attempts to categorize
projects based on risk profiles on various areas. Personally, I see the
suggested categorization more as a way of supporting workload assessments (how
many mandays of work will this be), but I digress.&lt;/p&gt;
&lt;p&gt;Since then, other publications came up which tried to document frameworks and
methodologies that facilitate project portfolio prioritization and management.
The focus often boils down to value or benefits realization. In &lt;a href="https://books.google.be/books/about/The_Information_Paradox.html?id=mk60QgAACAAJ&amp;amp;redir_esc=y&amp;amp;hl=en"&gt;The
Information Paradox&lt;/a&gt; John Thorp comes up with a benefits realization
approach, which enables organizations to better define and track benefits
realization - although it again boils down on larger transformation exercises
rather than the lower-level backlogs. The realm of &lt;a href="https://en.wikipedia.org/wiki/IT_portfolio_management"&gt;IT portfolio management&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Benefits_realisation_management"&gt;Benefits realization management&lt;/a&gt; gives interesting pointers as to
the lecture part of prioritizing projects.&lt;/p&gt;
&lt;p&gt;Still, although one can hardly state the resources are incorrect, a common
question is how to make this tangible. Personally, I tend to view the above on
the value stream level and solution train level.  Here, we have a strong
alignment with benefits and value for customers, and we can leverage the ideas
of past research.&lt;/p&gt;
&lt;p&gt;The information needed at this level often boils down to strategic insights and
business benefits, coarse-grained resource assessments, with an important focus
on quality of the resources. For instance, a solution delivery might take up
500 days of work (rough estimation) but will also require significant back-end
development resources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Handling value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we implement this on the highest level in the structure, it should be
conceivable that the overview of the value streams (a dozen or so) and
solutions (a handful per value stream) is manageable, and something that at an
executive level is feasible to work with. These are the larger efforts for
structuring and making strategic alignment. Formal methods for prioritization
are generally not implemented or described.&lt;/p&gt;
&lt;p&gt;In my company, there are exercises that are aligning with SAFe, but it isn't
company-wide. Still, there is a structure in place that (within IT) one could
map to value streams (with some twisting ;-) and, within value streams, there
are structures in place that one could map to the solution train exercises.&lt;/p&gt;
&lt;p&gt;We could assume that the enterprise knows about its resources (people, budget
...) and makes a high-level suggestion on how to distribute the resources in
the mid-term (such as the next 6 months to a year). This distribution is
challenged and worked out with the value stream owners. See also "lean
budgeting" in the SAFe approach for one way of dealing with this.&lt;/p&gt;
&lt;p&gt;There is no prioritization of value streams. The enterprise has already made
its decision on what it finds to be the important values and benefits and
decided those in value streams.&lt;/p&gt;
&lt;p&gt;Within a value stream, the owner works together with the customers (internal or
external) to position and bring out solutions. My experience here is that
prioritization is generally based on timings and expectations from the
customer. In case of resource contention, the most challenging decision to make
here is to put a solution down (meaning, not to pursue the delivery of a
solution), and such decisions are hardly taken.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prioritizing the projects and changes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the lower echelons of the project portfolio structure, we have the projects
and changes. Let's say that the levels here are projects (agile release trains)
and changes (team-level). Here, I tend to look at prioritization on project
level, and this is the level that has a more formal approach for
prioritization.&lt;/p&gt;
&lt;p&gt;Why? Because unlike the higher levels, where the prioritization is generally
quality-oriented on a manageable amount of streams and solutions, we have a
large quantity of projects and ideas. Hence, prioritization is more
quantity-oriented in which formal methods are more efficient to handle.&lt;/p&gt;
&lt;p&gt;The method that is used in my company uses scoring criteria on a per-project
level. This is not innovative per se, as past research has also revealed that
project categorization and mapping is a powerful approach for handling project
portfolio's. Just look for "categorizing priority projects it portfolio" in
Google and you'll find ample resources. Kendal's &lt;a href="https://www.amazon.com/Advanced-Project-Portfolio-Management-PMO/dp/1932159029"&gt;Advanced Project Portfolio
Management and the PMO&lt;/a&gt; (book) has several example project scoring
criteria's. But allow me to explain our approach.&lt;/p&gt;
&lt;p&gt;It basically is like so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each project selects three value drivers (list decided up front)&lt;/li&gt;
&lt;li&gt;For the value drivers, the projects check if they contribute to it slightly (low), moderately (medium) or fully (high)&lt;/li&gt;
&lt;li&gt;The value drivers have weights, as do the values. Sum the resulting products to get a priority score&lt;/li&gt;
&lt;li&gt;Have the priority score validated by a scoring team&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get to the details of it.&lt;/p&gt;
&lt;p&gt;For the IT projects within the infrastructure area (which is what I'm active
in), we have around 5 scoring criteria (value drivers) that are value-stream
agnostic, and then 3 to 5 scoring criteria that are value-stream specific. Each
scoring criteria has three potential values: low (2), medium (4) and high (9).
The numbers are the weights that are given to the value.&lt;/p&gt;
&lt;p&gt;A scoring criteria also has a weight. For instance, we have a scoring criteria
on efficiency (read: business case) which has a weight of 15, so a score of
medium within that criteria gives a total value of 60 (4 times 15). The
potential values here are based on the "return on investment" value, with low
being a return less than 2 years, medium within a year, and high within a few
months (don't hold me on the actual values, but you get the idea).&lt;/p&gt;
&lt;p&gt;The sum of all values gives a priority score. Now, hold your horses, because
we're not done yet. There is a scoring rule that says a project can only be
scored by at most 3 scoring criteria. Hence, project owners need to see what
scoring areas their project is mostly visible in, and use those scoring
criteria. This rule supports the notion that people don't bring around ideas
that will fix world hunger and make a cure for cancer, but specific, well
scoped ideas (the former are generally huge projects, while the latter requires
much less resources).&lt;/p&gt;
&lt;p&gt;OK, so you have a score - is that your priority? No. As a project always falls
within a particular value stream, we have a "scoring team" for each value
stream which does a number of things. First, it checks if your project really
belongs in the right value stream (but that's generally implied) and has a
deliverable that fits the solution or target within that stream. Projects that
don't give any value or aren't asked by customers are eliminated.&lt;/p&gt;
&lt;p&gt;Next, the team validates if the scoring that was used is correct: did you
select the right values (low, medium or high) matching the methodology for said
criteria? If not, then the score is adjusted.&lt;/p&gt;
&lt;p&gt;Finally, the team validates if the resulting score is perceived to be OK or
not. Sometimes, ideas just don't map correctly on scoring criteria, and even
though a project has a huge strategic importance or deliverable it might score
low. In those cases, the scoring team can adjust the score manually. However,
this is more of a fail-safe (due to the methodology) rather than the norm.
About one in 20 projects gets its score adjusted. If too many adjustments come
up, the scoring team will suggest a change in methodology to rectify the
situation.&lt;/p&gt;
&lt;p&gt;With the score obtained and validated by the scoring team, the project is given
a "go" to move to the project governance. It is the portfolio manager that then
uses the scores to see when a project can start.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Providing levers to management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, these scoring criteria are not established from a random number generator.
An initial suggestion was made on the scoring criteria, and their associated
weights, to the higher levels within the organization (read: the people in
charge of the prioritization and challenging of value streams and solutions).&lt;/p&gt;
&lt;p&gt;The same people are those that approve the weights on the scoring criteria. If
management (as this is often the level at which this is decided) feels that
business case is, overall, more important than risk reduction, then they will
be able to put a higher value in the business case scoring than in the risk
reduction.&lt;/p&gt;
&lt;p&gt;The only constraint is that the total value of all scoring criteria must be
fixed. So an increase on one scoring criteria implies a reduction on at least
one other scoring criteria. Also, changing the weights (or even the scoring
criteria themselves) cannot be done frequently. There is some inertia in
project prioritization: not the implementation (because that is a matter of
following through) but the support it will get in the organization itself.&lt;/p&gt;
&lt;p&gt;Management can then use external benchmarks and other sources to gauge the
level that an organization is at, and then - if needed - adjust the scoring
weights to fit their needs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resource allocation in teams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Portfolio managers use the scores assigned to the projects to drive their
decisions as to when (and which) projects to launch. The trivial approach is to
always pick the projects with the highest scores. But that's not all.&lt;/p&gt;
&lt;p&gt;Projects can have dependencies on other projects. If these dependencies are
"hard" and non-negotiable, then the upstream project (the one being dependent
on) inherits the priority of the downstream project (the one depending on the
first) if the downstream project has a higher priority. Soft dependencies
however need to validate if they can (or have to) wait, or can implement
workarounds if needed.&lt;/p&gt;
&lt;p&gt;Projects also have specific resource requirements. A project might have a high
priority, but if it requires expertise (say DBA knowledge) which is unavailable
(because those resources are already assigned to other ongoing projects) then
the project will need to wait (once resources are fully allocated and the
projects are started, then they need to finish - another reason why projects
have a narrow scope and an established timeframe).&lt;/p&gt;
&lt;p&gt;For engineers, operators, developers and other roles, this approach allows them
to see which workload is more important versus others. When their scope is
always within a single value stream, then the mentioned method is sufficient.
But what if a resource has two projects, each of a different value stream? As
each value stream has its own scoring criteria it can use (and weight), one
value stream could systematically have higher scores than others...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mixing and matching multiple value streams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To allow projects to be somewhat comparable in priority values, an additional
rule has been made in the scoring methodology: value streams must have a
comparable amount of scoring criteria (value drivers), and the total value of
all criteria must be fixed (as was already mentioned before). So if there are
four scoring criteria and the total value is fixed at 20, then one value stream
can have its criteria at (5,3,8,4) while another has it at (5,5,5,5).&lt;/p&gt;
&lt;p&gt;This is still not fully adequate, as one value stream could use a single
criteria with the maximum amount (20,0,0,0). However, we elected not to put in
an additional constraint, and have management work things out if the situation
ever comes out. Luckily, even managers are just human and they tend to follow
the notion of well-balanced value drivers.&lt;/p&gt;
&lt;p&gt;The result is that two projects will have priority values that are currently
sufficiently comparable to allow cross-value-stream experts to be exchangeable
without monopolizing these important resources to a single value stream
portfolio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Current state&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The scoring methodology has been around for a few years already. Initially, it
had fixed scoring criteria used by three value streams (out of seven, the other
ones did not use the same methodology), but this year we switched to support
both value stream agnostic criteria (like in the past) as well as value stream
specific ones.&lt;/p&gt;
&lt;p&gt;The methodology is furthest progressed in one value stream (with focus of around
1000 projects) and is being taken up by two others (they are still looking at
what their stream-specific criteria are before switching).&lt;/p&gt;</content><category term="Architecture"></category><category term="pmo"></category><category term="strategy"></category><category term="SAFe"></category><category term="prioritization"></category><category term="project"></category></entry><entry><title>Structuring infrastructural deployments</title><link href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/" rel="alternate"></link><published>2017-06-07T20:40:00+02:00</published><updated>2017-06-07T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-06-07:/2017/06/structuring-infrastructural-deployments/</id><summary type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;As always, start with the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From an infrastructure architect point of view, creating structure is one way
of dealing with the onslaught in complexity that is prevalent within the wider
organizational architecture. By creating a framework in which infrastructural
services can be positioned, architects and other stakeholders (such as
information security officers, process managers, service delivery owners, project
and team leaders ...) can support the wide organization in its endeavor of
becoming or remaining competitive.&lt;/p&gt;
&lt;p&gt;Structure can be provided through various viewpoints. As such, while creating
such framework, the initial intention is not to start drawing borders or
creating a complex graph. Instead, look at attributes that one would assign
to an infrastructural service, and treat those as labels. Create a nice
portfolio of attributes which will help guide the development of such framework.&lt;/p&gt;
&lt;p&gt;The following list gives some ideas in labels or attributes that one can use.
But be creative, and use experienced people in devising the "true" list of
attributes that fits the needs of your organization. Be sure to describe them
properly and unambiguously - the list here is just an example, as are the
descriptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;tenant&lt;/strong&gt; identifies the organizational aggregation of business units which are
  sufficiently similar in areas such as policies (same policies in use),
  governance (decision bodies or approval structure), charging, etc. It
  could be a hierarchical aspect (such as organization) as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;location&lt;/strong&gt; provides insight in the physical (if applicable) location of the
  service. This could be an actual building name, but can also be structured
  depending on the size of the environment. If it is structured, make sure to
  devise a structure up front. Consider things such as regions, countries,
  cities, data centers, etc. A special case location value could be the
  jurisdiction, if that is something that concerns the organization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;service type&lt;/strong&gt; tells you what kind of service an asset is. It can be a
  workstation, a server/host, server/guest, network device, virtual or
  physical appliance, sensor, tablet, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trust level&lt;/strong&gt; provides information on how controlled and trusted the service
  is. Consider the differences between unmanaged (no patching, no users doing
  any maintenance), open (one or more admins, but no active controlled
  maintenance), controlled (basic maintenance and monitoring, but with still
  administrative access by others), managed (actively maintained, no privileged
  access without strict control), hardened (actively maintained, additional
  security measures taken) and kiosk (actively maintained, additional security
  measures taken and limited, well-known interfacing).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compliance set&lt;/strong&gt; identifies specific compliance-related attributes, such as the
  PCI-DSS compliancy level that a system has to adhere to.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consumer group&lt;/strong&gt; informs about the main consumer group, active on the service.
  This could be an identification of the relationship that consumer group has
  with the organization (anonymous, customer, provider, partner, employee, ...)
  or the actual name of the consumer group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;architectural purpose&lt;/strong&gt; gives insight in the purpose of the service in
  infrastructural terms. Is it a client system, a gateway, a mid-tier system,
  a processing system, a data management system, a batch server, a reporting
  system, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt; could be interpreted as to the company purpose of the system. Is it for
  commercial purposes (such as customer-facing software), corporate functions
  (company management), development, infrastructure/operations ...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;production status&lt;/strong&gt; provides information about the production state of a
  service. Is it a production service, or a pre-production (final testing before
  going to production), staging (aggregation of multiple changes) or development
  environment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the final set of labels, the next step is to aggregate results to create
a high-level view of the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating a layered structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chances are high that you'll end up with several attributes, and many of these
will have multiple possible values. What we don't want is to end in an
N-dimensional infrastructure architecture overview. Sure, it sounds sexy to do
so, but you want to show the infrastructure architecture to several stakeholders
in your organization. And projecting an N-dimensional structure on a
2-dimensional slide is not only challenging - you'll possibly create a projection
which leaves out important details or make it hard to interpret.&lt;/p&gt;
&lt;p&gt;Instead, we looked at a &lt;em&gt;layered approach&lt;/em&gt;, with each layer handling one or two
requirements. The top layer represents the requirement which the organization
seems to see as the most defining attribute. It is the attribute where, if its
value changes, most of its architecture changes (and thus the impact of a service
relocation is the largest).&lt;/p&gt;
&lt;p&gt;Suppose for instance that the domain attribute is seen as the most defining one:
the organization has strict rules about placing corporate services and commercial
services in separate environments, or the security officers want to see the
commercial services, which are well exposed to many end users, be in a separate
environment from corporate services. Or perhaps the company offers commercial
services for multiple tenants, and as such wants several separate "commercial
services" environments while having a single corporate service domain.&lt;/p&gt;
&lt;p&gt;In this case, part of the infrastructure architecture overview on the top level
could look like so (hypothetical example):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Top level view" src="https://blog.siphos.be/images/201706/07-1-toplevelview.png"&gt;&lt;/p&gt;
&lt;p&gt;This also shows that, next to the corporate and commercial interests of the
organization, a strong development support focus is prevalent as well. This
of course depends on the type of organization or company and how significant
in-house development is, but in this example it is seen as a major decisive
factor for service positioning.&lt;/p&gt;
&lt;p&gt;These top-level blocks (depicted as locations, for those of you using Archimate)
are what we call "&lt;strong&gt;zones&lt;/strong&gt;". These are not networks, but clearly bounded areas in
which multiple services are positioned, and for which particular handling rules
exist. These rules are generally written down in policies and standards - more
about that later.&lt;/p&gt;
&lt;p&gt;Inside each of these zones, a substructure is made available as well, based on
another attribute. For instance, let's assume that this is the architectural
purpose. This could be because the company has a requirement on segregating
workstations and other client-oriented zones from the application hosting related
ones. Security-wise, the company might have a principle where mid-tier services
(API and presentation layer exposures) are separate from processing services,
and where data is located in a separate zone to ensure specific data access or
more optimal infrastructure services.&lt;/p&gt;
&lt;p&gt;This zoning result could then be depicted as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Detailed top-level view" src="https://blog.siphos.be/images/201706/07-1-detailedtoplevel.png"&gt;&lt;/p&gt;
&lt;p&gt;From this viewpoint, we can also deduce that this company provides separate
workstation services: corporate workstation services (most likely managed
workstations with focus on application disclosure, end user computing, etc.)
and development workstations (most likely controlled workstations but with more
open privileged access for the developer).&lt;/p&gt;
&lt;p&gt;By making this separation explicit, the organization makes it clear that the
development workstations will have a different position, and even a different
access profile toward other services within the company.&lt;/p&gt;
&lt;p&gt;We're not done yet. For instance, on the mid-tier level, we could look at the
consumer group of the services:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mid-tier explained" src="https://blog.siphos.be/images/201706/07-1-midtier.png"&gt;&lt;/p&gt;
&lt;p&gt;This separation can be established due to security reasons (isolating services
that are exposed to anonymous users from customer services or even partner
services), but one can also envision this to be from a management point of
view (availability requirements can differ, capacity management is more
uncertain for anonymous-facing services than authenticated, etc.)&lt;/p&gt;
&lt;p&gt;Going one layer down, we use a production status attribute as the defining
requirement:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anonymous user detail" src="https://blog.siphos.be/images/201706/07-1-anonymousdetail.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point, our company decided that the defined layers are sufficiently
established and make for a good overview. We used different defining properties
than the ones displayed above (again, find a good balance that fits the company
or organization that you're focusing on), but found that the ones we used were
mostly involved in existing policies and principles, while the other ones are
not that decisive for infrastructure architectural purposes. &lt;/p&gt;
&lt;p&gt;For instance, the tenant might not be selected as a deciding attribute, because
there will be larger tenants and smaller tenants (which could make the resulting
zone set very convoluted) or because some commercial services are offered toward
multiple tenants and the organizations' strategy would be to move toward
multi-tenant services rather than multiple deployments.&lt;/p&gt;
&lt;p&gt;Now, in the zoning structure there is still another layer, which from an
infrastructure architecture point is less about rules and guidelines and more
about manageability from an organizational point of view. For instance, in the
above example, a SAP deployment for HR purposes (which is obviously a corporate
service) might have its Enterprise Portal service in the &lt;code&gt;Corporate Services&lt;/code&gt; &amp;gt; 
&lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone. However, another service such as
an on-premise SharePoint deployment for group collaboration might be in &lt;code&gt;Corporate
Services&lt;/code&gt; &amp;gt; &lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone as well. Yet both
services are supported through different teams.&lt;/p&gt;
&lt;p&gt;This "final" layer thus enables grouping of services based on the supporting
team (again, this is an example), which is organizationally aligned with the
business units of the company, and potentially further isolation of services
based on other attributes which are not defining for all services. For instance,
the company might have a policy that services with a certain business impact
assessment score must be in isolated segments with no other deployments within
the same segment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about management services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, the above picture is missing some of the (in my opinion) most important
services: infrastructure support and management services. These services do not
shine in functional offerings (which many non-IT people generally look at) but
are needed for non-functional requirements: manageability, cost control,
security (if security can be defined as a non-functional - let's not discuss
that right now).&lt;/p&gt;
&lt;p&gt;Let's first consider &lt;em&gt;interfaces&lt;/em&gt; - gateways and other services which are
positioned between zones or the "outside world". In the past, we would speak of
a demilitarized zone (DMZ). In more recent publications, one can find this as
an interface zone, or a set of Zone Interface Points (ZIPs) for accessing and
interacting with the services within a zone.&lt;/p&gt;
&lt;p&gt;In many cases, several of these interface points and gateways are used in the
organization to support a number of non-functional requirements. They can be
used for intelligent load balancing, providing virtual patching capabilities,
validating content against malware before passing it on to the actual services,
etc.&lt;/p&gt;
&lt;p&gt;Depending on the top level zone, different gateways might be needed (i.e.
different requirements). Interfaces for commercial services will have a strong
focus on security and manageability. Those for the corporate services might be
more integration-oriented, and have different data leakage requirements than
those for commercial services.&lt;/p&gt;
&lt;p&gt;Also, inside such an interface zone, one can imagine a substructure to take
place as well: egress interfaces (for communication that is exiting the zone),
ingress interfaces (for communication that is entering the zone) and internal
interfaces (used for routing between the subzones within the zone).&lt;/p&gt;
&lt;p&gt;Yet, there will also be requirements which are company-wide. Hence, one could
envision a structure where there is a company-wide interface zone (with
mandatory requirements regardless of the zone that they support) as well as a
zone-specific interface zone (with the mandatory requirements specific to that
zone).&lt;/p&gt;
&lt;p&gt;Before I show a picture of this, let's consider &lt;em&gt;management services&lt;/em&gt;. Unlike
interfaces, these services are more oriented toward the operational management
of the infrastructure. Software deployment, configuration management, identity
&amp;amp; access management services, etc. Are services one can put under management
services.&lt;/p&gt;
&lt;p&gt;And like with interfaces, one can envision the need for both company-wide
management services, as well as zone-specific management services.&lt;/p&gt;
&lt;p&gt;This information brings us to a final picture, one that assists the
organization in providing a more manageable view on its deployment landscape.
It does not show the 3rd layer (i.e. production versus non-production
deployments) and only displays the second layer through specialization
information, which I've quickly made a few examples for (you don't want to make
such decisions in a few hours, like I did for this post).&lt;/p&gt;
&lt;p&gt;&lt;img alt="General overview" src="https://blog.siphos.be/images/201706/07-1-firstgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;If the organization took an alternative approach for structuring (different
requirements and grouping) the resulting diagram could look quite different:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alternative general overview" src="https://blog.siphos.be/images/201706/07-1-secondgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flows, flows and more flows&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the high-level picture ready, it is not a bad idea to look at how flows
are handled in such an architecture. As the interface layer is available on
both company-wide level as well as the next, flows will cross multiple zones.&lt;/p&gt;
&lt;p&gt;Consider the case of a corporate workstation connecting to a reporting server
(like a Cognos or PowerBI or whatever fancy tool is used), and this reporting
server is pulling data from a database system. Now, this database system is
positioned in the &lt;code&gt;Commercial&lt;/code&gt; zone, while the reporting server is in the
&lt;code&gt;Corporate&lt;/code&gt; zone. The flows could then look like so:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Flow example" src="https://blog.siphos.be/images/201706/07-1-flow.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;Note for the Archimate people: I'm sorry that I'm abusing the flow relation
here. I didn't want to create abstract services in the locations and then use
the "serves" or "used by" relation and then explaining readers that the arrows
are then inverse from what they imagine.&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;In this picture, the corporate workstation does not connect to the reporting
server directly. It goes through the internal interface layer for the corporate
zone. This internal interface layer can offer services such as reverse proxies
or intelligent load balancers. The idea here is that, if the organization
wants, it can introduce additional controls or supporting services in this
internal interface layer without impacting the system deployments themselves
much.&lt;/p&gt;
&lt;p&gt;But the true flow challenge is in the next one, where a processing system
connects to a data layer. Here, the processing server will first connect to the
egress interface for corporate, then through the company-wide internal
interface, toward the ingress interface of the commercial and then to the data
layer.&lt;/p&gt;
&lt;p&gt;Now, why three different interfaces, and what would be inside it?&lt;/p&gt;
&lt;p&gt;On the corporate level, the egress interface could be focusing on privacy
controls or data leakage controls. On the company-wide internal interface more
functional routing capabilities could be provided, while on the commercial
level the ingress could be a database activity monitoring (DAM) system such as
a database firewall to provide enhanced auditing and access controls.&lt;/p&gt;
&lt;p&gt;Does that mean that all flows need to have at least three gateways? No, this is
a functional picture. If the organization agrees, then one or more of these
interface levels can have a simple pass-through setup. It is well possible that
database connections only connect directly to a DAM service and that such flows
are allowed to immediately go through other interfaces.&lt;/p&gt;
&lt;p&gt;The importance thus is not to make flows more difficult to provide, but to
provide several areas where the organization can introduce controls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making policies and standards more visible&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the effects of having a better structure of the company-wide deployments
(i.e. a good zoning solution) is that one can start making policies more clear,
and potentially even simple to implement with supporting tools (such as
software defined network solutions).&lt;/p&gt;
&lt;p&gt;For instance, a company might want to protect its production data and establish
that it cannot be used for non-production use, but that there are no
restrictions for the other data environments. Another rule could be that
web-based access toward the mid-tier is only allowed through an interface.&lt;/p&gt;
&lt;p&gt;These are simple statements which, if a company has a good IP plan, are easy to
implement - one doesn't need zoning, although it helps. But it goes further
than access controls.&lt;/p&gt;
&lt;p&gt;For instance, the company might require corporate workstations to be under
heavy data leakage prevention and protection measures, while developer
workstations are more open (but don't have any production data access). This
not only reveals an access control, but also implies particular minimal
requirements (for the &lt;code&gt;Corporate&lt;/code&gt; &amp;gt; &lt;code&gt;Workstation&lt;/code&gt; zone) and services (for the
&lt;code&gt;Corporate&lt;/code&gt; interfaces).&lt;/p&gt;
&lt;p&gt;This zoning structure does not necessarily make any statements about the
location (assuming it isn't picked as one of the requirements in the
beginning). One can easily extend this to include cloud-based services or
services offered by third parties.&lt;/p&gt;
&lt;p&gt;Finally, it also supports making policies and standards more realistic. I often
see policies that make bold statements such as "all software deployments must
be done through the company software distribution tool", but the policies don't
consider development environments (production status) or unmanaged, open or
controlled deployments (trust level). When challenged, the policy owner might
shrug away the comment with "it's obvious that this policy does not apply to
our sandbox environment" or so.&lt;/p&gt;
&lt;p&gt;With a proper zoning structure, policies can establish the rules for the right
set of zones, and actually pin-point which zones are affected by a statement.
This is also important if a company has many, many policies. With a good zoning
structure, the policies can be assigned with meta-data so that affected roles
(such as project leaders, architects, solution engineers, etc.) can easily get
an overview of the policies that influence a given zone.&lt;/p&gt;
&lt;p&gt;For instance, if I want to position a new management service, I am less
concerned about workstation-specific policies. And if the management service is
specific for the development environment (such as a new version control system)
many corporate or commercially oriented policies don't apply either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above approach for structuring an organization is documented here in a
high-level manner. It takes many assumptions or hypothetical decisions which
are to be tailored toward the company itself. In my company, a different zoning
structure is selected, taking into account that it is a financial service
provider with entities in multiple countries, handling several thousand of
systems and with an ongoing effort to include cloud providers within its
infrastructure architecture.&lt;/p&gt;
&lt;p&gt;Yet the approach itself is followed in an equal fashion. We looked at
requirements, created a layered structure, and finished the zoning schema. Once
the schema was established, the requirements for all the zones were written out
further, and a mapping of existing deployments (as-is) toward the new zoning
picture is on-going. For those thinking that it is just slideware right now -
it isn't. Some of the structures that come out of the zoning exercise are
already prevalent in the organization, and new environments (due to mergers and
acquisitions) are directed to this new situation.&lt;/p&gt;
&lt;p&gt;Still, we know we have a large exercise ahead before it is finished, but I
believe that it will benefit us greatly, not only from a security point of
view, but also clarity and manageability of the environment.&lt;/p&gt;</content><category term="Architecture"></category><category term="segmentation"></category><category term="zoning"></category><category term="deployments"></category><category term="landscape"></category></entry><entry><title>Matching MD5 SSH fingerprint</title><link href="https://blog.siphos.be/2017/05/matching-md5-ssh-fingerprint/" rel="alternate"></link><published>2017-05-18T18:20:00+02:00</published><updated>2017-05-18T18:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-05-18:/2017/05/matching-md5-ssh-fingerprint/</id><summary type="html">&lt;p&gt;Today I was attempting to update a local repository, when SSH complained
about a changed fingerprint, something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:p4ZGs+YjsBAw26tn2a+HPkga1dPWWAWX+NEm4Cv4I9s.
Please contact your system administrator.
Add correct host key in /home/user/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/user/.ssh/known_hosts:9
ECDSA host key for 192.168.56.101 has changed and you have requested strict checking.
Host key verification failed.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</summary><content type="html">&lt;p&gt;Today I was attempting to update a local repository, when SSH complained
about a changed fingerprint, something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:p4ZGs+YjsBAw26tn2a+HPkga1dPWWAWX+NEm4Cv4I9s.
Please contact your system administrator.
Add correct host key in /home/user/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/user/.ssh/known_hosts:9
ECDSA host key for 192.168.56.101 has changed and you have requested strict checking.
Host key verification failed.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;I checked if the host was changed recently, or the alias through
which I connected switched host, or the SSH key changed. But that
wasn't the case. Or at least, it wasn't the case recently, and I
distinctly remember connecting to the same host two weeks ago.&lt;/p&gt;
&lt;p&gt;Now, what happened I don't know yet, but I do know I didn't want
to connect until I reviewed the received SSH key fingerprint. I
obtained the fingerprint from the administration (who graceously
documented it on the wiki)...&lt;/p&gt;
&lt;p&gt;... only to realize that the documented fingerprint are MD5
hashes (and in hexadecimal result) whereas the key shown by the
SSH command shows it in base64 SHA256 by default.&lt;/p&gt;
&lt;p&gt;Luckily, a quick search revealed this &lt;a href="https://superuser.com/questions/929566/sha256-ssh-fingerprint-given-by-the-client-but-only-md5-fingerprint-known-for-se"&gt;superuser&lt;/a&gt;
post which told me to connect to the host using the
&lt;code&gt;FingerprintHash md5&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -o FingerprintHash=md5 192.168.56.11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is SSH displaying the MD5 hashed fingerprint which I
can now validate against the documented one. Once I validated that
the key is the correct one, I accepted the change and continued
with my endeavour.&lt;/p&gt;
&lt;p&gt;I later discovered (or, more precisely, have strong assumptions)
that I had an old elliptic curve key registered in my &lt;code&gt;known_hosts&lt;/code&gt;
file, which was not used for the communication for quite some time.
I recently re-enabled elliptic curve support in OpenSSH (with Gentoo's
USE="-bindist") which triggered the validation of the old key.&lt;/p&gt;</content><category term="Security"></category><category term="openssh"></category><category term="fingerprint"></category><category term="md5"></category></entry><entry><title>Switched to Lineage OS</title><link href="https://blog.siphos.be/2017/04/switched-to-lineage-os/" rel="alternate"></link><published>2017-04-09T16:40:00+02:00</published><updated>2017-04-09T16:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-04-09:/2017/04/switched-to-lineage-os/</id><summary type="html">&lt;p&gt;I have been a long time user of &lt;a href="https://en.wikipedia.org/wiki/CyanogenMod"&gt;Cyanogenmod&lt;/a&gt;, 
which discontinued its services end of 2016. Due to lack of (continuous) time, I was not
able to switch over toward a different ROM. Also, I wasn't sure if
&lt;a href="https://www.lineageos.org/"&gt;LineageOS&lt;/a&gt; would remain the best choice for me or not. I wanted
to review other ROMs for my Samsung Galaxy SIII (the i9300 model) phone.&lt;/p&gt;
&lt;p&gt;Today, I made my choice and installed LineageOS.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I have been a long time user of &lt;a href="https://en.wikipedia.org/wiki/CyanogenMod"&gt;Cyanogenmod&lt;/a&gt;, 
which discontinued its services end of 2016. Due to lack of (continuous) time, I was not
able to switch over toward a different ROM. Also, I wasn't sure if
&lt;a href="https://www.lineageos.org/"&gt;LineageOS&lt;/a&gt; would remain the best choice for me or not. I wanted
to review other ROMs for my Samsung Galaxy SIII (the i9300 model) phone.&lt;/p&gt;
&lt;p&gt;Today, I made my choice and installed LineageOS.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The requirements list&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When looking for new ROMs to use, I had a number of requirements, some must-have, others
should-have or would-have (using the &lt;a href="https://en.wikipedia.org/wiki/MoSCoW_method"&gt;MoSCoW method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First of all, I want the ROM to be installable through ClockworkMod 6.4.0.something. This
is a mandatory requirement, because I don't want to venture out in installing a different
recovery (like &lt;a href="https://twrp.me/"&gt;TWRP&lt;/a&gt;). Not that much that I'm scared from it, but it might
require me to install stuff like Heimdal and update my SELinux policies on my system to allow
it to run, and has the additional risk that things still fail.&lt;/p&gt;
&lt;p&gt;I tried updating the recovery ROM in the past (a year or so ago) using the mobile application
approaches themselves (which require root access, that my phone had at the time) but it continuously
said that it failed and that I had to revert to the more traditional way of flashing the
recovery.&lt;/p&gt;
&lt;p&gt;Given that I know I need to upgrade within a day (and have other things planned today) I didn't
want to loose too much time in upgrading the recovery first.&lt;/p&gt;
&lt;p&gt;Second, the ROM had to allow OTA updates. With CyanogenMod, the OTA didn't fully work on
my phone (it downloaded and verified the images correctly, but couldn't install it
automatically - I had to reboot in recovery manually and install the ZIP), but it worked
sufficiently for me to easily update the phone on a weekly basis. I wanted to keep this luxury,
and who knows, move towards an end-to-end working OTA.&lt;/p&gt;
&lt;p&gt;Furthermore, the ROM had to support Android 7.1. I want the latest Android to see how long
this (nowadays aged) phone can handle things. Once the phone cannot get the latest Android
anymore, I'll probably move towards a new phone. But as long as I don't have to, I'll put
my money in other endeavours ;-)&lt;/p&gt;
&lt;p&gt;Finally, the ROM must be in active development. One of the reasons I want the latest Android
is also because I want to keep receiving the necessary security fixes. If a ROM doesn't
actively follow the security patches and code, then it might become (too) vulnerable for
comfort.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ROMs, ROMs everywhere (?)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, I visited the &lt;a href="https://forum.xda-developers.com/galaxy-s3/development"&gt;Galaxy S3 discussion&lt;/a&gt;
on the XDA-Developers site. This often contains enough material to find ROMs which have 
a somewhat active development base.&lt;/p&gt;
&lt;p&gt;I was still positively surprised by the activity on this quite old phone (the i9300 was
first released in May, 2012, making this phone almost 5 years old).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://forum.xda-developers.com/galaxy-s3/development/vanir-aosp-t3568393"&gt;Vanir&lt;/a&gt;
mod seemed to imply that TWRP was required, but past articles on Vanir showed that CWM
should also work. However, from the discussion I gathered that it is based on LineageOS.
Not that that's bad, but it makes LineageOS the "preferred" ROM first (default installed
software list, larger upstream community, etc.)&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://forum.resurrectionremix.com/"&gt;Ressurrection Remix&lt;/a&gt;
shows a very active discussion with good feedback from the developer(s). It is based on
a number of other resources (including CyanogenMod), so seems to borrow and implement
various other features. Although I got the slight impression that it would be a bit more
filled with applications I might not want, I kept it on the short-list.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forum.xda-developers.com/galaxy-s3/development/slimrom-t3580824"&gt;SLIMROM&lt;/a&gt; is
based on AOSP (the Android Open Source Project). It doesn't seem to support OTA though,
and its release history is currently still premature. However, I will keep an eye on this
one for future reference.&lt;/p&gt;
&lt;p&gt;After a while, I started looking for ROMs based on AOSP, as the majority of ROMs shown
are based on LineageOS (abbreviated to LOS). Apparently, for the Samsung S3, LineageOS
seems to be one of the most popular sources (and ROMs).&lt;/p&gt;
&lt;p&gt;So I put my attention to LineageOS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It &lt;a href="https://lineageosrom.org/install-lineageos-cwm/"&gt;supports CWM installations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It offers OTA update support&lt;/li&gt;
&lt;li&gt;It closely tracks upstream&lt;/li&gt;
&lt;li&gt;It is in active development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using LineageOS without root&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While deciding to use LineageOS or go through with additional ROM seeking, I stumbled
upon the installation instructions that showed that the ROM can be installed without
automatically enabling rooted Android access. I'm not sure if this was the case with
Cyanogenmod (I've been running with a rooted Cyanogenmod for too long to remember) but
it opened a possiblity for me...&lt;/p&gt;
&lt;p&gt;Personally, I don't mind having a rooted phone, as long as it is the user who decides
which applications can get root access and which can't. For me, the two applications
that used root access was an open source ad blocker called &lt;a href="https://adaway.org/"&gt;AdAway&lt;/a&gt;
and the Android shell (for troubleshooting purposes, such as killing the media server
if it locked my camera).&lt;/p&gt;
&lt;p&gt;But some applications seem to think that a rooted phone automatically means that the
phone is open access and full of malware. It is hard to find any trustworthy, academical
research on the actual secure state of rooted versus non-rooted devices. I believe 
that proper application vetting (don't install applications that aren't popular
and long-existing, check the application vendors, etc.) and keeping your phone up-to-date
is much more important than not rooting.&lt;/p&gt;
&lt;p&gt;And although these applications happily function on old, unpatched Android 4.x devices
they refuse to function on my (rooted) Android 7.1 phone. So, the ability to install
LineageOS without root (rooting actually requires flashing an additional package) is
a nice thing as I can start with a non-rooted device first, and switch back to a rooted
device if I need it later.&lt;/p&gt;
&lt;p&gt;With that, I decided to flash my phone with the latest LineageOS nightly for my phone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Switching password manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I tend to use such ROM switches (or, in case of CyanogenMod, major version upgrades)
as a time to revisit the mobile application list, and reduce it to what I really used
the last few months.&lt;/p&gt;
&lt;p&gt;One of the changes I did on my mobile application list is switch the password application.
I used to use &lt;a href="https://play.google.com/store/apps/details?id=ebeletskiy.gmail.com.passwords"&gt;Remember Passwords&lt;/a&gt;
but it hasn't seen updates for quite some time, and the backup import failed last
time I migrated to a higher CyanogenMod version (possibly Android version related).
Because I don't want to synchronize the passwords or see the application have any Internet
oriented activity, I now use &lt;a href="https://play.google.com/store/apps/details?id=keepass2android.keepass2android_nonet"&gt;Keepass2Android Offline&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is for passwords which I don't auto-generate using &lt;a href="https://chriszarate.github.io/supergenpass/"&gt;SuperGenPass&lt;/a&gt;,
my favorite password manager. I don't use the bookmarklet approach myself, but download
and run it separately when generating passwords - or use a &lt;a href="https://play.google.com/store/apps/details?id=info.staticfree.SuperGenPass&amp;amp;hl=en"&gt;SuperGenPass mobile application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First impressions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is too soon to say if it is fully functional or not. Most standard functionality
works OK (phone, SMS, camera) but it is only after a few days that specific issues
can come up.&lt;/p&gt;
&lt;p&gt;Only the first boot was very slow (probably because it was optimizing the application
list in the background), the second boot was well below half a minute. I didn't count it,
but it's fast enough for me.&lt;/p&gt;</content><category term="Misc"></category><category term="cyanogenmod"></category><category term="lineageos"></category><category term="mobile"></category><category term="android"></category></entry><entry><title>cvechecker 3.8 released</title><link href="https://blog.siphos.be/2017/03/cvechecker-3.8-released/" rel="alternate"></link><published>2017-03-27T19:00:00+02:00</published><updated>2017-03-27T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-27:/2017/03/cvechecker-3.8-released/</id><summary type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;


&lt;p&gt;It is recommended to remove any historical files from &lt;code&gt;/var/lib/cvechecker/cache&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rm /var/lib/cvechecker/cache/nvdcve-2.0-2017.*
~# rm /var/lib/cvechecker/cache/nvdcve-2.0-modified.*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will make sure that the next run of &lt;code&gt;pullcves pull&lt;/code&gt; will re-download those files, and attempt to load
the resulting CVEs back in the database.&lt;/p&gt;
&lt;p&gt;Sorry for this issue :-(&lt;/p&gt;</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Handling certificates in Gentoo Linux</title><link href="https://blog.siphos.be/2017/03/handling-certificates-in-gentoo-linux/" rel="alternate"></link><published>2017-03-06T22:20:00+01:00</published><updated>2017-03-06T22:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-06:/2017/03/handling-certificates-in-gentoo-linux/</id><summary type="html">&lt;p&gt;I recently created a new article on the Gentoo Wiki titled &lt;a href="https://wiki.gentoo.org/wiki/Certificates"&gt;Certificates&lt;/a&gt;
which talks about how to handle certificate stores on Gentoo Linux. The write-up
of the article (which might still change name later, because it does not handle
&lt;em&gt;everything&lt;/em&gt; about certificates, mostly how to handle certificate stores) was
inspired by the observation that I had to adjust the certificate stores of both
Chromium and Firefox separately, even though they both use NSS.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I recently created a new article on the Gentoo Wiki titled &lt;a href="https://wiki.gentoo.org/wiki/Certificates"&gt;Certificates&lt;/a&gt;
which talks about how to handle certificate stores on Gentoo Linux. The write-up
of the article (which might still change name later, because it does not handle
&lt;em&gt;everything&lt;/em&gt; about certificates, mostly how to handle certificate stores) was
inspired by the observation that I had to adjust the certificate stores of both
Chromium and Firefox separately, even though they both use NSS.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Certificates?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, when a secure communication is established from a browser to a site (or any
other interaction that uses SSL/TLS, but let's stay with the browser example for now)
part of the exchange is to ensure that the target site is actually the site it claims
to be. Don't want someone else to trick you into giving your e-mail credentials do you?&lt;/p&gt;
&lt;p&gt;To establish this, the certificate presented by the remote site is validated (alongside
other &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake"&gt;handshake steps&lt;/a&gt;).
A certificate contains a public key, as well as information about what the certificate can
be used for, and who (or what) the certificate represents. In case of a site, the identification
is (or should be) tied to the fully qualified domain name.&lt;/p&gt;
&lt;p&gt;Of course, everyone could create a certificate for accounts.google.com and try to trick you
into leaving your credentials. So, part of the validation of a certificate is to verify that
it is signed by a third party that you trust to only sign certificates that are trustworthy.
And to validate this signature, you hence need the certificate of this third party as well.&lt;/p&gt;
&lt;p&gt;So, what about this certificate? Well, turns out, this one is also often signed by
another certificate, and so on, until you reach the "top" of the certificate tree. This top
certificate is called the "root certificate". And because we still have to establish that this
certificate is trustworthy, we need another way to accomplish this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enter certificate stores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The root certificates of these trusted third parties (well, let us call them "Certificate Authorities"
from now onward, because they &lt;a href="https://en.wikipedia.org/wiki/DigiNotar"&gt;sometimes will lose your trust&lt;/a&gt;)
need to be reachable by the browser. The location where they are stored in is (often) called
the truststore (a naming that I came across when dealing with Java and which stuck).&lt;/p&gt;
&lt;p&gt;So, what I wanted to accomplish was to remove a particular CA certificate from the certificate
store. I assumed that, because Chromium and Firefox both use NSS as the library to support their
cryptographic uses, they would also both use the store location at &lt;code&gt;~/.pki/nssdb&lt;/code&gt;. That was wrong.&lt;/p&gt;
&lt;p&gt;Another assumption I had was that NSS also uses the &lt;code&gt;/etc/pki/nssdb&lt;/code&gt; location as a system-wide one.
Wrong again (not that NSS doesn't allow this, but it seems that it is very much up to, and often
ignored by, the NSS-implementing applications).&lt;/p&gt;
&lt;p&gt;Oh, and I also assumed that there wouldn't be a hard-coded list in the application. Yup. Wrong again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How NSS tracks root CA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Basically, NSS has a hard-coded root CA list inside the &lt;code&gt;libnssckbi.so&lt;/code&gt; file. On Gentoo, this
file is provided by the &lt;code&gt;dev-libs/nss&lt;/code&gt; package. Because it is hard-coded, it seemed like there
was little I could do to remove it, yet still through the user interfaces offered by Firefox and
Chromium I was able to remove the trust bits from the certificate.&lt;/p&gt;
&lt;p&gt;Turns out that Firefox (inside &lt;code&gt;~/.mozilla/firefox/*.default&lt;/code&gt;) and Chromium (inside &lt;code&gt;~/.pki/nssdb&lt;/code&gt;)
store the (modified) trust bits for those locations, so that the hardcoded list does not need to
be altered if all I want to do was revoke the trust on a specific CA. And it isn't that this hard-coded
list is a bad list: Mozilla has a &lt;a href="https://www.mozilla.org/en-US/about/governance/policies/security-group/certs/"&gt;CA Certificate Program&lt;/a&gt;
which controls the CAs that are accepted inside this store.&lt;/p&gt;
&lt;p&gt;Still, I find it sad that the system-wide location (at &lt;code&gt;/etc/pki/nssdb&lt;/code&gt;) is not by default used as
well (or I have something wrong on my system that makes it so). On a multi-user system, administrators
who want to have some control over the certificate stores might need to either use login scripts to
manipulate the user certificate stores, or adapt the user files directly currently.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="certificates"></category><category term="nss"></category></entry><entry><title>cvechecker 3.7 released</title><link href="https://blog.siphos.be/2017/03/cvechecker-3.7-released/" rel="alternate"></link><published>2017-03-02T10:00:00+01:00</published><updated>2017-03-02T10:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-02:/2017/03/cvechecker-3.7-released/</id><summary type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What has been changed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, what has changed? With this release (now at version 3.7) two bugs have been
fixed, one having a wrong URL in the CVE download and the other about the CVE
sequence numbers.&lt;/p&gt;
&lt;p&gt;The first bug was an annoying one, which I should have fixed a long time ago.
Well, it was fixed in the repository, but I didn't make a new release for it. 
When downloading the &lt;code&gt;nvdcve-2.0-Modified.xml&lt;/code&gt; file, the &lt;code&gt;pullcves&lt;/code&gt; command used
the lowercase filename, which doesn't exist.&lt;/p&gt;
&lt;p&gt;The second bug is about parsing the CVE sequence. On &lt;a href="https://cve.mitre.org/cve/identifiers/syntaxchange.html"&gt;January 2014&lt;/a&gt;
the syntax changed to allow for sequence identifiers longer than 4 digits. The
cvechecker tool however did a hard validation on the length of the identifier,
and cut off longer fields.&lt;/p&gt;
&lt;p&gt;That means that some CVE reports failed to parse in cvechecker, and thus cvechecker
didn't "know" about these vulnerabilities. This has been fixed in this release,
although I am not fully satisfied...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What still needs to be done&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The codebase for cvechecker is from 2010, and is actually based on a prototype
that I wrote which I decided not to rewrite into proper code. As a result, the
code is not up to par.&lt;/p&gt;
&lt;p&gt;I'm going to gradually improve and clean up the code in the next few [insert
timeperiod here]. I don't know if there will be feature improvements in the
next few releases (not that there aren't many feature enhancements needed) but
I hope that, once the code is improved, new functionality can be added more
easily.&lt;/p&gt;
&lt;p&gt;But that's for another time. Right now, enjoy the new release.&lt;/p&gt;</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>I missed FOSDEM</title><link href="https://blog.siphos.be/2017/02/i-missed-fosdem/" rel="alternate"></link><published>2017-02-07T17:06:00+01:00</published><updated>2017-02-07T17:06:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-02-07:/2017/02/i-missed-fosdem/</id><content type="html">&lt;p&gt;I sadly had to miss out on the FOSDEM event. The entire weekend was filled with
me being apathetic, feverish and overall zombie-like. Yes, sickness can be cruel.
It wasn't until today that I had the energy back to fire up my laptop.&lt;/p&gt;
&lt;p&gt;Sorry for the crew that I promised to meet at FOSDEM. I'll make it up, somehow.&lt;/p&gt;
</content><category term="Misc"></category><category term="gentoo"></category><category term="fosdem"></category></entry><entry><title>SELinux System Administration, 2nd Edition</title><link href="https://blog.siphos.be/2016/12/selinux-system-administration-2nd-edition/" rel="alternate"></link><published>2016-12-22T19:26:00+01:00</published><updated>2016-12-22T19:26:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-12-22:/2016/12/selinux-system-administration-2nd-edition/</id><content type="html">&lt;p&gt;While still working on a few other projects, one of the time consumers of the
past half year (haven't you noticed? my blog was quite silent) has come to an
end: the &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
book is now available. With almost double the amount of pages and a serious
update of the content, the book can now be bought either through Packt Publishing
itself, or the various online bookstores such as &lt;a href="https://www.amazon.com/SELinux-System-Administration-Sven-Vermeulen-ebook/dp/B01LWM02WI"&gt;Amazon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the holidays now approaching, I hope to be able to execute a few tasks
within the Gentoo community (and of the Gentoo Foundation) and get back on track.
Luckily, my absence was not jeopardizing the state of &lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;SELinux&lt;/a&gt;
in Gentoo thanks to the efforts of Jason Zaman.&lt;/p&gt;
</content><category term="SELinux"></category><category term="selinux"></category><category term="gentoo"></category><category term="rhel"></category><category term="redhat"></category><category term="packt"></category><category term="book"></category><category term="publishing"></category></entry><entry><title>GnuPG: private key suddenly missing?</title><link href="https://blog.siphos.be/2016/10/gnupg-private-key-suddenly-missing/" rel="alternate"></link><published>2016-10-12T18:56:00+02:00</published><updated>2016-10-12T18:56:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-10-12:/2016/10/gnupg-private-key-suddenly-missing/</id><summary type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can't find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can&amp;#39;t find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;GnuPG upgrade to 2.1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The update I did also upgraded GnuPG to the 2.1 series. This version has &lt;a href="https://www.gnupg.org/faq/whats-new-in-2.1.html"&gt;quite
a few updates&lt;/a&gt;, one of which is
a change towards a new private key storage approach. I thought that it might have
done a wrong conversion, or that the key which was used was of a particular method
or strength that suddenly wasn't supported anymore (PGP-2 is mentioned in the
article).&lt;/p&gt;
&lt;p&gt;But the key is a relatively standard RSA4096 one. Yet still, when I listed my
private keys, I did not see this key. I even tried to re-import the &lt;code&gt;secring.gpg&lt;/code&gt;
file, but it only found private keys that it already saw previously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I'm blind - the key never disappeared&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Luckily, when I tried to sign something with the key, &lt;code&gt;gpg-agent&lt;/code&gt; still asked me
for the passphraze that I had used for a while on that key. So it isn't gone. What
happened?&lt;/p&gt;
&lt;p&gt;Well, the key id is not my private key id, but the key id of one of the subkeys.
Previously, &lt;code&gt;gpg-agent&lt;/code&gt; sought and found the private key associated with the subkey,
but now it no longer does. I don't know if this is a bug in the past that I accidentally
used, or if this is a bug in the new version. I might investigate that a bit more,
but right now I'm happy that I found it.&lt;/p&gt;
&lt;p&gt;All I had to do was use the right key id in keychain, and things worked again.&lt;/p&gt;
&lt;p&gt;Good, now I can continue debugging networking issues with an azure-hosted system...&lt;/p&gt;</content><category term="Free-Software"></category><category term="gnupg"></category></entry><entry><title>We do not ship SELinux sandbox</title><link href="https://blog.siphos.be/2016/09/we-do-not-ship-selinux-sandbox/" rel="alternate"></link><published>2016-09-27T20:47:00+02:00</published><updated>2016-09-27T20:47:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-09-27:/2016/09/we-do-not-ship-selinux-sandbox/</id><summary type="html">&lt;p&gt;A few days ago a vulnerability was reported in the SELinux sandbox user space
utility. The utility is part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. Luckily, Gentoo's
&lt;code&gt;sys-apps/policycoreutils&lt;/code&gt; package is not vulnerable - and not because we were
clairvoyant about this issue, but because we don't ship this utility.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago a vulnerability was reported in the SELinux sandbox user space
utility. The utility is part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. Luckily, Gentoo's
&lt;code&gt;sys-apps/policycoreutils&lt;/code&gt; package is not vulnerable - and not because we were
clairvoyant about this issue, but because we don't ship this utility.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is the SELinux sandbox?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux sandbox utility, aptly named &lt;code&gt;sandbox&lt;/code&gt;, is a simple C application which
executes its arguments, but only after ensuring that the task it launches is
going to run in the &lt;code&gt;sandbox_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;This domain is specifically crafted to allow applications most standard privileges
needed for interacting with the user (so that the user can of course still use the
application) but removes many permissions that might be abused to either obtain 
information from the system, or use to try and exploit vulnerabilities to gain
more privileges. It also hides a number of resources on the system through
namespaces.&lt;/p&gt;
&lt;p&gt;It was &lt;a href="http://danwalsh.livejournal.com/28545.html"&gt;developed in 2009&lt;/a&gt; for Fedora
and Red Hat. Given the necessary SELinux policy support though, it was usable on
other distributions as well, and thus became part of the SELinux user space itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is the vulnerability about?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux sandbox utility used an execution approach that did not shield off
the users' terminal access sufficiently. In the &lt;a href="http://www.openwall.com/lists/oss-security/2016/09/25/1"&gt;POC post&lt;/a&gt;
we notice that characters could be sent to the terminal through the &lt;code&gt;ioctl()&lt;/code&gt;
function (which executes the ioctl system call used for input/output operations
against devices) which are eventually executed when the application finishes.&lt;/p&gt;
&lt;p&gt;That's bad of course. Hence the CVE-2016-7545 registration, and of course also
a possible &lt;a href="https://github.com/SELinuxProject/selinux/commit/acca96a135a4d2a028ba9b636886af99c0915379"&gt;fix has been committed upstream&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why isn't Gentoo vulnerable / shipping with SELinux sandbox?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There's some history involved why Gentoo does not ship the SELinux sandbox (anymore).&lt;/p&gt;
&lt;p&gt;First of all, Gentoo already has a command that is called &lt;code&gt;sandbox&lt;/code&gt;, installed through
the &lt;code&gt;sys-apps/sandbox&lt;/code&gt; application. So back in the days that we still shipped with
the SELinux sandbox, we continuously had to patch &lt;code&gt;policycoreutils&lt;/code&gt; to use a
different name for the sandbox application (we used &lt;code&gt;sesandbox&lt;/code&gt; then).&lt;/p&gt;
&lt;p&gt;But then we had a couple of security issues with the SELinux sandbox application.
In 2011, &lt;a href="http://www.cvedetails.com/cve/CVE-2011-1011/"&gt;CVE-2011-1011&lt;/a&gt;
came up in which the &lt;code&gt;seunshare_mount&lt;/code&gt; function had a security issue. And in 2014,
&lt;a href="http://www.cvedetails.com/cve/CVE-2014-3215/"&gt;CVE-2014-3215&lt;/a&gt; came up with - again -
a security issue with &lt;code&gt;seunshare&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At that point, I had enough of this sandbox utility. First of all, it never quite worked
enough on Gentoo as it is (as it also requires a policy which is not part of the
upstream release) and given its wide open access approach (it was meant to contain
various types of workloads, so security concessions had to be made), I decided to
&lt;a href="http://blog.siphos.be/2014/05/dropping-sesandbox-support/"&gt;no longer support the SELinux sandbox in Gentoo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;None of the Gentoo SELinux users ever approached me with the question to add it back.&lt;/p&gt;
&lt;p&gt;And that is why Gentoo is not vulnerable to this specific issue.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="sandbox"></category><category term="gentoo"></category><category term="vulnerability"></category><category term="seunshare"></category></entry><entry><title>Mounting QEMU images</title><link href="https://blog.siphos.be/2016/09/mounting-qemu-images/" rel="alternate"></link><published>2016-09-26T19:26:00+02:00</published><updated>2016-09-26T19:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-09-26:/2016/09/mounting-qemu-images/</id><summary type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Mounting qemu images on the system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To resolve this, I want to mount the image on my system, and edit the &lt;code&gt;/etc/shadow&lt;/code&gt;
file so that the root account is accessible. Once that is accomplished, I can
log on through the console and start setting up the system further.&lt;/p&gt;
&lt;p&gt;Images that are in the qcow2 format can be mounted through the nbd driver, but that
would require some updates on my local SELinux policy that I am too lazy to do right
now (I'll get to them eventually, but first need to finish the book). Still, if you
are interested in using nbd, see &lt;a href="https://www.kumari.net/index.php/system-adminstration/49-mounting-a-qemu-image"&gt;these instructions&lt;/a&gt;
or a &lt;a href="https://forums.gentoo.org/viewtopic-t-822672.html"&gt;related thread&lt;/a&gt; on the Gentoo
Forums.&lt;/p&gt;
&lt;p&gt;Luckily, storage is cheap (even SSD disks), so I quickly converted the qcow2 images
into raw images:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ qemu-img convert root.qcow2 root.raw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the image now available in raw format, I can use the loop devices to mount
the image(s) on my system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# losetup /dev/loop0 root.raw
~# kpartx -a /dev/loop0
~# mount /dev/mapper/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;kpartx&lt;/code&gt; command will detect the partitions and ensure that those are
available: the first partition becomes available at &lt;code&gt;/dev/loop0p1&lt;/code&gt;, the
second &lt;code&gt;/dev/loop0p2&lt;/code&gt; and so forth.&lt;/p&gt;
&lt;p&gt;With the image now mounted, let's update the &lt;code&gt;/etc/shadow&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Placing a new password hash in the shadow file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A google search quickly revealed that the following command generates
a shadow-compatible hash for a password:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl passwd -1 MyMightyPassword
$1$BHbMVz9i$qYHmULtXIY3dqZkyfW/oO.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The challenge wasn't to find the hash though, but to edit it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# vim /mnt/etc/shadow
vim: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The image that I downloaded used SELinux (of course), which meant that the &lt;code&gt;shadow&lt;/code&gt;
file was labeled with &lt;code&gt;shadow_t&lt;/code&gt; which I am not allowed to access. And I didn't
want to put SELinux in permissive mode just for this (sometimes I /do/ have some
time left, apparently).&lt;/p&gt;
&lt;p&gt;So I remounted the image, but now with the &lt;code&gt;context=&lt;/code&gt; mount option, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -o context=&amp;quot;system_u:object_r:var_t:s0: /dev/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now all files are labeled with &lt;code&gt;var_t&lt;/code&gt; which I do have permissions to edit. But
I also need to take care that the files that I edited get the proper label again.
There are a number of ways to accomplish this. I chose to create a &lt;code&gt;.autorelabel&lt;/code&gt;
file in the root of the partition. Red Hat based distributions will pick this up
and force a file system relabeling operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unmounting the file system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After making the changes, I can now unmount the file system again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# umount /mnt
~# kpart -d /dev/loop0
~# losetup -d /dev/loop0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that done, I had root access to the image and could start testing out
my own set of commands.&lt;/p&gt;
&lt;p&gt;It did trigger my interest in the cloud-init setup though...&lt;/p&gt;</content><category term="Free-Software"></category><category term="qemu"></category></entry><entry><title>Comparing Hadoop with mainframe</title><link href="https://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/" rel="alternate"></link><published>2016-06-15T20:55:00+02:00</published><updated>2016-06-15T20:55:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-06-15:/2016/06/comparing-hadoop-with-mainframe/</id><summary type="html">&lt;p&gt;At my work, I have the pleasure of being involved in a big data project that
uses Hadoop as the primary platform for several services. As an architect, I
try to get to know the platform's capabilities, its potential use cases, its
surrounding ecosystem, etc. And although the implementation at work is not in
its final form (yay agile infrastructure releases) I do start to get a grasp of
where we might be going.&lt;/p&gt;
&lt;p&gt;For many analysts and architects, this Hadoop platform is a new kid on the block
so I have some work explaining what it is and what it is capable of. Not for the
fun of it, but to help the company make the right decisions, to support management
and operations, to lift the fear of new environments. One thing I've once said is
that "Hadoop is the poor man's mainframe", because I notice some high-level
similarities between the two.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At my work, I have the pleasure of being involved in a big data project that
uses Hadoop as the primary platform for several services. As an architect, I
try to get to know the platform's capabilities, its potential use cases, its
surrounding ecosystem, etc. And although the implementation at work is not in
its final form (yay agile infrastructure releases) I do start to get a grasp of
where we might be going.&lt;/p&gt;
&lt;p&gt;For many analysts and architects, this Hadoop platform is a new kid on the block
so I have some work explaining what it is and what it is capable of. Not for the
fun of it, but to help the company make the right decisions, to support management
and operations, to lift the fear of new environments. One thing I've once said is
that "Hadoop is the poor man's mainframe", because I notice some high-level
similarities between the two.&lt;/p&gt;


&lt;p&gt;Somehow, it stuck, and I was asked to elaborate. So why not bring these points
into a nice blog post :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The big fat disclaimer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, before embarking on this comparison, I would like to state that I am &lt;strong&gt;not&lt;/strong&gt;
saying that Hadoop offers the same services, or even quality and functionality
of what can be found in mainframe environments. Considering how much time, effort
and experience was already put in the mainframe platform, it would be strange if
Hadoop could match the same. This post is to seek some similarities and, who knows,
learn a few more tricks from one or another.&lt;/p&gt;
&lt;p&gt;Second, I am not an avid mainframe knowledgeable person. I've been involved as
an IT architect in database and workload automation technical domains, which also
spanned the mainframe parts of it, but most of the effort was within the distributed
world. Mainframes remain somewhat opaque to me. Still, that shouldn't prevent me
from making any comparisons for those areas that I do have some grasp on.&lt;/p&gt;
&lt;p&gt;And if my current understanding is just wrong, I'm sure that I'll learn from the
comments that you can leave behind!&lt;/p&gt;
&lt;p&gt;With that being said, here it goes...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reliability, Availability, Serviceability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's start with some of the promises that both platforms make - and generally are
also able to deliver. Those promises are of reliability, availability and serviceability.&lt;/p&gt;
&lt;p&gt;For the mainframe platform, these quality attributes are shown as the &lt;a href="https://www.ibm.com/support/knowledgecenter/zosbasics/com.ibm.zos.zmainframe/zconc_RAS.htm"&gt;mainframe strengths&lt;/a&gt;.
The platform's hardware has extensive self-checking and self-recovery
capabilities, the systems can recover from failed components without service
interruption, and failures can be quickly determined and resolved. On the mainframes,
this is done through a good balance and alignment of hardware and software, design
decisions and - in my opinion - tight control over the various components and
services.&lt;/p&gt;
&lt;p&gt;I notice the same promises on Hadoop. Various components are checking the state
of the hardware and other components, and when something fails, it is often 
automatically recovered without impacting services. Instead of tight control
over the components and services, Hadoop uses a service architecture and APIs
with Java virtual machine abstractions.&lt;/p&gt;
&lt;p&gt;Let's consider hardware changes. &lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;hardware failure and component substitutions&lt;/strong&gt;, both platforms are capable
of dealing with those without service disruption.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mainframe probably has a better reputation in this matter, as its components
  have a very high Mean Time Between Failure (MTBF), and many - if not all - of
  the components are set up in a redundant fashion. Lots of error detection and
  failure detection processes try to detect if a component is close to failure,
  and ensure proper transitioning of any workload towards the other components
  without impact.&lt;/li&gt;
&lt;li&gt;Hadoop uses redundancy on a server level. If a complete server fails, Hadoop
  is usually able to deal with this without impact. Either the sensor-like
  services disable a node before it goes haywire, or the workload and data that
  was running on the failed node is restarted on a different node. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hardware (component) failures on the mainframe side will not impact the services
and running transactions. Component failures on Hadoop might have a noticeable
impact (especially if it is OLTP-like workload), but will be quickly recovered.&lt;/p&gt;
&lt;p&gt;Failures are more likely to happen on Hadoop clusters though, as it was designed
to work with many systems that have a worse MTBF design than a mainframe. The
focus within Hadoop is on resiliency and fast recoverability. Depending on the
service that is being used, active redundancy can be in use (so disruptions are
not visible to the user).&lt;/p&gt;
&lt;p&gt;If the Hadoop workload includes anything that resembles online transactional
processing, you're still better off with enterprise-grade hardware such as ECC
memory to at least allow improved hardware failure detection (and perform
proactive workload management). CPU failures are not that common (at least not
those without any upfront Machine Check Exception - MCE), and disk/controller
failures are handled through the abstraction of HDFS anyway.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;system substitutions&lt;/strong&gt;, I think both platforms can deal with this in a
dynamic fashion as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the mainframe side (and I'm guessing here) it is possible to switch
  machines with no service impact &lt;em&gt;if&lt;/em&gt; the services are running on LPARs
  that are joined together in a Parallel Sysplex setup (sort-of clustering
  through the use of the Coupling Facilities of mainframe, which is supported
  through high-speed data links and services for handling data sharing and
  IPC across LPARs). My company 
  &lt;a href="https://www-03.ibm.com/press/us/en/pressrelease/47812.wss"&gt;switched to the z13 mainframe&lt;/a&gt;
  last year, and was able to keep core services available during the migration.&lt;/li&gt;
&lt;li&gt;For Hadoop systems, the redundancy on system level is part of its design.
  Extending clusters, removing nodes, moving services, ... can be done with
  no impact. For instance, switching the active HiveServer2 instance means
  de-registering it in the ZooKeeper service. New client connects are then no
  longer served by that HiveServer2 instance, while active client connections
  remain until finished.
  There are also in-memory data grid solutions such as through the Ignite
  project, allowing for data sharing and IPC across nodes, as well as
  building up memory-based services with Arrow, allowing for efficient
  memory transfers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, also &lt;strong&gt;application level code failures&lt;/strong&gt; tend to only disrupt that
application, and not the other users. Be it because of different address
spaces and tight runtime control (mainframe) or the use of different
containers / JVMs for the applications (Hadoop), this is a good feat to have
(even though it is not something that differentiates these platforms from
other platforms or operating systems).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let's talk workloads&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When we look at a mainframe setup, we generally look at different workload
patterns as well. There are basically two main workload approaches for the
mainframe: batch, and On-Line Transactional Processing (OLTP) workload. In
the OLTP type, there is often an additional distinction between synchronous
OLTP and asynchronous OLTP (usually message-based). &lt;/p&gt;
&lt;p&gt;Well, we have the same on Hadoop. It was once a pure batch-driven platform
(and many of its components are still using batches or micro-batches in their
underlying designs) but now also provides OLTP workload capabilities. Most of
the OLTP workload on Hadoop is in the form of SQL-like or NoSQL database
management systems with transaction manager support though.&lt;/p&gt;
&lt;p&gt;To manage these (different) workloads, and to deal with prioritization of
the workload, both platforms offer the necessary services to make things both
managed as well as business (or "fit for purpose") focused.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using the Workload Manager (WLM) on the mainframe, policies can be set on
  the workload classes so that an over-demand of resources (cross-LPARs) results
  in the "right" amount of allocations for the "right" workload. To actually 
  manage jobs themselves, the Job Entry Subsystem (JES) to receive jobs and
  schedule then for processing on z/OS. For transactional workload, WLM
  provides the right resources to for instance the involved IMS regions.&lt;/li&gt;
&lt;li&gt;On Hadoop, workload management is done through Yet Another Resource 
  Negotiator (YARN), which uses (logical) queues for the different workloads.
  Workload (Application Containers) running through these queues can be, 
  resource-wise, controlled both on the queue level (high-level resource
  control) as well as process level (low-level resource control) through
  the use of Linux Control Groups (CGroups - when using Linux based systems
  course).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If I would try to compare both against each other, one might say that the
YARN queues are like WLMs service classes, and for batch applications, the
initiators on mainframe are like the Application Containers within YARN
queues. The latter can also be somewhat compared to IMS regions in case
of long-running Application Containers.&lt;/p&gt;
&lt;p&gt;The comparison will not hold completely though. WLM can be tuned based on
goals and will do dynamic decision making on the workloads depending on its
parameters, and even do live adjustments on the resources (through the 
System Resources Manager - SRM). Heavy focus on workload management on
mainframe environments is feasible because extending the available resources
on mainframes is usually expensive (additional Million Service Units - MSU).
On Hadoop, large cluster users who notice resource contention just tend
to extend the cluster further. It's a different approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Files and file access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another thing that tends to confuse some new users on Hadoop is its approach
to files. But when you know some things about the mainframe, this does remain
understandable.&lt;/p&gt;
&lt;p&gt;Both platforms have a sort-of master repository where data sets (mainframe)
or files (Hadoop) are registered in. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the mainframe, the catalog translates data set names into the right
  location (or points to other catalogs that do the same)&lt;/li&gt;
&lt;li&gt;On Hadoop, the Hadoop Distributed File System (HDFS) NameNode is
  responsible for tracking where files (well, blocks) are located across
  the various systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considering the use of the repository, both platforms thus require the
allocation of files and offer the necessary APIs to work with them. But
this small comparison does not end here.&lt;/p&gt;
&lt;p&gt;Depending on what you want to store (or access), the file format you use
is important as well.
- On mainframe, Virtual Storage Access Method (VSAM) provides both the
  methods (think of it as API) as well as format for a particular data
  organization. Inside a VSAM, multiple data entries can be stored in a
  structured way. Besides VSAM, there is also Partitioned Data Set/Extended
  (PDSE), which is more like a directory of sorts. Regular files are Physical
  Sequential (PS) data sets.
- On Hadoop, a number of file formats are supported which optimize the use of
  the files across the services. One is Avro, which holds both methods and
  format (not unlike VSAM), another is Optimized Row Columnar (ORC).  HDFS also
  has a number of options that can be enabled or set on certain locations (HDFS
  uses a folder-like structure) such as encryption, or on files themselves,
  such as replication factor.&lt;/p&gt;
&lt;p&gt;Although I don't say VSAM versus Avro are very similar (Hadoop focuses more on
the concept of files and then the file structure, whereas mainframe focuses on
the organization and allocation aspect if I'm not mistaken) they seem to be
sufficiently similar to get people's attention back on the table.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services all around&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What makes a platform tick is its multitude of supported services. And even
here can we find similarities between the two platforms.&lt;/p&gt;
&lt;p&gt;On mainframe, DBMS services can be offered my a multitude of softwares.
Relational DBMS services can be provided by IBM DB2, CA Datacom/DB, NOMAD, ...
while other database types are rendered by titles such as CA IDMS and ADABAS.
All these titles build upon the capabilities of the underlying components and
services to extend the platform's abilities.&lt;/p&gt;
&lt;p&gt;On Hadoop, several database technologies exist as well. Hive offers a SQL layer
on top of Hadoop managed data (so does Drill btw), HBase is a non-relational
database (mainly columnar store), Kylin provides distributed analytics, MapR-DB
offers a column-store NoSQL database, etc.&lt;/p&gt;
&lt;p&gt;When we look at transaction processing, the mainframe platform shows its decades
of experience with solutions such as CICS and IMS. Hadoop is still very much
at its infancy here, but with projects such as Omid or commercial software solutions
such as Splice Machine, transactional processing is coming here as well. Most
of these are based on underlying database management systems which are extended with
transactional properties.&lt;/p&gt;
&lt;p&gt;And services that offer messaging and queueing are also available on both
platforms: mainframe can enjoy Tibco Rendezvous and IBM WebSphere MQ, while
Hadoop is hitting the news with projects such as Kafka and Ignite.&lt;/p&gt;
&lt;p&gt;Services extend even beyond the ones that are directly user facing. For instance,
both platforms can easily be orchestrated using workload automation tooling.
Mainframe has a number of popular schedulers up its sleeve (such as IBM TWS,
BMC Control-M or CA Workload Automation) whereas Hadoop is generally easily
extended with the scheduling and workload automation software of the distributed
world (which, given its market, is dominated by the same vendors, although many
smaller ones exist as well). Hadoop also has its "own" little scheduling
infrastructure called Oozie.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Programming for the platforms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Platforms however are more than just the sum of the services and the properties
that it provides. Platforms are used to build solutions on, and that is true for
both mainframe as well as Hadoop.&lt;/p&gt;
&lt;p&gt;Let's first look at scripting - using interpreted languages. On mainframe, you
can use the Restructed Extended Executor (REXX) or CLIST (Command LIST). Hadoop
gives you Tez and Pig, as well as Python and R (through PySpark and SparkR).&lt;/p&gt;
&lt;p&gt;If you want to directly interact with the systems, mainframe offers the Time
Sharing Option/Extensions (TSO/E) and Interactive System Productivity Facility
(ISPF). For Hadoop, regular shells can be used, as well as service-specific
ones such as Spark shell. However, for end users, web-based services such 
as Ambari UI (Ambari Views) are generally better suited.&lt;/p&gt;
&lt;p&gt;If you're more fond of compiled code, mainframe supports you with COBOL, Java
(okay, it's "a bit" interpreted, but also compiled - don't shoot me here), C/C++
and all the other popular programming languages. Hadoop builds on top of Java,
but supports other languages such as Scala and allows you to run native
applications as well - it's all about using the right APIs.&lt;/p&gt;
&lt;p&gt;To support development efforts, Integrated Development Environments (IDEs) are
provided for both platforms as well. You can use Cobos, Micro Focus Enterprise
Developer, Rational Developer for System z, Topaz Workbench and more for
mainframe development. Hadoop has you covered with web-based notebook solutions
such as Zeppelin and JupyterHub, as well as client-level IDEs such as Eclipse
(with the Hadoop Development Tools plugins) and IntelliJ.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Governing and managing the platforms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, there is also the aspect of managing the platforms.&lt;/p&gt;
&lt;p&gt;When working on the mainframe, management tooling such as the Hardware
Management Console (HMC) and z/OS Management Facility (z/OSMF) cover operations
for both hardware and system resources. On Hadoop, central management
software such as Ambari, Cloudera Manager or Zettaset Orchestrator try to
cover the same needs - although most of these focus more on the software
side than on the hardware level.&lt;/p&gt;
&lt;p&gt;Both platforms also have a reasonable use for multiple roles: application
developers, end users, system engineers, database adminstrators, operators, 
system administrators, production control, etc. who all need some kind of access
to the platform to support their day-to-day duties. And when you talk roles,
you talk authorizations.&lt;/p&gt;
&lt;p&gt;On the mainframe, the Resource Access Control Facility (RACF) provides
access control and auditing facilities, and supports a multitude of services on
the mainframe (such as DB2, MQ, JES, ...). Many major Hadoop services, such
as HDFS, YARN, Hive and HBase support Ranger, providing a single pane for
security controls on the Hadoop platform.&lt;/p&gt;
&lt;p&gt;Both platforms also offer the necessary APIs or hooks through which system
developers can fine-tune the platform to fit the needs of the business, or
develop new integrated solutions - including security oriented ones. 
Hadoop's extensive plugin-based design (not explicitly
named) or mainframe's Security Access Facility (SAF) are just examples of this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Playing around&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Going for a mainframe or a Hadoop platform will always be a management decision.
Both platforms have specific roles and need particular profiles in order to
support them. They are both, in my opinion, also difficult to migrate away from
once you are really using them actively (lock-in) although it is more digestible
for Hadoop given its financial implications.&lt;/p&gt;
&lt;p&gt;Once you want to start meddling with it, getting access to a full platform used
to be hard (the coming age of cloud services makes that this is no longer the
case though), and both therefore had some potential "small deployment" uses.
Mainframe experience could be gained through the Hercules 390 emulator, whereas
most Hadoop distributions have a single-VM sandbox available for download.&lt;/p&gt;
&lt;p&gt;To do a full scale roll-out however is much harder to do by your own. You'll
need to have quite some experience or even expertise on so many levels that 
you will soon see that you need teams (plural) to get things done.&lt;/p&gt;
&lt;p&gt;This concludes my (apparently longer than expected) write-down of this matter.
If you don't agree, or are interested in some insights, be sure to comment!&lt;/p&gt;</content><category term="Hadoop"></category><category term="hadoop"></category><category term="mainframe"></category></entry><entry><title>Template was specified incorrectly</title><link href="https://blog.siphos.be/2016/03/template-was-specified-incorrectly/" rel="alternate"></link><published>2016-03-27T13:32:00+02:00</published><updated>2016-03-27T13:32:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-27:/2016/03/template-was-specified-incorrectly/</id><summary type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv 'testing', u'salt://top.sls'
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to 'merge' and multiple top files found. Top file merging order is undefined; for better results use 'same' option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what's all this about?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv &amp;#39;testing&amp;#39;, u&amp;#39;salt://top.sls&amp;#39;
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to &amp;#39;merge&amp;#39; and multiple top files found. Top file merging order is undefined; for better results use &amp;#39;same&amp;#39; option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So what's all this about?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;When using a single top file is preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to stick with a single top file, then the first error is (or at least, in my case)
caused by my environments not having a fall-back definition.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;/etc/salt/master&lt;/code&gt; configuration file had the following &lt;code&gt;file_roots&lt;/code&gt; setting:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The problem is that Salt expects ''a'' top file through the environment. What I had to do was to
set the fallback directory to the base directory again, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
    - /srv/salt/base
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this set, the error disappeared and both salt and myself were happy again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When multiple top files are preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you really want to use multiple top files (which is also a use case in my configuration),
then first we need to make sure that the top files of all environments correctly isolate the
minion matches. If two environments would match the same minion, then this approach becomes
more troublesome.&lt;/p&gt;
&lt;p&gt;On the one hand, we can just let saltstack merge the top files (default behavior) but the order
of the merging is undefined (and no, you can't set it using &lt;code&gt;env_order&lt;/code&gt;) which might result in 
salt states being executed in an unexpected order. If the definitions are done to such an extend
that this is not a problem, then you can just ignore the warning. See also
&lt;a href="https://github.com/saltstack/salt/issues/29104"&gt;bug 29104&lt;/a&gt; about the warning itself.&lt;/p&gt;
&lt;p&gt;But better would be to have the top files of the environment(s) isolated so that each environment
top file completely manages the entire environment. When that is the case, then we tell salt that
only the top file of the affected environment should be used. This is done using the following
setting in &lt;code&gt;/etc/salt/master&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;top_file_merging_strategy: same
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this is used, then the &lt;code&gt;env_order&lt;/code&gt; setting is used to define in which order the environments
are processed. &lt;/p&gt;
&lt;p&gt;Oh and if you're using &lt;code&gt;salt-ssh&lt;/code&gt;, then be sure to set the environment of the minion in the roster
file, as there is no running minion on the target system that informs salt about the environment 
to use otherwise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# In /etc/salt/roster
testserver:
  host: testserver.example.com
  minion_opts:
    environment: testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free-Software"></category><category term="salt"></category></entry><entry><title>Using salt-ssh with agent forwarding</title><link href="https://blog.siphos.be/2016/03/using-salt-ssh-with-agent-forwarding/" rel="alternate"></link><published>2016-03-26T19:57:00+01:00</published><updated>2016-03-26T19:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-26:/2016/03/using-salt-ssh-with-agent-forwarding/</id><summary type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Using SSH agent forwarding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anyway, I quickly got stuck with accessing minions over the SSH interface as it seemed that
salt requires its own SSH keys (I don't enable password-only authentication, most of the systems
use the &lt;a href="https://blog.flameeyes.eu/2013/03/openssh-6-2-adds-support-for-two-factor-authentication"&gt;AuthenticationMethods&lt;/a&gt;
approach to chain both key and passwords). But first things first, the current target uses regular
ssh key authentication (no chained approach, that's for later). But I don't want to assign
such a powerful key to my salt master (especially not if it would later also document the
passwords). I would like to use SSH agent forwarding.&lt;/p&gt;
&lt;p&gt;Luckily, salt does support that, it just &lt;a href="https://github.com/saltstack/salt/pull/31328/commits/024439186a0c51c0ac1242b38d6584d2abd1a534"&gt;forgot to document&lt;/a&gt;
it. Basically, what you need to do is update the roster file with the &lt;code&gt;priv:&lt;/code&gt; parameter
set to &lt;code&gt;agent-forwarding&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;myminion:
  host: myminion.example.com
  priv: agent-forwarding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It will use the &lt;code&gt;known_hosts&lt;/code&gt; file of the currently logged on user (the one executing
the &lt;code&gt;salt-ssh&lt;/code&gt; command) so make sure that the system's key is already known.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ salt-ssh myminion test.ping
myminion:
    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free-Software"></category><category term="salt"></category></entry><entry><title>Trying out imapsync</title><link href="https://blog.siphos.be/2016/03/trying-out-imapsync/" rel="alternate"></link><published>2016-03-13T12:57:00+01:00</published><updated>2016-03-13T12:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-13:/2016/03/trying-out-imapsync/</id><summary type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What I required&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The migration required that all mails, except for the spam and trash e-mails, were
migrated to another mail server. The migrated mails had to retain their status flags
(so unread mails had to remain unread while read mails had to remain read), and the
migration had to be done in two waves: one while the primary mail server was still
in use (where most of the mails where synchronized) and then, after switching the
mail servers (which was done through DNS changes) re-sync to fetch the final ones.&lt;/p&gt;
&lt;p&gt;I did not get access to the credentials of all mail boxes, but together with the
main administrator we enabled a sort-of shadow authentication system (a temporary
OpenLDAP installation) in which the same users were enabled, but with passwords that
will be used during the synchronization. The mailservers were then configured to
have a secondary interface available which used this OpenLDAP rather than the primary
authentication that was being used by the end users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using imapsync&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;imapsync&lt;/code&gt; is simple. It is a command-line application, and everything
configurable is done through command arguments. The basic ones are of course the
source and target definitions, as well as the authentication information for both
sides.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync \
  --host1 src-host --user1 src-user --password1 src-pw --authmech1 LOGIN --ssl1 \
  --host2 dst-host --user2 dst-user --password2 dst-pw --authmech2 LOGIN --ssl2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The use of the &lt;code&gt;--ssl1&lt;/code&gt; and &lt;code&gt;--ssl2&lt;/code&gt; is not to enable an older or newer version of
the SSL/TLS protocol. It just enables the use of SSL/TLS for the source host (&lt;code&gt;--ssl1&lt;/code&gt;)
and destination host (&lt;code&gt;--ssl2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This would just start synchronizing messages, but we need to include the necessary
directives to skip trash and spam mailboxes for instance. For this, the &lt;code&gt;--exclude&lt;/code&gt; parameter
can be used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --exclude &amp;quot;Trash|Spam|Drafts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is also possible to transform some mailbox names. For instance, if the source host
uses &lt;code&gt;Sent&lt;/code&gt; as the mailbox for sent mail, while the target has &lt;code&gt;Sent Items&lt;/code&gt;, then the
following would enable migrating mails between the right folders:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --folder &amp;quot;Sent&amp;quot; --regextrans2 &amp;#39;s/Sent/Sent Items/&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Conclusions and interesting resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the application was a breeze. I do recommend to create a test account on both sides
so that you can easily see the available folders, source and target naming conventions as
well as test if rerunning the application works flawlessly.&lt;/p&gt;
&lt;p&gt;In my case for instance, I had to add &lt;code&gt;--skipsize&lt;/code&gt; so that the application does not use the
mail sizes for comparing if a mail is already transferred or not, as the target mailserver
showed different mail sizes for the same mails. This was luckily often documented on the
various online tutorials about &lt;code&gt;imapsync&lt;/code&gt;, such as &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://seagrief.co.uk/2010/12/moving-to-google-apps-with-imapsync/"&gt;Moving to Google Apps with imapsync&lt;/a&gt; on seagrief.co.uk&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.zimbra.com/wiki/Guide_to_imapsync"&gt;Guide to imapsync&lt;/a&gt; on wiki.zimbra.com&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The migration took a while, but without major issues. Within a few hours, the mailboxes of all
users where correctly migrated.&lt;/p&gt;</content><category term="Free-Software"></category><category term="imapsync"></category></entry><entry><title>New cvechecker release</title><link href="https://blog.siphos.be/2015/11/new-cvechecker-release/" rel="alternate"></link><published>2015-11-07T11:07:00+01:00</published><updated>2015-11-07T11:07:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-11-07:/2015/11/new-cvechecker-release/</id><content type="html">&lt;p&gt;A short while ago I got the notification that pulling new CVE information was
no longer possible. The reason was that the NVD site did not support uncompressed
downloads anymore. The fix for cvechecker was simple, and it also gave me a reason
to push out a new release (after two years) which also includes various updates by
Christopher Warner.&lt;/p&gt;
&lt;p&gt;So &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker 3.6&lt;/a&gt; is now available
for general consumption.&lt;/p&gt;
</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Switching focus at work</title><link href="https://blog.siphos.be/2015/09/switching-focus-at-work/" rel="alternate"></link><published>2015-09-20T13:29:00+02:00</published><updated>2015-09-20T13:29:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-20:/2015/09/switching-focus-at-work/</id><summary type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;


&lt;p&gt;This means looking at the investments related to those domains, implementation
details, standards of use, features that we will or will not use, positioning
of products and so forth. To do this from an architecture point of view means
that I not only had to focus on the details of the technology and understand 
all their use, but also become a sort-of subject matter expert on those topics.
Luckily, I had (well, still have) great teams of DBAs (for the databases) and
batch teams (for the scheduling/workload automation) to keep things in the right
direction. &lt;/p&gt;
&lt;p&gt;I helped them with a (hopefully sufficiently) clear roadmap, investment track,
procurement, contract/terms and conditions for use, architectural decisions and
positioning and what not. And they helped me with understanding the various
components, learn about the best use of these, and of course implement the 
improvements that we collaboratively put on the roadmap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Times, they are changing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last week, I flipped over a page at work. Although I remain an IT architect
within the same architecture team, my focus shifts entirely. Instead of a fixed
domain, my focus is now more volatile. I leave behind the stability of 
organizationally anchored technology domains and go forward in a more tense
environment.&lt;/p&gt;
&lt;p&gt;Instead of looking at just two technology domains, I need to look at all of them,
and find the right balance between high flexibility demands (which might not want
to use current "standard" offerings) which come up from a very agile context, and
the almost non-negotionable requirements that are typical for financial institutions.&lt;/p&gt;
&lt;p&gt;The focus is also not primarily technology oriented anymore. I'll be part of an 
enterprise architecture team with direct business involvement and although my
main focus will be on the technology side, it'll also involve information
management, business processes and applications.&lt;/p&gt;
&lt;p&gt;The end goal is to set up a future-proof architecture in an agile, fast-moving
environment (contradictio in terminis ?) which has a main focus in data analytics
and information gathering/management. Yes, "big data", but more applied than what
some of the vendors try to sell us ;-)&lt;/p&gt;
&lt;p&gt;I'm currently finishing off the high-level design and use of a Hadoop platform,
and the next focus will be on a possible micro-service architecture using Docker.
I've been working on this Hadoop design for a while now (but then it was together
with my previous function at work) and given the evolving nature of Hadoop (and
the various services that surround it) I'm confident that it will not be the last
time I'm looking at it. &lt;/p&gt;
&lt;p&gt;Now let me hope I can keep things manageable ;-)&lt;/p&gt;</content><category term="Architecture"></category><category term="work"></category><category term="hadoop"></category><category term="docker"></category></entry><entry><title>Getting su to work in init scripts</title><link href="https://blog.siphos.be/2015/09/getting-su-to-work-in-init-scripts/" rel="alternate"></link><published>2015-09-14T16:37:00+02:00</published><updated>2015-09-14T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-14:/2015/09/getting-su-to-work-in-init-scripts/</id><summary type="html">&lt;p&gt;While developing an init script which has to switch user, I got a couple of
errors from SELinux and the system itself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~# rc-service hadoop-namenode format&lt;/span&gt;
&lt;span class="go"&gt;Authenticating root.&lt;/span&gt;
&lt;span class="go"&gt; * Formatting HDFS ...&lt;/span&gt;
&lt;span class="go"&gt;su: Authentication service cannot retrieve authentication info&lt;/span&gt;
&lt;span class="gp gp-VirtualEnv"&gt;(Ignored)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</summary><content type="html">&lt;p&gt;While developing an init script which has to switch user, I got a couple of
errors from SELinux and the system itself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~# rc-service hadoop-namenode format&lt;/span&gt;
&lt;span class="go"&gt;Authenticating root.&lt;/span&gt;
&lt;span class="go"&gt; * Formatting HDFS ...&lt;/span&gt;
&lt;span class="go"&gt;su: Authentication service cannot retrieve authentication info&lt;/span&gt;
&lt;span class="gp gp-VirtualEnv"&gt;(Ignored)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;The authentication log shows entries such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sep 14 20:20:05 localhost unix_chkpwd[5522]: could not obtain user info (hdfs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I've always had issues with getting su to work properly again. Now that I have
what I think is a working set, let me document it for later (as I still need to
review why they are needed):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Allow initrc_t to use unix_chkpwd to check entries&lt;/span&gt;
&lt;span class="c1"&gt;# Without it gives the retrieval failure&lt;/span&gt;
&lt;span class="n"&gt;auth_domtrans_chk_passwd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Allow initrc_t to query selinux access, otherwise avc assertion&lt;/span&gt;
&lt;span class="n"&gt;allow&lt;/span&gt; &lt;span class="n"&gt;initrc_t&lt;/span&gt; &lt;span class="nb"&gt;self&lt;/span&gt;&lt;span class="ss"&gt;:netlink_selinux_socket&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;create&lt;/span&gt; &lt;span class="n"&gt;read&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="n"&gt;selinux_compute_access_vector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Allow initrc_t to honor the pam_rootok setting&lt;/span&gt;
&lt;span class="n"&gt;allow&lt;/span&gt; &lt;span class="n"&gt;initrc_t&lt;/span&gt; &lt;span class="nb"&gt;self&lt;/span&gt;&lt;span class="ss"&gt;:passwd&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;passwd&lt;/span&gt; &lt;span class="n"&gt;rootok&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With these SELinux rules, switching the user works as expected from within an
init script.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="initrc"></category></entry><entry><title>Custom CIL SELinux policies in Gentoo</title><link href="https://blog.siphos.be/2015/09/custom-cil-selinux-policies-in-gentoo/" rel="alternate"></link><published>2015-09-10T07:13:00+02:00</published><updated>2015-09-10T07:13:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-10:/2015/09/custom-cil-selinux-policies-in-gentoo/</id><summary type="html">&lt;p&gt;In Gentoo, we have been supporting &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;custom policy packages&lt;/a&gt;
for a while now. Unlike most other distributions, which focus on binary packages,
Gentoo has always supported source-based packages as default (although 
&lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary packages&lt;/a&gt; are 
supported as well).&lt;/p&gt;
&lt;p&gt;A recent &lt;a href="https://gitweb.gentoo.org/repo/gentoo.git/commit/?id=8f2aa45db35bbf3a74f8db09ece9edac60e79ee4"&gt;commit&lt;/a&gt;
now also allows CIL files to be used.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In Gentoo, we have been supporting &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;custom policy packages&lt;/a&gt;
for a while now. Unlike most other distributions, which focus on binary packages,
Gentoo has always supported source-based packages as default (although 
&lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary packages&lt;/a&gt; are 
supported as well).&lt;/p&gt;
&lt;p&gt;A recent &lt;a href="https://gitweb.gentoo.org/repo/gentoo.git/commit/?id=8f2aa45db35bbf3a74f8db09ece9edac60e79ee4"&gt;commit&lt;/a&gt;
now also allows CIL files to be used.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Policy ebuilds, how they work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gentoo provides its own SELinux policy, based on the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Reference_policy"&gt;reference policy&lt;/a&gt;, 
and provides per-module ebuilds (packages). For instance, the SELinux policy for
the &lt;a href="https://packages.gentoo.org/package/app-misc/screen"&gt;screen&lt;/a&gt; package is
provided by the &lt;a href="https://packages.gentoo.org/package/sec-policy/selinux-screen"&gt;sec-policy/selinux-screen&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;The package itself is pretty straight forward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Copyright 1999-2015 Gentoo Foundation&lt;/span&gt;
&lt;span class="c1"&gt;# Distributed under the terms of the GNU General Public License v2&lt;/span&gt;
&lt;span class="c1"&gt;# $Id$&lt;/span&gt;
&lt;span class="nv"&gt;EAPI&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;IUSE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;MODS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;screen&amp;quot;&lt;/span&gt;

inherit selinux-policy-2

&lt;span class="nv"&gt;DESCRIPTION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELinux policy for screen&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$PV&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;9999&lt;/span&gt;* &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        &lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;~amd64 ~x86&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The real workhorse lays within a &lt;a href="https://devmanual.gentoo.org/eclass-writing/"&gt;Gentoo eclass&lt;/a&gt;,
something that can be seen as a library for ebuilds. It allows consolidation of functions and
activities so that a large set of ebuilds can be simplified. The more ebuilds are standardized,
the more development can be put inside an eclass instead of in the ebuilds. As a result, some
ebuilds are extremely simple, and the SELinux policy ebuilds are a good example of this.&lt;/p&gt;
&lt;p&gt;The eclass for SELinux policy ebuilds is called &lt;a href="https://devmanual.gentoo.org/eclass-reference/selinux-policy-2.eclass/index.html"&gt;selinux-policy-2.eclass&lt;/a&gt;
and holds a number of functionalities. One of these (the one we focus on right now)
is to support custom SELinux policy modules.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom SELinux policy ebuilds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever a user has a SELinux policy that is not part of the Gentoo policy repository,
then the user might want to provide these policies through packages still. This has
the advantage that Portage (or whatever package manager is used) is aware of the
policies on the system, and proper dependencies can be built in.&lt;/p&gt;
&lt;p&gt;To use a custom policy, the user needs to create an ebuild which informs the eclass
not only about the module name (through the &lt;code&gt;MODS&lt;/code&gt; variable) but also about the
policy files themselves. These files are put in the &lt;code&gt;files/&lt;/code&gt; location of the ebuild,
and referred to through the &lt;code&gt;POLICY_FILES&lt;/code&gt; variable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Copyright 1999-2015 Gentoo Foundation&lt;/span&gt;
&lt;span class="c1"&gt;# Distributed under the terms of the GNU General Public License v2&lt;/span&gt;
&lt;span class="c1"&gt;# $Id$&lt;/span&gt;
&lt;span class="nv"&gt;EAPI&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;IUSE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;MODS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;oracle&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;POLICY_FILES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;oracle.te oracle.if oracle.fc&amp;quot;&lt;/span&gt;

inherit selinux-policy-2

&lt;span class="nv"&gt;DESCRIPTION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELinux policy for screen&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$PV&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;9999&lt;/span&gt;* &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        &lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;~amd64 ~x86&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The eclass generally will try to build the policies, converting them into &lt;code&gt;.pp&lt;/code&gt;
files. With CIL, this is no longer needed. Instead, what we do is copy the &lt;code&gt;.cil&lt;/code&gt;
files straight into the location where we place the &lt;code&gt;.pp&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;From that point onwards, managing the &lt;code&gt;.cil&lt;/code&gt; files is similar to &lt;code&gt;.pp&lt;/code&gt; files.
They are loaded with &lt;code&gt;semodule -i&lt;/code&gt; and unloaded with &lt;code&gt;semodule -r&lt;/code&gt; when needed.&lt;/p&gt;
&lt;p&gt;Enabling CIL in our ebuilds is a small improvement (after the heavy workload
to support the 2.4 userspace) which allows Gentoo to stay ahead in the SELinux
world.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="cil"></category><category term="selinux"></category><category term="ebuild"></category><category term="eclass"></category></entry><entry><title>Using multiple OpenSSH daemons</title><link href="https://blog.siphos.be/2015/09/using-multiple-openssh-daemons/" rel="alternate"></link><published>2015-09-06T16:37:00+02:00</published><updated>2015-09-06T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-06:/2015/09/using-multiple-openssh-daemons/</id><summary type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Running multiple instances&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although I might get a similar result with a single OpenSSH instance, I
prefer to have multiple instances for this. The default OpenSSH port is used
for the non-administrative access whereas administrative access is on a
non-default port. This has a number of advantages...&lt;/p&gt;
&lt;p&gt;First of all, the SSH configurations are simple and clean. No complex
configurations, and more importantly: easy to manage through configuration
management tools like &lt;a href="http://saltstack.com/"&gt;SaltStack&lt;/a&gt;, my current favorite
orchestration/automation tool.&lt;/p&gt;
&lt;p&gt;Different instances also allow for different operational support services.
There is different monitoring for end-user SSH access versus administrative
SSH access. Also the &lt;a href="https://wiki.gentoo.org/wiki/Fail2ban"&gt;fail2ban&lt;/a&gt; configuration
is different for these instances.&lt;/p&gt;
&lt;p&gt;I can also easily shut down the non-administrative service while ensuring that
administrative access remains operational - something important in case of
changes and maintenance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with multiple instances and SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Beyond enabling a non-default port for SSH (i.e. by marking it as &lt;code&gt;ssh_port_t&lt;/code&gt;
as well) there is little additional tuning necessary, but that doesn't mean that
there is no additional tuning possible.&lt;/p&gt;
&lt;p&gt;For instance, we could leverage MCS' categories to only allow users (and thus the
SSH daemon) access to the files assigned only that category (and not the rest)
whereas the administrative SSH daemon can access all categories.&lt;/p&gt;
&lt;p&gt;On an MLS enabled system we could even use different sensitivity levels, allowing
the administrative SSH to access the full scala whereas the user-facing SSH can
only access the lowest sensitivity level. But as I don't use MLS myself, I won't go
into detail for this.&lt;/p&gt;
&lt;p&gt;A third possibility would be to fine-tune the permissions of the SSH daemons. However,
that would require different types for the daemon, which requires the daemons to be
started through different scripts (so that we first transition to dedicated 
types) before they execute the SSHd binary (which has the &lt;code&gt;sshd_exec_t&lt;/code&gt; type
assigned).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requiring pubkey and password authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recent OpenSSH daemons allow &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining multiple authentication methods&lt;/a&gt;
before access is granted. This allows the systems to force SSH key authentication first, and then -
after succesful authentication - require the password to be passed on as well. Or a
second step such as &lt;a href="https://wiki.archlinux.org/index.php/Google_Authenticator"&gt;Google Authenticator&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;AuthenticationMethods publickey,password
PasswordAuthentication yes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I don't use the Google Authenticator, but the &lt;a href="https://developers.yubico.com/yubico-pam/"&gt;Yubico PAM module&lt;/a&gt;
to require additional authentication through my U2F dongle (so ssh key, password
and u2f key). Don't consider this three-factor authentication: one thing I know
(password) and two things I have (U2F and ssh key). It's more that I have a couple
of devices with a valid SSH key (laptop, tablet, mobile) which are of course targets
for theft.&lt;/p&gt;
&lt;p&gt;The chance that both one of those devices is stolen &lt;em&gt;together&lt;/em&gt; with the U2F
dongle (which I don't keep attached to those devices of course) is somewhat less.&lt;/p&gt;</content><category term="Free-Software"></category><category term="openssh"></category><category term="ssh"></category><category term="u2f"></category><category term="selinux"></category></entry><entry><title>Maintaining packages and backporting</title><link href="https://blog.siphos.be/2015/09/maintaining-packages-and-backporting/" rel="alternate"></link><published>2015-09-02T20:33:00+02:00</published><updated>2015-09-02T20:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-02:/2015/09/maintaining-packages-and-backporting/</id><summary type="html">&lt;p&gt;A few days ago I committed a small update to &lt;code&gt;policycoreutils&lt;/code&gt;, a SELinux related
package that provides most of the management utilities for SELinux systems. The
fix was to get two patches (which are committed upstream) into the existing
release so that our users can benefit from the fixed issues without having to
wait for a new release.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago I committed a small update to &lt;code&gt;policycoreutils&lt;/code&gt;, a SELinux related
package that provides most of the management utilities for SELinux systems. The
fix was to get two patches (which are committed upstream) into the existing
release so that our users can benefit from the fixed issues without having to
wait for a new release.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Getting the patches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To capture the patches, I used &lt;code&gt;git&lt;/code&gt; together with the commit id:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ git format-patch -n -1 73b7ff41&lt;/span&gt;
&lt;span class="go"&gt;0001-Only-invoke-RPM-on-RPM-enabled-Linux-distributions.patch&lt;/span&gt;
&lt;span class="go"&gt;~$ git format-patch -n -1 4fbc6623&lt;/span&gt;
&lt;span class="go"&gt;0001-Set-self.sename-to-sename-after-calling-semanage_seu.patch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The two generated patch files contain all information about the commit. Thanks
to the &lt;code&gt;epatch&lt;/code&gt; support in the &lt;code&gt;eutils.eclass&lt;/code&gt;, these patch files are
immediately usable within Gentoo's ebuilds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Updating the ebuilds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux userspace ebuilds in Gentoo all have &lt;a href="http://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/"&gt;live ebuilds&lt;/a&gt;
available which are immediately usable for releases. The idea with those live
ebuilds is that we can simply copy them and commit in order to make a new release.&lt;/p&gt;
&lt;p&gt;So, in case of the patch backporting, the necessary patch files are first moved
into the &lt;code&gt;files/&lt;/code&gt; subdirectory of the package. Then, the live ebuild is updated
to use the new patches:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gu"&gt;@@ -88,6 +85,8 @@ src_prepare() {&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0070-remove-symlink-attempt-fails-with-gentoo-sandbox-approach.patch&amp;quot;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0110-build-mcstrans-bug-472912.patch&amp;quot;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0120-build-failure-for-mcscolor-for-CONTEXT__CONTAINS.patch&amp;quot;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+               epatch &amp;quot;${FILESDIR}/0130-Only-invoke-RPM-on-RPM-enabled-Linux-distributions-bug-534682.patch&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+               epatch &amp;quot;${FILESDIR}/0140-Set-self.sename-to-sename-after-calling-semanage-bug-557370.patch&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;       fi&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;       # rlpkg is more useful than fixfiles&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The patches themselves do not apply for the live ebuilds themselves (they are
ignored there) as we want the live ebuilds to be as close to the upstream
project as possible. But because the ebuilds are immediately usable for
releases, we add the necessary information there first.&lt;/p&gt;
&lt;p&gt;Next, the new release is created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ cp policycoreutils-9999.ebuild policycoreutils-2.4-r2.ebuild&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Testing the changes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new release is then tested. I have a couple of scripts that I use
for automated testing. So first I update these scripts to also try out
the functionality that was failing before. On existing systems, these
tests should fail:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Running task semanage (Various semanage related operations).
  ...
    Executing step &amp;quot;perm_port_on   : Marking portage_t as a permissive domain                              &amp;quot; -&amp;gt; ok
    Executing step &amp;quot;perm_port_off  : Removing permissive mark from portage_t                               &amp;quot; -&amp;gt; ok
    Executing step &amp;quot;selogin_modify : Modifying a SELinux login definition                                  &amp;quot; -&amp;gt; failed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, on a test system where the new package has been installed, the same
testset is executed (together with all other tests) to validate if the problem
is fixed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pushing out the new release&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, with the fixes in and validated, the new release is pushed out (into
~arch first of course) and the bugs are marked as &lt;code&gt;RESOLVED:TEST-REQUEST&lt;/code&gt;. Users
can confirm that it works (which would move it to &lt;code&gt;VERIFIED:TEST-REQUEST&lt;/code&gt;) or
we stabilize it after the regular testing period is over (which moves it to
&lt;code&gt;RESOLVED:FIXED&lt;/code&gt; or &lt;code&gt;VERIFIED:FIXED&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I do still have to get used to Gentoo using git as its repository now. The
&lt;a href="https://wiki.gentoo.org/wiki/Gentoo_git_workflow"&gt;workflow&lt;/a&gt; to use is
documented though. Luckily, because I often get that the &lt;code&gt;git push&lt;/code&gt; fails
(due to changes to the tree since my last pull). So I need to run &lt;code&gt;git
pull --rebase=preserve&lt;/code&gt; followed by &lt;code&gt;repoman full&lt;/code&gt; and then the push again
sufficiently quick after each other).&lt;/p&gt;
&lt;p&gt;This simple flow is easy to get used to. Thanks to the existing foundation
for package maintenance (such as &lt;code&gt;epatch&lt;/code&gt; for patching, live ebuilds that
can be immediately used for releases and the ability to just cherry pick
patches towards our repository) we can serve updates with just a few minutes
of work.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="ebuild"></category><category term="patching"></category></entry><entry><title>Doing away with interfaces</title><link href="https://blog.siphos.be/2015/08/doing-away-with-interfaces/" rel="alternate"></link><published>2015-08-29T11:30:00+02:00</published><updated>2015-08-29T11:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-29:/2015/08/doing-away-with-interfaces/</id><summary type="html">&lt;p&gt;CIL is SELinux' Common Intermediate Language, which brings on a whole new set of
possibilities with policy development. I hardly know CIL but am (slowly)
learning. Of course, the best way to learn is to try and do lots of things with
it, but real-life work and time-to-market for now forces me to stick with the
M4-based refpolicy one.&lt;/p&gt;
&lt;p&gt;Still, I do try out some things here and there, and one of the things I wanted
to look into was how CIL policies would deal with interfaces.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;CIL is SELinux' Common Intermediate Language, which brings on a whole new set of
possibilities with policy development. I hardly know CIL but am (slowly)
learning. Of course, the best way to learn is to try and do lots of things with
it, but real-life work and time-to-market for now forces me to stick with the
M4-based refpolicy one.&lt;/p&gt;
&lt;p&gt;Still, I do try out some things here and there, and one of the things I wanted
to look into was how CIL policies would deal with interfaces.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Recap on interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the M4 based reference policy, interfaces are M4 macros that expand into
the standard SELinux rules. They are used by the reference policy to provide 
a way to isolate module-specific code and to have "public" calls.&lt;/p&gt;
&lt;p&gt;Policy modules are not allowed (by convention) to call types or domains that
are not defined by the same module. If they want to interact with those modules,
then they need to call the interface(s):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# module &amp;quot;ntp&amp;quot;
# domtrans: when executing an ntpd_exec_t binary, the resulting process 
#           runs in ntpd_t
interface(`ntp_domtrans&amp;#39;,`
  domtrans_pattern($1, ntpd_exec_t, ntpd_t)
)

# module &amp;quot;hal&amp;quot;
ntp_domtrans(hald_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the purpose is to have &lt;code&gt;hald_t&lt;/code&gt; be able to execute
binaries labeled as &lt;code&gt;ntpd_exec_t&lt;/code&gt; and have the resulting process run as the
&lt;code&gt;ntpd_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;The following would not be allowed inside the hal module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;domtrans_pattern(hald_t, ntpd_exec_t, ntpd_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This would imply that both &lt;code&gt;hald_t&lt;/code&gt;, &lt;code&gt;ntpd_exec_t&lt;/code&gt; and &lt;code&gt;ntpd_t&lt;/code&gt; are defined
by the same module, which is not the case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interfaces in CIL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It &lt;em&gt;seems&lt;/em&gt; that CIL will not use interface files. Perhaps some convention
surrounding it will be created - to know this, we'll have to wait until a
"cilrefpolicy" is created. However, functionally, this is no longer necessary.&lt;/p&gt;
&lt;p&gt;Consider the &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; declaration from a &lt;a href="http://blog.siphos.be/2015/08/filtering-network-access-per-application/"&gt;previous post&lt;/a&gt;.
In it, we wanted to allow &lt;code&gt;mozilla_t&lt;/code&gt; to send and receive these packets. The 
example didn't use an interface-like construction for this, so let's see
how this would be dealt with.&lt;/p&gt;
&lt;p&gt;First, the module is slightly adjusted to create a &lt;em&gt;macro&lt;/em&gt; called &lt;code&gt;myhttp_sendrecv_client_packet&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;macro&lt;/span&gt; &lt;span class="nv"&gt;myhttp_sendrecv_client_packet&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;cil_gen_require&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt; &lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt; &lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Another module would then call this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;call&lt;/span&gt; &lt;span class="nv"&gt;myhttp_sendrecv_client_packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's it. When the policy modules are both loaded, then the &lt;code&gt;mozilla_t&lt;/code&gt; domain is able
to send and receive &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; labeled packets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's more: namespaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But it doesn't end there. Whereas the reference policy had a single namespace
for the interfaces, CIL is able to use namespaces. It allows to create an almost
object-like approach for policy development.&lt;/p&gt;
&lt;p&gt;The above &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; definition could be written as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;block&lt;/span&gt; &lt;span class="nv"&gt;myhttp&lt;/span&gt;
  &lt;span class="c1"&gt;; MyHTTP client packet&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;roletype&lt;/span&gt; &lt;span class="nv"&gt;object_r&lt;/span&gt; &lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;client_packet_type&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;packet_type&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;macro&lt;/span&gt; &lt;span class="nv"&gt;sendrecv_client_packet&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;cil_gen_require&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt; &lt;span class="nv"&gt;client_packet_t&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt; &lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The other module looks as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;block&lt;/span&gt; &lt;span class="nv"&gt;mozilla&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;cil_gen_require&lt;/span&gt; &lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;call&lt;/span&gt; &lt;span class="nv"&gt;myhttp.sendrecv_client_packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is similar, but not fully the same. The packet is no longer called
&lt;code&gt;myhttp_client_packet_t&lt;/code&gt; but &lt;code&gt;myhttp.client_packet_t&lt;/code&gt;. In other words, a period (&lt;code&gt;.&lt;/code&gt;)
is used to separate the object name (&lt;code&gt;myhttp&lt;/code&gt;) and the object/type (&lt;code&gt;client_packet_t&lt;/code&gt;)
as well as interface/macro (&lt;code&gt;sendrecv_client_packet&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ sesearch -s mozilla_t -c packet -p send -Ad&lt;/span&gt;
&lt;span class="go"&gt;  ...&lt;/span&gt;
&lt;span class="go"&gt;  allow mozilla_t myhttp.client_packet_t : packet { send recv };&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And it looks that namespace support goes even further than that, but I still
need to learn more about it first.&lt;/p&gt;
&lt;p&gt;Still, I find this a good evolution. With CIL interfaces are no longer separate
from the module definition: everything is inside the CIL file. I secretly hope
that tools such as &lt;code&gt;seinfo&lt;/code&gt; would support querying macros as well.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="cil"></category></entry><entry><title>Slowly converting from GuideXML to HTML</title><link href="https://blog.siphos.be/2015/08/slowly-converting-from-guidexml-to-html/" rel="alternate"></link><published>2015-08-25T11:30:00+02:00</published><updated>2015-08-25T11:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-25:/2015/08/slowly-converting-from-guidexml-to-html/</id><summary type="html">&lt;p&gt;Gentoo has removed its support of the older GuideXML format in favor of using
the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; and a new content management system
for the main site (or is it static pages, I don't have the faintest idea to be
honest). I do still have a few GuideXML pages in my development space, which I
am going to move to HTML pretty soon.&lt;/p&gt;
&lt;p&gt;In order to do so, I make use of the &lt;a href="https://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl?view=log"&gt;guidexml2wiki&lt;/a&gt;
stylesheet I &lt;a href="http://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"&gt;developed&lt;/a&gt;.
But instead of migrating it to wiki syntax, I want to end with HTML.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Gentoo has removed its support of the older GuideXML format in favor of using
the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; and a new content management system
for the main site (or is it static pages, I don't have the faintest idea to be
honest). I do still have a few GuideXML pages in my development space, which I
am going to move to HTML pretty soon.&lt;/p&gt;
&lt;p&gt;In order to do so, I make use of the &lt;a href="https://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl?view=log"&gt;guidexml2wiki&lt;/a&gt;
stylesheet I &lt;a href="http://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"&gt;developed&lt;/a&gt;.
But instead of migrating it to wiki syntax, I want to end with HTML.&lt;/p&gt;


&lt;p&gt;So what I do is first convert the file from GuideXML to MediaWiki with &lt;code&gt;xsltproc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, I use &lt;code&gt;pandoc&lt;/code&gt; to convert this to restructured text. The idea is that the main
pages on my devpage are now restructured text based. I was hoping to use markdown, but
the conversion from markdown to HTML is not what I hoped it was.&lt;/p&gt;
&lt;p&gt;The restructured text is then converted to HTML using &lt;code&gt;rst2html.py&lt;/code&gt;. In the end,
I use the following function (for conversion, once):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Convert GuideXML to RestructedText and to HTML&lt;/span&gt;
gxml2html&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="p"&gt;%%.xml&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="c1"&gt;# Convert to Mediawiki syntax&lt;/span&gt;
  xsltproc ~/dev-cvs/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl &lt;span class="nv"&gt;$1&lt;/span&gt; &amp;gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="c1"&gt;# Convert to restructured text&lt;/span&gt;
    pandoc -f mediawiki -t rst -s -S -o &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;fi&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="c1"&gt;# Use your own stylesheet links (use full https URLs for this)&lt;/span&gt;
    rst2html.py  --stylesheet&lt;span class="o"&gt;=&lt;/span&gt;link-to-bootstrap.min.css,link-to-tyrian.min.css --link-stylesheet &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.html
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Is it perfect? No, but &lt;a href="http://dev.gentoo.org/~swift/snapshots/"&gt;it works&lt;/a&gt;.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="guidexml"></category><category term="xml"></category><category term="xslt"></category><category term="rst"></category><category term="mediawiki"></category><category term="html"></category></entry><entry><title>Making the case for multi-instance support</title><link href="https://blog.siphos.be/2015/08/making-the-case-for-multi-instance-support/" rel="alternate"></link><published>2015-08-22T12:45:00+02:00</published><updated>2015-08-22T12:45:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-22:/2015/08/making-the-case-for-multi-instance-support/</id><summary type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Examples of multi-instance technologies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first example that I'm pretty familiar with is multi-instance database
deployments: Oracle DBs, SQL Servers, PostgreSQLs, etc. The consolidation
of databases while still keeping multiple instances around (instead of
consolidating into a single instance itself) is mainly for operational 
reasons (changes should not influence other database/schema's) or
technical reasons (different requirements in parameters, locales, etc.)&lt;/p&gt;
&lt;p&gt;Other examples are web servers (for web hosting companies), which next to
virtual host support (which is still part of a single instance) could
benefit from multi-instance deployments for security reasons (vulnerabilities
might be better contained then) as well as performance tuning. Same goes
for web application servers (such as TomCat deployments).&lt;/p&gt;
&lt;p&gt;But even other technologies like mail servers can benefit from multiple
instance deployments. Postfix has a &lt;a href="http://www.postfix.org/MULTI_INSTANCE_README.html"&gt;nice guide&lt;/a&gt;
on multi-instance deployments and also covers some of the use cases for it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages of multi-instance setups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The primary objective that most organizations have when dealing with multiple
instances is the consolidation to reduce cost. Especially expensive, 
propriatary software which is CPU licensed gains a lot from consolidation 
(and don't think a CPU is a CPU, each company
&lt;a href="http://www-01.ibm.com/software/passportadvantage/pvu_licensing_for_customers.html"&gt;has&lt;/a&gt;
&lt;a href="http://www.oracle.com/us/corporate/contracts/processor-core-factor-table-070634.pdf"&gt;its&lt;/a&gt; (PDF)
&lt;a href="go.microsoft.com/fwlink/?LinkID=229882"&gt;own&lt;/a&gt; (PDF) core weight table to
get the most money out of their customers).&lt;/p&gt;
&lt;p&gt;But beyond cost savings, using multi-instance deployments also provides for
resource sharing. A high-end server can be used to host the multiple instances,
with for instance SSD disks (or even flash cards), more memory, high-end CPUs,
high-speed network connnectivity and more. This improves performance considerably,
because most multi-instance technologies don't need all resources continuously.&lt;/p&gt;
&lt;p&gt;Another advantage, if properly designed, is that multi-instance capable software
can often leverage the multi-instance deployments for fast changes. A database
might be easily patched (remove vulnerabilities) by creating a second codebase
deployment, patching that codebase, and then migrating the database from one
instance to another. Although it often still requires downtime, it can be made
considerably less, and roll-back of such changes is very easy.&lt;/p&gt;
&lt;p&gt;A last advantage that I see is security. Instances can be running as different
runtime accounts, through different SELinux contexts, bound on different
interfaces or chrooted into different locations. This is not an advantage
compared to dedicated systems of course, but more an advantage compared
to full consolidation (everything in a single instance).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don't always focus on multi-instance setups though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Multiple instances isn't a silver bullet. Some technologies are generally much
better when there is a single instance on a single operating system. Personally,
I find that such technologies should know better. If they are really designed to
be suboptimal in case of multi-instance deployments, then there is a design error.&lt;/p&gt;
&lt;p&gt;But when the advantages of multiple instances do not exist (no license cost,
hardware cost is low, etc.) then organizations might focus on single-instance
deployments, because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-instance deployments might require more users to access the system
  (especially when it is multi-tenant)&lt;/li&gt;
&lt;li&gt;operational activities might impact other instances (for instance updating 
  kernel parameters for one instance requires a reboot which affects other
  instances)&lt;/li&gt;
&lt;li&gt;the software might not be properly "multi-instance aware" and as such
  starts fighting for resources with its own sigbling instances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that properly designed architectures are well capable of using
virtualization (and in the future containerization) moving towards
single-instance deployments becomes more and more interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What should multi-instance software consider?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Software should, imo, always consider multi-instance deployments. Even
when the administrator decides to stick with a single instance, all that
that takes is that the software ends up with a "single instance" setup
(it is &lt;em&gt;much&lt;/em&gt; easier to support multiple instances and deploy a single one,
than to support single instances and deploy multiple ones).&lt;/p&gt;
&lt;p&gt;The first thing software should take into account is that it might (and
will) run with different runtime accounts - service accounts if you whish.
That means that the software should be well aware that file locations are
separate, and that these locations will have different access control settings
on them (if not just a different owner).&lt;/p&gt;
&lt;p&gt;So instead of using &lt;code&gt;/etc/foo&lt;/code&gt; as the mandatory location, consider supporting
&lt;code&gt;/etc/foo/instance1&lt;/code&gt;, &lt;code&gt;/etc/foo/instance2&lt;/code&gt; if full directories are needed, or
just have &lt;code&gt;/etc/foo1.conf&lt;/code&gt; and &lt;code&gt;/etc/foo2.conf&lt;/code&gt;. I prefer the directory approach,
because it makes management much easier. It then also makes sense that the log
location is &lt;code&gt;/var/log/foo/instance1&lt;/code&gt;, the data files are at &lt;code&gt;/var/lib/foo/instance1&lt;/code&gt;,
etc.&lt;/p&gt;
&lt;p&gt;The second is that, if a service is network-facing (which most of them
are), it must be able to either use multihomed systems easily (bind to
different interfaces) or use different ports. The latter is a challenge
I often come across with software - the way to configure the software to
deal with multiple deployments and multiple ports is often a lengthy
trial-and-error setup.&lt;/p&gt;
&lt;p&gt;What's so difficult with using a &lt;em&gt;base port&lt;/em&gt; setting, and document how the
other ports are derived from this base port. &lt;a href="http://neo4j.com/docs/stable/ha-setup-tutorial.html"&gt;Neo4J&lt;/a&gt;
needs 3 ports for its enterprise services (transactions, cluster management
and online backup), but they all need to be explicitly configured if you
want a multi-instance deployment. What if one could just set &lt;code&gt;baseport = 5001&lt;/code&gt;
with the software automatically selecting 5002 and 5003 as other ports (or 6001
and 7001). If the software in the future needs another port, there is no need
to update the configuration (assuming the administrator leaves sufficient room).&lt;/p&gt;
&lt;p&gt;Also consider the service scripts (&lt;code&gt;/etc/init.d&lt;/code&gt;) or similar (depending on the
init system used). Don't provide a single one which only deals with one instance.
Instead, consider supporting symlinked service scripts which automatically obtain
the right configuration from its name.&lt;/p&gt;
&lt;p&gt;For instance, a service script called &lt;code&gt;pgsql-inst1&lt;/code&gt; which is a symlink to
&lt;code&gt;/etc/init.d/postgresql&lt;/code&gt; could then look for its configuration in &lt;code&gt;/var/lib/postgresql/pgsql-inst1&lt;/code&gt;
(or &lt;code&gt;/etc/postgresql/pgsql-inst1&lt;/code&gt;). &lt;/p&gt;
&lt;p&gt;Just like supporting &lt;a href="http://blog.siphos.be/2013/05/the-linux-d-approach/"&gt;.d directories&lt;/a&gt;,
I consider multi-instance support an important non-functional requirement for software.&lt;/p&gt;</content><category term="Architecture"></category></entry><entry><title>Switching OpenSSH to ed25519 keys</title><link href="https://blog.siphos.be/2015/08/switching-openssh-to-ed25519-keys/" rel="alternate"></link><published>2015-08-19T18:26:00+02:00</published><updated>2015-08-19T18:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-19:/2015/08/switching-openssh-to-ed25519-keys/</id><summary type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The issue with DSA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You might find DSA a bit slower than RSA:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl speed rsa1024 rsa2048 dsa1024 dsa2048
...
                  sign    verify    sign/s verify/s
rsa 1024 bits 0.000127s 0.000009s   7874.0 111147.6
rsa 2048 bits 0.000959s 0.000029s   1042.9  33956.0
                  sign    verify    sign/s verify/s
dsa 1024 bits 0.000098s 0.000103s  10213.9   9702.8
dsa 2048 bits 0.000293s 0.000339s   3407.9   2947.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, RSA verification outperforms DSA in verification, while signing
with DSA is better than RSA. But for what OpenSSH is concerned, this speed
difference should not be noticeable on the vast majority of OpenSSH servers.&lt;/p&gt;
&lt;p&gt;So no, it is not the speed, but the secure state of the DSS standard.&lt;/p&gt;
&lt;p&gt;The OpenSSH developers find that &lt;a href="http://www.openssh.com/legacy.html"&gt;ssh-dss (DSA) is too weak&lt;/a&gt;,
which is followed by &lt;a href="http://meyering.net/nuke-your-DSA-keys/"&gt;various&lt;/a&gt; 
&lt;a href="https://docs.moodle.org/dev/SSH_key"&gt;sources&lt;/a&gt;. Considering the impact of these keys,
it is important that they follow the state-of-the-art cryptographic services. &lt;/p&gt;
&lt;p&gt;Instead, they suggest to switch to elliptic curve cryptography based algorithms,
with Ed25519 and &lt;a href="https://en.wikipedia.org/wiki/Curve25519"&gt;Curve25519&lt;/a&gt; coming out
on top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Switch to RSA or ED25519?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that RSA is still considered very secure, one of the questions is of
course if &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; is the right choice here or not.
I don't consider myself anything in cryptography, but I do like to validate stuff
through academic and (hopefully) reputable sources for information (not that I don't
trust the OpenSSH and OpenSSL folks, but more from a broader interest in the subject).&lt;/p&gt;
&lt;p&gt;Ed25519 should be written fully as &lt;em&gt;Ed25519-SHA-512&lt;/em&gt; and is a signature
algorithm. It uses elliptic curve cryptography as explained on the
&lt;a href="https://en.wikipedia.org/wiki/EdDSA"&gt;EdDSA wikipedia page&lt;/a&gt;. An often cited
paper is &lt;a href="http://aspartame.shiftleft.org/papers/fff/fff.pdf"&gt;Fast and compact elliptic-curve cryptography&lt;/a&gt;
by Mike Hamburg, which talks about the performance improvements, but the main
paper is called &lt;a href="http://ed25519.cr.yp.to/ed25519-20110705.pdf"&gt;High-speed high-security signatures&lt;/a&gt;
which introduces the Ed25519 implementation.&lt;/p&gt;
&lt;p&gt;Of the references I was able to (quickly) go through (not all papers are
publicly reachable) none showed any concerns about the secure state of the 
algorithm. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The (simple) process of switching&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Switching to Ed25519 is simple. First, generate the (new) SSH key (below
just an example run):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh-keygen -t ed25519
Generating public/private ed25519 key pair.
Enter file in which to save the key (/home/testuser/.ssh/id_ed25519): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/testuser/.ssh/id_ed25519.
Your public key has been saved in /home/testuser/.ssh/id_ed25519.pub.
The key fingerprint is:
SHA256:RDaEw3tNAKBGMJ2S4wmN+6P3yDYIE+v90Hfzz/0r73M testuser@testserver
The key&amp;#39;s randomart image is:
+--[ED25519 256]--+
|o*...o.+*.       |
|*o+.  +o ..      |
|o++    o.o       |
|o+    ... .      |
| +     .S        |
|+ o .            |
|o+.o . . o       |
|oo+o. . . o ....E|
| oooo.     ..o+=*|
+----[SHA256]-----+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, make sure that the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; file contains the public key
(as generated as &lt;code&gt;id_ed25519.pub&lt;/code&gt;). Don't remove the other keys yet until the
communication is validated. For me, all I had to do was to update the file in
the Salt repository and have the master push the changes to all nodes (starting
with non-production first of course).&lt;/p&gt;
&lt;p&gt;Next, try to log on to the system using the Ed25519 key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -i ~/.ssh/id_ed25519 testuser@testserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Make sure that your SSH agent is not running as it might still try to revert
back to another key if the Ed25519 one does not work. You can validate if the
connection was using Ed25519 through the &lt;code&gt;auth.log&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ sudo tail -f auth.log&lt;/span&gt;
&lt;span class="go"&gt;Aug 17 21:20:48 localhost sshd[13962]: Accepted publickey for root from \&lt;/span&gt;
&lt;span class="go"&gt;  192.168.100.1 port 43152 ssh2: ED25519 SHA256:-------redacted----------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this communication succeeds, then you can remove the old key from the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;On the client level, you might want to hide &lt;code&gt;~/.ssh/id_dsa&lt;/code&gt; from the SSH agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Obsolete - keychain ~/.ssh/id_dsa&lt;/span&gt;
keychain ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If a server update was forgotten, then the authentication will fail and, depending
on the configuration, either fall back to the regular authentication or fail
immediately. This gives a nice heads-up to you to update the server, while keeping
the key handy just in case. Just refer to the old &lt;code&gt;id_dsa&lt;/code&gt; key during the authentication
and fix up the server.&lt;/p&gt;</content><category term="Free-Software"></category><category term="openssh"></category><category term="ssh"></category><category term="gentoo"></category></entry><entry><title>Updates on my Pelican adventure</title><link href="https://blog.siphos.be/2015/08/updates-on-my-pelican-adventure/" rel="alternate"></link><published>2015-08-16T19:50:00+02:00</published><updated>2015-08-16T19:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-16:/2015/08/updates-on-my-pelican-adventure/</id><summary type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The full article view on index page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the features I wanted was to have my latest blog post to be fully
readable from the front page (called the &lt;em&gt;index&lt;/em&gt; page within Pelican). Sadly,
I could not find a plugin of setting that would do this, but I did find
a plugin that I can use to work around this: the &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/summary"&gt;summary&lt;/a&gt;
plugin.&lt;/p&gt;
&lt;p&gt;Enabling the plugin was a breeze. Extract the plugin sources in the &lt;code&gt;plugin/&lt;/code&gt;
folder, and enable it in &lt;code&gt;pelicanconf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;summary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this plug-in, articles can use inline comments to tell the system at which
point the summary of the article stops. Usually, the summary (which is displayed
on index pages) is a first paragraph (or set of paragraphs). What I do is I now
manually set the summmary to the entire blog post for the latest post, and adjust
later when a new post comes up.&lt;/p&gt;
&lt;p&gt;It might be some manual labour, but it fits nicely and doesn't hack around in the
code too much.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Commenting with Disqus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I had some remarks that the &lt;a href="https://disqus.com/home/welcome/"&gt;Disqus&lt;/a&gt; integration
is not as intuitive as expected. Some readers had difficulties finding out how
to comment as a guest (without the need to log on through popular social media
or through Disqus itself).&lt;/p&gt;
&lt;p&gt;Agreed, it is not easy to see at first sight that people need to start typing
their name in the &lt;em&gt;Or sign up with disqus&lt;/em&gt; before they can select &lt;em&gt;I'd rather post
as guest&lt;/em&gt;. As I don't have any way of controlling the format and rendered code
with Disqus, I updated the theme a bit to add in two paragraphs on commenting.
The first paragraph tells how to comment as guest.&lt;/p&gt;
&lt;p&gt;The second paragraph for now informs readers that non-verified comments are put
in the moderation queue. Once I get a feeling of how the spam and bots act on the
commenting system, I will adjust the filters and also allow guest comments to be
readily accessible (no moderation queue). Give it a few more weeks to get myself
settled and I'll adjust it.&lt;/p&gt;
&lt;p&gt;If the performance of the site is slowed down due to the Disqus javascripts: both
Firefox (excuse me, Aurora) and Chromium have this at the initial load. Later, the
scripts are properly cached and load in relatively fast (a quick test shows
all pages I tried load in less than 2 seconds - WordPress was at 4). And if you're
not interested in commenting, then you can even use &lt;a href="https://noscript.net/"&gt;NoScript&lt;/a&gt;
or similar plugins to disallow any remote javascript.&lt;/p&gt;
&lt;p&gt;Still, I will continue to look at how to make commenting easier. I recently allowed
unmoderated comments (unless a number of keywords are added, and comments with links
are also put in the moderation queue). If someone knows of another comment-like
system that I could integrate I'm happy to hear about it as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My issue with Tipue Search has been fixed by reverting a change in &lt;code&gt;tipue_search.py&lt;/code&gt;
(the plugin) where the URL was assigned to the &lt;code&gt;loc&lt;/code&gt; key instead of &lt;code&gt;url&lt;/code&gt;. It is
probably a mismatch between the plugin and the theme (the change of the key was done
in May in Tipue Search itself).&lt;/p&gt;
&lt;p&gt;With this minor issue changed, the search capabilities are back on track on my blog.
Enabling is was a matter of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;tipue_search&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;DIRECT_TEMPLATES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;search&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tags and categories&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;WordPress supports multiple categories, but Pelican does not. So I went through
the various posts that had multiple categories and decided on a single one. While
doing so, I also reduced the &lt;a href="http://blog.siphos.be/categories.html"&gt;categories&lt;/a&gt; to
a small set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Databases&lt;/li&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;Free Software&lt;/li&gt;
&lt;li&gt;Gentoo&lt;/li&gt;
&lt;li&gt;Misc&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;SELinux&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will try to properly tag all posts so that, if someone is interested in a very
particular topic, such as &lt;a href="http://blog.siphos.be/tag/postgresql/index.html"&gt;PostgreSQL&lt;/a&gt;, he can reach
those posts through the tag.&lt;/p&gt;</content><category term="Free-Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>Finding a good compression utility</title><link href="https://blog.siphos.be/2015/08/finding-a-good-compression-utility/" rel="alternate"></link><published>2015-08-13T19:15:00+02:00</published><updated>2015-08-13T19:15:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-13:/2015/08/finding-a-good-compression-utility/</id><summary type="html">&lt;p&gt;I recently came across a &lt;a href="http://catchchallenger.first-world.info//wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO"&gt;wiki page&lt;/a&gt;
written by &lt;a href="http://catchchallenger.first-world.info/wiki/User:Alpha_one_x86"&gt;Herman Brule&lt;/a&gt;
which gives a quick benchmark on a couple of compression methods / algorithms.
It gave me the idea of writing a quick script that tests out a wide number of
compression utilities available in Gentoo (usually through the &lt;code&gt;app-arch&lt;/code&gt;
category), with also a number of options (in case multiple options are
possible).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I recently came across a &lt;a href="http://catchchallenger.first-world.info//wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO"&gt;wiki page&lt;/a&gt;
written by &lt;a href="http://catchchallenger.first-world.info/wiki/User:Alpha_one_x86"&gt;Herman Brule&lt;/a&gt;
which gives a quick benchmark on a couple of compression methods / algorithms.
It gave me the idea of writing a quick script that tests out a wide number of
compression utilities available in Gentoo (usually through the &lt;code&gt;app-arch&lt;/code&gt;
category), with also a number of options (in case multiple options are
possible).&lt;/p&gt;


&lt;p&gt;The currently supported packages are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;app-arch/bloscpack      app-arch/bzip2          app-arch/freeze
app-arch/gzip           app-arch/lha            app-arch/lrzip
app-arch/lz4            app-arch/lzip           app-arch/lzma
app-arch/lzop           app-arch/mscompress     app-arch/p7zip
app-arch/pigz           app-arch/pixz           app-arch/plzip
app-arch/pxz            app-arch/rar            app-arch/rzip
app-arch/xar            app-arch/xz-utils       app-arch/zopfli
app-arch/zpaq
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The script should keep the best compression information: duration, compression
ratio, compression command, as well as the compressed file itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the "best" compression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is not my intention to find the most optimal compression, as that would
require heuristic optimizations (which has triggered my interest in seeking
such software, or writing it myself) while trying out various optimization
parameters.&lt;/p&gt;
&lt;p&gt;No, what I want is to find the "best" compression for a given file, with "best"
being either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;most reduced size (which I call &lt;em&gt;compression delta&lt;/em&gt; in my script)&lt;/li&gt;
&lt;li&gt;best reduction obtained per time unit (which I call the &lt;em&gt;efficiency&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For me personally, I think I would use it for the various raw image files that
I have through the photography hobby. Those image files are difficult to
compress (the Nikon DS3200 I use is an entry-level camera which applies
lossy compression already for its raw files) but their total size is considerable,
and it would allow me to better use the storage I have available both on my
laptop (which is SSD-only) as well as backup server.&lt;/p&gt;
&lt;p&gt;But next to the best compression ratio, the efficiency is also an important
metric as it shows how efficient the algorithm works in a certain time aspect.
If one compression method yields 80% reduction in 5 minutes, and another one
yields 80,5% in 45 minutes, then I might want to prefer the first one even
though that is not the best compression at all.&lt;/p&gt;
&lt;p&gt;Although the script could be used to get the most compression (without
resolving to an optimization algorithm for the compression commands) for 
each file, this is definitely not the use case. A single run can take hours
for files that are compressed in a handful of seconds. But it can show the
best algorithms for a particular file type (for instance, do a few runs on
a couple of raw image files and see which method is most succesful).&lt;/p&gt;
&lt;p&gt;Another use case I'm currently looking into is how much improvement I can
get when multiple files (all raw image files) are first grouped in a single
archive (&lt;code&gt;.tar&lt;/code&gt;). Theoretically, this should improve the compression, but
by how much?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How the script works&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The script does not contain much intelligence. It iterates over a wide set of
compression commands that I tested out, checks the final compressed file size,
and if it is better than a previous one it keeps this compressed file (and
its statistics).&lt;/p&gt;
&lt;p&gt;I tried to group some of the compressions together based on the algorithm used,
but as I don't really know the details of the algorithms (it's based on manual
pages and internet sites) and some of them combine multiple algorithms, it is
more of a high-level selection than anything else.&lt;/p&gt;
&lt;p&gt;The script can also only run the compressions of a single application (which I
use when I'm fine-tuning the parameter runs).&lt;/p&gt;
&lt;p&gt;A run shows something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Original file (test.nef) size 20958430 bytes
      package name                                                 command      duration                   size compr.Δ effic.:
      ------------                                                 -------      --------                   ---- ------- -------
app-arch/bloscpack                                               blpk -n 4           0.1               20947097 0.00054 0.00416
app-arch/bloscpack                                               blpk -n 8           0.1               20947097 0.00054 0.00492
app-arch/bloscpack                                              blpk -n 16           0.1               20947097 0.00054 0.00492
    app-arch/bzip2                                                   bzip2           2.0               19285616 0.07982 0.03991
    app-arch/bzip2                                                bzip2 -1           2.0               19881886 0.05137 0.02543
    app-arch/bzip2                                                bzip2 -2           1.9               19673083 0.06133 0.03211
...
    app-arch/p7zip                                      7za -tzip -mm=PPMd           5.9               19002882 0.09331 0.01592
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=24           5.7               19002882 0.09331 0.01640
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=25           6.4               18871933 0.09955 0.01551
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=26           7.7               18771632 0.10434 0.01364
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=27           9.0               18652402 0.11003 0.01224
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=28          10.0               18521291 0.11628 0.01161
    app-arch/p7zip                                       7za -t7z -m0=PPMd           5.7               18999088 0.09349 0.01634
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=24           5.8               18999088 0.09349 0.01617
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=25           6.5               18868478 0.09972 0.01534
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=26           7.5               18770031 0.10442 0.01387
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=27           8.6               18651294 0.11008 0.01282
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=28          10.6               18518330 0.11643 0.01100
      app-arch/rar                                                     rar           0.9               20249470 0.03383 0.03980
      app-arch/rar                                                 rar -m0           0.0               20958497 -0.00000        -0.00008
      app-arch/rar                                                 rar -m1           0.2               20243598 0.03411 0.14829
      app-arch/rar                                                 rar -m2           0.8               20252266 0.03369 0.04433
      app-arch/rar                                                 rar -m3           0.8               20249470 0.03383 0.04027
      app-arch/rar                                                 rar -m4           0.9               20248859 0.03386 0.03983
      app-arch/rar                                                 rar -m5           0.8               20248577 0.03387 0.04181
    app-arch/lrzip                                                lrzip -z          13.1               19769417 0.05673 0.00432
     app-arch/zpaq                                                    zpaq           0.2               20970029 -0.00055        -0.00252
The best compression was found with 7za -t7z -m0=PPMd:mem=28.
The compression delta obtained was 0.11643 within 10.58 seconds.
This file is now available as test.nef.7z.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the test file was around 20 MByte. The best compression
compression command that the script found was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ 7za -t7z -m0=PPMd:mem=28 a test.nef.7z test.nef
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting file (&lt;code&gt;test.nef.7z&lt;/code&gt;) is 18 MByte, a reduction of 11,64%. The
compression command took almost 11 seconds to do its thing, which gave an
efficiency rating of 0,011, which is definitely not a fast one.&lt;/p&gt;
&lt;p&gt;Some other algorithms don't do bad either with a better efficiency. For
instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;   app-arch/pbzip2                                                  pbzip2           0.6               19287402 0.07973 0.13071
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the &lt;code&gt;pbzip2&lt;/code&gt; command got almost 8% reduction in less than a
second, which is considerably more efficient than the 11-seconds long &lt;code&gt;7za&lt;/code&gt;
run.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Want to try it out yourself?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've pushed the script to my &lt;a href="https://github.com/sjvermeu/small.coding/tree/master/sw_comprbest"&gt;github&lt;/a&gt;
location. Do a quick review of the code first (to see that I did not include
anything malicious) and then execute it to see how it works:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sw_comprbest -h
Usage: sw_comprbest --infile=&amp;lt;inputfile&amp;gt; [--family=&amp;lt;family&amp;gt;[,...]] [--command=&amp;lt;cmd&amp;gt;]
       sw_comprbest -i &amp;lt;inputfile&amp;gt; [-f &amp;lt;family&amp;gt;[,...]] [-c &amp;lt;cmd&amp;gt;]

Supported families: blosc bwt deflate lzma ppmd zpaq. These can be provided comma-separated.
Command is an additional filter - only the tests that use this base command are run.

The output shows
  - The package (in Gentoo) that the command belongs to
  - The command run
  - The duration (in seconds)
  - The size (in bytes) of the resulting file
  - The compression delta (percentage) showing how much is reduced (higher is better)
  - The efficiency ratio showing how much reduction (percentage) per second (higher is better)

When the command supports multithreading, we use the number of available cores on the system (as told by /proc/cpuinfo).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For instance, to try it out against a PDF file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sw_comprbest -i MEA6-Sven_Vermeulen-Research_Summary.pdf
Original file (MEA6-Sven_Vermeulen-Research_Summary.pdf) size 117763 bytes
...
The best compression was found with zopfli --deflate.
The compression delta obtained was 0.00982 within 0.19 seconds.
This file is now available as MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So in this case, the resulting file is hardly better compressed - the PDF
itself is already compressed. Let's try it against the uncompressed PDF:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ pdftk MEA6-Sven_Vermeulen-Research_Summary.pdf output test.pdf uncompress
~$ sw_comprbest -i test.pdf
Original file (test.pdf) size 144670 bytes
...
The best compression was found with lrzip -z.
The compression delta obtained was 0.27739 within 0.18 seconds.
This file is now available as test.pdf.lrz.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is somewhat better:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ls -l MEA6-Sven_Vermeulen-Research_Summary.pdf* test.pdf*
-rw-r--r--. 1 swift swift 117763 Aug  7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf
-rw-r--r--. 1 swift swift 116606 Aug  7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate
-rw-r--r--. 1 swift swift 144670 Aug  7 14:34 test.pdf
-rw-r--r--. 1 swift swift 104540 Aug  7 14:35 test.pdf.lrz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting file is 11,22% reduced from the original one.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="compression"></category></entry><entry><title>Why we do confine Firefox</title><link href="https://blog.siphos.be/2015/08/why-we-do-confine-firefox/" rel="alternate"></link><published>2015-08-11T19:18:00+02:00</published><updated>2015-08-11T19:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-11:/2015/08/why-we-do-confine-firefox/</id><summary type="html">&lt;p&gt;If you're a bit following the SELinux development community you will know
&lt;a href="http://danwalsh.livejournal.com"&gt;Dan Walsh&lt;/a&gt;, a &lt;a href="http://people.redhat.com/dwalsh/"&gt;Red Hat&lt;/a&gt;
security engineer. Today he &lt;a href="http://danwalsh.livejournal.com/72697.html"&gt;blogged&lt;/a&gt; 
about &lt;em&gt;CVE-2015-4495 and SELinux, or why doesn't SELinux confine Firefox&lt;/em&gt;. He 
should've asked why the &lt;em&gt;reference policy&lt;/em&gt; or &lt;em&gt;Red Hat/Fedora policy&lt;/em&gt; does not
confine Firefox, because SELinux is, as I've
&lt;a href="http://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/"&gt;mentioned before&lt;/a&gt;,
not the same as its policy.&lt;/p&gt;
&lt;p&gt;In effect, Gentoo's SELinux policy &lt;em&gt;does&lt;/em&gt; confine Firefox by default. One of the
principles we focus on in Gentoo Hardened is to
&lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/Development_policy#Develop_desktop_policies"&gt;develop desktop policies&lt;/a&gt;
in order to reduce exposure and information leakage of user documents. We might
not have the manpower to confine all desktop applications, but I do think it is
worthwhile to at least attempt to do this, even though what Dan Walsh mentioned
is also correct: desktops are notoriously difficult to use a mandatory access
control system on.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;If you're a bit following the SELinux development community you will know
&lt;a href="http://danwalsh.livejournal.com"&gt;Dan Walsh&lt;/a&gt;, a &lt;a href="http://people.redhat.com/dwalsh/"&gt;Red Hat&lt;/a&gt;
security engineer. Today he &lt;a href="http://danwalsh.livejournal.com/72697.html"&gt;blogged&lt;/a&gt; 
about &lt;em&gt;CVE-2015-4495 and SELinux, or why doesn't SELinux confine Firefox&lt;/em&gt;. He 
should've asked why the &lt;em&gt;reference policy&lt;/em&gt; or &lt;em&gt;Red Hat/Fedora policy&lt;/em&gt; does not
confine Firefox, because SELinux is, as I've
&lt;a href="http://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/"&gt;mentioned before&lt;/a&gt;,
not the same as its policy.&lt;/p&gt;
&lt;p&gt;In effect, Gentoo's SELinux policy &lt;em&gt;does&lt;/em&gt; confine Firefox by default. One of the
principles we focus on in Gentoo Hardened is to
&lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/Development_policy#Develop_desktop_policies"&gt;develop desktop policies&lt;/a&gt;
in order to reduce exposure and information leakage of user documents. We might
not have the manpower to confine all desktop applications, but I do think it is
worthwhile to at least attempt to do this, even though what Dan Walsh mentioned
is also correct: desktops are notoriously difficult to use a mandatory access
control system on.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;How Gentoo wants to support more confined desktop applications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What Gentoo Hardened tries to do is to support the
&lt;a href="http://standards.freedesktop.org/basedir-spec/basedir-spec-0.8.html"&gt;XDG Base Directory Specification&lt;/a&gt;
for several documentation types. Downloads are marked as &lt;code&gt;xdg_downloads_home_t&lt;/code&gt;,
pictures are marked as &lt;code&gt;xdg_pictures_home_t&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;With those types defined, we grant the regular user domains full access to
those types, but start removing access to user content from applications. Rules
such as the following are commented out or removed from the policies:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# userdom_manage_user_home_content_dirs(mozilla_t)
# userdom_manage_user_home_content_files(mozilla_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Instead, we add in a call to a template we have defined ourselves:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;userdom_user_content_access_template(mozilla, { mozilla_t mozilla_plugin_t })
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This call makes access to user content optional through SELinux booleans. For
instance, for the &lt;code&gt;mozilla_t&lt;/code&gt; domain (which is used for Firefox), the following
booleans are created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Read generic (user_home_t) user content
mozilla_read_generic_user_content       -&amp;gt;      true

# Read all user content
mozilla_read_all_user_content           -&amp;gt;      false

# Manage generic (user_home_t) user content
mozilla_manage_generic_user_content     -&amp;gt;      false

# Manage all user content
mozilla_manage_all_user_content         -&amp;gt;      false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the default setting is that Firefox can read user content, but
only non-specific types. So &lt;code&gt;ssh_home_t&lt;/code&gt;, which is used for the SSH related
files, is not readable by Firefox with our policy &lt;em&gt;by default&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;By changing these booleans, the policy is fine-tuned to the requirements of
the administrator. On my systems, &lt;code&gt;mozilla_read_generic_user_content&lt;/code&gt; is switched
off.&lt;/p&gt;
&lt;p&gt;You might ask how we can then still support a browser if it cannot access user
content to upload or download. Well, as mentioned before, we support the XDG
types. The browser is allowed to manage &lt;code&gt;xdg_download_home_t&lt;/code&gt; files and
directories. For the majority of cases, this is sufficient. I also don't mind
copying over files to the &lt;code&gt;~/Downloads&lt;/code&gt; directory just for uploading files. But
I am well aware that this is not what the majority of users would want, which
is why the default is as it is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is much more work to be done sadly&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As said earlier, the default policy will allow &lt;em&gt;reading&lt;/em&gt; of user files if those
files are not typed specifically. Types that are protected by our policy (but not
by the reference policy standard) includes SSH related files at &lt;code&gt;~/.ssh&lt;/code&gt; and
GnuPG files at &lt;code&gt;~/.gnupg&lt;/code&gt;. Even other configuration files, such as for my Mutt
configuration (&lt;code&gt;~/.muttrc&lt;/code&gt;) which contains a password for an IMAP server I connect
to, are not reachable.&lt;/p&gt;
&lt;p&gt;However, it is still far from perfect. One of the reasons is that many desktop
applications are not "converted" yet to our desktop policy approach. Yes, Chromium
is also already converted, and policies we've added such as for Skype also do not
allow direct access unless the user explicitly enabled it. But Evolution for instance
isn't yet.&lt;/p&gt;
&lt;p&gt;Converting desktop policies to a more strict setup requires lots of testing, which
translates to many human resources. Within Gentoo, only a few developers and 
contributors are working on policies, and considering that this is not a change
that is already part of the (upstream) reference policy, some contributors also
do not want to put lots of focus on it either. But without having done the works,
it will not be easy (nor probably acceptable) to upstream this (the XDG patch has
been submitted a few times already but wasn't deemed ready yet then).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Having a more restrictive policy isn't the end&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As the blog post of Dan rightly mentioned, there are still quite some other
ways of accessing information that we might want to protect. An application 
might not have access to user files, but can be able to communicate (for instance
through DBus) with an application that does, and through that instruct it to
pass on the data.&lt;/p&gt;
&lt;p&gt;Plugins might require permissions which do not match with the principles set up
earlier. When we tried out Google Talk (needed for proper Google Hangouts support)
we noticed that it requires many, many more privileges. Luckily, we were able to
write down and develop a policy for the Google Talk plugin (&lt;code&gt;googletalk_plugin_t&lt;/code&gt;)
so it is still properly confined. But this is just a single plugin, and I'm sure
that more plugins exist which will have similar requirements. Which leads to more
policy development.&lt;/p&gt;
&lt;p&gt;But having workarounds does not make the effort we do worthless. Being able to
work around a firewall through application data does not make the firewall
useless, it is just one of the many security layers. The same is true with SELinux
policies.&lt;/p&gt;
&lt;p&gt;I am glad that we at least try to confine desktop applications more, and
that Gentoo Hardened users who use SELinux are at least somewhat more protected
from the vulnerability (even with the default case) and that our investment for
this is sound.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="policy"></category><category term="firefox"></category><category term="cve"></category><category term="vulnerability"></category><category term="xdg"></category></entry><entry><title>Can SELinux substitute DAC?</title><link href="https://blog.siphos.be/2015/08/can-selinux-substitute-dac/" rel="alternate"></link><published>2015-08-09T14:48:00+02:00</published><updated>2015-08-09T14:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-09:/2015/08/can-selinux-substitute-dac/</id><summary type="html">&lt;p&gt;A nice &lt;a href="https://twitter.com/sjvermeu/status/630107879123623936"&gt;twitter discussion&lt;/a&gt;
with &lt;a href="https://twitter.com/erlheldata"&gt;Erling Hellenäs&lt;/a&gt; caught my full attention later
when I was heading home: Can SELinux substitute DAC? I know it can't and doesn't
in the current implementation, but why not and what would be needed?&lt;/p&gt;
&lt;p&gt;SELinux is implemented through the &lt;a href="https://en.wikipedia.org/wiki/Linux_Security_Modules"&gt;Linux Security Modules framework&lt;/a&gt;
which allows for different security systems to be implemented and integrated
in the Linux kernel. Through LSM, various security-sensitive operations can be
secured further through &lt;em&gt;additional&lt;/em&gt; access checks. This criteria was made to
have LSM be as minimally invasive as possible.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A nice &lt;a href="https://twitter.com/sjvermeu/status/630107879123623936"&gt;twitter discussion&lt;/a&gt;
with &lt;a href="https://twitter.com/erlheldata"&gt;Erling Hellenäs&lt;/a&gt; caught my full attention later
when I was heading home: Can SELinux substitute DAC? I know it can't and doesn't
in the current implementation, but why not and what would be needed?&lt;/p&gt;
&lt;p&gt;SELinux is implemented through the &lt;a href="https://en.wikipedia.org/wiki/Linux_Security_Modules"&gt;Linux Security Modules framework&lt;/a&gt;
which allows for different security systems to be implemented and integrated
in the Linux kernel. Through LSM, various security-sensitive operations can be
secured further through &lt;em&gt;additional&lt;/em&gt; access checks. This criteria was made to
have LSM be as minimally invasive as possible.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The LSM design&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic LSM design paper, called &lt;a href="http://www.kroah.com/linux/talks/usenix_security_2002_lsm_paper/lsm.pdf"&gt;Linux Security Modules: General Security
Support for the Linux Kernel&lt;/a&gt;
as presented in 2002, is still one of the better references for learning and
understanding LSM. It does show that there was a whish-list from the community
where LSM hooks could override DAC checks, and that it has been partially
implemented through permissive hooks (not to be mistaken with SELinux' 
permissive mode).&lt;/p&gt;
&lt;p&gt;However, this definitely is &lt;em&gt;partially&lt;/em&gt; implemented because there are quite
a few restrictions. One of them is that, if a request is made towards a
resource and the UIDs match (see page 3, figure 2 of the paper) then
the LSM hook is not consulted. When they don't match, a permissive LSM
hook can be implemented. Support for permissive hooks is implemented
for capabilities, a powerful DAC control that Linux supports and which is
implemented &lt;a href="http://www.hep.by/gnu/kernel/lsm/cap.html"&gt;through LSM&lt;/a&gt; as
well. I have &lt;a href="http://blog.siphos.be/tag/capabilities/index.html"&gt;blogged&lt;/a&gt;
about this nice feature a while ago.&lt;/p&gt;
&lt;p&gt;These restrictions are also why some other security-conscious developers,
such as &lt;a href="http://grsecurity.net/lsm.php"&gt;grsecurity's team&lt;/a&gt; and &lt;a href="https://www.rsbac.org/documentation/why_rsbac_does_not_use_lsm"&gt;RSBAC&lt;/a&gt;
do not use the LSM system. Well, it's not only through these restrictions
of course - other reasons play a role in them as well. But knowing what
LSM can (and cannot) do also shows what SELinux can and cannot do.&lt;/p&gt;
&lt;p&gt;The LSM design itself is already a reason why SELinux cannot substitute
DAC controls. But perhaps we could disable DAC completely and thus only
rely on SELinux?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disabling DAC in Linux would be an excessive workload&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discretionary access controls in the Linux kernel are not easy to remove.
They are often part of the code itself (just grep through the source code
after &lt;code&gt;-EPERM&lt;/code&gt;). Some subsystems which use a common standard approach (such
as VFS operations) can rely on good integrated security controls, but these
too often allow the operation if DAC allows it, and will only consult the LSM
hooks otherwise.&lt;/p&gt;
&lt;p&gt;VFS operations are the most known ones, but DAC controls go beyond file access.
It also entails reading program memory, sending signals to applications,
accessing hardware and more. But let's focus on the easier controls (as in,
easier to use examples for), such as sharing files between users, restricting
access to personal documents and authorizing operations in applications based
on the user id (for instance, the owner can modify while other users can only
read the file).&lt;/p&gt;
&lt;p&gt;We could "work around" the Linux DAC controls by running everything as a single user
(the root user) and having all files and resources be fully accessible by this
user. But the problem with that is that SELinux would not be able to take
over controls either, because you will need some user-based access controls,
and within SELinux this implies that a mapping is done from a user to a 
SELinux user. Also, access controls based on the user id would no longer work,
and unless the application is made SELinux-aware it would lack any authorization
system (or would need to implement it itself).&lt;/p&gt;
&lt;p&gt;With DAC Linux also provides quite some "freedom" which is well established
in the Linux (and Unix) environment: a simple security model where the user
and group membership versus the owner-privileges, group-privileges and
"rest"-privileges are validated. Note that SELinux does not really know
what a "group" is. It knows SELinux users, roles, types and sensitivities.&lt;/p&gt;
&lt;p&gt;So, suppose we would keep multi-user support in Linux but completely remove
the DAC controls and rely solely on LSM (and SELinux). Is this something
reusable?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using SELinux for DAC-alike rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the use case of two users. One user wants another user to read a few
of his files. With DAC controls, he can "open up" the necessary resources
(files and directories) through &lt;a href="https://wiki.gentoo.org/wiki/Filesystem/Access_Control_List_Guide"&gt;extended access control lists&lt;/a&gt;
so that the other user can access it. No need to involve administrators.&lt;/p&gt;
&lt;p&gt;With a MAC(-only) system, updates on the MAC policy usually require the security
administrator to write additional policy rules to allow something. With SELinux
(and without DAC) it would require the users to be somewhat isolated from each
other (otherwise the users can just access everything from each other), which
SELinux can do through &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Constraints#UBAC_-_User_Based_Access_Control"&gt;User Based Access Control&lt;/a&gt;,
but the target resource itself should be labeled with a type that is not managed
through the UBAC control. Which means that the users will need the privilege to
change labels to this type (which is possible!), &lt;em&gt;assuming&lt;/em&gt; such a type is already
made available for them. Users can't create new types themselves.&lt;/p&gt;
&lt;p&gt;UBAC is by default disabled in many distributions, because it has some nasty
side-effects that need to be taken into consideration. Just recently one of these
&lt;a href="http://oss.tresys.com/pipermail/refpolicy/2015-August/007704.html"&gt;came up on the refpolicy mailinglist&lt;/a&gt;.
But even with UBAC enabled (I have it enabled on most of my systems, but considering
that I only have a couple of users to manage and am administrator on these systems
to quickly "update" rules when necessary) it does not provide equal functionality as
DAC controls.&lt;/p&gt;
&lt;p&gt;As mentioned before, SELinux does not know group membership. In order to create
something group-like, we will probably need to consider roles. But in SELinux,
roles are used to define what types are transitionable towards - it is not a
membership approach. A type which is usable by two roles (for instance, the
&lt;code&gt;mozilla_t&lt;/code&gt; type which is allowed for &lt;code&gt;staff_r&lt;/code&gt; and &lt;code&gt;user_r&lt;/code&gt;) does not care about
the role. This is unlike group membership.&lt;/p&gt;
&lt;p&gt;Also, roles only focus on &lt;em&gt;transitionable&lt;/em&gt; types (known as domains). It does not
care about &lt;em&gt;accessible&lt;/em&gt; resources (regular file types for instance). In order to
allow one person to read a certain file type but not another, SELinux will need
to control that one person can read this file through a particular domain while
the other user can't. And given that domains are part of the SELinux policy, any
situation that the policy has not thought about before will not be easily adaptable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, we can't do it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, I'm pretty sure that a very extensive policy and set of rules can be made
for SELinux which would make a number of DAC permissions obsolete, and that we could
theoretically remove DAC from the Linux kernel.&lt;/p&gt;
&lt;p&gt;End users would require a huge training to work with this system, and it would not
be reusable across other systems in different environments, because the policy
will be too specific to the system (unlike the current reference policy based ones,
which are quite reusable across many distributions).&lt;/p&gt;
&lt;p&gt;Furthermore, the effort to create these policies would be extremely high, whereas
the DAC permissions are very simple to implement, and have been proven to be
well suitable for many secured systems. &lt;/p&gt;
&lt;p&gt;So no, unless you do massive engineering, I do not believe it is possible to
substitute DAC with SELinux-only controls.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="refpolicy"></category><category term="linux"></category><category term="dac"></category><category term="lsm"></category></entry><entry><title>Filtering network access per application</title><link href="https://blog.siphos.be/2015/08/filtering-network-access-per-application/" rel="alternate"></link><published>2015-08-07T03:49:00+02:00</published><updated>2015-08-07T03:49:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-07:/2015/08/filtering-network-access-per-application/</id><summary type="html">&lt;p&gt;Iptables (and the successor nftables) is a powerful packet filtering system in
the Linux kernel, able to create advanced firewall capabilities. One of the 
features that it &lt;em&gt;cannot&lt;/em&gt; provide is per-application filtering. Together with
SELinux however, it is possible to implement this on a &lt;em&gt;per domain&lt;/em&gt; basis.&lt;/p&gt;
&lt;p&gt;SELinux does not know applications, but it knows domains. If we ensure that each
application runs in its own domain, then we can leverage the firewall
capabilities with SELinux to only allow those domains access that we need.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Iptables (and the successor nftables) is a powerful packet filtering system in
the Linux kernel, able to create advanced firewall capabilities. One of the 
features that it &lt;em&gt;cannot&lt;/em&gt; provide is per-application filtering. Together with
SELinux however, it is possible to implement this on a &lt;em&gt;per domain&lt;/em&gt; basis.&lt;/p&gt;
&lt;p&gt;SELinux does not know applications, but it knows domains. If we ensure that each
application runs in its own domain, then we can leverage the firewall
capabilities with SELinux to only allow those domains access that we need.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;SELinux network control: packet types&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic network control we need to enable is SELinux' packet types. Most
default policies will grant application domains the right set of packet types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# sesearch -s mozilla_t -c packet -A
Found 13 semantic av rules:
   allow mozilla_t ipp_client_packet_t : packet { send recv } ; 
   allow mozilla_t soundd_client_packet_t : packet { send recv } ; 
   allow nsswitch_domain dns_client_packet_t : packet { send recv } ; 
   allow mozilla_t speech_client_packet_t : packet { send recv } ; 
   allow mozilla_t ftp_client_packet_t : packet { send recv } ; 
   allow mozilla_t http_client_packet_t : packet { send recv } ; 
   allow mozilla_t tor_client_packet_t : packet { send recv } ; 
   allow mozilla_t squid_client_packet_t : packet { send recv } ; 
   allow mozilla_t http_cache_client_packet_t : packet { send recv } ; 
 DT allow mozilla_t server_packet_type : packet recv ; [ mozilla_bind_all_unreserved_ports ]
 DT allow mozilla_t server_packet_type : packet send ; [ mozilla_bind_all_unreserved_ports ]
 DT allow nsswitch_domain ldap_client_packet_t : packet recv ; [ authlogin_nsswitch_use_ldap ]
 DT allow nsswitch_domain ldap_client_packet_t : packet send ; [ authlogin_nsswitch_use_ldap ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the &lt;code&gt;mozilla_t&lt;/code&gt; domain is able to send and receive packets of
type &lt;code&gt;ipp_client_packet_t&lt;/code&gt;, &lt;code&gt;soundd_client_packet_t&lt;/code&gt;, &lt;code&gt;dns_client_packet_t&lt;/code&gt;, 
&lt;code&gt;speech_client_packet_t&lt;/code&gt;, &lt;code&gt;ftp_client_packet_t&lt;/code&gt;, &lt;code&gt;http_client_packet_t&lt;/code&gt;, 
&lt;code&gt;tor_client_packet_t&lt;/code&gt;, &lt;code&gt;squid_client_packet_t&lt;/code&gt; and &lt;code&gt;http_cache_client_packet_t&lt;/code&gt;.
If the SELinux booleans mentioned at the end are enabled, additional packet
types are alloed to be used as well.&lt;/p&gt;
&lt;p&gt;But even with this default policy in place, SELinux is not being consulted for
filtering. To accomplish this, &lt;code&gt;iptables&lt;/code&gt; will need to be told to label the
incoming and outgoing packets. This is the &lt;a href="http://blog.siphos.be/2013/05/secmark-and-selinux/"&gt;SECMARK&lt;/a&gt;
functionality that I've blogged about earlier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling SECMARK filtering through iptables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To enable SECMARK filtering, we use the &lt;code&gt;iptables&lt;/code&gt; command and tell it to label
SSH incoming and outgoing packets as &lt;code&gt;ssh_server_packet_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -A INPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore
~# iptables -t mangle -A INPUT -p tcp --dport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0
~# iptables -t mangle -A OUTPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore
~# iptables -t mangle -A OUTPUT -p tcp --sport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But be warned: the moment iptables starts with its SECMARK support, &lt;em&gt;all packets&lt;/em&gt;
will be labeled. Those that are not explicitly labeled through one of the above
commands will be labeled with the &lt;code&gt;unlabeled_t&lt;/code&gt; type, and most domains are not
allowed any access to &lt;code&gt;unlabeled_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are two things we can do to improve this situation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define the necessary SECMARK rules for all supported ports (which is something
   that &lt;a href="https://www.linux.com/learn/tutorials/421152:using-selinux-and-iptables-together"&gt;secmarkgen&lt;/a&gt;
   does), and/or&lt;/li&gt;
&lt;li&gt;Allow &lt;code&gt;unlabeled_t&lt;/code&gt; for all domains.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To allow the latter, we can load a SELinux rule like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt; &lt;span class="nv"&gt;domain&lt;/span&gt; &lt;span class="nv"&gt;unlabeled_t&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt; &lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will allow all domains to send and receive packets of the &lt;code&gt;unlabeled_t&lt;/code&gt; type.
Although this is something that might be security-sensitive, it might be a good idea
to allow at start, together with proper auditing (you can use &lt;code&gt;(auditallow ...)&lt;/code&gt; to
audit all granted packet communication) so that the right set of packet types can be
enabled. This way, administrators can iteratively improve the SECMARK rules and finally
remove the &lt;code&gt;unlabeled_t&lt;/code&gt; privilege from the &lt;code&gt;domain&lt;/code&gt; attribute.&lt;/p&gt;
&lt;p&gt;To list the current SECMARK rules, list the firewall rules for the &lt;code&gt;mangle&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -nvL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Only granting one application network access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These two together allow for creating a firewall that only allows a single domain
access to a particular target.&lt;/p&gt;
&lt;p&gt;For instance, suppose that we only want the &lt;code&gt;mozilla_t&lt;/code&gt; domain to connect to the
company proxy (10.15.10.5). We can't enable the &lt;code&gt;http_client_packet_t&lt;/code&gt; for this
connection, as all other web browsers and other HTTP-aware applications will have
policy rules enabled to send and receive that packet type. Instead, we are going
to create a new packet type to use.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;;; Definition of myhttp_client_packet_t&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;roletype&lt;/span&gt; &lt;span class="nv"&gt;object_r&lt;/span&gt; &lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;client_packet_type&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;packet_type&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;;; Grant the use to mozilla_t&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt; &lt;span class="nv"&gt;cil_gen_require&lt;/span&gt; &lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt; &lt;span class="nv"&gt;mozilla_t&lt;/span&gt; &lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt; &lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Putting the above in a &lt;code&gt;myhttppacket.cil&lt;/code&gt; file and loading it allows the type
to be used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i myhttppacket.cil
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, the &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; type can be used in &lt;code&gt;iptables&lt;/code&gt; rules. Also, 
only the &lt;code&gt;mozilla_t&lt;/code&gt; domain is allowed to send and receive these packets,
effectively creating an application-based firewall, as all we now need to do
is to mark the outgoing packets towards the proxy as &lt;code&gt;myhttp_client_packet_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -A OUTPUT -p tcp --dport 80 -d 10.15.10.5 -j SECMARK --selctx system_u:object_r:myhttp_client_packet_t:s0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This shows that it is &lt;em&gt;possible&lt;/em&gt; to create such firewall rules with SELinux. It
is however not an out-of-the-box solution, requiring thought and development of
both firewall rules and SELinux code constructions. Still, with some advanced
scripting experience this will lead to a powerful addition to a hardened
system.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="network"></category><category term="iptables"></category></entry><entry><title>My application base: Obnam</title><link href="https://blog.siphos.be/2015/08/my-application-base-obnam/" rel="alternate"></link><published>2015-08-05T22:35:00+02:00</published><updated>2015-08-05T22:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-05:/2015/08/my-application-base-obnam/</id><summary type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;


&lt;p&gt;I found attic to be faster, but with a small supporting community. Obnam was
slower, but seems to have a more active community which I find important for 
infrastructure that is meant to live quite long (you don't want to switch 
backup solutions every year). I also found it pretty easy to work with, and
to restore files back, and Gentoo provides the &lt;a href="https://packages.gentoo.org/package/app-backup/obnam"&gt;app-backup/obnam&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;I think both are decent solutions, so I had to make one choice and ended up
with obnam. So, how does it work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring what to backup&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic configuration file for obnam is &lt;code&gt;/etc/obnam.conf&lt;/code&gt;. Inside this file,
I tell which directories need to be backed up, as well as which subdirectories
or files (through expressions) can be left alone. For instance, I don't want
obnam to backup ISO files as those have been downloaded anyway.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[config]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="na"&gt;repository&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;/srv/backup&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="na"&gt;root&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;/root, /etc, /var/lib/portage, /srv/virt/gentoo, /home&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="na"&gt;exclude&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;\.img$, \.iso$, /home/[^/]*/Development/Centralized/.*&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="na"&gt;exclude-caches&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;yes&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="na"&gt;keep&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;8h,14d,10w,12m,10y&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;root&lt;/code&gt; parameter tells obnam which directories (and subdirectories) to
back up. With &lt;code&gt;exclude&lt;/code&gt; a particular set of files or directories can be
excluded, for instance because these contain downloaded resources (and as such
do not need to be inside the backup archives).&lt;/p&gt;
&lt;p&gt;Obnam also supports the &lt;a href="http://www.brynosaurus.com/cachedir/spec.html"&gt;CACHEDIR.TAG&lt;/a&gt;
specification, which I use for the various cache directories. With the use of 
these cache tag files I do not need to update the &lt;code&gt;obnam.conf&lt;/code&gt; file with every
new cache directory (or software build directory).&lt;/p&gt;
&lt;p&gt;The last parameter in the configuration that I want to focus on is the &lt;code&gt;keep&lt;/code&gt;
parameter. Every time obnam takes a backup, it creates what it calls a new
&lt;em&gt;generation&lt;/em&gt;. When the backup storage becomes too big, administrators can run
&lt;code&gt;obnam forget&lt;/code&gt; to drop generations. The &lt;code&gt;keep&lt;/code&gt; parameter informs obnam which
generations can be removed and which ones can be kept.&lt;/p&gt;
&lt;p&gt;In my case, I want to keep one backup per hour for the last 8 hours (I normally
take one backup per day, but during some development sprees or photo
manipulations I back up multiple times), one per day for the last two weeks, 
one per week for the last 10 weeks, one per month for the last 12 months and
one per year for the last 10 years.&lt;/p&gt;
&lt;p&gt;Obnam will clean up only when &lt;code&gt;obnam forget&lt;/code&gt; is executed. As storage is cheap,
and the performance of obnam is sufficient for me, I do not need to call this
very often.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backing up and restoring files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My backup strategy is to backup to an external disk, and then synchronize this
disk with a personal backup server somewhere else. This backup server runs no
other software beyond OpenSSH (to allow secure transfer of the backups) and both
the backup server disks and the external disk is &lt;a href="https://wiki.gentoo.org/wiki/Dm-crypt"&gt;LUKS&lt;/a&gt;
encrypted. Considering that I don't have government secrets I opted not to encrypt
the backup files themselves, but Obnam does support that (through GnuPG).&lt;/p&gt;
&lt;p&gt;All backup enabled systems use cron jobs which execute &lt;code&gt;obnam backup&lt;/code&gt; to take
the backup, and use rsync to synchronize the finished backup with the backup
server. If I need to restore a file, I use &lt;code&gt;obnam ls&lt;/code&gt; to see which file(s) I
need to restore (add in a &lt;code&gt;--generation=&lt;/code&gt; to list the files of a different
backup generation than the last one).&lt;/p&gt;
&lt;p&gt;Then, the command to restore is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/var/restore /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or I can restore immediately to the directory again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/home/swift/Images/Processing /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To support multiple clients, obnam by default identifies each client through
the hostname. It is possible to use different names, but hostnames tend to be
a common best practice which I don't deviate from either. Obnam is able to share
blocks between clients (it is not mandatory, but supported nonetheless).&lt;/p&gt;</content><category term="Free-Software"></category><category term="mab"></category><category term="backup"></category><category term="obnam"></category></entry><entry><title>Don't confuse SELinux with its policy</title><link href="https://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/" rel="alternate"></link><published>2015-08-03T01:49:00+02:00</published><updated>2015-08-03T01:49:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-03:/2015/08/dont-confuse-selinux-with-its-policy/</id><summary type="html">&lt;p&gt;With the increased attention that SELinux is getting thanks to its inclusion in
recent &lt;a href="https://source.android.com/devices/tech/security/selinux/"&gt;Android&lt;/a&gt;
releases, more and more people are understanding that SELinux is not a singular
security solution. Many administrators are still disabling SELinux on their 
servers because it does not play well with their day-to-day operations. But
the Android inclusion shows that SELinux itself is not the culprit for this:
it is the policy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the increased attention that SELinux is getting thanks to its inclusion in
recent &lt;a href="https://source.android.com/devices/tech/security/selinux/"&gt;Android&lt;/a&gt;
releases, more and more people are understanding that SELinux is not a singular
security solution. Many administrators are still disabling SELinux on their 
servers because it does not play well with their day-to-day operations. But
the Android inclusion shows that SELinux itself is not the culprit for this:
it is the policy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Policy versus enforcement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SELinux has conceptually segregated the enforcement from the rules/policy. 
There is an in-kernel enforcement (the SELinux subsystem) which is configured
through an administrator-provided policy (the SELinux rules). As long as 
SELinux was being used on servers, chances are very high that the policy that
is being used is based on the &lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;SELinux Reference Policy&lt;/a&gt;
as this is, as far as I know, the only policy implementation for Linux systems
that is widely usable.&lt;/p&gt;
&lt;p&gt;The reference policy project aims to provide a well designed, broadly usable
yet still secure set of rules. And through this goal, it has to play ball with
all possible use cases that the various software titles require. Given the open
ecosystem of the free software world, and the Linux based ones in particular, 
managing such a policy is not for beginners. New policy development requires 
insight in the technology for which the policy is created, as well as knowledge
of how the reference policy works.&lt;/p&gt;
&lt;p&gt;Compare this to the Android environment. Applications have to follow more
rigid guidelines before they are accepted on Android systems. Communication
between applications and services is governed through Intents and Activities
which are managed by the &lt;a href="http://www.cubrid.org/blog/dev-platform/binder-communication-mechanism-of-android-processes/"&gt;Binder&lt;/a&gt;
application. Interactions with the user are based on well defined interfaces.
Heck, the Android OS even holds a number of permissions that applications
have to subscribe to before they can use it.&lt;/p&gt;
&lt;p&gt;Such an environment is much easier to create policies for, because it allows
policies to be created almost on-the-fly, with the application permissions
being mapped to predefined SELinux rules. Because the freedom of
implementations is limited (in order to create a manageable environment which
is used by millions of devices over the world) policies can be made more
strictly and yet enjoy the static nature of the environment: no continuous
updates on existing policies, something that Linux distributions have to do
on an almost daily basis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aiming for a policy development ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having SELinux active on Android shows that one should not confuse SELinux
with its policies. SELinux is a nice security subsystem in the Linux kernel,
and can be used and tuned to cover whatever use case is given to it. The slow
adoption of SELinux by Linux distributions might be attributed to its lack
of policy diversification, which results in few ecosystems where additional
(and perhaps innovative) policies could be developed.&lt;/p&gt;
&lt;p&gt;It is however a huge advantage that a reference policy exists, so that
distributions can enjoy a working policy without having to put resources
into its own policy development and maintenance. Perhaps we should try to
further enhance the existing policies while support new policy ecosystems
and development initiatives.&lt;/p&gt;
&lt;p&gt;The maturation of the &lt;a href="https://github.com/SELinuxProject/cil/wiki"&gt;CIL&lt;/a&gt;
language by the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux userland libraries and tools&lt;/a&gt;
might be a good catalyst for this. At one point, policies will need to be
migrated to CIL (although this can happen gradually as the userland utilities
can deal with CIL and other languages such as the legacy &lt;code&gt;.pp&lt;/code&gt; files 
simultaneously) and there are a few developers considering a renewal
of the reference policy. This would make use of the new benefits of the CIL
language and implementation: some restrictions that where applicable to the legacy
format no longer holds on CIL, such as rules which previously were only allowed
in the base policy which can now be made part of the modules as well.&lt;/p&gt;
&lt;p&gt;But next to renewing existing policies, there is plenty of room left for
innovative policy ideas and developments. The &lt;a href="http://selinuxproject.org/page/PolicyLanguage"&gt;SELinux language&lt;/a&gt;
is very versatile, and just like with programming languages we notice that only
a few set of constructs are used. Some applications might even benefit from
using SELinux as their decision and enforcement system (something that
&lt;a href="https://wiki.postgresql.org/wiki/SEPostgreSQL_Introduction"&gt;SEPostgreSQL&lt;/a&gt; has
tried).&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://freecomputerbooks.com/The-SELinux-Notebook-The-Foundations.html"&gt;SELinux Notebook&lt;/a&gt; by
Richard Haines is an excellent resource for developers that want to work more
closely with the SELinux language constructs. Just skimming through this resource
also shows how very open SELinux itself is, and that most of the users'
experience with SELinux is based on a singular policy implementation. This is
a prime reason why having a more open policy ecosystem makes perfect sense.&lt;/p&gt;
&lt;p&gt;If you don't like a particular car, do you ditch driving at all? No, you try out
another car. Let's create other cars in the SELinux world as well.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="policy"></category><category term="cil"></category></entry><entry><title>Switching to Pelican</title><link href="https://blog.siphos.be/2015/08/switching-to-pelican/" rel="alternate"></link><published>2015-08-02T04:09:00+02:00</published><updated>2015-08-02T04:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-02:/2015/08/switching-to-pelican/</id><summary type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;


&lt;p&gt;Today I finally moved the blog, imported all past articles as well as
comments. For the commenting, I now use &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;disqus&lt;/a&gt;
which integrates nicely with Pelican and has a fluid feel to it. I wanted to
use the &lt;a href="http://www.tipue.com/search/"&gt;Tipue Search&lt;/a&gt; plug-in as well for
searching through the blog, but I had to put that on hold as I couldn't get
the results of a search to display nicely (all I got were links to
"undefined"). But I'll work on this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring Pelican&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pelican configuration is done through &lt;code&gt;pelicanconf.py&lt;/code&gt; and &lt;code&gt;publishconf.py&lt;/code&gt;. 
The former contains all definitions and settings for the site which are also
useful when previewing changes. The latter contains additional (or overruled)
settings related to publication.&lt;/p&gt;
&lt;p&gt;In order to keep the same links as before (to keep web crawlers happy, as well
as links to the blog from other sites and even the comments themselves) I did
had to update some variables, but the Internet was strong on this one and I had
little problems finding the right settings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Link structure of the site&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next challenges were (and still are, I will have to check if this is working
or not soon by checking the blog aggregation sites I am usually aggregated on)
the RSS and Atom feeds. From the access logs of my previous blog, I believe that
most of the aggregation sites are using the &lt;code&gt;/feed/&lt;/code&gt;, &lt;code&gt;/feed/atom&lt;/code&gt; and 
&lt;code&gt;/category/*/feed&lt;/code&gt; links.&lt;/p&gt;
&lt;p&gt;Now, I would like to move the aggregations to XML files, so that the RSS feed is
available at &lt;code&gt;/feed/rss.xml&lt;/code&gt; and the Atom feed at &lt;code&gt;/feed/atom.xml&lt;/code&gt;, but then the
existing aggregations would most likely fail because they currently don't use
these URLs. To fix this, I am now trying to generate the XML files as I would
like them to be, and create symbolic links afterwards from &lt;code&gt;index.html&lt;/code&gt; to the
right XML file.&lt;/p&gt;
&lt;p&gt;The RSS/ATOM settings I am currently using are as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;CATEGORY_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TRANSLATION_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Hopefully, the existing aggregations still work, and I can then start asking
the planets to move to the XML URL itself. Some tracking on the access logs
should allow me to see how well this is going.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing to make sure is happening correctly is the blog aggregation and
the comment system. Then, a few tweaks are still on the pipeline.&lt;/p&gt;
&lt;p&gt;One is to optimize the front page a bit. Right now, all articles are
summarized, and I would like to have the last (or last few) article(s) fully
expanded whereas the rest is summarized. If that isn't possible, I'll probably
switch to fully expanded articles (which is a matter of setting a single
variable).&lt;/p&gt;
&lt;p&gt;Next, I really want the search functionality to work again. Enabling the Tipue
search worked almost flawlessly - search worked as it should, and the resulting
search entries are all correct. The problem is that the URLs that the entries
point to (which is what users will click on) all point to an invalid
("undefined") URL.&lt;/p&gt;
&lt;p&gt;Finally, I want the printer-friendly one to be without the social / links on
the top right. This is theme-oriented, and I'm happily using
&lt;a href="https://github.com/DandyDev/pelican-bootstrap3"&gt;pelican-bootstrap3&lt;/a&gt; right now,
so I don't expect this to be much of a hassle. But considering that my blog is
mainly technology oriented for now (although I am planning on expanding that)
being able to have the articles saved in PDF or printed in a nice format is
an important use case for me.&lt;/p&gt;</content><category term="Free-Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>Loading CIL modules directly</title><link href="https://blog.siphos.be/2015/07/loading-cil-modules-directly/" rel="alternate"></link><published>2015-07-15T15:54:00+02:00</published><updated>2015-07-15T15:54:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-15:/2015/07/loading-cil-modules-directly/</id><summary type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/"&gt;previous
post&lt;/a&gt;
I used the &lt;code&gt;secilc&lt;/code&gt; binary to load an additional test policy. Little did
I know (and that's actually embarrassing because it was one of the
things I complained about) that you can just use the CIL policy as
modules directly.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;With this I mean that a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/"&gt;previous
post&lt;/a&gt;
I used the &lt;code&gt;secilc&lt;/code&gt; binary to load an additional test policy. Little did
I know (and that's actually embarrassing because it was one of the
things I complained about) that you can just use the CIL policy as
modules directly.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;With this I mean that a CIL policy as mentioned in the previous post can
be loaded like a prebuilt &lt;code&gt;.pp&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i test.cil
~# semodule -l | grep test
test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's all that is to it. Loading the module resulted in the test port
to be immediately declared and available:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage port -l | grep test
test_port_t                    tcp      1440
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In hindsight, it makes sense that it is this easy. After all, support
for the old-style policy language is done by converting it into CIL when
calling &lt;code&gt;semodule&lt;/code&gt; so it makes sense to immediately put the module (in
CIL code) ready to be taken up.&lt;/p&gt;</content><category term="SELinux"></category><category term="cil"></category><category term="selinux"></category></entry><entry><title>Restricting even root access to a folder</title><link href="https://blog.siphos.be/2015/07/restricting-even-root-access-to-a-folder/" rel="alternate"></link><published>2015-07-11T14:09:00+02:00</published><updated>2015-07-11T14:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-11:/2015/07/restricting-even-root-access-to-a-folder/</id><summary type="html">&lt;p&gt;In a
&lt;a href="http://blog.siphos.be/2014/01/private-key-handling-and-selinux-protection/comment-page-1/#comment-143323"&gt;comment&lt;/a&gt;
Robert asked how to use SELinux to prevent even root access to a
directory. The trivial solution would be not to assign an administrative
role to the root account (which is definitely possible, but you want
some way to gain administrative access otherwise ;-)&lt;/p&gt;
&lt;p&gt;Restricting root is one of the commonly referred features of a MAC
(Mandatory Access Control) system. With a well designed user management
and sudo environment, it is fairly trivial - but if you need to start
from the premise that a user has direct root access, it requires some
thought to implement it correctly. The main "issue" is not that it is
difficult to implement policy-wise, but that most users will start from
a pre-existing policy (such as the reference policy) and build on top of
that.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In a
&lt;a href="http://blog.siphos.be/2014/01/private-key-handling-and-selinux-protection/comment-page-1/#comment-143323"&gt;comment&lt;/a&gt;
Robert asked how to use SELinux to prevent even root access to a
directory. The trivial solution would be not to assign an administrative
role to the root account (which is definitely possible, but you want
some way to gain administrative access otherwise ;-)&lt;/p&gt;
&lt;p&gt;Restricting root is one of the commonly referred features of a MAC
(Mandatory Access Control) system. With a well designed user management
and sudo environment, it is fairly trivial - but if you need to start
from the premise that a user has direct root access, it requires some
thought to implement it correctly. The main "issue" is not that it is
difficult to implement policy-wise, but that most users will start from
a pre-existing policy (such as the reference policy) and build on top of
that.&lt;/p&gt;


&lt;p&gt;The use of a pre-existing policy means that some roles are already
identified and privileges are already granted to users - often these
higher privileged roles are assigned to the Linux root user as not to
confuse users. But that does mean that restricting root access to a
folder means that some additional countermeasures need to be
implemented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The policy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But first things first. Let's look at a simple policy for restricting
access to &lt;code&gt;/etc/private&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(myprivate, 1.0)

type etc_private_t;
fs_associate(etc_private_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This simple policy introduces a type (&lt;code&gt;etc_private_t&lt;/code&gt;) which is allowed
to be used for files (it associates with a file system). &lt;em&gt;Do not&lt;/em&gt; use
the &lt;code&gt;files_type()&lt;/code&gt; interface as this would assign a set of attributes
that many user roles get read access on.&lt;/p&gt;
&lt;p&gt;Now, it is not sufficient to have the type available. If we want to
assign it to a type, someone or something needs to have the privileges
to change the security context of a file and directory to this type. If
we would just load this policy and try to do this from a privileged
account, it would fail:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# chcon -t etc_private_t /etc/private
chcon: failed to change context of &amp;#39;/etc/private&amp;#39; to &amp;#39;system_u:object_r:etc_private_t:s0&amp;#39;: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the following rule, the &lt;code&gt;sysadm_t&lt;/code&gt; domain (which I use for system
administration) is allowed to change the context to &lt;code&gt;etc_private_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow sysadm_t etc_private_t:{dir file} relabelto;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this in place, the administrator can label resources as
&lt;code&gt;etc_private_t&lt;/code&gt; without having read access to these resources
afterwards. Also, as long as there are no &lt;em&gt;relabelfrom&lt;/em&gt; privileges
assigned, the administrator cannot revert the context back to a type
that he has read access to.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The countermeasures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But this policy is not sufficient. One way that administrators can
easily access the resources is to disable SELinux controls (as in, put
the system in permissive mode):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# cat /etc/private/README
cat: /etc/private/README: Permission denied
~# setenforce 0
~# cat /etc/private/README
Hello World!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To prevent this, enable the &lt;em&gt;secure_mode_policyload&lt;/em&gt; SELinux boolean:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# setsebool secure_mode_policyload on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will prevent any policy and SELinux state manipulation... including
permissive mode, but also including loading additional SELinux policies
or changing booleans. Definitely experiment with this setting without
persisting (i.e. do not use &lt;code&gt;-P&lt;/code&gt; in the above command yet) to make sure
it is manageable for you.&lt;/p&gt;
&lt;p&gt;Still, this isn't sufficient. Don't forget that the administrator is
otherwise a full administrator - if he cannot access the &lt;code&gt;/etc/private&lt;/code&gt;
location directly, then he might be able to access it indirectly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the resource is on a non-critical file system, he can unmount the
    file system and remount it with a &lt;code&gt;context=&lt;/code&gt; mount option. This will
    override the file-level contexts. Bind-mounting does not seem to
    allow overriding the context.&lt;/li&gt;
&lt;li&gt;If the resource is on a file system that cannot be unmounted, the
    administrator can still reboot the system in a mode where he can
    access the file system regardless of SELinux controls (either
    through editing &lt;code&gt;/etc/selinux/config&lt;/code&gt; or by booting with
    &lt;code&gt;enforcing=0&lt;/code&gt;, etc.&lt;/li&gt;
&lt;li&gt;The administrator can still access the block device files on which
    the resources are directly. Specialized tools can allow for
    extracting files and directories without actually (re)mounting
    the device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more extensive list of methods to potentially gain access to such
resources is iterated in &lt;a href="http://blog.siphos.be/2013/12/limiting-file-access-with-selinux-alone/"&gt;Limiting file access with SELinux
alone&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This set of methods for gaining access is due to the administrative role
already assigned by the existing policy. To further mitigate these risks
with SELinux (although SELinux will never completely mitigate all risks)
the roles assigned to the users need to be carefully revisited. If you
grant people administrative access, but you don't want them to be able
to reboot the system, (re)mount file systems, access block devices, etc.
then create a user role that does not have these privileges at all.&lt;/p&gt;
&lt;p&gt;Creating such user roles does not require leaving behind the policy that
is already active. Additional user domains can be created and granted to
Linux accounts (including root). But in my experience, when you need to
allow a user to log on as the "root" account directly, you probably need
him to have true administrative privileges. Otherwise you'd work with
personal accounts and a well-designed &lt;code&gt;/etc/sudoers&lt;/code&gt; file.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Intermediate policies</title><link href="https://blog.siphos.be/2015/07/intermediate-policies/" rel="alternate"></link><published>2015-07-05T18:17:00+02:00</published><updated>2015-07-05T18:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-05:/2015/07/intermediate-policies/</id><summary type="html">&lt;p&gt;When developing SELinux policies for new software (or existing ones
whose policies I don't agree with) it is often more difficult to finish
the policies so that they are broadly usable. When dealing with personal
policies, having them "just work" is often sufficient. To make the
policies reusable for distributions (or for the upstream project), a
number of things are necessary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try structuring the policy using the style as suggested by refpolicy
    or Gentoo&lt;/li&gt;
&lt;li&gt;Add the role interfaces that are most likely to be used or required,
    or which are in the current draft implemented differently&lt;/li&gt;
&lt;li&gt;Refactor some of the policies to use refpolicy/Gentoo style
    interfaces&lt;/li&gt;
&lt;li&gt;Remove the comments from the policies (as refpolicy does not want
    too verbose policies)&lt;/li&gt;
&lt;li&gt;Change or update the file context definitions for default
    installations (rather than the custom installations I use)&lt;/li&gt;
&lt;/ul&gt;
</summary><content type="html">&lt;p&gt;When developing SELinux policies for new software (or existing ones
whose policies I don't agree with) it is often more difficult to finish
the policies so that they are broadly usable. When dealing with personal
policies, having them "just work" is often sufficient. To make the
policies reusable for distributions (or for the upstream project), a
number of things are necessary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try structuring the policy using the style as suggested by refpolicy
    or Gentoo&lt;/li&gt;
&lt;li&gt;Add the role interfaces that are most likely to be used or required,
    or which are in the current draft implemented differently&lt;/li&gt;
&lt;li&gt;Refactor some of the policies to use refpolicy/Gentoo style
    interfaces&lt;/li&gt;
&lt;li&gt;Remove the comments from the policies (as refpolicy does not want
    too verbose policies)&lt;/li&gt;
&lt;li&gt;Change or update the file context definitions for default
    installations (rather than the custom installations I use)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This often takes quite some effort. Some of these changes (such as the
style updates and commenting) are even counterproductive for me
personally (in the sense that I don't gain any value from doing so and
would have to start maintaining two different policy files for the same
policy), and necessary only for upstreaming policies. As a result, I
often finish with policies that I just leave for me personally or
somewhere on a public repository (like these
&lt;a href="https://github.com/sjvermeu/small.coding/tree/master/selinux-modules/neo4j"&gt;Neo4J&lt;/a&gt;
and
&lt;a href="https://github.com/sjvermeu/small.coding/tree/master/selinux-modules/ceph"&gt;Ceph&lt;/a&gt;
policies), without any activities already scheduled to attempt to
upstream those.&lt;/p&gt;
&lt;p&gt;But not contributing the policies to a broader public means that the
effort is not known, and other contributors might be struggling with
creating policies for their favorite (or necessary) technologies. So the
majority of policies that I write I still hope to eventually push them
out. But I noticed that these last few steps for upstreaming (the ones
mentioned above) might only take a few hours of work, but take me over 6
months (or more) to accomplish (as I often find other stuff more
interesting to do).&lt;/p&gt;
&lt;p&gt;I don't know yet how to change the process to make it more interesting
to use. However, I do have a couple of wishes that might make it easier
for me, and perhaps others, to contribute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of reacting on contribution suggestions, work on a common
    repository together. Just like with a wiki, where we don't aim for a
    100% correct and well designed document from the start, we should
    use the strength of the community to continuously improve policies
    (and to allow multiple people to work on the same policy). Right
    now, policies are a one-man publication with a number of people
    commenting on the suggested changes and asking the one person to
    refactor or update the change himself.&lt;/li&gt;
&lt;li&gt;Document the style guide properly, but don't disallow contributions
    if they do not adhere to the style guide completely. Instead, merge
    and update. On successful wikis there are even people that update
    styles without content updates, and their help is greatly
    appreciated by the community.&lt;/li&gt;
&lt;li&gt;If a naming convention is to be followed (which is the case
    with policies) make it clear. Too often the name of an interface is
    something that takes a few days of discussion. That's not productive
    for policy development.&lt;/li&gt;
&lt;li&gt;Find a way to truly create a "core" part of the policy and a
    modular/serviceable approach to handle additional policies. The idea
    of the &lt;code&gt;contrib/&lt;/code&gt; repository was like that, but failed to live up to
    its expectations: the number of people who have commit access to the
    contrib is almost the same as to the core, a few exceptions
    notwithstanding, and whenever policies are added to contrib they
    often require changes on the core as well. Perhaps even support
    overlay-type approaches to policies so that intermediate policies
    can be "staged" and tested by a larger audience before they are
    vetted into the upstream reference policy.&lt;/li&gt;
&lt;li&gt;Settle on how to deal with networking controls. My suggestion would
    be to immediately support the TCP/UDP ports as assigned by IANA (or
    another set of sources) so that additional policies do not need to
    wait for the base policy to support the ports. Or find and support a
    way for contributions to declare the port types themselves (we
    probably need to focus on CIL for this).&lt;/li&gt;
&lt;li&gt;Document "best practices" on policy development where certain types
    of policies are documented in more detail. For instance, desktop
    application profiles, networked daemons, user roles, etc. These best
    practices should not be mandatory and should in fact support a broad
    set of privilege isolation. With the latter, I mean that there are
    policies who cover an entire category of systems (init systems, web
    servers), a single software package or even the sub-commands and
    sub-daemons of that package. It would surprise me if this can't be
    supported better out-of-the-box (as in, through a well
    thought-through base policy framework and styleguide).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I believe that this might create a more active community surrounding
policy development.&lt;/p&gt;</content><category term="SELinux"></category><category term="community"></category><category term="contributions"></category><category term="policy-development"></category><category term="selinux"></category></entry><entry><title>Where does CIL play in the SELinux system?</title><link href="https://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/" rel="alternate"></link><published>2015-06-13T23:12:00+02:00</published><updated>2015-06-13T23:12:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-06-13:/2015/06/where-does-cil-play-in-the-selinux-system/</id><summary type="html">&lt;p&gt;SELinux policy developers already have a number of file formats to work
with. Currently, policy code is written in a set of three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.te&lt;/code&gt; file contains the SELinux policy code (type
    enforcement rules)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.if&lt;/code&gt; file contains functions which turn a set of arguments into
    blocks of SELinux policy code (interfaces). These functions are
    called by other interface files or type enforcement files&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.fc&lt;/code&gt; file contains mappings of file path expressions towards
    labels (file contexts)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These files are compiled into loadable modules (or a base module) which
are then transformed to an active policy. But this is not a single-step
approach.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;SELinux policy developers already have a number of file formats to work
with. Currently, policy code is written in a set of three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.te&lt;/code&gt; file contains the SELinux policy code (type
    enforcement rules)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.if&lt;/code&gt; file contains functions which turn a set of arguments into
    blocks of SELinux policy code (interfaces). These functions are
    called by other interface files or type enforcement files&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.fc&lt;/code&gt; file contains mappings of file path expressions towards
    labels (file contexts)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These files are compiled into loadable modules (or a base module) which
are then transformed to an active policy. But this is not a single-step
approach.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Transforming policy code into policy file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the Linux kernel SELinux subsystem, only a single file matters - the
&lt;code&gt;policy.##&lt;/code&gt; file (for instance &lt;code&gt;policy.29&lt;/code&gt;). The suffix denotes the
binary format used as higher numbers mean that additional SELinux
features are supported which require different binary formats for the
SELinux code in the Linux kernel.&lt;/p&gt;
&lt;p&gt;With the 2.4 userspace, the transformation of the initial files as
mentioned above towards a policy file is done as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SELinux transformation diagram" src="http://dev.gentoo.org/~swift/blog/201506/formats_selinux.png"&gt;&lt;/p&gt;
&lt;p&gt;When a developer builds a policy module, first &lt;code&gt;checkmodule&lt;/code&gt; is used to
build a &lt;code&gt;.mod&lt;/code&gt; intermediate file. This file contains the type
enforcement rules with the expanded rules of the various interface
files. Next, &lt;code&gt;semodule_package&lt;/code&gt; is called which transforms this
intermediate file, together with the file context file, into a &lt;code&gt;.pp&lt;/code&gt;
file.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;.pp&lt;/code&gt; file is, in the 2.4 userspace, called a "high level language"
file. There is little high-level about it, but the idea is that such
high-level language files are then transformed into &lt;code&gt;.cil&lt;/code&gt; files (CIL
stands for &lt;em&gt;Common Intermediate Language&lt;/em&gt;). If at any moment other
frameworks come around, they could create high-level languages
themselves and provide a transformation engine to convert these HLL
files into CIL files.&lt;/p&gt;
&lt;p&gt;For the current &lt;code&gt;.pp&lt;/code&gt; files, this transformation is supported through
the &lt;code&gt;/usr/libexec/selinux/hll/pp&lt;/code&gt; binary which, given a &lt;code&gt;.pp&lt;/code&gt; file,
outputs CIL code.&lt;/p&gt;
&lt;p&gt;Finally, all CIL files (together) are compiled into a binary &lt;code&gt;policy.29&lt;/code&gt;
file. All the steps coming from a &lt;code&gt;.pp&lt;/code&gt; file towards the final binary
file are handled by the &lt;code&gt;semodule&lt;/code&gt; command. For instance, if an
administrator loads an additional &lt;code&gt;.pp&lt;/code&gt; file, its (generated) CIL code
is added to the other active CIL code and together, a new policy binary
file is created.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adding some CIL code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux userspace development repository contains a &lt;code&gt;secilc&lt;/code&gt; command
which can compile CIL code into a binary policy file. As such, it can
perform the (very) last step of the file conversions above. However, it
is not &lt;em&gt;integrated&lt;/em&gt; in the sense that, if additional code is added, the
administrator can "play" with it as he would with SELinux policy
modules.&lt;/p&gt;
&lt;p&gt;Still, that shouldn't prohibit us from playing around with it to
experiment with the CIL language construct. Consider the following CIL
SELinux policy code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;; Declare a test_port_t type
(type test_port_t)
; Assign the type to the object_r role
(roletype object_r test_port_t)

; Assign the right set of attributes to the port
(typeattributeset defined_port_type test_port_t)
(typeattributeset port_type test_port_t)

; Declare tcp:1440 as test_port_t
(portcon tcp 1440 (system_u object_r test_port_t ((s0) (s0))))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code declares a port type (&lt;code&gt;test_port_t&lt;/code&gt;) and uses it for the TCP
port 1440.&lt;/p&gt;
&lt;p&gt;In order to use this code, we have to build a policy file which includes
all currently active CIL code, together with the test code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ secilc -c 29 /var/lib/selinux/mcs/active/modules/400/*/cil testport.cil
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is a &lt;code&gt;policy.29&lt;/code&gt; (the command forces version 29 as the
current Linux kernel used on this system does not support version 30)
file, which can now be copied to &lt;code&gt;/etc/selinux/mcs/policy&lt;/code&gt;. Then, after
having copied the file, load the new policy file using &lt;code&gt;load_policy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;And lo and behold, the port type is now available:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage port -l | grep 1440
test_port_t           tcp      1440
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To verify that it really is available and not just parsed by the
userspace, let's connect to it and hope for a nice denial message:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -p 1440 localhost
ssh: connect to host localhost port 1440: Permission denied

~$ sudo ausearch -ts recent
time-&amp;gt;Thu Jun 11 19:35:45 2015
type=PROCTITLE msg=audit(1434044145.829:296): proctitle=737368002D700031343430006C6F63616C686F7374
type=SOCKADDR msg=audit(1434044145.829:296): saddr=0A0005A0000000000000000000000000000000000000000100000000
type=SYSCALL msg=audit(1434044145.829:296): arch=c000003e syscall=42 success=no exit=-13 a0=3 a1=6d4d1ce050 a2=1c a3=0 items=0 ppid=2005 pid=18045 auid=1001 uid=1001 gid=1001 euid=1001 suid=1001 fsuid=1001 egid=1001 sgid=1001 fsgid=1001 tty=pts0 ses=1 comm=&amp;quot;ssh&amp;quot; exe=&amp;quot;/usr/bin/ssh&amp;quot; subj=staff_u:staff_r:ssh_t:s0 key=(null)
type=AVC msg=audit(1434044145.829:296): avc:  denied  { name_connect } for  pid=18045 comm=&amp;quot;ssh&amp;quot; dest=1440 scontext=staff_u:staff_r:ssh_t:s0 tcontext=system_u:object_r:test_port_t:s0 tclass=tcp_socket permissive=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="cil"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Live SELinux userspace ebuilds</title><link href="https://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/" rel="alternate"></link><published>2015-06-10T20:07:00+02:00</published><updated>2015-06-10T20:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-06-10:/2015/06/live-selinux-userspace-ebuilds/</id><summary type="html">&lt;p&gt;In between courses, I pushed out live ebuilds for the SELinux userspace
applications: libselinux, policycoreutils, libsemanage, libsepol,
sepolgen, checkpolicy and secilc. These live ebuilds (with Gentoo
version 9999) pull in the current development code of the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux
userspace&lt;/a&gt; so that developers
and contributors can already work with in-progress code developments as
well as see how they work on a Gentoo platform.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In between courses, I pushed out live ebuilds for the SELinux userspace
applications: libselinux, policycoreutils, libsemanage, libsepol,
sepolgen, checkpolicy and secilc. These live ebuilds (with Gentoo
version 9999) pull in the current development code of the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux
userspace&lt;/a&gt; so that developers
and contributors can already work with in-progress code developments as
well as see how they work on a Gentoo platform.&lt;/p&gt;


&lt;p&gt;That being said, I do not recommend using the live ebuilds for anyone
else except developers and contributors in development zones (definitely
not on production). One of the reasons is that the ebuilds do not apply
Gentoo-specific patches to the ebuilds. I would also like to remove the
Gentoo-specific manipulations that we do, such as small Makefile
adjustments, but let's start with just ignoring the Gentoo patches.&lt;/p&gt;
&lt;p&gt;Dropping the patches makes sure that we track upstream libraries and
userspace closely, and allows developers to try and send out patches to
the SELinux project to fix Gentoo related build problems. But as not all
packages can be deployed successfully on a Gentoo system some patches
need to be applied anyway. For this, users can drop the necessary
patches inside &lt;code&gt;/etc/portage/patches&lt;/code&gt; as all userspace ebuilds use the
&lt;em&gt;epatch_user&lt;/em&gt; method.&lt;/p&gt;
&lt;p&gt;Finally, observant users will notice that "secilc" is also provided.
This is a new package, which is probably going to have an official
release with a new userspace release. It allows for building CIL-based
SELinux policy code, and was one of the drivers for me to create the
live ebuilds as I'm experimenting with the CIL constructions. So expect
more on that later.&lt;/p&gt;</content><category term="Gentoo"></category><category term="cil"></category><category term="Gentoo"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>PostgreSQL with central authentication and authorization</title><link href="https://blog.siphos.be/2015/05/postgresql-with-central-authentication-and-authorization/" rel="alternate"></link><published>2015-05-25T12:07:00+02:00</published><updated>2015-05-25T12:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-25:/2015/05/postgresql-with-central-authentication-and-authorization/</id><summary type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;


&lt;p&gt;Centralized management is an important principle whenever deployments
become very dispersed. For authentication and authorization, having a
high-available LDAP is one of the more powerful components in any
architecture. It isn't the only method though - it is also possible to
use a distributed approach where the master data is centrally managed,
but the proper data is distributed to the various systems that need it.
Such a distributed approach allows for high availability without the
need for a highly available central infrastructure (user ids, group
membership and passwords are distributed to the servers rather than
queried centrally). Here, I'm going to focus on a mixture of both
methods: central authentication for password verification, and
distributed authorization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PostgreSQL default uses in-database credentials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By default, PostgreSQL uses in-database credentials for the
authentication and authorization. When a &lt;code&gt;CREATE ROLE&lt;/code&gt; (or
&lt;code&gt;CREATE USER&lt;/code&gt;) command is issued with a password, it is stored in the
&lt;code&gt;pg_catalog.pg_authid&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# select rolname, rolpassword from pg_catalog.pg_authid;
    rolname     |             rolpassword             
----------------+-------------------------------------
 postgres_admin | 
 dmvsl          | 
 johan          | 
 hdc_owner      | 
 hdc_reader     | 
 hdc_readwrite  | 
 hadoop         | 
 swift          | 
 sean           | 
 hdpreport      | 
 postgres       | md5c127bc9fc185daf0e06e785876e38484
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;this cannot be moved outside):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# \l db_hadoop
                                   List of databases
   Name    |   Owner   | Encoding |  Collate   |   Ctype    |     Access privileges     
-----------+-----------+----------+------------+------------+---------------------------
 db_hadoop | hdc_owner | UTF8     | en_US.utf8 | en_US.utf8 | hdc_owner=CTc/hdc_owner  +
           |           |          |            |            | hdc_reader=c/hdc_owner   +
           |           |          |            |            | hdc_readwrite=c/hdc_owner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Furthermore, PostgreSQL has some additional access controls through its
&lt;code&gt;pg_hba.conf&lt;/code&gt; file, in which the access towards the PostgreSQL service
itself can be governed based on context information (such as originating
IP address, target database, etc.).&lt;/p&gt;
&lt;p&gt;For more information about the standard setups for PostgreSQL,
&lt;em&gt;definitely&lt;/em&gt; go through the &lt;a href="http://www.postgresql.org/docs/9.4/static/index.html"&gt;official PostgreSQL
documentation&lt;/a&gt; as
it is well documented and kept up-to-date.&lt;/p&gt;
&lt;p&gt;Now, for central management, in-database settings become more difficult
to handle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using PAM for authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first step to move the management of authentication and
authorization outside the database is to look at a way to authenticate
users (password verification) outside the database. I tend not to use a
distributed password approach (where a central component is responsible
for changing passwords on multiple targets), instead relying on a
high-available LDAP setup, but with local caching (to catch short-lived
network hick-ups) and local password use for last-hope accounts (such as
root and admin accounts).&lt;/p&gt;
&lt;p&gt;PostgreSQL can be configured to directly interact with an LDAP, but I
like to use &lt;a href="http://www.linux-pam.org/"&gt;Linux PAM&lt;/a&gt; whenever I can. For
my systems, it is a standard way of managing the authentication of many
services, so the same goes for PostgreSQL. And with the
&lt;a href="https://packages.gentoo.org/package/sys-auth/pam_ldap"&gt;sys-auth/pam_ldap&lt;/a&gt;
package integrating multiple services with LDAP is a breeze. So the
first step is to have PostgreSQL use PAM for authentication. This is
handled through its &lt;code&gt;pg_hba.conf&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# TYPE  DATABASE        USER    ADDRESS         METHOD          [OPTIONS]
local   all             all                     md5
host    all             all     all             pam             pamservice=postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will have PostgreSQL use the &lt;code&gt;postgresql&lt;/code&gt; PAM service for
authentication. The PAM configuration is thus in
&lt;code&gt;/etc/pam.d/postgresql&lt;/code&gt;. In it, we can either directly use the LDAP PAM
modules, or use the SSSD modules and have SSSD work with LDAP.&lt;/p&gt;
&lt;p&gt;Yet, this isn't sufficient. We still need to tell PostgreSQL which users
can be authenticated - the users need to be defined in the database
(just without password credentials because that is handled externally
now). This is done together with the authorization handling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Users and group membership&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every service on the systems I maintain has dedicated groups in which
for instance its administrators are listed. For instance, for the
PostgreSQL services:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqladmin
gpgsqladmin:x:413843:swift,dmvsl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A local batch job (ran through cron) queries this group (which I call
the &lt;em&gt;masterlist&lt;/em&gt;, as well as queries which users in PostgreSQL are
assigned the &lt;code&gt;postgres_admin&lt;/code&gt; role (which is a superuser role like
postgres and is used as the intermediate role to assign to
administrators of a PostgreSQL service), known as the &lt;em&gt;slavelist&lt;/em&gt;.
Delta's are then used to add the user or remove it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Note: membersToAdd / membersToRemove / _psql are custom functions
#       so do not vainly search for them on your system ;-)
for member in $(membersToAdd ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;CREATE USER ${member} LOGIN INHERIT;&amp;quot; postgres
  _psql &amp;quot;GRANT postgres_admin TO ${member};&amp;quot; postgres
done

for member in $(membersToRemove ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;REVOKE postgres_admin FROM ${member};&amp;quot; postgres
  _psql &amp;quot;DROP USER ${member};&amp;quot; postgres
done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;postgres_admin&lt;/code&gt; role is created whenever I create a PostgreSQL
instance. Likewise, for databases, a number of roles are added as well.
For instance, for the &lt;code&gt;db_hadoop&lt;/code&gt; database, the &lt;code&gt;hdc_owner&lt;/code&gt;,
&lt;code&gt;hdc_reader&lt;/code&gt; and &lt;code&gt;hdc_readwrite&lt;/code&gt; roles are created with the right set of
privileges. Users are then granted this role if they belong to the right
group in the LDAP. For instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqlhdc_own
gpgsqlhdc_own:x:413850:hadoop,johan,christov,sean
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this simple approach, granting users access to a database is a
matter of adding the user to the right group (like &lt;code&gt;gpgsqlhdc_ro&lt;/code&gt; for
read-only access to the Hadoop related database(s)) and either wait for
the cron-job to add it, or manually run the authorization
synchronization. By standardizing on infrastructural roles (admin,
auditor) and data roles (owner, rw, ro) managing multiple databases is a
breeze.&lt;/p&gt;</content><category term="Free-Software"></category><category term="postgresql"></category></entry><entry><title>Testing with permissive domains</title><link href="https://blog.siphos.be/2015/05/testing-with-permissive-domains/" rel="alternate"></link><published>2015-05-18T13:40:00+02:00</published><updated>2015-05-18T13:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-18:/2015/05/testing-with-permissive-domains/</id><summary type="html">&lt;p&gt;When testing out new technologies or new setups, not having (proper)
SELinux policies can be a nuisance. Not only are the number of SELinux
policies that are available through the standard repositories limited,
some of these policies are not even written with the same level of
confinement that an administrator might expect. Or perhaps the
technology to be tested is used in a completely different manner.&lt;/p&gt;
&lt;p&gt;Without proper policies, any attempt to start such a daemon or
application might or will cause permission violations. In many cases,
developers or users tend to disable SELinux enforcing then so that they
can continue playing with the new technology. And why not? After all,
policy development is to be done &lt;em&gt;after&lt;/em&gt; the technology is understood.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When testing out new technologies or new setups, not having (proper)
SELinux policies can be a nuisance. Not only are the number of SELinux
policies that are available through the standard repositories limited,
some of these policies are not even written with the same level of
confinement that an administrator might expect. Or perhaps the
technology to be tested is used in a completely different manner.&lt;/p&gt;
&lt;p&gt;Without proper policies, any attempt to start such a daemon or
application might or will cause permission violations. In many cases,
developers or users tend to disable SELinux enforcing then so that they
can continue playing with the new technology. And why not? After all,
policy development is to be done &lt;em&gt;after&lt;/em&gt; the technology is understood.&lt;/p&gt;


&lt;p&gt;But completely putting the system in permissive mode is overshooting. It
is much easier to make a very simple policy to start with, and then mark
the domain as a permissive domain. What happens is that the software
then, after transitioning into the "simple" domain, is not part of the
SELinux enforcements anymore whereas the rest of the system remains in
SELinux enforcing mode.&lt;/p&gt;
&lt;p&gt;For instance, create a minuscule policy like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(testdom, 1.0)

type testdom_t;
type testdom_exec_t;
init_daemon_domain(testdom_t, testdom_exec_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Mark the executable for the daemon as &lt;code&gt;testdom_exec_t&lt;/code&gt; (after building
and loading the minuscule policy):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# chcon -t testdom_exec_t /opt/something/bin/daemond
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, tell SELinux that &lt;code&gt;testdom_t&lt;/code&gt; is to be seen as a permissive
domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage permissive -a testdom_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When finished, don't forget to remove the permissive bit
(&lt;code&gt;semanage permissive -d testdom_t&lt;/code&gt;) and unload/remove the SELinux
policy module.&lt;/p&gt;
&lt;p&gt;And that's it. If the daemon is now started (through a standard init
script) it will run as &lt;code&gt;testdom_t&lt;/code&gt; and everything it does will be
logged, but not enforced by SELinux. That might even help in
understanding the application better.&lt;/p&gt;</content><category term="SELinux"></category><category term="permissive"></category><category term="policy"></category><category term="selinux"></category><category term="semanage"></category><category term="test"></category></entry><entry><title>Audit buffering and rate limiting</title><link href="https://blog.siphos.be/2015/05/audit-buffering-and-rate-limiting/" rel="alternate"></link><published>2015-05-10T14:18:00+02:00</published><updated>2015-05-10T14:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-10:/2015/05/audit-buffering-and-rate-limiting/</id><summary type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be logged
through the audit subsystem. Depending on the system configuration, they
might be either ignored, sent through the kernel logging infrastructure
or even have the system panic. And if the messages are sent to the
kernel log then they might show up, but even that log has its
limitations, which can lead to output similar to the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;__ratelimit: 53 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this post, I want to give some pointers in configuring the audit
subsystem as well as understand these messages...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is auditd and kauditd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you take a look at the audit processes running on the system, you'll
notice that (assuming Linux auditing is used of course) two processes
are running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# ps -ef | grep audit
root      1483     1  0 10:11 ?        00:00:00 /sbin/auditd
root      1486     2  0 10:11 ?        00:00:00 [kauditd]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;/sbin/auditd&lt;/code&gt; daemon is the user-space audit daemon. It &lt;a href="http://man7.org/linux/man-pages/man3/audit_open.3.html"&gt;registers
itself&lt;/a&gt; with the
Linux kernel audit subsystem (through the audit netlink system), which
responds with spawning the &lt;code&gt;kauditd&lt;/code&gt; kernel thread/process. The fact
that the process is a kernel-level one is why the &lt;code&gt;kauditd&lt;/code&gt; is
surrounded by brackets in the &lt;code&gt;ps&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;Once this is done, audit messages are communicated through the netlink
socket to the user-space audit daemon. For the detail-oriented people
amongst you, look for the &lt;em&gt;kauditd_send_skb()&lt;/em&gt; method in the
&lt;a href="http://lxr.free-electrons.com/source/kernel/audit.c"&gt;kernel/audit.c&lt;/a&gt;
file. Now, generated audit event messages are not directly relayed to
the audit daemon - they are first queued in a sort-of backlog, which is
where the backlog-related messages above come from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audit backlog queue&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the kernel-level audit subsystem, a socket buffer queue is used to
hold audit events. Whenever a new audit event is received, it is logged
and prepared to be added to this queue. Adding to this queue can be
controlled through a few parameters.&lt;/p&gt;
&lt;p&gt;The first parameter is the backlog limit. Be it through a kernel boot
parameter (&lt;code&gt;audit_backlog_limit=N&lt;/code&gt;) or through a message relayed by the
user-space audit daemon (&lt;code&gt;auditctl -b N&lt;/code&gt;), this limit will ensure that a
queue cannot grow beyond a certain size (expressed in the amount of
messages). If an audit event is logged which would grow the queue beyond
this limit, then a failure occurs and is handled according to the system
configuration (more on that later).&lt;/p&gt;
&lt;p&gt;The second parameter is the rate limit. When more audit events are
logged within a second than set through this parameter (which can be
controlled through a message relayed by the user-space audit system,
using &lt;code&gt;auditctl -r N&lt;/code&gt;) then those audit events are not added to the
queue. Instead, a failure occurs and is handled according to the system
configuration.&lt;/p&gt;
&lt;p&gt;Only when the limits are not reached is the message added to the queue,
allowing the user-space audit daemon to consume those events and log
those according to the audit configuration. There are some good
resources on audit configuration available on the Internet. I find &lt;a href="http://webapp5.rrz.uni-hamburg.de/SuSe-Dokumentation/manual/sles-manuals_en/cha.audit.comp.html"&gt;this
SuSe
chapter&lt;/a&gt;
worth reading, but many others exist as well.&lt;/p&gt;
&lt;p&gt;There is a useful command related to the subject of the audit backlog
queue. It queries the audit subsystem for its current status:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# auditctl -s
AUDIT_STATUS: enabled=1 flag=1 pid=1483 rate_limit=0 backlog_limit=8192 lost=3 backlog=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The command displays not only the audit state (enabled or not) but also
the settings for rate limits (on the audit backlog) and backlog limit.
It also shows how many events are currently still waiting in the backlog
queue (which is zero in our case, so the audit user-space daemon has
properly consumed and logged the audit events).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Failure handling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If an audit event cannot be logged, then this failure needs to be
resolved. The Linux audit subsystem can be configured do either silently
discard the message, switch to the kernel log subsystem, or panic. This
can be configured through the audit user-space (&lt;code&gt;auditctl -f [0..2]&lt;/code&gt;),
but is usually left at the default (which is 1, being to switch to the
kernel log subsystem).&lt;/p&gt;
&lt;p&gt;Before that is done, the message is displayed which reveals the cause of
the failure handling:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the backlog queue was set to contain at most 320 entries
(which is low for a production system) and more messages were being
added (the Linux kernel in certain cases allows to have a few more
entries than configured for performance and consistency reasons). The
number of events already lost is displayed, as well as the current
limitation settings. The message "backlog limit exceeded" can be "rate
limit exceeded" if that was the limitation that was triggered.&lt;/p&gt;
&lt;p&gt;Now, if the system is not configured to silently discard it, or to panic
the system, then the "dropped" messages are sent to the kernel log
subsystem. The calls however are also governed through a configurable
limitation: it uses a rate limit which can be set through &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# sysctl -a | grep kernel.printk_rate
kernel.printk_ratelimit = 5
kernel.printk_ratelimit_burst = 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, this system allows one message every 5 seconds,
but does allow a burst of up to 10 messages at once. When the rate
limitation kicks in, then the kernel will log (at most one per second)
the number of suppressed events:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[40676.545099] __ratelimit: 246 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although this limit is kernel-wide, not all kernel log events are
governed through it. It is the caller subsystem (in our case, the audit
subsystem) which is responsible for having its events governed through
this rate limitation or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing up&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before waving goodbye, I would like to point out that the backlog queue
is a memory queue (and not &lt;a href="https://access.redhat.com/solutions/19327"&gt;on disk, Red
Hat&lt;/a&gt;), just in case it wasn't
obvious. Increasing the queue size can result in more kernel memory
consumption. Apparently, a &lt;a href="https://www.redhat.com/archives/linux-audit/2011-October/msg00007.html"&gt;practical size
estimate&lt;/a&gt;
is around 9000 bytes per message. On production systems, it is advised
not to make this setting too low. I personally set it to 8192.&lt;/p&gt;
&lt;p&gt;Lost audit events might result in difficulties for troubleshooting,
which is the case when dealing with new or experimental SELinux
policies. It would also result in missing security-important events. It
is the audit subsystem, after all. So tune it properly, and enjoy the
power of Linux' audit subsystem.&lt;/p&gt;</content><category term="Free-Software"></category><category term="audit"></category><category term="kernel"></category><category term="security"></category><category term="selinux"></category></entry><entry><title>Use change management when you are using SELinux to its fullest</title><link href="https://blog.siphos.be/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/" rel="alternate"></link><published>2015-04-30T20:58:00+02:00</published><updated>2015-04-30T20:58:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-04-30:/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/</id><summary type="html">&lt;p&gt;If you are using SELinux on production systems (with which I mean
systems that you offer services with towards customers or other parties
beyond you, yourself and your ego), please consider proper change
management if you don't do already. SELinux is a very sensitive security
subsystem - not in the sense …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are using SELinux on production systems (with which I mean
systems that you offer services with towards customers or other parties
beyond you, yourself and your ego), please consider proper change
management if you don't do already. SELinux is a very sensitive security
subsystem - not in the sense that it easily fails, but because it is
very fine-grained and as such can easily stop applications from running
when their behavior changes just a tiny bit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sensitivity of SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SELinux is a wonderful security measure for Linux systems that can
prevent successful exploitation of vulnerabilities or misconfigurations.
Of course, it is not the sole security measure that systems should take.
Proper secure configuration of services, least privilege accounts,
kernel-level mitigations such as grSecurity and more are other measures
that certainly need to be taken if you really find system security to be
a worthy goal to attain. But I'm not going to talk about those others
right now. What I am going to focus on is SELinux, and how sensitive it
is to changes.&lt;/p&gt;
&lt;p&gt;An important functionality of SELinux to understand is that it
segregates the security control system itself (the SELinux subsystem)
from its configuration (the policy). The security control system itself
is relatively small, and focuses on enforcement of the policy and
logging (either because the policy asks to log something, or because
something is prevented, or because an error occurred). The most
difficult part of handling SELinux on a system is not enabling or
interacting with it. No, it is its policy.&lt;/p&gt;
&lt;p&gt;The policy is also what makes SELinux so darn sensitive for small system
changes (or behavior that is not either normal, or at least not allowed
through the existing policy). Let me explain with a small situation that
I recently had.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case in point: Switching an IP address&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A case that beautifully shows how sensitive SELinux can be is an IP
address change. My systems all obtain their IP address (at least for
IPv4) from a DHCP system. This is of course acceptable behavior as
otherwise my systems would never be able to boot up successfully anyway.
The SELinux policy that I run also allows this without any hindrance. So
that was not a problem.&lt;/p&gt;
&lt;p&gt;Yet recently I had to switch an IP address for a system in production.
All the services I run are set up in a dual-active mode, so I started
with the change by draining the services to the second system, shutting
down the service and then (after reconfiguring the DHCP system to now
provide a different IP address) reload the network configuration. And
then it happened - the DHCP client just stalled.&lt;/p&gt;
&lt;p&gt;As the change failed, I updated the DHCP system again to deliver the old
IP address and then reloaded the network configuration on the client.
Again, it failed. Dumbstruck, I looked at the AVC denials and lo and
behold, I notice a &lt;code&gt;dig&lt;/code&gt; process running in a DHCP client related domain
that is trying to do UDP binds, which the policy (at that time) did not
allow. But why now suddenly, after all - this system was running happily
for more than a year already (and with occasional reboots for kernel
updates).&lt;/p&gt;
&lt;p&gt;I won't bore you with the investigation. It boils down to the fact that
the DHCP client detected a change compared to previous startups, and was
configured to run a few hooks as additional steps in the IP lease setup.
As these hooks were never ran previously, the policy was never
challenged to face this. And since the address change occurred a revert
to the previous situation didn't work either (as its previous state
information was already deleted).&lt;/p&gt;
&lt;p&gt;I was able to revert the client (which is a virtual guest in KVM) to the
situation right before the change (thank you &lt;code&gt;savevm&lt;/code&gt; and &lt;code&gt;loadvm&lt;/code&gt;
functionality) so that I could work on the policy first in a
non-production environment so that the next change attempt was
successful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The previous situation might be "solved" by temporarily putting the DHCP
client domain in permissive mode just for the change and then back. But
that is ignoring the issue, and unless you have perfect operational
documentation that you always read before making system or configuration
changes, I doubt that you'll remember this for the next time.&lt;/p&gt;
&lt;p&gt;The case is also a good example on the sensitivity of SELinux. It is not
just when software is being upgraded. Every change (be it in
configuration, behavior or operational activity) might result in a
situation that is new for the loaded SELinux policy. As the default
action in SELinux is to deny everything, this will result in unexpected
results on the system. Sometimes very visible (no IP address obtained),
sometimes hidden behind some weird behavior (hostname correctly set but
not the domainname) or perhaps not even noticed until far later. Compare
it to the firewall rule configurations: you might be able to easily
confirm that standard flows are still passed through, but how are you
certain that fallback flows or one-in-a-month connection setups are not
suddenly prevented from happening.&lt;/p&gt;
&lt;p&gt;A somewhat better solution than just temporarily disabling SELinux
access controls for a domain is to look into proper change management.
Whenever a change has to be done, make sure that you&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can easily revert the change back to the previous
    situation (backups!)&lt;/li&gt;
&lt;li&gt;have tested the change on a non-vital (preproduction) system first&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two principles are pretty vital when you are serious about using
SELinux in production. I'm not talking about a system that hardly has
any fine-grained policies, like where most of the system's services are
running in "unconfined" domains (although that's still better than not
running with SELinux at all), but where you are truly trying to put a
least privilege policy in place for all processes and services.&lt;/p&gt;
&lt;p&gt;Being able to revert a change allows you to quickly get a service up and
running again so that customers are not affected by the change (and
potential issues) for long time. First fix the service, then fix the
problem. If you are an engineer like me, you might rather focus on the
problem (and a permanent, correct solution) first. But that's wrong -
always first make sure that the customers are not affected by it. Revert
and put the service back up, and then investigate so that the next
change attempt will not go wrong anymore.&lt;/p&gt;
&lt;p&gt;Having a multi-master setup might give some more leeway into
investigating issues (as the service itself is not disrupted) so in the
case mentioned above I would probably have tried fixing the issue
immediately anyway if it wasn't policy-based. But most users do not have
truly multi-master service setups.&lt;/p&gt;
&lt;p&gt;Being able to test (and retest) changes in non-production also allows
you to focus on automation (so that changes can be done faster and in a
repeated, predictable and qualitative manner), regression testing as
well as change accumulation testing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You don't have time for that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Be honest with yourself. If you support services for others (be it in a
paid-for manner or because you support an organization in your free
time) you'll quickly learn that service availability is one of the most
qualitative aspects of what you do. No matter what mess is behind it,
most users don't see all that. All they see is the service itself (and
its performance / features). If a change you wanted to make made a
service unavailable for hours, users will notice. And if the change
wasn't communicated up front or it is the n-th time that this downtime
occurs, they will start asking questions you rather not hear.&lt;/p&gt;
&lt;p&gt;Using a non-production environment is not that much of an issue if the
infrastructure you work with supports bare metal restores, or
snapshot/cloning (in case of VMs). After doing those a couple of times,
you'll easily find that you can create a non-production environment from
the production one. Or, you can go for a permanent non-production
environment (although you'll need to take care that this environment is
at all times representative for the production systems).&lt;/p&gt;
&lt;p&gt;And regarding qualitative changes, I really recommend to use a
configuration management system. I recently switched from Puppet to
Saltstack and have yet to use the latter to its fullest set (most of
what I do is still scripted), but it is growing on me and I'm pretty
convinced that I'll have the majority of my change management scripts
removed by the end of this year towards Saltstack-based configurations.
And that'll allow me to automate changes and thus provide a more
qualitative service offering.&lt;/p&gt;
&lt;p&gt;With SELinux, of course.&lt;/p&gt;</content><category term="SELinux"></category><category term="change-management"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Moving closer to 2.4 stabilization</title><link href="https://blog.siphos.be/2015/04/moving-closer-to-2-4-stabilization/" rel="alternate"></link><published>2015-04-27T19:18:00+02:00</published><updated>2015-04-27T19:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-04-27:/2015/04/moving-closer-to-2-4-stabilization/</id><summary type="html">&lt;p&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki"&gt;SELinux userspace&lt;/a&gt;
project has released version 2.4 in february this year, after release
candidates have been tested for half a year. After its release, we at
the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened&lt;/a&gt;
project have been working hard to integrate it within Gentoo. This
effort has been made a bit more difficult …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki"&gt;SELinux userspace&lt;/a&gt;
project has released version 2.4 in february this year, after release
candidates have been tested for half a year. After its release, we at
the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened&lt;/a&gt;
project have been working hard to integrate it within Gentoo. This
effort has been made a bit more difficult due to the migration of the
policy store from one location to another while at the same time
switching to HLL- and CIL based builds.&lt;/p&gt;
&lt;p&gt;Lately, 2.4 itself has been pretty stable, and we're focusing on the
proper migration from 2.3 to 2.4. The SELinux policy has been adjusted
to allow the migrations to work, and a few final fixes are being tested
so that we can safely transition our stable users from 2.3 to 2.4.
Hopefully we'll be able to stabilize the userspace this month or
beginning of next month.&lt;/p&gt;</content><category term="Gentoo"></category><category term="2.4"></category><category term="Gentoo"></category><category term="hardened"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Trying out Pelican, part one</title><link href="https://blog.siphos.be/2015/03/trying-out-pelican-part-one/" rel="alternate"></link><published>2015-03-06T20:02:00+01:00</published><updated>2015-03-06T20:02:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-03-06:/2015/03/trying-out-pelican-part-one/</id><summary type="html">&lt;p&gt;One of the goals I've set myself to do this year (not as a new year
resolution though, I *really* want to accomplish this ;-) is to move
my blog from Wordpress to a statically built website. And
&lt;a href="http://docs.getpelican.com/en/3.5.0/"&gt;Pelican&lt;/a&gt; looks to be a good
solution to do so. It's based on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the goals I've set myself to do this year (not as a new year
resolution though, I *really* want to accomplish this ;-) is to move
my blog from Wordpress to a statically built website. And
&lt;a href="http://docs.getpelican.com/en/3.5.0/"&gt;Pelican&lt;/a&gt; looks to be a good
solution to do so. It's based on Python, which is readily available and
supported on Gentoo, and is quite readable. Also, it looks to be very
active in development and support. And also: it supports taking data
from an existing Wordpress installation, so that none of the posts are
lost (with some rounding error that's inherit to such migrations of
course).&lt;/p&gt;
&lt;p&gt;Before getting Pelican ready (which is available through Gentoo btw) I
also needed to install &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;pandoc&lt;/a&gt;, and
that became more troublesome than expected. While installing &lt;code&gt;pandoc&lt;/code&gt; I
got hit by its massive amount of dependencies towards &lt;code&gt;dev-haskell/*&lt;/code&gt;
packages, and many of those packages really failed to install. It does
some internal dependency checking and fails, informing me to run
&lt;code&gt;haskell-updater&lt;/code&gt;. Sadly, multiple re-runs of said command did not
resolve the issue. In fact, it wasn't until I hit a &lt;a href="http://forums.gentoo.org/viewtopic-p-7712250.html?sid=7707e62264dadf8bad4b8a1273b19f77"&gt;forum post about
the same
issue&lt;/a&gt;
that a first step to a working solution was found.&lt;/p&gt;
&lt;p&gt;It turns out that the &lt;code&gt;~arch&lt;/code&gt; versions of the haskell packages are
better working. So I enabled &lt;code&gt;dev-haskell/*&lt;/code&gt; in my
&lt;code&gt;package.accept_keywords&lt;/code&gt; file. And then started updating the
packages... which also failed. Then I ran &lt;code&gt;haskell-updater&lt;/code&gt; multiple
times, but that also failed. After a while, I had to run the following
set of commands (in random order) just to get everything to build fine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge -u $(qlist -IC dev-haskell) --keep-going
~# for n in $(qlist -IC dev-haskell); do emerge -u $n; done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It took quite some reruns, but it finally got through. I never thought I
had this much Haskell-related packages installed on my system (89
packages here to be exact), as I never intended to do any Haskell
development since I left the university. Still, I finally got &lt;code&gt;pandoc&lt;/code&gt;
to work. So, on to the migration of my Wordpress site... I thought.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a good time to ask for stabilization requests (I'll look into
it myself as well of course) but also to see if you can help out our
arch testing teams to support the stabilization requests on Gentoo! We
need you!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I started with the &lt;a href="http://docs.getpelican.com/en/latest/importer.html"&gt;official docs on
importing&lt;/a&gt;. Looks
promising, but it didn't turn out too well for me. Importing was okay,
but then immediately building the site again resulted in issues about
wrong arguments (file names being interpreted as an argument name or
function when an underscore was used) and interpretation of code inside
the posts. Then I found Jason Antman's &lt;a href="http://blog.jasonantman.com/2014/02/converting-wordpress-posts-to-pelican-markdown/"&gt;converting wordpress posts to
pelican
markdown&lt;/a&gt;
post to inform me I had to try using markdown instead of restructured
text. And lo and behold - that's much better.&lt;/p&gt;
&lt;p&gt;The first builds look promising. Of all the posts that I made on
Wordpress, only one gives a build failure. The next thing to investigate
is theming, as well as seeing how good the migration goes (it isn't
because there are no errors otherwise that the migration is successful
of course) so that I know how much manual labor I have to take into
consideration when I finally switch (right now, I'm still running
Wordpress).&lt;/p&gt;</content><category term="Gentoo"></category><category term="blog"></category><category term="Gentoo"></category><category term="haskell"></category><category term="pandoc"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>CIL and attributes</title><link href="https://blog.siphos.be/2015/02/cil-and-attributes/" rel="alternate"></link><published>2015-02-15T15:49:00+01:00</published><updated>2015-02-15T15:49:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-02-15:/2015/02/cil-and-attributes/</id><summary type="html">&lt;p&gt;I keep on struggling to remember this, so let's make a blog post out of
it ;-)&lt;/p&gt;
&lt;p&gt;When the SELinux policy is being built, recent userspace (2.4 and
higher) will convert the policy into CIL language, and then build the
binary policy. When the policy supports type attributes, these are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I keep on struggling to remember this, so let's make a blog post out of
it ;-)&lt;/p&gt;
&lt;p&gt;When the SELinux policy is being built, recent userspace (2.4 and
higher) will convert the policy into CIL language, and then build the
binary policy. When the policy supports type attributes, these are of
course also made available in the CIL code. For instance the
&lt;code&gt;admindomain&lt;/code&gt; attribute from the &lt;code&gt;userdomain&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;...
(typeattribute admindomain)
(typeattribute userdomain)
(typeattribute unpriv_userdomain)
(typeattribute user_home_content_type)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Interfaces provided by the module are also applied. You won't find the
interface CIL code in &lt;code&gt;/var/lib/selinux/mcs/active/modules&lt;/code&gt; though; the
code at that location is already "expanded" and filled in. So for the
&lt;code&gt;sysadm_t&lt;/code&gt; domain we have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Equivalent of
# gen_require(`
#   attribute admindomain;
#   attribute userdomain;
# &amp;#39;)
# typeattribute sysadm_t admindomain;
# typeattribute sysadm_t userdomain;

(typeattributeset cil_gen_require admindomain)
(typeattributeset admindomain (sysadm_t ))
(typeattributeset cil_gen_require userdomain)
(typeattributeset userdomain (sysadm_t ))
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, when checking which domains use the &lt;code&gt;admindomain&lt;/code&gt; attribute,
notice the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# seinfo -aadmindomain -x
ERROR: Provided attribute (admindomain) is not a valid attribute name.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But don't panic - this has a reason: as long as there is no SELinux rule
applied towards the &lt;code&gt;admindomain&lt;/code&gt; attribute, then the SELinux policy
compiler will drop the attribute from the final policy. This can be
confirmed by adding a single, cosmetic rule, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## allow admindomain admindomain:process sigchld;

~# seinfo -aadmindomain -x
   admindomain
      sysadm_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So there you go. That does mean that if something previously used the
attribute assignation for any decisions (like "for each domain assigned
the userdomain attribute, do something") will need to make sure that the
attribute is really used in a policy rule.&lt;/p&gt;</content><category term="SELinux"></category><category term="attribute"></category><category term="cil"></category><category term="selinux"></category></entry><entry><title>Have dhcpcd wait before backgrounding</title><link href="https://blog.siphos.be/2015/02/have-dhcpcd-wait-before-backgrounding/" rel="alternate"></link><published>2015-02-08T16:50:00+01:00</published><updated>2015-02-08T16:50:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-02-08:/2015/02/have-dhcpcd-wait-before-backgrounding/</id><summary type="html">&lt;p&gt;Many of my systems use DHCP for obtaining IP addresses. Even though they
all receive a static IP address, it allows me to have them moved over
(migrations), use TFTP boot, cloning (in case of quick testing), etc.
But one of the things that was making my efforts somewhat more …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many of my systems use DHCP for obtaining IP addresses. Even though they
all receive a static IP address, it allows me to have them moved over
(migrations), use TFTP boot, cloning (in case of quick testing), etc.
But one of the things that was making my efforts somewhat more difficult
was that the &lt;code&gt;dhcpcd&lt;/code&gt; service continued (the &lt;code&gt;dhcpcd&lt;/code&gt; daemon immediately
went in the background) even though no IP address was received yet.
Subsequent service scripts that required a working network connection
failed to start then.&lt;/p&gt;
&lt;p&gt;The solution is to configure &lt;code&gt;dhcpcd&lt;/code&gt; to wait for an IP address. This is
done through the &lt;code&gt;-w&lt;/code&gt; option, or the &lt;code&gt;waitip&lt;/code&gt; instruction in the
&lt;code&gt;dhcpcd.conf&lt;/code&gt; file. With that in place, the service script now waits
until an IP address is assigned.&lt;/p&gt;</content><category term="Gentoo"></category><category term="dhcp"></category><category term="dhcpcd"></category><category term="Gentoo"></category></entry><entry><title>Old Gentoo system? Not a problem...</title><link href="https://blog.siphos.be/2015/01/old-gentoo-system-not-a-problem/" rel="alternate"></link><published>2015-01-21T23:05:00+01:00</published><updated>2015-01-21T23:05:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-21:/2015/01/old-gentoo-system-not-a-problem/</id><summary type="html">&lt;p&gt;If you have a very old Gentoo system that you want to upgrade, you might
have some issues with too old software and Portage which can't just
upgrade to a recent state. Although many methods exist to work around
it, one that I have found to be very useful is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you have a very old Gentoo system that you want to upgrade, you might
have some issues with too old software and Portage which can't just
upgrade to a recent state. Although many methods exist to work around
it, one that I have found to be very useful is to have access to old
Portage snapshots. It often allows the administrator to upgrade the
system in stages (say in 6-months blocks), perhaps not the entire world
but at least the system set.&lt;/p&gt;
&lt;p&gt;Finding old snapshots might be difficult though, so at one point I
decided to create &lt;a href="http://dev.gentoo.org/~swift/snapshots/"&gt;a list of old
snapshots&lt;/a&gt;, two months apart,
together with the GPG signature (so people can verify that the snapshot
was not tampered with by me in an attempt to create a Gentoo botnet). I
haven't needed it in a while anymore, but I still try to update the list
every two months, which I just did with the snapshot of January 20th
this year.&lt;/p&gt;
&lt;p&gt;I hope it at least helps a few other admins out there.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="portage"></category><category term="snapshot"></category><category term="tree"></category></entry><entry><title>SELinux is great for enterprises (but many don't know it yet)</title><link href="https://blog.siphos.be/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/" rel="alternate"></link><published>2015-01-03T13:36:00+01:00</published><updated>2015-01-03T13:36:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-03:/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/</id><summary type="html">&lt;p&gt;Large companies that handle their own IT often have internal support
teams for many of the technologies that they use. Most of the time, this
is for reusable components like database technologies, web application
servers, operating systems, middleware components (like file transfers,
messaging infrastructure, ...) and more. All components that are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Large companies that handle their own IT often have internal support
teams for many of the technologies that they use. Most of the time, this
is for reusable components like database technologies, web application
servers, operating systems, middleware components (like file transfers,
messaging infrastructure, ...) and more. All components that are used
and deployed multiple times, and thus warrant the expenses of a
dedicated engineering team.&lt;/p&gt;
&lt;p&gt;Such teams often have (or need to write) secure configuration deployment
guides, so that these components are installed in the organization with
as little misconfigurations as possible. A wrongly configured component
is often worse than a vulnerable component, because vulnerabilities are
often fixed with the software upgrades (you do patch your software,
right?) whereas misconfigurations survive these updates and remain
exploitable for longer periods. Also, misuse of components is harder to
detect than exploiting vulnerabilities because they are often seen as
regular user behavior.&lt;/p&gt;
&lt;p&gt;But next to the redeployable components, most business services are
provided by a single application. Most companies don't have the budget
and resources to put dedicated engineering teams on each and every
application that is deployed in the organization. Even worse, many
companies hire external consultants to help in the deployment of the
component, and then the consultants hand over the maintenance of that
software to internal teams. Some consultants don't fully bother with
secure configuration deployment guides, or even feel the need to disable
security constraints put forth by the organization (policies and
standards) because "it is needed". A deployment is often seen as
successful when the software functionally works, which not necessarily
means that it is misconfiguration-free.&lt;/p&gt;
&lt;p&gt;As a recent example that I came across, consider an application that
needs &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt;. A consultancy firm is hired to set
up the infrastructure, and given full administrative rights on the
operating system to make sure that this particular component is deployed
fast (because the company wants to have the infrastructure in production
before the end of the week). Security is initially seen as less of a
concern, and the consultancy firm informs the customer (without any
guarantees though) that it will be set up "according to common best
practices". The company itself has no engineering team for Node.js nor
wants to invest in the appropriate resources (such as training) for
security engineers to review Node.js configurations. Yet the application
that is deployed on the Node.js application server is internet-facing,
so has a higher risk associated with it than a purely internal
deployment.&lt;/p&gt;
&lt;p&gt;So, how to ensure that these applications cannot be exploited or, if an
exploit is done, how to ensure that the risks involved with the exploit
are contained? Well, this is where I believe SELinux has a great
potential. And although I'm talking about SELinux here, the same goes
for other similar technologies like &lt;a href="http://en.wikipedia.org/wiki/TOMOYO_Linux"&gt;TOMOYO
Linux&lt;/a&gt;, &lt;a href="http://en.wikibooks.org/wiki/Grsecurity/The_RBAC_System"&gt;grSecurity's RBAC
system&lt;/a&gt;,
&lt;a href="http://www.rsbac.org/"&gt;RSBAC&lt;/a&gt; and more.&lt;/p&gt;
&lt;p&gt;SELinux can provide a container, decoupled from the application itself
(but of course built for that particular application) which restricts
the behavior of that application on the system to those activities that
are expected. The application itself is not SELinux-aware (or does not
need to be - some applications are, but those that I am focusing on here
usually don't), but the SELinux access controls ensure that exploits on
the application cannot reach beyond those activities/capabilities that
are granted to it.&lt;/p&gt;
&lt;p&gt;Consider the Node.js deployment from before. The Node.js application
server might need to connect to a &lt;a href="http://www.mongodb.org/"&gt;MongoDB&lt;/a&gt;
cluster, so we can configure SELinux to allow just that, but all other
connections that originate from the Node.js deployment should be
forbidden. Worms (if any) cannot use this deployment then to spread out.
Same with access to files - the Node.js application probably only needs
access to the application files and not to other system files. Instead
of trying to run the application in a chroot (which requires engineering
effort from those people implementing Node.js, which could be a
consultancy firm that does not know or want to deploy within a chroot)
SELinux is configured to disallow any file access beyond the application
files.&lt;/p&gt;
&lt;p&gt;With SELinux, the application can be deployed relatively safely while
ensuring that exploits (or abuse of misconfigurations) cannot spread.
All that the company itself has to do is to provide resources for a
SELinux engineering team (which can be just a responsibility of the
Linux engineering teams, but can be specialized as well). Such a team
does not need to be big, as policy development effort is usually only
needed during changes (for instance when the application is updated to
also send e-mails, in which case the SELinux policy can be adjusted to
allow that as well), and given enough experience, the SELinux
engineering team can build flexible policies that the administration
teams (those that do the maintenance of the servers) can tune the policy
as needed (for instance through SELinux booleans) without the need to
have the SELinux team work on the policies again.&lt;/p&gt;
&lt;p&gt;Using SELinux also has a number of additional advantages which other,
sometimes commercial tools (like Symantecs SPE/SCSP - really Symantec,
you ask customers to disable SELinux?) severly lack.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SELinux is part of a default Linux installation in many cases.
    RedHat Enterprise Linux ships with SELinux by default, and actively
    supports SELinux when customers have any problems with it. This also
    improves the likelihood for SELinux to be accepted, as other, third
    party solutions might not be supported. Ever tried getting support
    for a system on which both McAfee AV for Linux and Symantec SCSP are
    running (if you got it to work together at all)? At least McAfee
    gives pointers to how to update &lt;a href="https://kc.mcafee.com/corporate/index?page=content&amp;amp;id=KB67360"&gt;SELinux
    settings&lt;/a&gt;
    when they would interfere with McAfee processes.&lt;/li&gt;
&lt;li&gt;SELinux is widely known and many resources exist for users,
    administrators and engineers to learn more about it. The resources
    are freely available, and often kept up2date by a very
    motivated community. Unlike commercial products, whose support pages
    are hidden behind paywalls, customers are usually prevented from
    interacting with each other and tips and tricks for using the
    product are often not found on the Internet, SELinux information can
    be found almost everywhere. And if you like books, I have a couple
    for you to read: &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
    Administration&lt;/a&gt;
    and &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
    Cookbook&lt;/a&gt;,
    written by yours truly.&lt;/li&gt;
&lt;li&gt;Using SELinux is widely supported by third party configuration
    management tools, especially in the free software world.
    &lt;a href="http://puppetlabs.com/"&gt;Puppet&lt;/a&gt;, &lt;a href="https://www.chef.io/chef/"&gt;Chef&lt;/a&gt;,
    &lt;a href="http://www.ansible.com/home"&gt;Ansible&lt;/a&gt;,
    &lt;a href="http://www.saltstack.com/"&gt;SaltStack&lt;/a&gt; and others all support
    SELinux and/or have modules that integrate SELinux support in the
    management system.&lt;/li&gt;
&lt;li&gt;Using SELinux incurs no additional licensing costs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, SELinux is definitely not a holy grail. It has its limitations, so
security should still be seen as a global approach where SELinux is just
playing one specific role in. For instance, SELinux does not prevent
application behavior that is allowed by the policy. If a user abuses a
configuration and can have an application expose information that the
user usually does not have access to, but the application itself does
(for instance because other users on that application might) SELinux
cannot do anything about it (well, not as long as the application is not
made SELinux-aware). Also, vulnerabilities that exploit application
internals are not controlled by SELinux access controls. It is the
application behavior ("external view") that SELinux controls. To
mitigate in-application vulnerabilities, other approaches need to be
considered (such as memory protections for free software solutions,
which can protect against some kinds of exploits - see
&lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; as one of the solutions that could
be used).&lt;/p&gt;
&lt;p&gt;Still, I believe that SELinux can definitely provide additional
protections for such "one-time deployments" where a company cannot
invest in resources to provide engineering services on those
deployments. The SELinux security controls do not require engineering on
the application side, making investments in SELinux engineering very
much reusable.&lt;/p&gt;</content><category term="SELinux"></category><category term="companies"></category><category term="configuration"></category><category term="engineering"></category><category term="enterprise"></category><category term="selinux"></category></entry><entry><title>Gentoo Wiki is growing</title><link href="https://blog.siphos.be/2015/01/gentoo-wiki-is-growing/" rel="alternate"></link><published>2015-01-03T10:09:00+01:00</published><updated>2015-01-03T10:09:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-03:/2015/01/gentoo-wiki-is-growing/</id><summary type="html">&lt;p&gt;Perhaps it is because of the winter holidays, but the last weeks I've
noticed a lot of updates and edits on the Gentoo wiki.&lt;/p&gt;
&lt;p&gt;The move to the
&lt;a href="https://wiki.gentoo.org/wiki/Project:Website/Tyrian"&gt;Tyrian&lt;/a&gt; layout,
whose purpose is to eventually become the unified layout for all Gentoo
resources, happened first. Then, three common templates (&lt;code&gt;Code …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Perhaps it is because of the winter holidays, but the last weeks I've
noticed a lot of updates and edits on the Gentoo wiki.&lt;/p&gt;
&lt;p&gt;The move to the
&lt;a href="https://wiki.gentoo.org/wiki/Project:Website/Tyrian"&gt;Tyrian&lt;/a&gt; layout,
whose purpose is to eventually become the unified layout for all Gentoo
resources, happened first. Then, three common templates (&lt;code&gt;Code&lt;/code&gt;, &lt;code&gt;File&lt;/code&gt;
and &lt;code&gt;Kernel&lt;/code&gt;) where deprecated in favor of their "*Box" counterparts
(&lt;code&gt;CodeBox&lt;/code&gt;, &lt;code&gt;FileBox&lt;/code&gt; and &lt;code&gt;KernelBox&lt;/code&gt;). These provide better parameter
support (which should make future updates on the templates easier to
implement) as well as syntax highlighting.&lt;/p&gt;
&lt;p&gt;But the wiki also saw a number of contributions being added. I added a
short article on &lt;a href="https://wiki.gentoo.org/wiki/Efibootmgr"&gt;Efibootmgr&lt;/a&gt;
as the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbook&lt;/a&gt; now also uses
it for its EFI related instructions, but other users added quite a few
additional articles as well. As they come along, articles are being
marked by editors for translation. For me, that's a trigger.&lt;/p&gt;
&lt;p&gt;Whenever a wiki article is marked for translations, it shows up on the
&lt;a href="https://wiki.gentoo.org/wiki/Special:PageTranslation"&gt;PageTranslation&lt;/a&gt;
list. When I have time, I pick one of these articles and try to update
it to move to a common style (the
&lt;a href="https://wiki.gentoo.org/wiki/Gentoo_Wiki:Guidelines"&gt;Guidelines&lt;/a&gt; page
is the "official" one, and I have a
&lt;a href="https://wiki.gentoo.org/wiki/User:SwifT/Styleguide"&gt;Styleguide&lt;/a&gt; in
which I elaborate a bit more on the use). Having a common style gives a
better look and feel to the articles (as they are then more alike),
gives a common documentation development approach (so everyone can join
in and update documentation in a similar layout/structure) and - most
importantly - reduces the number of edits that do little more than
switch from one formatting to another.&lt;/p&gt;
&lt;p&gt;When an article has been edited, I mark it for translation, and then the
real workhorse on the wiki starts. We have several active translators on
the Gentoo wiki, who we cannot thank hard enough for their work (I used
to start at Gentoo as a translator, I have some feeling about their
work). They make the Gentoo documentation reachable for a broader
audience. Thanks to the use of the translation extension (kindly offered
by the Gentoo wiki admins, who have been working quite hard the last few
weeks on improving the wiki infrastructure) translations are easier to
handle and follow through.&lt;/p&gt;
&lt;p&gt;The advantage of a translation-marked article is that any change on the
article also shows up on the list again, allowing me to look at the
change and perform edits when necessary. For the end user, this is
behind the scenes - an update on an article shows up immediately, which
is fine. But for me (and perhaps other editors as well) this gives a
nice overview of changes to articles (watchlists can only go so far) and
also shows the changes in a simple yet efficient manner. Thanks to this
approach, we can more actively follow up on edits and improve where
necessary.&lt;/p&gt;
&lt;p&gt;Now, editing is not always just a few minutes of work. Consider the
&lt;a href="https://wiki.gentoo.org/wiki/GRUB2"&gt;GRUB2&lt;/a&gt; article on the wiki. It was
marked for translation, but had some issues with its style. It was very
verbose (which is not a bad thing, but suggests to split information
towards multiple articles) and quite a few open discussions on its
&lt;a href="https://wiki.gentoo.org/wiki/Talk:GRUB2"&gt;Discussions&lt;/a&gt; page. I started
editing the article around 13.12h local time, and ended at 19.40h.
Unlike with offline documentation, the entire process of the editing can
be followed through the page'
&lt;a href="https://wiki.gentoo.org/index.php?title=GRUB2&amp;amp;offset=&amp;amp;limit=100&amp;amp;action=history"&gt;history&lt;/a&gt;).
And although I'm still not 100% satisfied with the result, it is imo
easier to follow through and read.&lt;/p&gt;
&lt;p&gt;However, don't get me wrong - I do not feel that the article was wrong
in any way. Although I would appreciate articles that immediately follow
a style, I rather see more contributions (which we can then edit towards
the new style) than that we would start penalizing contributors that
don't use the style. That would work contra-productive, because it is
far easier to update the style of an article than to write articles. We
should try and get more contributors to document aspects of their Gentoo
journey.&lt;/p&gt;
&lt;p&gt;So, please keep them coming. If you find a lack of (good) information
for something, start jotting down what you know in an article. We'll
gladly help you out with editing and improving the article then, but the
content is something you are probably best to write down.&lt;/p&gt;</content><category term="Documentation"></category><category term="documentation"></category><category term="Gentoo"></category><category term="wiki"></category></entry><entry><title>Why does it access /etc/shadow?</title><link href="https://blog.siphos.be/2014/12/why-does-it-access-etcshadow/" rel="alternate"></link><published>2014-12-30T22:48:00+01:00</published><updated>2014-12-30T22:48:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-30:/2014/12/why-does-it-access-etcshadow/</id><summary type="html">&lt;p&gt;While updating the SELinux policy for the Courier IMAP daemon, I noticed
that it (well, the authdaemon that is part of Courier) wanted to access
&lt;code&gt;/etc/shadow&lt;/code&gt;, which is of course a big no-no. It doesn't take long to
know that this is through the PAM support (more specifically,
&lt;code&gt;pam_unix …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;While updating the SELinux policy for the Courier IMAP daemon, I noticed
that it (well, the authdaemon that is part of Courier) wanted to access
&lt;code&gt;/etc/shadow&lt;/code&gt;, which is of course a big no-no. It doesn't take long to
know that this is through the PAM support (more specifically,
&lt;code&gt;pam_unix.so&lt;/code&gt;). But why? After all, &lt;code&gt;pam_unix.so&lt;/code&gt; should try to execute
&lt;code&gt;unix_chkpwd&lt;/code&gt; to verify a password and not read in the shadow file
directly (which would require all PAM-aware applications to be granted
access to the shadow file).&lt;/p&gt;
&lt;p&gt;So I dived into the PAM-Linux sources (yay free software).&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;pam_unix_passwd.c&lt;/code&gt;, the &lt;em&gt;_unix_run_verify_binary()&lt;/em&gt; method is
called but only if the &lt;em&gt;get_account_info()&lt;/em&gt; method returns
&lt;code&gt;PAM_UNIX_RUN_HELPER&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;static int _unix_verify_shadow(pam_handle_t *pamh, const char *user, unsigned int ctrl)
{
...
        retval = get_account_info(pamh, user, &amp;amp;pwent, &amp;amp;spent);
...
        if (retval == PAM_UNIX_RUN_HELPER) {
                retval = _unix_run_verify_binary(pamh, ctrl, user, &amp;amp;daysleft);
                if (retval == PAM_AUTH_ERR || retval == PAM_USER_UNKNOWN)
                        return retval;
        }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code&gt;passverify.c&lt;/code&gt; this method will check the password entry file and, if
the entry is a shadow file, will return &lt;code&gt;PAM_UNIX_RUN_HELPER&lt;/code&gt; if the
current user id is not root, or if SELinux is enabled:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;PAMH_ARG_DECL(int get_account_info,
        const char *name, struct passwd **pwd, struct spwd **spwdent)
{
        /* UNIX passwords area */
        *pwd = pam_modutil_getpwnam(pamh, name);        /* Get password file entry... */
        *spwdent = NULL;

        if (*pwd != NULL) {
...
                } else if (is_pwd_shadowed(*pwd)) {
                        /*
                         * ...and shadow password file entry for this user,
                         * if shadowing is enabled
                         */
#ifndef HELPER_COMPILE
                        if (geteuid() || SELINUX_ENABLED)
                                return PAM_UNIX_RUN_HELPER;
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;SELINUX_ENABLED&lt;/code&gt; is a C macro defined in the same file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#ifdef WITH_SELINUX
#include 
#define SELINUX_ENABLED is_selinux_enabled()&amp;gt;0
#else
#define SELINUX_ENABLED 0
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this is where my "aha" moment came forth: the Courier authdaemon
runs as root, so its user id is 0. The &lt;em&gt;geteuid()&lt;/em&gt; method will return 0,
so the &lt;code&gt;SELINUX_ENABLED&lt;/code&gt; macro must return non-zero for the proper path
to be followed. A quick check in the audit logs, after disabling
&lt;em&gt;dontaudit&lt;/em&gt; lines, showed that the Courier IMAPd daemon wants to get the
attribute(s) of the &lt;code&gt;security_t&lt;/code&gt; file system (on which the SELinux
information is exposed). As this was denied, the call to
&lt;em&gt;is_selinux_enabled()&lt;/em&gt; returns -1 (error) which, through the macro,
becomes 0.&lt;/p&gt;
&lt;p&gt;So granting &lt;em&gt;selinux_getattr_fs(courier_authdaemon_t)&lt;/em&gt; was enough to
get it to use the &lt;code&gt;unix_chkpwd&lt;/code&gt; binary again.&lt;/p&gt;
&lt;p&gt;To fix this properly, we need to grant this to all PAM using
applications. There is an interface called &lt;em&gt;auth_use_pam()&lt;/em&gt; in the
policies, but that isn't used by the Courier policy. Until now, that is
;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="chkpwd"></category><category term="pam"></category><category term="selinux"></category><category term="shadow"></category><category term="unix_chkpwd"></category></entry><entry><title>Added UEFI instructions to AMD64/x86 handbooks</title><link href="https://blog.siphos.be/2014/12/added-uefi-instructions-to-amd64x86-handbooks/" rel="alternate"></link><published>2014-12-23T18:08:00+01:00</published><updated>2014-12-23T18:08:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-23:/2014/12/added-uefi-instructions-to-amd64x86-handbooks/</id><summary type="html">&lt;p&gt;I just finished up adding some UEFI instructions to the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbooks&lt;/a&gt; for AMD64
and x86 (I don't know how many systems are still using x86 instead of
the AMD64 one, and if those support UEFI, but the instructions are
shared and they don't collide). The entire EFI stuff can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just finished up adding some UEFI instructions to the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbooks&lt;/a&gt; for AMD64
and x86 (I don't know how many systems are still using x86 instead of
the AMD64 one, and if those support UEFI, but the instructions are
shared and they don't collide). The entire EFI stuff can probably be
improved a lot, but basically the things that were added are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;boot the system using UEFI already if possible (which is needed for
    efibootmgr to access the EFI variables). This is not entirely
    mandatory (as efibootmgr is not mandatory to boot a system)
    but recommended.&lt;/li&gt;
&lt;li&gt;use vfat for the &lt;code&gt;/boot/&lt;/code&gt; location, as this now becomes the EFI
    System Partition.&lt;/li&gt;
&lt;li&gt;configure the Linux kernel to support EFI stub and EFI variables&lt;/li&gt;
&lt;li&gt;install the Linux kernel as the &lt;code&gt;bootx64.efi&lt;/code&gt; file to boot the
    system with&lt;/li&gt;
&lt;li&gt;use efibootmgr to add boot options (if required) and create an EFI
    boot entry called "Gentoo"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you find grave errors, please do mention them (either on a talk page
on the wiki, as a &lt;a href="https://bugs.gentoo.org"&gt;bug&lt;/a&gt; or through IRC) so it
is picked up. All developers and trusted contributors on the wiki have
access to the files so can edit where needed (but do take care that, if
something is edited, that it is either architecture-specific or shared
across all architectures - check the page when editing; if it is
&lt;em&gt;Handbook:Parts&lt;/em&gt; then it is shared, and &lt;em&gt;Handbook:AMD64&lt;/em&gt; is specific for
the architecture). And if I'm online I'll of course act on it quickly.&lt;/p&gt;
&lt;p&gt;Oh, and no - it is not a bug that there is a (now not used) &lt;code&gt;/dev/sda1&lt;/code&gt;
"bios" partition. Due to the differences with the possible installation
alternatives, it is easier for us (me) to just document a common
partition layout than to try and write everything out (making it just
harder for new users to follow the instructions).&lt;/p&gt;</content><category term="Documentation"></category><category term="efi"></category><category term="Gentoo"></category><category term="handbook"></category><category term="uefi"></category></entry><entry><title>Handbooks moved</title><link href="https://blog.siphos.be/2014/12/handbooks-moved/" rel="alternate"></link><published>2014-12-14T14:42:00+01:00</published><updated>2014-12-14T14:42:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-14:/2014/12/handbooks-moved/</id><summary type="html">&lt;p&gt;Yesterday the move of the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
Wiki&lt;/a&gt; for the Gentoo
handbooks (whose most important part are the installation instructions
for the various supported architectures) has been concluded, with a
last-minute addition being the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page#Viewing_the_handbook"&gt;one-page
views&lt;/a&gt;
so that users who want to can view the installation instructions
completely within one view …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday the move of the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
Wiki&lt;/a&gt; for the Gentoo
handbooks (whose most important part are the installation instructions
for the various supported architectures) has been concluded, with a
last-minute addition being the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page#Viewing_the_handbook"&gt;one-page
views&lt;/a&gt;
so that users who want to can view the installation instructions
completely within one view.&lt;/p&gt;
&lt;p&gt;Because we use lots of
&lt;a href="http://www.mediawiki.org/wiki/Transclusion"&gt;transclusions&lt;/a&gt; (i.e.
including different wiki articles inside another article) to support a
common documentation base for the various architectures, I did hit a
limit that prevented me from creating a single-page for the entire
handbook (i.e. "Installing Gentoo Linux", "Working with Gentoo",
"Working with portage" and "Network configuration" together), but I
could settle with one page per part. I think that matches most of the
use cases.&lt;/p&gt;
&lt;p&gt;With the move now done, it is time to start tackling the various bugs
that were reported against the handbook, as well as initiate
improvements where needed.&lt;/p&gt;
&lt;p&gt;I did make a (probably more - but this one is fresh in my memory)
mistake in the move though. I had to do a lot of the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Content went missing when switching blog technology :-(
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Without this, transcluded parts would suddenly show the translation tags
as regular text. Only afterwards (I'm talking about more than &lt;a href="https://wiki.gentoo.org/wiki/Project:Documentation/HandbookDevelopment"&gt;400
different
pages&lt;/a&gt;)
did I read that I should transclude the &lt;code&gt;/en&lt;/code&gt; pages (like
&lt;code&gt;Handbook:Parts/Installation/About/en&lt;/code&gt; instead of
&lt;code&gt;Handbook:Parts/Installation/About&lt;/code&gt;) as those do not have the
translation specifics in them. Sigh.&lt;/p&gt;</content><category term="Documentation"></category><category term="Gentoo"></category><category term="handbook"></category><category term="wiki"></category></entry><entry><title>Gentoo Handbooks almost moved to wiki</title><link href="https://blog.siphos.be/2014/12/gentoo-handbooks-almost-moved-to-wiki/" rel="alternate"></link><published>2014-12-12T17:35:00+01:00</published><updated>2014-12-12T17:35:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-12:/2014/12/gentoo-handbooks-almost-moved-to-wiki/</id><summary type="html">&lt;p&gt;Content-wise, the move is done. I've done a few checks on the content to
see if the structure still holds, translations are enabled on all pages,
the use of partitions is sufficiently consistent for each architecture,
and so on. The result can be seen on &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;the gentoo handbook main
page …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Content-wise, the move is done. I've done a few checks on the content to
see if the structure still holds, translations are enabled on all pages,
the use of partitions is sufficiently consistent for each architecture,
and so on. The result can be seen on &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;the gentoo handbook main
page&lt;/a&gt;, from which the
various architectural handbooks are linked.&lt;/p&gt;
&lt;p&gt;I sent a &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.project/4141"&gt;sort-of
announcement&lt;/a&gt;
to the gentoo-project mailinglist (which also includes the motivation of
the move). If there are no objections, I will update the current
handbooks to link to the wiki ones, as well as update the links on the
website (and in wiki articles) to point to the wiki.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="handbook"></category><category term="wiki"></category></entry><entry><title>Sometimes I forget how important communication is</title><link href="https://blog.siphos.be/2014/12/sometimes-i-forget-how-important-communication-is/" rel="alternate"></link><published>2014-12-10T20:38:00+01:00</published><updated>2014-12-10T20:38:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-10:/2014/12/sometimes-i-forget-how-important-communication-is/</id><summary type="html">&lt;p&gt;Free software (and documentation) developers don't always have all the
time they want. Instead, they grab whatever time they have to do what
they believe is the most productive - be it documentation editing,
programming, updating ebuilds, SELinux policy improvements and what not.
But they often don't take the time to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Free software (and documentation) developers don't always have all the
time they want. Instead, they grab whatever time they have to do what
they believe is the most productive - be it documentation editing,
programming, updating ebuilds, SELinux policy improvements and what not.
But they often don't take the time to communicate. And communication is
important.&lt;/p&gt;
&lt;p&gt;For one, communication is needed to reach a larger audience than those
that follow the commit history in whatever repository work is being
done. Yes, there are developers that follow &lt;a href="http://news.gmane.org/gmane.linux.gentoo.cvs"&gt;each
commit&lt;/a&gt;, but development
isn't just done for developers, it is also for end users. And end users
deserve frequent updates and feedback. Be it through blog posts, Google+
posts, tweets or instragrams (well, I'm not sure how to communicate a
software or documentation change through Instagram, but I'm sure people
find lots of creative ways to do so), telling the broader world what has
changed is important.&lt;/p&gt;
&lt;p&gt;Perhaps a (silent or not) user was waiting for this change. Perhaps he
or she is even actually trying to fix things himself/herself but is
struggling with it, and would really benefit (time-wise) from a quick
fix. Without communicating about the change, (s)he does not know that no
further attempts are needed, actually reducing the efficiency in
overall.&lt;/p&gt;
&lt;p&gt;But communication is just one-way. Better is to get feedback as well. In
that sense, communication is just one part of the feedback loop - once
developers receive feedback on what they are doing (or did recently)
they might even improve results faster. With feedback loops, the wisdom
of the crowd (in the positive sense) can be used to improve solutions
beyond what the developer originally intended. And even a simple "cool"
and "I like" is good information for a developer or contributor.&lt;/p&gt;
&lt;p&gt;Still, I often forget to do it - or don't have the time to focus on
communication. And that's bad. So, let me quickly state what things I
forgot to communicate more broadly about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.project/4129"&gt;new
    developer&lt;/a&gt;
    joined the Gentoo ranks: Jason Zaman. Now developers join Gentoo
    more often than just once in a while, but Jason is one of
    my "recruits". In a sense, he became a developer because I was tired
    of pulling his changes in and proxy-committing stuff. Of course,
    that's only half the truth; he is also a very active contributor in
    other areas (and was already a maintainer for a few packages through
    the proxy-maintainer project) and is a tremendous help in the Gentoo
    Hardened project. So welcome onboard Jason (or perfinion as he calls
    himself online).&lt;/li&gt;
&lt;li&gt;I've started with &lt;a href="https://wiki.gentoo.org/wiki/Project:Documentation/HandbookDevelopment"&gt;copying the Gentoo handbook to the
    wiki&lt;/a&gt;.
    This is still an on-going project, but was long overdue. There are
    many reasons why the move to the wiki is interesting. For me
    personally, it is to attract a larger audience to update
    the handbook. Although the document will be restricted for editing
    by developers and trusted contributors only (it does contain the
    installation instructions and is a primary entry point for
    many users) that's still a whole lot more than when just a handful
    (one or two actually) developers update the handbook.&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;SELinux
    userspace&lt;/a&gt;
    (2.4 release) is looking more stable; there are no specific
    regressions anymore (upstream is at release candidate 7) although I
    must admit that I have not implemented it on the majority of test
    systems that I maintain. Not due to fears, but mostly because I
    struggle a bit with available time so I can do without testing
    upgrades that are not needed. I do plan on moving towards 2.4 in a
    week or two.&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;reference
    policy&lt;/a&gt; has
    released a new version of the policy. Gentoo quickly followed
    through (Jason did the honors of creating the ebuilds).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, apologies for not communicating sooner, and I promise I'll try to
uplift the communication frequency.&lt;/p&gt;</content><category term="Gentoo"></category><category term="communication"></category><category term="developer"></category><category term="Gentoo"></category><category term="selinux"></category><category term="time"></category></entry><entry><title>No more DEPENDs for SELinux policy package dependencies</title><link href="https://blog.siphos.be/2014/11/no-more-depends-for-selinux-policy-package-dependencies/" rel="alternate"></link><published>2014-11-02T14:51:00+01:00</published><updated>2014-11-02T14:51:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-11-02:/2014/11/no-more-depends-for-selinux-policy-package-dependencies/</id><summary type="html">&lt;p&gt;I just finished updating 102 packages. The change? Removing the
following from the ebuilds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DEPEND=&amp;quot;selinux? ( sec-policy/selinux-${packagename} )&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the past, we needed this construction in both DEPEND and RDEPEND.
Recently however, the SELinux eclass got updated with some logic to
relabel files after the policy package is deployed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just finished updating 102 packages. The change? Removing the
following from the ebuilds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DEPEND=&amp;quot;selinux? ( sec-policy/selinux-${packagename} )&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the past, we needed this construction in both DEPEND and RDEPEND.
Recently however, the SELinux eclass got updated with some logic to
relabel files after the policy package is deployed. As a result, the
DEPEND variable no longer needs to refer to the SELinux policy package.&lt;/p&gt;
&lt;p&gt;This change also means that for those moving from a regular Gentoo
installation to an SELinux installation will have much less packages to
rebuild. In the past, getting &lt;code&gt;USE="selinux"&lt;/code&gt; (through the SELinux
profiles) would rebuild all packages that have a DEPEND dependency to
the SELinux policy package. No more - only packages that depend on the
SELinux libraries (like &lt;code&gt;libselinux&lt;/code&gt;) or utilities rebuild. The rest
will just pull in the proper policy package.&lt;/p&gt;</content><category term="Gentoo"></category><category term="DEPEND"></category><category term="ebuild"></category><category term="Gentoo"></category><category term="RDEPEND"></category><category term="selinux"></category></entry><entry><title>Using multiple priorities with modules</title><link href="https://blog.siphos.be/2014/10/using-multiple-priorities-with-modules/" rel="alternate"></link><published>2014-10-31T18:24:00+01:00</published><updated>2014-10-31T18:24:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-31:/2014/10/using-multiple-priorities-with-modules/</id><summary type="html">&lt;p&gt;One of the new features of the 2.4 SELinux userspace is support for
module priorities. The idea is that distributions and administrators can
override a (pre)loaded SELinux policy module with another module without
removing the previous module. This lower-version module will remain in
the store, but will not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the new features of the 2.4 SELinux userspace is support for
module priorities. The idea is that distributions and administrators can
override a (pre)loaded SELinux policy module with another module without
removing the previous module. This lower-version module will remain in
the store, but will not be active until the higher-priority module is
disabled or removed again.&lt;/p&gt;
&lt;p&gt;The "old" modules (pre-2.4) are loaded with priority 100. When policy
modules with the 2.4 SELinux userspace series are loaded, they get
loaded with priority 400. As a result, the following message occurs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i screen.pp
libsemanage.semanage_direct_install_info: Overriding screen module at lower priority 100 with module at priority 400
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So unlike the previous situation, where the older module is substituted
with the new one, we now have two "screen" modules loaded; the last one
gets priority 400 and is active. To see all installed modules and
priorities, use the &lt;code&gt;--list-modules&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule --list-modules=all | grep screen
100 screen     pp
400 screen     pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Older versions of modules can be removed by specifying the priority:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -X 100 -r screen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="priorities"></category><category term="priority"></category><category term="selinux"></category><category term="semodule"></category></entry><entry><title>Migrating to SELinux userspace 2.4 (small warning for users)</title><link href="https://blog.siphos.be/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/" rel="alternate"></link><published>2014-10-30T19:44:00+01:00</published><updated>2014-10-30T19:44:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-30:/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/</id><summary type="html">&lt;p&gt;In a few moments, SELinux users which have the \~arch KEYWORDS set
(either globally or for the SELinux utilities in particular) will notice
that the SELinux userspace will upgrade to version 2.4 (release
candidate 5 for now). This upgrade comes with a manual step that needs
to be performed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a few moments, SELinux users which have the \~arch KEYWORDS set
(either globally or for the SELinux utilities in particular) will notice
that the SELinux userspace will upgrade to version 2.4 (release
candidate 5 for now). This upgrade comes with a manual step that needs
to be performed after upgrade. The information is mentioned as
post-installation message of the &lt;code&gt;policycoreutils&lt;/code&gt; package, and
basically sais that you need to execute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The reason is that the SELinux utilities expect the SELinux policy
module store (and the semanage related files) to be in
&lt;code&gt;/var/lib/selinux&lt;/code&gt; and no longer in &lt;code&gt;/etc/selinux&lt;/code&gt;. Note that this does
not mean that the SELinux policy itself is moved outside of that
location, nor is the basic configuration file (&lt;code&gt;/etc/selinux/config&lt;/code&gt;).
It is what tools such as &lt;strong&gt;semanage&lt;/strong&gt; manage that is moved outside that
location.&lt;/p&gt;
&lt;p&gt;I tried to automate the migration as part of the packages themselves,
but this would require the &lt;code&gt;portage_t&lt;/code&gt; domain to be able to move,
rebuild and load policies, which it can't (and to be honest, shouldn't).
Instead of augmenting the policy or making updates to the migration
script as delivered by the upstream project, we currently decided to
have the migration done manually. It is a one-time migration anyway.&lt;/p&gt;
&lt;p&gt;If for some reason end users forget to do the migration, then that does
not mean that the system breaks or becomes unusable. SELinux still
works, SELinux aware applications still work; the only thing that will
fail are updates on the SELinux configuration through tools like
&lt;strong&gt;semanage&lt;/strong&gt; or &lt;strong&gt;setsebool&lt;/strong&gt; - the latter when you want to persist
boolean changes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage fcontext -l
ValueError: SELinux policy is not managed or store cannot be accessed.

~# setsebool -P allow_ptrace on
Cannot set persistent booleans without managed policy.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you get those errors or warnings, all that is left to do is to do the
migration. Note in the following that there is a warning about 'else'
blocks that are no longer supported: that's okay, as far as I know (and
it was mentioned on the upstream mailinglist as well as not something to
worry about) it does not have any impact.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store
Migrating from /etc/selinux/mcs/modules/active to /var/lib/selinux/mcs/active
Attempting to rebuild policy from /var/lib/selinux
sysnetwork: Warning: &amp;#39;else&amp;#39; blocks in optional statements are unsupported in CIL. Dropping from output.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also add in &lt;code&gt;-c&lt;/code&gt; so that the old policy module store is cleaned
up. You can also rerun the command multiple times:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store -c
warning: Policy type mcs has already been migrated, but modules still exist in the old store. Skipping store.
Attempting to rebuild policy from /var/lib/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can manually clean up the old policy module store like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rm -rf /etc/selinux/mcs/modules
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So... don't worry - the change is small and does not break stuff. And
for those wondering about CIL I'll talk about it in one of my next
posts.&lt;/p&gt;</content><category term="Gentoo"></category><category term="cil"></category><category term="Gentoo"></category><category term="migrate"></category><category term="selinux"></category><category term="semanage"></category><category term="upgrade"></category><category term="userspace"></category></entry><entry><title>Lots of new challenges ahead</title><link href="https://blog.siphos.be/2014/10/lots-of-new-challenges-ahead/" rel="alternate"></link><published>2014-10-19T16:01:00+02:00</published><updated>2014-10-19T16:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-19:/2014/10/lots-of-new-challenges-ahead/</id><summary type="html">&lt;p&gt;I've been pretty busy lately, albeit behind the corners, which leads to
a lower activity within the free software communities that I'm active
in. Still, I'm not planning any exit, on the contrary. Lots of ideas are
just waiting for some free time to engage. So what are the challenges …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been pretty busy lately, albeit behind the corners, which leads to
a lower activity within the free software communities that I'm active
in. Still, I'm not planning any exit, on the contrary. Lots of ideas are
just waiting for some free time to engage. So what are the challenges
that have been taking up my time?&lt;/p&gt;
&lt;p&gt;One of them is that I recently moved. And with moving comes a lot of
work in getting the place into a good shape and getting settled. Today I
finished the last job that I wanted to finish in my appartment in a
short amount of time, so that's one thing off my TODO list.&lt;/p&gt;
&lt;p&gt;Another one is that I started an intensive master-after-master programme
with the subject of &lt;em&gt;Enterprise Architecture&lt;/em&gt;. This not only takes up
quite some ex-cathedra time, but also additional hours of studying (and
for the moment also exams). But I'm really satisfied that I can take up
this course, as I've been wandering around in the world of enterprise
architecture for some time now and want to grow even further in this
field.&lt;/p&gt;
&lt;p&gt;But that's not all. One of my side activities has been blooming a lot,
and I recently reached the 200th server that I'm administering (although
I think this number will reduce to about 120 as I'm helping one
organization with handing over management of their 80+ systems to their
own IT staff). Together with some friends (who also have non-profit
customers' IT infrastructure management as their side-business) we're
now looking at consolidating our approach to system administration (and
engineering).&lt;/p&gt;
&lt;p&gt;I'm also looking at investing time and resources in a start-up,
depending on the business plan and required efforts. But more
information on this later when things are more clear :-)&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>After SELinux System Administration, now the SELinux Cookbook</title><link href="https://blog.siphos.be/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/" rel="alternate"></link><published>2014-09-24T20:10:00+02:00</published><updated>2014-09-24T20:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-09-24:/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/</id><summary type="html">&lt;p&gt;Almost an entire year ago (just a few days apart) I
&lt;a href="http://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/"&gt;announced&lt;/a&gt;
my first published book, called &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
Administration&lt;/a&gt;.
The book covered SELinux administration commands and focuses on Linux
administrators that need to interact with SELinux-enabled systems.&lt;/p&gt;
&lt;p&gt;An important part of SELinux was only covered very briefly in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Almost an entire year ago (just a few days apart) I
&lt;a href="http://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/"&gt;announced&lt;/a&gt;
my first published book, called &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
Administration&lt;/a&gt;.
The book covered SELinux administration commands and focuses on Linux
administrators that need to interact with SELinux-enabled systems.&lt;/p&gt;
&lt;p&gt;An important part of SELinux was only covered very briefly in the book:
policy development. So in the spring this year, Packt approached me and
asked if I was interested in authoring a second book for them, called
&lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
Cookbook&lt;/a&gt;.
This book focuses on policy development and tuning of SELinux to fit the
needs of the administrator or engineer, and as such is a logical
follow-up to the previous book. Of course, given my affinity with the
wonderful Gentoo Linux distribution, it is mentioned in the book (and
even the reference platform) even though the book itself is checked
against Red Hat Enterprise Linux and Fedora as well, ensuring that every
recipe in the book works on all distributions. Luckily (or perhaps not
surprisingly) the approach is quite distribution-agnostic.&lt;/p&gt;
&lt;p&gt;Today, I got word that the &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
Cookbook&lt;/a&gt;
is now officially published. The book uses a recipe-based approach to
SELinux development and tuning, so it is quickly hands-on. It gives my
view on SELinux policy development while keeping the methods and
processes aligned with the upstream policy development project (the
&lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;reference policy&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It's been a pleasure (but also somewhat a pain, as this is done in free
time, which is scarce already) to author the book. Unlike the first
book, where I struggled a bit to keep the page count to the requested
amount, this book was not limited. Also, I think the various stages of
the book development contributed well to the final result (something
that I overlooked a bit in the first time, so I re-re-reviewed changes
over and over again this time - after the first editorial reviews, then
after the content reviews, then after the language reviews, then after
the code reviews).&lt;/p&gt;
&lt;p&gt;You'll see me blog a bit more about the book later (as the marketing
phase is now starting) but for me, this is a major milestone which
allowed me to write down more of my SELinux knowledge and experience. I
hope it is as good a read for you as I hope it to be.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Showing return code in PS1</title><link href="https://blog.siphos.be/2014/08/showing-return-code-in-ps1/" rel="alternate"></link><published>2014-08-31T01:14:00+02:00</published><updated>2014-08-31T01:14:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-08-31:/2014/08/showing-return-code-in-ps1/</id><summary type="html">&lt;p&gt;If you do daily management on Unix/Linux systems, then checking the
return code of a command is something you'll do often. If you do SELinux
development, you might not even notice that a command has failed without
checking its return code, as policies might prevent the application from
showing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you do daily management on Unix/Linux systems, then checking the
return code of a command is something you'll do often. If you do SELinux
development, you might not even notice that a command has failed without
checking its return code, as policies might prevent the application from
showing any output.&lt;/p&gt;
&lt;p&gt;To make sure I don't miss out on application failures, I wanted to add
the return code of the last executed command to my PS1 (i.e. the prompt
displayed on my terminal).&lt;br&gt;
I wasn't able to add it to the prompt easily - in fact, I had to use a
bash feature called the &lt;em&gt;prompt command&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When the &lt;code&gt;PROMPT_COMMMAND&lt;/code&gt; variable is defined, then bash will execute
its content (which I declare as a function) to generate the prompt.
Inside the function, I obtain the return code of the last command (&lt;code&gt;$?&lt;/code&gt;)
and then add it to the PS1 variable. This results in the following code
snippet inside my &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;export PROMPT_COMMAND=__gen_ps1

function __gen_ps1() {
  local EXITCODE=&amp;quot;$?&amp;quot;;
  # Enable colors for ls, etc.  Prefer ~/.dir_colors #64489
  if type -P dircolors &amp;gt;/dev/null ; then
    if [[ -f ~/.dir_colors ]] ; then
      eval $(dircolors -b ~/.dir_colors)
    elif [[ -f /etc/DIR_COLORS ]] ; then
      eval $(dircolors -b /etc/DIR_COLORS)
    fi
  fi

  if [[ ${EUID} == 0 ]] ; then
    PS1=&amp;quot;RC=${EXITCODE} \[\033[01;31m\]\h\[\033[01;34m\] \W \$\[\033[00m\] &amp;quot;
  else
    PS1=&amp;quot;RC=${EXITCODE} \[\033[01;32m\]\u@\h\[\033[01;34m\] \w \$\[\033[00m\] &amp;quot;
  fi
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With it, my prompt now nicely shows the return code of the last executed
command. Neat.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; Sean Patrick Santos showed me my utter failure in that this can
be accomplished with the &lt;code&gt;PS1&lt;/code&gt; variable immediately, without using the
overhead of the &lt;code&gt;PROMPT_COMMAND&lt;/code&gt;. Just make sure to properly escape the
&lt;code&gt;$&lt;/code&gt; sign which I of course forgot in my late-night experiments :-(.&lt;/p&gt;</content><category term="Gentoo"></category><category term="bash"></category><category term="ps1"></category><category term="rc"></category><category term="shell"></category></entry><entry><title>Gentoo Hardened august meeting</title><link href="https://blog.siphos.be/2014/08/gentoo-hardened-august-meeting/" rel="alternate"></link><published>2014-08-29T16:43:00+02:00</published><updated>2014-08-29T16:43:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-08-29:/2014/08/gentoo-hardened-august-meeting/</id><summary type="html">&lt;p&gt;Another month has passed, so we had another online meeting to discuss
the progress within Gentoo Hardened.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lead elections&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The yearly lead elections within Gentoo Hardened were up again. Zorry
(Magnus Granberg) was re-elected as project lead so doesn't need to
update his LinkedIn profile yet ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;blueness (Anthony G …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another month has passed, so we had another online meeting to discuss
the progress within Gentoo Hardened.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lead elections&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The yearly lead elections within Gentoo Hardened were up again. Zorry
(Magnus Granberg) was re-elected as project lead so doesn't need to
update his LinkedIn profile yet ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;blueness (Anthony G. Basile) has been working on the uclibc stages for
some time. Due to the configurable nature of these setups, many
&lt;code&gt;/etc/portage&lt;/code&gt; files were provided as part of the stages, which
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=519686"&gt;shouldn't&lt;/a&gt; happen. Work
is on the way to update this accordingly.&lt;/p&gt;
&lt;p&gt;For the musl setup, blueness is also rebuilding the stages to use a
symbolic link to the dynamic linker (&lt;code&gt;/lib/ld-linux-arch.so&lt;/code&gt;) as
recommended by the musl maintainers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grsecurity with PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=520198"&gt;bug&lt;/a&gt; has been
submitted which shows that large binary files (in the bug, a chrome
binary with debug information is shown to be more than 2 Gb in size)
cannot be pax-mark'ed, with &lt;code&gt;paxctl&lt;/code&gt; informing the user that the file is
too big. The problem is when the PAX marks are in ELF (as the
application mmaps the binary) - users of extended attributes based PaX
markings do not have this problem. blueness is working on making things
a bit more intelligent, and to fix this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I have been making a few changes to the SELinux setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The live ebuilds (those with version 9999 which use the repository
    policy rather than snapshots of the policies) are now being used as
    "master" in case of releases: the ebuilds can just be copied to the
    right version to support the releases. The release script inside the
    repository is adjusted to reflect this as well.&lt;/li&gt;
&lt;li&gt;The SELinux eclass now supports two variables, &lt;code&gt;SELINUX_GIT_REPO&lt;/code&gt;
    and &lt;code&gt;SELINUX_GIT_BRANCH&lt;/code&gt;, which allows users to use their own
    repository, and developers to work in specific branches together. By
    setting the right value in the users' &lt;code&gt;make.conf&lt;/code&gt; switching policy
    repositories or branches is now a breeze.&lt;/li&gt;
&lt;li&gt;Another change in the SELinux eclass is that, after the installation
    of SELinux policies, we will check the reverse dependencies of the
    policy package and relabel the files of these packages. This allows
    us to only have &lt;code&gt;RDEPEND&lt;/code&gt; dependencies towards the SELinux policy
    packages (if the application itself does not otherwise link with
    &lt;em&gt;libselinux&lt;/em&gt;), making the dependency tree within the package manager
    more correct. We still need to update these packages to drop the
    &lt;code&gt;DEPEND&lt;/code&gt; dependency, which is something we will focus on in the next
    few months.&lt;/li&gt;
&lt;li&gt;In order to support improved cooperation between SELinux developers
    in the Gentoo Hardened team - perfinion (Jason Zaman) is in the
    queue for becoming a new developer in our mids - a &lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/CodingStyle"&gt;coding style for
    SELinux
    policies&lt;/a&gt;
    is being drafted up. This is of course based on the coding style of
    the reference policy, but with some Gentoo specific improvements and
    more clarifications.&lt;/li&gt;
&lt;li&gt;perfinion has been working on improving the SELinux support in
    OpenRC (release 0.13 and higher), making some of the additions that
    we had to make in the past - such as the &lt;code&gt;selinux_gentoo&lt;/code&gt; init
    script - obsolete.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The meeting also discussed a few bugs in more detail, but if you really
want to know, just hang on and wait for the IRC logs ;-) Other usual
sections (system integrity and profiles) did not have any notable topics
to describe.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category></entry><entry><title>Switching to new laptop</title><link href="https://blog.siphos.be/2014/08/switching-to-new-laptop/" rel="alternate"></link><published>2014-08-19T22:11:00+02:00</published><updated>2014-08-19T22:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-08-19:/2014/08/switching-to-new-laptop/</id><summary type="html">&lt;p&gt;I'm slowly but surely starting to switch to a new laptop. The old one
hasn't completely died (yet) but given that I had to force its CPU
frequency at the lowest Hz or the CPU would burn (and the system
suddenly shut down due to heat issues), and that the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm slowly but surely starting to switch to a new laptop. The old one
hasn't completely died (yet) but given that I had to force its CPU
frequency at the lowest Hz or the CPU would burn (and the system
suddenly shut down due to heat issues), and that the connection between
the battery and laptop fails (so even new battery didn't help out) so I
couldn't use it as a laptop... well, let's say the new laptop is welcome
;-)&lt;/p&gt;
&lt;p&gt;Building Gentoo isn't an issue (having only a few hours per day to work
on it is) and while I'm at it, I'm also experimenting with EFI
(currently still without secure boot, but with EFI) and such.
Considering that the Gentoo Handbook needs quite a few updates (and I'm
thinking to do more than just small updates) knowing how EFI works is a
Good Thing (tm).&lt;/p&gt;
&lt;p&gt;For those interested - the &lt;a href="https://wiki.gentoo.org/wiki/EFI_stub_kernel"&gt;EFI stub
kernel&lt;/a&gt; instructions in
the article on the wiki, and also in Greg's wonderful post on &lt;a href="http://kroah.com/log/blog/2013/09/02/booting-a-self-signed-linux-kernel/"&gt;booting a
self-signed Linux
kernel&lt;/a&gt;
(which I will do later) work pretty well. I didn't try out the "Adding
more kernels" section in it, as I need to be able to (sometimes) edit
the boot options (which isn't easy to accomplish with EFI
stub-supporting kernels afaics). So I installed
&lt;a href="https://wiki.gentoo.org/wiki/Gummiboot"&gt;Gummiboot&lt;/a&gt; (and created a wiki
article on it).&lt;/p&gt;
&lt;p&gt;Lots of things still planned, so little time. But at least building
chromium is now a bit faster - instead of 5 hours and 16 minutes, I can
now enjoy the newer versions after little less than 40 minutes.&lt;/p&gt;</content><category term="Gentoo"></category><category term="efi"></category><category term="Gentoo"></category><category term="laptop"></category></entry><entry><title>Some changes under the hood</title><link href="https://blog.siphos.be/2014/08/some-changes-under-the-hood/" rel="alternate"></link><published>2014-08-09T21:45:00+02:00</published><updated>2014-08-09T21:45:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-08-09:/2014/08/some-changes-under-the-hood/</id><summary type="html">&lt;p&gt;In between conferences, technical writing jobs and traveling, we did a
few changes under the hood for SELinux in Gentoo.&lt;/p&gt;
&lt;p&gt;First of all, new policies are bumped and also stabilized (2.20130411-r3
is now stable, 2.20130411-r5 is \~arch). These have a few updates
(mergers from upstream), and r5 also …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In between conferences, technical writing jobs and traveling, we did a
few changes under the hood for SELinux in Gentoo.&lt;/p&gt;
&lt;p&gt;First of all, new policies are bumped and also stabilized (2.20130411-r3
is now stable, 2.20130411-r5 is \~arch). These have a few updates
(mergers from upstream), and r5 also has preliminary support for
&lt;a href="http://www.freedesktop.org/software/systemd/man/tmpfiles.d.html"&gt;tmpfiles&lt;/a&gt;
(at least the OpenRC implementation of it), which is made part of the
&lt;a href="http://packages.gentoo.org/package/sec-policy/selinux-base-policy"&gt;selinux-base-policy&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;The ebuilds to support new policy releases now are relatively simple
copies of the live ebuilds (which always contain the latest policies) so
that bumping (either by me or other developers) is easy enough. There's
also a release script in our policy repository which tags the right git
commit (the point at which the release is made), creates the necessary
patches, uploads them, etc.&lt;/p&gt;
&lt;p&gt;One of the changes made is to "drop" the &lt;code&gt;BASEPOL&lt;/code&gt; variable. In the
past, &lt;code&gt;BASEPOL&lt;/code&gt; was a variable inside the ebuilds that pointed to the
right patchset (and base policy) as we initially supported policy
modules of different base releases. However, that was a mistake and we
quickly moved to bumping all policies with every releaes, but kept the
&lt;code&gt;BASEPOL&lt;/code&gt; variable in it. Now, &lt;code&gt;BASEPOL&lt;/code&gt; is "just" the &lt;code&gt;${PVR}&lt;/code&gt; value of
the ebuild so no longer needs to be provided. In the future, I'll
probably remove &lt;code&gt;BASEPOL&lt;/code&gt; from the internal eclass and the
&lt;code&gt;selinux-base*&lt;/code&gt; packages as well.&lt;/p&gt;
&lt;p&gt;A more important change to the eclass is support for the
&lt;code&gt;SELINUX_GIT_REPO&lt;/code&gt; and &lt;code&gt;SELINUX_GIT_BRANCH&lt;/code&gt; variables (for live ebuilds,
i.e. those with the 9999 version). If set, then they pull from the
mentioned repository (and branch) instead of the default
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-refpolicy.git;a=summary"&gt;hardened-refpolicy.git&lt;/a&gt;
repository. This allows for developers to do some testing on a different
branch easily, or for other users to use their own policy repository
while still enjoying the SELinux integration support in Gentoo through
the &lt;code&gt;sec-policy/*&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;Finally, I wrote up a first attempt at our &lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/CodingStyle"&gt;coding
style&lt;/a&gt;,
heavily based on the coding style from the reference policy of course
(as our policy is still following this upstream project). This should
allow the team to work better together and to decide on namings
autonomously (instead of hours of discussing and settling for something
as silly as an interface or boolean name ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="eclass"></category><category term="Gentoo"></category><category term="git"></category><category term="hardened"></category><category term="refpolicy"></category><category term="selinux"></category></entry><entry><title>Gentoo Hardened July meeting</title><link href="https://blog.siphos.be/2014/08/gentoo-hardened-july-meeting/" rel="alternate"></link><published>2014-08-01T21:48:00+02:00</published><updated>2014-08-01T21:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-08-01:/2014/08/gentoo-hardened-july-meeting/</id><summary type="html">&lt;p&gt;I failed to show up myself (I fell asleep - kids are fun, but deplete
your energy source quickly), but that shouldn't prevent me from making a
nice write-up of the meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.9 gives some issues with kernel compilations and other components.
Lately, breakage has been reported with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I failed to show up myself (I fell asleep - kids are fun, but deplete
your energy source quickly), but that shouldn't prevent me from making a
nice write-up of the meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.9 gives some issues with kernel compilations and other components.
Lately, breakage has been reported with GCC 4.9.1 compiling MySQL or
with debugging symbols. So for hardened, we'll wait this one out until
the bugs are fixed.&lt;/p&gt;
&lt;p&gt;For GCC 4.10, the
&lt;a href="https://gcc.gnu.org/ml/gcc-patches/2014-07/msg02231.html"&gt;--enable-default-pie&lt;/a&gt;
patch has been sent upstream. If that is accepted, the SSP one will be
sent as well.&lt;/p&gt;
&lt;p&gt;In uclibc land, stages are being developed for PPC. This is the final
architecture that is often used in embedded worlds that needed support
for it in Gentoo, and that's now being finalized. Go blueness!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;libpcre&lt;/code&gt; upgrade broke relabeling operations on SELinux enabled
systems. A fix for this has been made part of libselinux, but a little
too late, so some users will be affected by the problem. It's easily
worked around (removing the &lt;code&gt;*.bin&lt;/code&gt; files in the &lt;code&gt;contexts/files/&lt;/code&gt;
directory of the SELinux configuration) and hopefully will never occur
again.&lt;/p&gt;
&lt;p&gt;The 2.3 userland has finally been stabilized (we had a few dependencies
that we were waiting for - and we were a dependency ourselves for other
packages as well).&lt;/p&gt;
&lt;p&gt;Finally, some &lt;a href="http://article.gmane.org/gmane.linux.gentoo.hardened/6266"&gt;thought
discussion&lt;/a&gt;
is being done (not that there's much feedback on it, but every
documented step is a good step imo) on the SELinux policy within Gentoo
(and the principles that we'll follow that are behind it).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grsecurity / PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Due to some security issues, the Linux kernel sources have been
stabilized more rapidly than usual, which left little time for broad
validation and regression testing. Updates and fixes have been applied
since and new stabilizations occurred. Hopefully we're now at the right,
stable set again.&lt;/p&gt;
&lt;p&gt;The C-based &lt;code&gt;install-xattr&lt;/code&gt; application (which is performance-wise a big
improvement over the Python-based one) is working well in "lab
environments" (some developers are using it exclusively). It is included
in the Portage repository
^(if\ I\ understand\ the\ chat\ excerpts\ correctly)^ but as such
not available for broader usage yet.&lt;/p&gt;
&lt;p&gt;An update against &lt;code&gt;elfix&lt;/code&gt; is made as well as there was a dependency
mismatch when building with &lt;code&gt;USE=-ptpax&lt;/code&gt;. This will be corrected in
elfix-0.9.&lt;/p&gt;
&lt;p&gt;Finally, blueness is also working on a GLEP (Gentoo Linux Enhancement
Proposal) to export VDB information (especially &lt;code&gt;NEEDED.ELF.2&lt;/code&gt;) as this
is important for ELF/library graph information (as used by revdep-pax,
migrate-pax, etc.). Although Portage already does this, this is not part
of the PMS and as such other package managers might not do this (such as
Paludis).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Updates on the profiles has been made to properly include multilib
related variables and other metadata. For some profiles, this went as
easy as expected (nice stacking), but other profiles have inheritance
troubles making it much harder to include the necessary information.
Although some talks have arised on the gentoo-dev mailinglist about
refactoring how Gentoo handles profiles, there hasn't been done much
more than just talking :-( But I'm sure we haven't heard the last of
this yet.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Blueness has added information on &lt;code&gt;EMULTRAMP&lt;/code&gt; in the kernel
configuration, especially noting to the user that it is needed for
Python support in Gentoo Hardened. It is also in the &lt;a href="https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart"&gt;PaX
Quickstart&lt;/a&gt;
document, although this document is becoming a very large one and users
might overlook it.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category></entry><entry><title>Segmentation fault when emerging packages after libpcre upgrade?</title><link href="https://blog.siphos.be/2014/07/segmentation-fault-when-emerging-packages-after-libpcre-upgrade/" rel="alternate"></link><published>2014-07-09T20:35:00+02:00</published><updated>2014-07-09T20:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-07-09:/2014/07/segmentation-fault-when-emerging-packages-after-libpcre-upgrade/</id><summary type="html">&lt;p&gt;SELinux users might be facing failures when emerge is merging a package
to the file system, with an error that looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Setting SELinux security labels
/usr/lib64/portage/bin/misc-functions.sh: line 1112: 23719 Segmentation fault      /usr/sbin/setfiles &amp;quot;${file_contexts_path}&amp;quot; -r &amp;quot;${D}&amp;quot; &amp;quot;${D}&amp;quot;
 * ERROR: dev-libs/libpcre-8.35::gentoo …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;SELinux users might be facing failures when emerge is merging a package
to the file system, with an error that looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Setting SELinux security labels
/usr/lib64/portage/bin/misc-functions.sh: line 1112: 23719 Segmentation fault      /usr/sbin/setfiles &amp;quot;${file_contexts_path}&amp;quot; -r &amp;quot;${D}&amp;quot; &amp;quot;${D}&amp;quot;
 * ERROR: dev-libs/libpcre-8.35::gentoo failed:
 *   Failed to set SELinux security labels.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This has been &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=516608"&gt;reported as bug
516608&lt;/a&gt; and, after some
investigation, the cause is found. First the quick workaround:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# cd /etc/selinux/strict/contexts/files
~# rm *.bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And do the same for the other SELinux policy stores on the system
(targeted, mcs, mls, ...).&lt;/p&gt;
&lt;p&gt;Now, what is happening... Inside the mentioned directory, binary files
exist such as &lt;code&gt;file_contexts.bin&lt;/code&gt;. These files contain the compiled
regular expressions of the non-binary files (like &lt;code&gt;file_contexts&lt;/code&gt;). By
using the precompiled versions, regular expression matching by the
SELinux utilities is a lot faster. Not that it is massively slow
otherwise, but it is a nice speed improvement nonetheless.&lt;/p&gt;
&lt;p&gt;However, when pcre updates occur, then the basic structures that pcre
uses internally might change. For instance, a number might switch from a
signed integer to an unsigned integer. As pcre is meant to be used
within the same application run, most applications do not have any
issues with such changes. However, the SELinux utilities effectively
serialize these structures and later read them back in. If the new pcre
uses a changed structure, then the read-in structures are incompatible
and even corrupt.&lt;/p&gt;
&lt;p&gt;Hence the segmentation faults.&lt;/p&gt;
&lt;p&gt;To resolve this, &lt;a href="http://marc.info/?l=selinux&amp;amp;m=140492568205937&amp;amp;w=2"&gt;Stephen
Smalley&lt;/a&gt; created a
patch that includes PCRE version checking. This patch is now included in
&lt;a href="http://packages.gentoo.org/package/sys-libs/libselinux"&gt;sys-libs/libselinux&lt;/a&gt;
version 2.3-r1. The package also recompiles the existing &lt;code&gt;*.bin&lt;/code&gt; files
so that the older binary files are no longer on the system. But there is
a significant chance that this update will not trickle down to the users
in time, so the workaround might be needed.&lt;/p&gt;
&lt;p&gt;I considered updating the pcre ebuilds as well with this workaround, but
considering that libselinux is most likely to be stabilized faster than
any libpcre bump I let it go.&lt;/p&gt;
&lt;p&gt;At least we have a solution for future upgrades; sorry for the noise.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; &lt;code&gt;libselinux-2.2.2-r5&lt;/code&gt; also has the fix included.&lt;/p&gt;</content><category term="SELinux"></category><category term="file_contexts"></category><category term="fix"></category><category term="Gentoo"></category><category term="libselinux"></category><category term="pcre"></category></entry><entry><title>Multilib in Gentoo</title><link href="https://blog.siphos.be/2014/07/multilib-in-gentoo/" rel="alternate"></link><published>2014-07-02T21:03:00+02:00</published><updated>2014-07-02T21:03:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-07-02:/2014/07/multilib-in-gentoo/</id><summary type="html">&lt;p&gt;One of the areas in Gentoo that is seeing lots of active development is
its ongoing effort to have proper &lt;a href="https://wiki.gentoo.org/wiki/Project:Multilib"&gt;multilib
support&lt;/a&gt; throughout the
tree. In the past, this support was provided through special emulation
packages, but those have the (serious) downside that they are often
outdated, sometimes even having …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the areas in Gentoo that is seeing lots of active development is
its ongoing effort to have proper &lt;a href="https://wiki.gentoo.org/wiki/Project:Multilib"&gt;multilib
support&lt;/a&gt; throughout the
tree. In the past, this support was provided through special emulation
packages, but those have the (serious) downside that they are often
outdated, sometimes even having security issues.&lt;/p&gt;
&lt;p&gt;But this active development is not because we all just started looking
in the same direction. No, it's thanks to a few developers that have put
their shoulders under this effort, directing the development workload
where needed and pressing other developers to help in this endeavor. And
pushing is more than just creating
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=454644"&gt;bugreports&lt;/a&gt; and telling
developers to do something.&lt;/p&gt;
&lt;p&gt;It is also about
&lt;a href="http://article.gmane.org/gmane.linux.gentoo.devel/91125"&gt;communicating&lt;/a&gt;,
&lt;a href="http://article.gmane.org/gmane.linux.gentoo.devel/91770"&gt;giving
feedback&lt;/a&gt; and
patiently helping developers when they have questions.&lt;/p&gt;
&lt;p&gt;I can only hope that other activities within Gentoo and its potential
broad impact work on this as well. Kudos to all involved, as well as all
developers that have undoubtedly put numerous hours of development
effort in the hope to make their ebuilds multilib-capable (I know I had
to put lots of effort in it, but I find it is worthwhile and a big
learning opportunity).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>D-Bus and SELinux</title><link href="https://blog.siphos.be/2014/06/d-bus-and-selinux/" rel="alternate"></link><published>2014-06-30T20:07:00+02:00</published><updated>2014-06-30T20:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-06-30:/2014/06/d-bus-and-selinux/</id><summary type="html">&lt;p&gt;After a &lt;a href="http://blog.siphos.be/2014/06/d-bus-quick-recap/"&gt;post about
D-Bus&lt;/a&gt; comes the
inevitable related post about SELinux with D-Bus.&lt;/p&gt;
&lt;p&gt;Some users might not know that D-Bus is an SELinux-aware application.
That means it has SELinux-specific code in it, which has the D-Bus
behavior based on the SELinux policy (and might not necessarily honor
the "permissive …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After a &lt;a href="http://blog.siphos.be/2014/06/d-bus-quick-recap/"&gt;post about
D-Bus&lt;/a&gt; comes the
inevitable related post about SELinux with D-Bus.&lt;/p&gt;
&lt;p&gt;Some users might not know that D-Bus is an SELinux-aware application.
That means it has SELinux-specific code in it, which has the D-Bus
behavior based on the SELinux policy (and might not necessarily honor
the "permissive" flag). This code is used as an additional
authentication control within D-Bus.&lt;/p&gt;
&lt;p&gt;Inside the SELinux policy, a &lt;em&gt;dbus&lt;/em&gt; permission class is supported, even
though the Linux kernel doesn't do anything with this class. The class
is purely for D-Bus, and it is D-Bus that checks the permission
(although work is being made to &lt;a href="https://lwn.net/Articles/580194/"&gt;implement D-Bus in kernel
(kdbus)&lt;/a&gt;). The class supports two
permission checks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;acquire_svc&lt;/em&gt; which tells the domain(s) allowed to "own" a service
    (which might, thanks to the SELinux support, be different from the
    domain itself)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;send_msg&lt;/em&gt; which tells which domain(s) can send messages to a
    service domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inside the D-Bus security configuration (the &lt;code&gt;busconfig&lt;/code&gt; XML file,
remember) a service configuration might tell D-Bus that the service
itself is labeled differently from the process that owned the service.
The default is that the service inherits the label from the domain, so
when &lt;code&gt;dnsmasq_t&lt;/code&gt; registers a service on the system bus, then this
service also inherits the &lt;code&gt;dnsmasq_t&lt;/code&gt; label.&lt;/p&gt;
&lt;p&gt;The necessary permission checks for the &lt;code&gt;sysadm_t&lt;/code&gt; user domain to send
messages to the dnsmasq service, and the dnsmasq service itself to
register it as a service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow dnsmasq_t self:dbus { acquire_svc send_msg };
allow sysadm_t dnsmasq_t:dbus send_msg;
allow dnsmasq_t sysadm_t:dbus send_msg;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For the &lt;code&gt;sysadm_t&lt;/code&gt; domain, the two rules are needed as we usually not
only want to send a message to a D-Bus service, but also receive a reply
(which is also handled through a &lt;em&gt;send_msg&lt;/em&gt; permission but in the
inverse direction).&lt;/p&gt;
&lt;p&gt;However, with the following XML snippet inside its service configuration
file, owning a certain resource is checked against a different label:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;busconfig&amp;gt;
  &amp;lt;selinux&amp;gt;
    &amp;lt;associate
      own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;
      context=&amp;quot;system_u:object_r:dnsmasq_dbus_t:s0&amp;quot; /&amp;gt;
  &amp;lt;/selinux&amp;gt;
&amp;lt;/busconfig&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this, the rules would become as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow dnsmasq_t dnsmasq_dbus_t:dbus acquire_svc;
allow dnsmasq_t self:dbus send_msg;
allow sysadm_t dnsmasq_t:dbus send_msg;
allow dnsmasq_t sysadm_t:dbus send_msg;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;em&gt;only&lt;/em&gt; the access for acquiring a service based on a name
(i.e. owning a service) is checked based on the different label. Sending
and receiving messages is still handled by the domains of the processes
(actually the labels of the connections, but these are always the
process domains).&lt;/p&gt;
&lt;p&gt;I am not aware of any policy implementation that uses a different label
for owning services, and the implementation is more suited to "force"
D-Bus to only allow services with a correct label. This ensures that
other domains that might have enough privileges to interact with D-Bus
and own a service cannot own these particular services. After all, other
services don't usually have the privileges (policy-wise) to
&lt;em&gt;acquire_svc&lt;/em&gt; a service with a different label than their own label.&lt;/p&gt;</content><category term="SELinux"></category><category term="busconfig"></category><category term="d-bus"></category><category term="dbus"></category><category term="linux"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>D-Bus, quick recap</title><link href="https://blog.siphos.be/2014/06/d-bus-quick-recap/" rel="alternate"></link><published>2014-06-29T19:16:00+02:00</published><updated>2014-06-29T19:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-06-29:/2014/06/d-bus-quick-recap/</id><summary type="html">&lt;p&gt;I've never fully investigated the what and how of D-Bus. I know it is
some sort of IPC, but higher level than the POSIX IPC methods. After
some reading, I think I start to understand how it works and how
administrators can work with it. So a quick write-down is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've never fully investigated the what and how of D-Bus. I know it is
some sort of IPC, but higher level than the POSIX IPC methods. After
some reading, I think I start to understand how it works and how
administrators can work with it. So a quick write-down is in place so I
don't forget in the future.&lt;/p&gt;
&lt;p&gt;There is one &lt;em&gt;system&lt;/em&gt; bus and, for each X session of a user, also a
&lt;em&gt;session&lt;/em&gt; bus.&lt;/p&gt;
&lt;p&gt;A bus is governed by a &lt;code&gt;dbus-daemon&lt;/code&gt; process. A bus itself has objects
on it, which are represented through path-like constructs (like
&lt;code&gt;/org/freedesktop/ConsoleKit&lt;/code&gt;). These objects are provided by a service
(application). Applications "own" such services, and identify these
through a namespace-like value (such as &lt;code&gt;org.freedesktop.ConsoleKit&lt;/code&gt;).&lt;br&gt;
Applications can send signals to the bus, or messages through methods
exposed by the service. If methods are invoked (i.e. messages send) then
the application must specify the interface (such as
&lt;code&gt;org.freedesktop.ConsoleKit.Manager.Stop&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Administrators can monitor the bus through &lt;strong&gt;dbus-monitor&lt;/strong&gt;, or send
messages through &lt;strong&gt;dbus-send&lt;/strong&gt;. For instance, the following command
invokes the &lt;code&gt;org.freedesktop.ConsoleKit.Manager.Stop&lt;/code&gt; method provided by
the object at &lt;code&gt;/org/freedesktop/ConsoleKit&lt;/code&gt; owned by the
service/application at &lt;code&gt;org.freedesktop.ConsoleKit&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ dbus-send --system --print-reply 
  --dest=org.freedesktop.ConsoleKit 
  /org/freedesktop/ConsoleKit/Manager 
  org.freedesktop.ConsoleKit.Manager.Stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What I found most interesting however was to query the busses. You can
do this with &lt;strong&gt;dbus-send&lt;/strong&gt; although it is much easier to use tools such
as &lt;strong&gt;d-feet&lt;/strong&gt; or &lt;strong&gt;qdbus&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To list current services on the system bus:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system
:1.1
 org.freedesktop.ConsoleKit
:1.10
:1.2
:1.3
 org.freedesktop.PolicyKit1
:1.36
 fi.epitest.hostap.WPASupplicant
 fi.w1.wpa_supplicant1
:1.4
:1.42
:1.5
:1.6
:1.7
 org.freedesktop.UPower
:1.8
:1.9
org.freedesktop.DBus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The numbers are generated by D-Bus itself, the namespace-like strings
are taken by the objects. To see what is provided by a particular
service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system org.freedesktop.PolicyKit1
/
/org
/org/freedesktop
/org/freedesktop/PolicyKit1
/org/freedesktop/PolicyKit1/Authority
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The methods made available through one of these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system org.freedesktop.PolicyKit1 /org/freedesktop/PolicyKit1/Authority
method QDBusVariant org.freedesktop.DBus.Properties.Get(QString interface_name, QString property_name)
method QVariantMap org.freedesktop.DBus.Properties.GetAll(QString interface_name)
...
property read uint org.freedesktop.PolicyKit1.Authority.BackendFeatures
property read QString org.freedesktop.PolicyKit1.Authority.BackendName
property read QString org.freedesktop.PolicyKit1.Authority.BackendVersion
method void org.freedesktop.PolicyKit1.Authority.AuthenticationAgentResponse(QString cookie, QDBusRawType::(sa{sv} identity)
method void org.freedesktop.PolicyKit1.Authority.CancelCheckAuthorization(QString cancellation_id)
signal void org.freedesktop.PolicyKit1.Authority.Changed()
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Access to methods and interfaces is governed through XML files in
&lt;code&gt;/etc/dbus-1/system.d&lt;/code&gt; (or &lt;code&gt;session.d&lt;/code&gt; depending on the bus). Let's look
at &lt;code&gt;/etc/dbus-1/system.d/dnsmasq.conf&lt;/code&gt; as an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;busconfig&amp;gt;
        &amp;lt;policy user=&amp;quot;root&amp;quot;&amp;gt;
                &amp;lt;allow own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;allow send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
        &amp;lt;policy user=&amp;quot;dnsmasq&amp;quot;&amp;gt;
                &amp;lt;allow own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;allow send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
        &amp;lt;policy context=&amp;quot;default&amp;quot;&amp;gt;
                &amp;lt;deny own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;deny send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
&amp;lt;/busconfig&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The configuration mentions that only the root Linux user can 'assign' a
service/application to the &lt;code&gt;uk.org.thekelleys.dnsmasq&lt;/code&gt; name, and root
can send messages to this same service/application name. The default is
that no-one can own and send to this service/application name. As a
result, only the Linux root user can interact with this object.&lt;/p&gt;
&lt;p&gt;D-Bus also supports starting of services when a method is invoked
(instead of running this service immediately). This is configured
through &lt;code&gt;*.service&lt;/code&gt; files inside &lt;code&gt;/usr/share/dbus-1/system-services/&lt;/code&gt;.&lt;/p&gt;</content><category term="Free-Software"></category><category term="dbus"></category><category term="linux"></category></entry><entry><title>Chroots for SELinux enabled applications</title><link href="https://blog.siphos.be/2014/06/chroots-for-selinux-enabled-applications/" rel="alternate"></link><published>2014-06-22T20:16:00+02:00</published><updated>2014-06-22T20:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-06-22:/2014/06/chroots-for-selinux-enabled-applications/</id><summary type="html">&lt;p&gt;Today I had to prepare a chroot jail (thank you grsecurity for the neat
additional chroot protection features) for a SELinux-enabled
application. As a result, "just" making a chroot was insufficient: the
application needed access to &lt;code&gt;/sys/fs/selinux&lt;/code&gt;. Of course, granting
access to &lt;code&gt;/sys&lt;/code&gt; is not something I like …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I had to prepare a chroot jail (thank you grsecurity for the neat
additional chroot protection features) for a SELinux-enabled
application. As a result, "just" making a chroot was insufficient: the
application needed access to &lt;code&gt;/sys/fs/selinux&lt;/code&gt;. Of course, granting
access to &lt;code&gt;/sys&lt;/code&gt; is not something I like to see for a chroot jail.&lt;/p&gt;
&lt;p&gt;Luckily, all other accesses are not needed, so I was able to create a
static &lt;code&gt;/sys/fs/selinux&lt;/code&gt; directory structure in the chroot, and then
just mount the SELinux file system on that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -t selinuxfs none /var/chroot/sys/fs/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In hindsight, I probably could just have created a &lt;code&gt;/selinux&lt;/code&gt; location
as that location, although deprecated, is still checked by the SELinux
libraries.&lt;/p&gt;
&lt;p&gt;Anyway, there was a second requirement: access to &lt;code&gt;/etc/selinux&lt;/code&gt;.
Luckily it was purely for read operations, so I was first contemplating
of copying the data and doing a &lt;strong&gt;chmod -R a-w
/var/chroot/etc/selinux&lt;/strong&gt;, but then considered a bind-mount:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -o bind,ro /etc/selinux /var/chroot/etc/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Alas, bad luck - the read-only flag is ignored during the mount, and the
bind-mount is still read-write. A &lt;a href="http://lwn.net/Articles/281157/"&gt;simple article on
lwn.net&lt;/a&gt; informed me about the
solution: I need to do a remount afterwards to enable the read-only
state:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -o remount,ro /var/chroot/etc/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great! And because my brain isn't what it used to be, I just make a
quick blog for future reference ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="bind-mount"></category><category term="bindmount"></category><category term="mount"></category><category term="read-only"></category><category term="ro"></category><category term="selinux"></category></entry><entry><title>Gentoo Hardened, June 2014</title><link href="https://blog.siphos.be/2014/06/gentoo-hardened-june-2014/" rel="alternate"></link><published>2014-06-15T21:28:00+02:00</published><updated>2014-06-15T21:28:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-06-15:/2014/06/gentoo-hardened-june-2014/</id><summary type="html">&lt;p&gt;Friday the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project had its
monthly online meeting to talk about the progress within the various
tools, responsibilities and subprojects.&lt;/p&gt;
&lt;p&gt;On the &lt;strong&gt;toolchain&lt;/strong&gt; part, Zorry mentioned that GCC 4.9 and 4.8.3 will
have SSP enabled by default. The hardened profiles will still have a
different …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Friday the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project had its
monthly online meeting to talk about the progress within the various
tools, responsibilities and subprojects.&lt;/p&gt;
&lt;p&gt;On the &lt;strong&gt;toolchain&lt;/strong&gt; part, Zorry mentioned that GCC 4.9 and 4.8.3 will
have SSP enabled by default. The hardened profiles will still have a
different SSP setting than the default (so yes, there will still be
differences between the two) but this will help in securing the Gentoo
default installations.&lt;/p&gt;
&lt;p&gt;Zorry is also working on upstreaming the PIE patches for GCC 4.10.&lt;/p&gt;
&lt;p&gt;Next to the regular toolchain, blueness also mentioned his intentions to
launch a &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened_musl"&gt;Hardened
musl&lt;/a&gt; subproject
which will focus on the musl C library (rather than glibc or uclibc) and
hardening.&lt;/p&gt;
&lt;p&gt;On the &lt;strong&gt;kernel&lt;/strong&gt; side, two recent kernel vulnerabilities in the vanilla
kernel Linux (pty race and privilege escalation through futex code)
painted the discussions on IRC recently. Some versions of the hardened
kernels are still available in the tree, but the more recent
(non-vulnerable) kernels have proven not to be as stable as we'd hoped.&lt;/p&gt;
&lt;p&gt;The pty race vulnerability is possibly not applicable to hardened
kernels thanks to grSecurity, due to its protection to access the kernel
symbols.&lt;/p&gt;
&lt;p&gt;The latest kernels should not be used with KSTACKOVERFLOW on production
systems though; there are some issues reported with virtio network
interface support (on the guests) and ZFS.&lt;/p&gt;
&lt;p&gt;Also, on the &lt;strong&gt;Pax&lt;/strong&gt; support, the &lt;code&gt;install-xattr&lt;/code&gt; saga continues. The
new wrapper that blueness worked in dismissed some code to keep the
&lt;code&gt;PWD&lt;/code&gt; so the &lt;code&gt;$S&lt;/code&gt; directory knowledge was "lost". This is now fixed. All
that is left is to have the wrapper included and stabilized.&lt;/p&gt;
&lt;p&gt;On &lt;strong&gt;SELinux&lt;/strong&gt; side, it was the usual set of progress. Policy
stabilization and user land application and library stabilization. The
latter is waiting a bit because of the multilib support that's now being
integrated in the ebuilds as well (and thus has a larger set of
dependencies to go through) but no show-stoppers there. Also, the
&lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;SELinux documentation portal&lt;/a&gt; on
the wiki was briefly mentioned.&lt;/p&gt;
&lt;p&gt;Also, the &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-3215"&gt;policycoreutils
vulnerability&lt;/a&gt;
has been &lt;a href="http://blog.siphos.be/2014/05/dropping-sesandbox-support/"&gt;worked
around&lt;/a&gt; so it
is no longer applicable to us.&lt;/p&gt;
&lt;p&gt;On the hardened &lt;strong&gt;profiles&lt;/strong&gt;, we had a nice discussion on enabling
capabilities support (and move towards capabilities instead of setuid
binaries), which klondike will try to tackle during the summer holidays.&lt;/p&gt;
&lt;p&gt;As I didn't take notes during the meeting, this post might miss a few
(and I forgot to enable logging as well) but as Zorry sends out the
meeting logs anyway later, you can read up there ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category></entry><entry><title>Visualizing constraints</title><link href="https://blog.siphos.be/2014/05/visualizing-constraints/" rel="alternate"></link><published>2014-05-31T03:47:00+02:00</published><updated>2014-05-31T03:47:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-05-31:/2014/05/visualizing-constraints/</id><summary type="html">&lt;p&gt;SELinux constraints are an interesting way to implement specific, well,
constraints on what SELinux allows. Most SELinux rules that users come
in contact with are purely type oriented: allow something to do
something against something. In fact, most of the SELinux rules applied
on a system are such &lt;code&gt;allow&lt;/code&gt; rules …&lt;/p&gt;</summary><content type="html">&lt;p&gt;SELinux constraints are an interesting way to implement specific, well,
constraints on what SELinux allows. Most SELinux rules that users come
in contact with are purely type oriented: allow something to do
something against something. In fact, most of the SELinux rules applied
on a system are such &lt;code&gt;allow&lt;/code&gt; rules.&lt;/p&gt;
&lt;p&gt;The restriction of such &lt;code&gt;allow&lt;/code&gt; rules is that they only take into
consideration the &lt;em&gt;type&lt;/em&gt; of the contexts that participate. This is the
&lt;a href="https://wiki.gentoo.org/wiki/SELinux/Type_enforcement"&gt;type
enforcement&lt;/a&gt; part
of the SELinux mandatory access control system.&lt;/p&gt;
&lt;p&gt;Constraints on the other hand work on the user, role and type part of a
context. Consider this piece of constraint code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;constrain file all_file_perms (
  u1 == u2
  or u1 == system_u
  or u2 == system_u
  or t1 != ubac_constrained_type
  or t2 != ubac_constrained_type
);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This particular constraint definition tells the SELinux subsystem that,
when an operation against a &lt;em&gt;file&lt;/em&gt; class is performed (any operation, as
&lt;em&gt;all_file_perms&lt;/em&gt; is used, but individual, specific permissions can be
listed as well), this is denied if none of the following conditions are
met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SELinux user of the subject and object are the same&lt;/li&gt;
&lt;li&gt;The SELinux user of the subject or object is &lt;code&gt;system_u&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The SELinux type of the subject does not have the
    &lt;code&gt;ubac_constrained_type&lt;/code&gt; attribute set&lt;/li&gt;
&lt;li&gt;The SELinux type of the object does not have the
    &lt;code&gt;ubac_constrained_type&lt;/code&gt; attribute set&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If none of the conditions are met, then the action is denied, regardless
of the &lt;code&gt;allow&lt;/code&gt; rules set otherwise. If at least one condition is met,
then the &lt;code&gt;allow&lt;/code&gt; rules (and other SELinux rules) decide if an action can
be taken or not.&lt;/p&gt;
&lt;p&gt;Constraints are currently difficult to query though. There is &lt;strong&gt;seinfo
--constrain&lt;/strong&gt; which gives all constraints, using the Reverse Polish
Notation - not something easily readable by users:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ seinfo --constrain
constrain { sem } { create destroy getattr setattr read write associate unix_read unix_write  } 
(  u1 u2 ==  u1 system_u ==  ||  u2 system_u ==  ||  t1 { screen_var_run_t gnome_xdg_config_home_t admin_crontab_t 
links_input_xevent_t gpg_pinentry_tmp_t virt_content_t print_spool_t crontab_tmp_t httpd_user_htaccess_t ssh_keysign_t 
remote_input_xevent_t gnome_home_t mozilla_tmpfs_t staff_gkeyringd_t consolekit_input_xevent_t user_mail_tmp_t 
chromium_xdg_config_t mozilla_input_xevent_t chromium_tmp_t httpd_user_script_exec_t gnome_keyring_tmp_t links_tmpfs_t 
skype_tmp_t user_gkeyringd_t svirt_home_t sysadm_su_t virt_home_t skype_home_t wireshark_tmp_t xscreensaver_xproperty_t 
consolekit_xproperty_t user_home_dir_t gpg_pinentry_xproperty_t mplayer_home_t mozilla_plugin_input_xevent_t mozilla_plugin_tmp_t 
mozilla_xproperty_t xdm_input_xevent_t chromium_input_xevent_t java_tmpfs_t googletalk_plugin_xproperty_t sysadm_t gorg_t gpg_t 
java_t links_t staff_dbusd_t httpd_user_ra_content_t httpd_user_rw_content_t googletalk_plugin_tmp_t gpg_agent_tmp_t 
ssh_agent_tmp_t sysadm_ssh_agent_t user_fonts_cache_t user_tmp_t googletalk_plugin_input_xevent_t user_dbusd_t xserver_tmpfs_t 
iceauth_home_t qemu_input_xevent_t xauth_home_t mutt_home_t sysadm_dbusd_t remote_xproperty_t gnome_xdg_config_t screen_home_t 
chromium_xproperty_t chromium_tmpfs_t wireshark_tmpfs_t xdg_videos_home_t pulseaudio_input_xevent_t krb5_home_t 
pulseaudio_xproperty_t xscreensaver_input_xevent_t gpg_pinentry_input_xevent_t httpd_user_script_t gnome_xdg_cache_home_t 
mozilla_plugin_tmpfs_t user_home_t user_sudo_t ssh_input_xevent_t ssh_tmpfs_t xdg_music_home_t gconf_tmp_t flash_home_t 
java_home_t skype_tmpfs_t xdg_pictures_home_t xdg_data_home_t gnome_keyring_home_t wireshark_home_t chromium_renderer_xproperty_t 
gpg_pinentry_t mozilla_t session_dbusd_tmp_t staff_sudo_t xdg_config_home_t user_su_t pan_input_xevent_t user_devpts_t 
mysqld_home_t pan_tmpfs_t root_input_xevent_t links_home_t sysadm_screen_t pulseaudio_tmpfs_t sysadm_gkeyringd_t mail_home_rw_t 
gconf_home_t mozilla_plugin_xproperty_t mutt_tmp_t httpd_user_content_t mozilla_xdg_cache_t mozilla_home_t alsa_home_t 
pulseaudio_t mencoder_t admin_crontab_tmp_t xdg_documents_home_t user_tty_device_t java_tmp_t gnome_xdg_data_home_t wireshark_t 
mozilla_plugin_home_t googletalk_plugin_tmpfs_t user_cron_spool_t mplayer_input_xevent_t skype_input_xevent_t xxe_home_t 
mozilla_tmp_t gconfd_t lpr_t mutt_t pan_t ssh_t staff_t user_t xauth_t skype_xproperty_t mozilla_plugin_config_t 
links_xproperty_t mplayer_xproperty_t xdg_runtime_home_t cert_home_t mplayer_tmpfs_t user_fonts_t user_tmpfs_t mutt_conf_t 
gpg_secret_t gpg_helper_t staff_ssh_agent_t pulseaudio_tmp_t xscreensaver_t googletalk_plugin_xdg_config_t staff_screen_t 
user_fonts_config_t ssh_home_t staff_su_t screen_tmp_t mozilla_plugin_t user_input_xevent_t xserver_tmp_t wireshark_xproperty_t 
user_mail_t pulseaudio_home_t xdg_cache_home_t user_ssh_agent_t xdg_downloads_home_t chromium_renderer_input_xevent_t cronjob_t 
crontab_t pan_home_t session_dbusd_home_t gpg_agent_t xauth_tmp_t xscreensaver_tmpfs_t iceauth_t mplayer_t chromium_xdg_cache_t 
lpr_tmp_t gpg_pinentry_tmpfs_t pan_xproperty_t ssh_xproperty_t xdm_xproperty_t java_xproperty_t sysadm_sudo_t qemu_xproperty_t 
root_xproperty_t user_xproperty_t mail_home_t xserver_t java_input_xevent_t user_screen_t wireshark_input_xevent_t } !=  ||  t2 { 
screen_var_run_t gnome_xdg_config_home_t admin_crontab_t links_input_xevent_t gpg_pinentry_tmp_t virt_content_t print_spool_t 
crontab_tmp_t httpd_user_htaccess_t ssh_keysign_t remote_input_xevent_t gnome_home_t mozilla_tmpfs_t staff_gkeyringd_t 
consolekit_input_xevent_t user_mail_tmp_t chromium_xdg_config_t mozilla_input_xevent_t chromium_tmp_t httpd_user_script_exec_t 
gnome_keyring_tmp_t links_tmpfs_t skype_tmp_t user_gkeyringd_t svirt_home_t sysadm_su_t virt_home_t skype_home_t wireshark_tmp_t 
xscreensaver_xproperty_t consolekit_xproperty_t user_home_dir_t gpg_pinentry_xproperty_t mplayer_home_t 
mozilla_plugin_input_xevent_t mozilla_plugin_tmp_t mozilla_xproperty_t xdm_input_xevent_t chromium_input_xevent_t java_tmpfs_t 
googletalk_plugin_xproperty_t sysadm_t gorg_t gpg_t java_t links_t staff_dbusd_t httpd_user_ra_content_t httpd_user_rw_content_t 
googletalk_plugin_tmp_t gpg_agent_tmp_t ssh_agent_tmp_t sysadm_ssh_agent_t user_fonts_cache_t user_tmp_t 
googletalk_plugin_input_xevent_t user_dbusd_t xserver_tmpfs_t iceauth_home_t qemu_input_xevent_t xauth_home_t mutt_home_t 
sysadm_dbusd_t remote_xproperty_t gnome_xdg_config_t screen_home_t chromium_xproperty_t chromium_tmpfs_t wireshark_tmpfs_t 
xdg_videos_home_t pulseaudio_input_xevent_t krb5_home_t pulseaudio_xproperty_t xscreensaver_input_xevent_t 
gpg_pinentry_input_xevent_t httpd_user_script_t gnome_xdg_cache_home_t mozilla_plugin_tmpfs_t user_home_t user_sudo_t 
ssh_input_xevent_t ssh_tmpfs_t xdg_music_home_t gconf_tmp_t flash_home_t java_home_t skype_tmpfs_t xdg_pictures_home_t 
xdg_data_home_t gnome_keyring_home_t wireshark_home_t chromium_renderer_xproperty_t gpg_pinentry_t mozilla_t session_dbusd_tmp_t 
staff_sudo_t xdg_config_home_t user_su_t pan_input_xevent_t user_devpts_t mysqld_home_t pan_tmpfs_t root_input_xevent_t 
links_home_t sysadm_screen_t pulseaudio_tmpfs_t sysadm_gkeyringd_t mail_home_rw_t gconf_home_t mozilla_plugin_xproperty_t 
mutt_tmp_t httpd_user_content_t mozilla_xdg_cache_t mozilla_home_t alsa_home_t pulseaudio_t mencoder_t admin_crontab_tmp_t 
xdg_documents_home_t user_tty_device_t java_tmp_t gnome_xdg_data_home_t wireshark_t mozilla_plugin_home_t 
googletalk_plugin_tmpfs_t user_cron_spool_t mplayer_input_xevent_t skype_input_xevent_t xxe_home_t mozilla_tmp_t gconfd_t lpr_t 
mutt_t pan_t ssh_t staff_t user_t xauth_t skype_xproperty_t mozilla_plugin_config_t links_xproperty_t mplayer_xproperty_t 
xdg_runtime_home_t cert_home_t mplayer_tmpfs_t user_fonts_t user_tmpfs_t mutt_conf_t gpg_secret_t gpg_helper_t staff_ssh_agent_t 
pulseaudio_tmp_t xscreensaver_t googletalk_plugin_xdg_config_t staff_screen_t user_fonts_config_t ssh_home_t staff_su_t 
screen_tmp_t mozilla_plugin_t user_input_xevent_t xserver_tmp_t wireshark_xproperty_t user_mail_t pulseaudio_home_t 
xdg_cache_home_t user_ssh_agent_t xdg_downloads_home_t chromium_renderer_input_xevent_t cronjob_t crontab_t pan_home_t 
session_dbusd_home_t gpg_agent_t xauth_tmp_t xscreensaver_tmpfs_t iceauth_t mplayer_t chromium_xdg_cache_t lpr_tmp_t 
gpg_pinentry_tmpfs_t pan_xproperty_t ssh_xproperty_t xdm_xproperty_t java_xproperty_t sysadm_sudo_t qemu_xproperty_t 
root_xproperty_t user_xproperty_t mail_home_t xserver_t java_input_xevent_t user_screen_t wireshark_input_xevent_t } !=  ||  t1 
 ==  || );
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There RPN notation however isn't the only reason why constraints are
difficult to read. The other reason is that &lt;strong&gt;seinfo&lt;/strong&gt; does not know
(anymore) about the attributes used to generate the constraints. As a
result, we get a huge list of all possible types that match a common
attribute - but we don't know which anymore.&lt;/p&gt;
&lt;p&gt;Not everyone can read the source files in which the constraints are
defined, so I hacked together a script that generates
&lt;a href="http://graphviz.org/"&gt;GraphViz&lt;/a&gt; dot file based on the &lt;strong&gt;seinfo
--constrain&lt;/strong&gt; output for a given &lt;em&gt;class&lt;/em&gt; and &lt;em&gt;permission&lt;/em&gt; and,
optionally, limiting the huge list of types to a set that the user (err,
that is me ;-) is interested in.&lt;/p&gt;
&lt;p&gt;For instance, to generate a graph of the constraints related to file
reads, limited to the &lt;code&gt;user_t&lt;/code&gt; and &lt;code&gt;staff_t&lt;/code&gt; types if huge lists would
otherwise be shown:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ seshowconstraint file read &amp;quot;user_t staff_t&amp;quot; &amp;gt; constraint-file.dot
~$ dot -Tsvg -O constraint-file.dot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This generates the following graph:&lt;/p&gt;
&lt;p&gt;!SELinux constraint flow&lt;a href="http://dev.gentoo.org/~swift/blog/201405/constraint-flow.svg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in the (ugly) script that does this, you can find
it on my
&lt;a href="https://github.com/sjvermeu/small.coding/blob/master/se_scripts/seshowconstraint"&gt;github&lt;/a&gt;
location.&lt;/p&gt;
&lt;p&gt;There are some patches laying around to support naming constraints and
taking the name up in the policy, so that denials based on constraints
can at least give feedback to the user which constraint is holding an
access back (rather than just a denial that the user doesn't know why).
Hopefully such patches can be made available in the kernel and user
space utilities soon.&lt;/p&gt;</content><category term="SELinux"></category><category term="constrain"></category><category term="constraints"></category><category term="dot"></category><category term="graphviz"></category><category term="seinfo"></category><category term="selinux"></category></entry><entry><title>Revamped our SELinux documentation</title><link href="https://blog.siphos.be/2014/05/revamped-our-selinux-documentation/" rel="alternate"></link><published>2014-05-12T22:15:00+02:00</published><updated>2014-05-12T22:15:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-05-12:/2014/05/revamped-our-selinux-documentation/</id><summary type="html">&lt;p&gt;In the move to the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo wiki&lt;/a&gt;, I have
updated and revamped most of our SELinux documentation. The end result
can be seen through the &lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;main SELinux
page&lt;/a&gt;. Most of the content is
below this page (as subpages).&lt;/p&gt;
&lt;p&gt;We start with a new &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Quick_introduction"&gt;introduction to
SELinux&lt;/a&gt;
article which goes over …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the move to the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo wiki&lt;/a&gt;, I have
updated and revamped most of our SELinux documentation. The end result
can be seen through the &lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;main SELinux
page&lt;/a&gt;. Most of the content is
below this page (as subpages).&lt;/p&gt;
&lt;p&gt;We start with a new &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Quick_introduction"&gt;introduction to
SELinux&lt;/a&gt;
article which goes over a large set of SELinux' features and concepts.
Next, we cover the various concepts within SELinux. This is mostly
SELinux features but explained more in-depth. Then we go on to the user
guides. We start of course with the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Installation"&gt;installation of SELinux on
Gentoo&lt;/a&gt; and then
cover the remainder of administrative topics within SELinux (user
management, handling AVC denials, label management, booleans, etc.)&lt;/p&gt;
&lt;p&gt;The above is most likely sufficient for the majority of SELinux users. A
few more expert-specific documents are provided as well (some of them
still work in progress, but I didn't want to wait to get some feedback)
and there is also a section specific for (Gentoo) developers.&lt;/p&gt;
&lt;p&gt;Give it a review and tell me what you think.&lt;/p&gt;</content><category term="Gentoo"></category><category term="documentation"></category><category term="Gentoo"></category><category term="selinux"></category><category term="wiki"></category></entry><entry><title>Dropping sesandbox support</title><link href="https://blog.siphos.be/2014/05/dropping-sesandbox-support/" rel="alternate"></link><published>2014-05-09T21:03:00+02:00</published><updated>2014-05-09T21:03:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-05-09:/2014/05/dropping-sesandbox-support/</id><summary type="html">&lt;p&gt;A &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-3215"&gt;vulnerability in
seunshare&lt;/a&gt;,
part of &lt;code&gt;policycoreutils&lt;/code&gt;, came to light recently (through &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=509896"&gt;bug
509896&lt;/a&gt;). The issue is
within &lt;code&gt;libcap-ng&lt;/code&gt; actually, but the specific situation in which the
vulnerability can be exploited is only available in &lt;code&gt;seunshare&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, &lt;code&gt;seunshare&lt;/code&gt; is not built by default on Gentoo. You need to define
&lt;code&gt;USE …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-3215"&gt;vulnerability in
seunshare&lt;/a&gt;,
part of &lt;code&gt;policycoreutils&lt;/code&gt;, came to light recently (through &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=509896"&gt;bug
509896&lt;/a&gt;). The issue is
within &lt;code&gt;libcap-ng&lt;/code&gt; actually, but the specific situation in which the
vulnerability can be exploited is only available in &lt;code&gt;seunshare&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, &lt;code&gt;seunshare&lt;/code&gt; is not built by default on Gentoo. You need to define
&lt;code&gt;USE="sesandbox"&lt;/code&gt;, which I implemented as an optional build because I
see no need for the &lt;code&gt;seunshare&lt;/code&gt; command and the &lt;em&gt;SELinux sandbox
(sesandbox)&lt;/em&gt; support. Upstream (Fedora/RedHat) calls it &lt;em&gt;sandbox&lt;/em&gt;, which
Gentoo translates to &lt;em&gt;sesandbox&lt;/em&gt; as it collides with the Gentoo sandbox
support otherwise. But I digress.&lt;/p&gt;
&lt;p&gt;The build of the SELinux sandbox support is optional, mostly because we
do not have a direct reason to support it. There are no Gentoo users
that I'm aware of that use it. It is used to start an application in a
chroot-like environment, based on Linux namespaces and a specific
SELinux policy called &lt;code&gt;sandbox_t&lt;/code&gt;. The idea isn't that bad, but I rather
focus on proper application confinement and full system enforcement
support (rather than specific services). The SELinux sandbox makes a bit
more sense when the system supports unconfined domains (and users are in
the &lt;code&gt;unconfined_t&lt;/code&gt; domain), but Gentoo focuses on strict policy support.&lt;/p&gt;
&lt;p&gt;Anyway, this isn't the first vulnerability in &lt;code&gt;seunshare&lt;/code&gt;. In 2011,
another privilege escalation vulnerability was found in the application
(see &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=374897"&gt;bug 374897&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But having a vulnerability in the application (or its interaction with
&lt;code&gt;libcap-ng&lt;/code&gt;) doesn't mean an exploitable vulnerability. Most users will
not even have &lt;code&gt;seunshare&lt;/code&gt;, and those that do have it will not be able to
call it if you are running with SELinux in &lt;em&gt;strict&lt;/em&gt; or have
&lt;code&gt;USE="-unconfined"&lt;/code&gt; set for the other policies. If &lt;code&gt;USE="unconfined"&lt;/code&gt; is
set and you run &lt;em&gt;mcs&lt;/em&gt;, &lt;em&gt;targeted&lt;/em&gt; or &lt;em&gt;mls&lt;/em&gt; (which isn't default either,
the default is &lt;em&gt;strict&lt;/em&gt;) then if your users are still mapped to the
regular user domains (&lt;code&gt;user_t&lt;/code&gt;, &lt;code&gt;staff_t&lt;/code&gt; or even &lt;code&gt;sysadm_t&lt;/code&gt;) then
&lt;code&gt;seunshare&lt;/code&gt; doesn't work as the SELinux policy prevents its behavior
before the vulnerability is triggered.&lt;/p&gt;
&lt;p&gt;Assuming you &lt;em&gt;do&lt;/em&gt; have a &lt;em&gt;targeted&lt;/em&gt; policy with users mapped to
&lt;code&gt;unconfined_t&lt;/code&gt; and you have built &lt;code&gt;policycoreutils&lt;/code&gt; with
&lt;code&gt;USE="sesandbox"&lt;/code&gt; or you run in SELinux in permissive mode, then please
tell me if you can trigger the exploit. On my systems, &lt;code&gt;seunshare&lt;/code&gt; fails
with the message that it can't drop its privileges and thus exits
(instead of executing the exploit code as it suggested by the reports).&lt;/p&gt;
&lt;p&gt;Since I mentioned that most user don't use SELinux sandbox, and because
I can't even get it to work (regardless of the vulnerability), I decided
to drop support for it from the builds. That also allows me to more
quickly introduce the new userspace utilities as I don't need to
refactor the code to switch from &lt;code&gt;sandbox&lt;/code&gt; to &lt;code&gt;sesandbox&lt;/code&gt; anymore.&lt;/p&gt;
&lt;p&gt;So, &lt;code&gt;policycoreutils-2.2.5-r4&lt;/code&gt; and &lt;code&gt;policycoreutils-2.3_rc1-r1&lt;/code&gt; are now
available which do not build &lt;code&gt;seunshare&lt;/code&gt; anymore. And now I can focus on
providing the full &lt;em&gt;2.3&lt;/em&gt; userspace that has been announced today.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="policycoreutils"></category><category term="selinux"></category><category term="seunshare"></category><category term="vulnerability"></category></entry><entry><title>Stepping through the build process with ebuild</title><link href="https://blog.siphos.be/2014/04/stepping-through-the-build-process-with-ebuild/" rel="alternate"></link><published>2014-04-20T11:59:00+02:00</published><updated>2014-04-20T11:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-04-20:/2014/04/stepping-through-the-build-process-with-ebuild/</id><summary type="html">&lt;p&gt;Today I had to verify a patch that I pushed upstream but which was
slightly modified. As I don't use the tool myself (it was a
user-reported issue) I decided to quickly drum up a live ebuild for the
application and install it (as the patch was in the upstream …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I had to verify a patch that I pushed upstream but which was
slightly modified. As I don't use the tool myself (it was a
user-reported issue) I decided to quickly drum up a live ebuild for the
application and install it (as the patch was in the upstream repository
but not in a release yet). The patch is for
&lt;a href="http://fcron.free.fr/"&gt;fcron&lt;/a&gt;'s SELinux support, so the file I created
is
&lt;a href="https://github.com/sjvermeu/gentoo.overlay/tree/master/sys-process/fcron"&gt;fcron-9999.ebuild&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sadly, the build failed at the documentation generation (something about
"No targets to create en/HTML/index.html"). That's unfortunate, because
that means I'm not going to ask to push the live ebuild to the Portage
tree itself (yet). But as my primary focus is to validate the patch (and
not create a live ebuild) I want to ignore this error and go on. I don't
need the fcron documentation right now, so how about I just continue?&lt;/p&gt;
&lt;p&gt;To do so, I start using the &lt;strong&gt;ebuild&lt;/strong&gt; command. As the failure occurred
in the build phase (&lt;em&gt;compile&lt;/em&gt;) and at the end (documentation was the
last step), I tell Portage that it should assume the build has
completed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# touch /var/portage/portage/sys-process/fcron-9999/.compiled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I tell Portage to install the (built) files into the &lt;code&gt;images/&lt;/code&gt;
directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# ebuild /home/swift/dev/gentoo.overlay/sys-process/fcron/fcron-9999.ebuild install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The installation phase fails again (with the same error as during the
build, which is logical as the &lt;code&gt;Makefile&lt;/code&gt; can't install files that
haven't been properly build yet.) As documentation is the last step, I
tell Portage to assume the installation phase has completed as well,
continuing with the merging of the files to the life file system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# touch /var/portage/portage/sys-process/fcron-9999/.installed
~# ebuild /home/swift/dev/gentoo.overlay/sys-process/fcron/fcron-9999.ebuild qmerge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Et voila, &lt;code&gt;fcron-9999&lt;/code&gt; is now installed on the system, ready to validate
the patch I had to check.&lt;/p&gt;</content><category term="Gentoo"></category><category term="ebuild"></category><category term="phase"></category><category term="portage"></category></entry><entry><title>If things are weird, check for policy.29</title><link href="https://blog.siphos.be/2014/04/if-things-are-weird-check-for-policy-29/" rel="alternate"></link><published>2014-04-17T21:01:00+02:00</published><updated>2014-04-17T21:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-04-17:/2014/04/if-things-are-weird-check-for-policy-29/</id><summary type="html">&lt;p&gt;Today we analyzed a weird issue one of our SELinux users had with their
system. He had a denial when calling &lt;strong&gt;audit2allow&lt;/strong&gt;, informing us that
&lt;code&gt;sysadm_t&lt;/code&gt; had no rights to read the SELinux policy. This is a known
issue that has been resolved in our current SELinux policy repository
but …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today we analyzed a weird issue one of our SELinux users had with their
system. He had a denial when calling &lt;strong&gt;audit2allow&lt;/strong&gt;, informing us that
&lt;code&gt;sysadm_t&lt;/code&gt; had no rights to read the SELinux policy. This is a known
issue that has been resolved in our current SELinux policy repository
but which needs to be pushed to the tree (which is my job, sorry about
that). The problem however is when he added the policy - it didn't work.&lt;/p&gt;
&lt;p&gt;Even worse, &lt;strong&gt;sesearch&lt;/strong&gt; told us that the policy has been modified
correctly - but it still doesn't work. Check your policy with
&lt;strong&gt;sestatus&lt;/strong&gt; and &lt;strong&gt;seinfo&lt;/strong&gt; and they're all saying things are working
well. And yet ... things don't. Apparently, all policy changes are
ignored.&lt;/p&gt;
&lt;p&gt;The reason? There was a &lt;code&gt;policy.29&lt;/code&gt; file in &lt;code&gt;/etc/selinux/mcs/policy&lt;/code&gt;
which was always loaded, even though the user already edited
&lt;code&gt;/etc/selinux/semanage.conf&lt;/code&gt; to have &lt;code&gt;policy-version&lt;/code&gt; set to 28.&lt;/p&gt;
&lt;p&gt;It is already a problem that we need to tell users to edit
&lt;code&gt;semanage.conf&lt;/code&gt; to a fixed version (because binary version 29 is not
supported by most Linux kernels as it has been very recently introduced)
but having &lt;strong&gt;load_policy&lt;/strong&gt; (which is called by &lt;strong&gt;semodule&lt;/strong&gt; when a
policy needs to be loaded) loading a stale &lt;code&gt;policy.29&lt;/code&gt; file is just...
disappointing.&lt;/p&gt;
&lt;p&gt;Anyway - if you see weird behavior, check both the &lt;code&gt;semanage.conf&lt;/code&gt; file
(and set &lt;code&gt;policy-version = 28&lt;/code&gt;) as well as the contents of your
&lt;code&gt;/etc/selinux/*/policy&lt;/code&gt; directory. If you see any &lt;code&gt;policy.*&lt;/code&gt; that isn't
version 28, delete them.&lt;/p&gt;</content><category term="SELinux"></category><category term="load_policy"></category><category term="policy"></category><category term="selinux"></category><category term="semanage"></category></entry><entry><title>What is that net-pf-## thingie?</title><link href="https://blog.siphos.be/2014/04/what-is-that-net-pf-thingie/" rel="alternate"></link><published>2014-04-01T19:46:00+02:00</published><updated>2014-04-01T19:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-04-01:/2014/04/what-is-that-net-pf-thingie/</id><summary type="html">&lt;p&gt;When checking audit logs, you might come across applications that
request loading of a &lt;code&gt;net-pf-##&lt;/code&gt; module, with &lt;code&gt;##&lt;/code&gt; being an integer.
Having requests for &lt;code&gt;net-pf-10&lt;/code&gt; is a more known cause (enable IPv6) but
what about &lt;code&gt;net-pf-34&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;The answer can be found in &lt;code&gt;/usr/src/linux/include/linux/socket.h&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#define AF_ATMPVC …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;When checking audit logs, you might come across applications that
request loading of a &lt;code&gt;net-pf-##&lt;/code&gt; module, with &lt;code&gt;##&lt;/code&gt; being an integer.
Having requests for &lt;code&gt;net-pf-10&lt;/code&gt; is a more known cause (enable IPv6) but
what about &lt;code&gt;net-pf-34&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;The answer can be found in &lt;code&gt;/usr/src/linux/include/linux/socket.h&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#define AF_ATMPVC       8       /* ATM PVCs                     */
#define AF_X25          9       /* Reserved for X.25 project    */
#define AF_INET6        10      /* IP version 6                 */
#define AF_ROSE         11      /* Amateur Radio X.25 PLP       */
#define AF_DECnet       12      /* Reserved for DECnet project  */
...
#define AF_BLUETOOTH    31      /* Bluetooth sockets            */
#define AF_IUCV         32      /* IUCV sockets                 */
#define AF_RXRPC        33      /* RxRPC sockets                */
#define AF_ISDN         34      /* mISDN sockets                */
#define AF_PHONET       35      /* Phonet sockets               */
#define AF_IEEE802154   36      /* IEEE802154 sockets           */
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So next time you get such a weird module load request, check &lt;code&gt;socket.h&lt;/code&gt;
for more information.&lt;/p&gt;</content><category term="Free-Software"></category><category term="linux"></category><category term="module_request"></category><category term="net-pf"></category></entry><entry><title>Proof of concept for USE enabled policies</title><link href="https://blog.siphos.be/2014/03/proof-of-concept-for-use-enabled-policies/" rel="alternate"></link><published>2014-03-31T18:33:00+02:00</published><updated>2014-03-31T18:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-31:/2014/03/proof-of-concept-for-use-enabled-policies/</id><summary type="html">&lt;p&gt;&lt;em&gt;tl;dr:&lt;/em&gt; Some (&lt;code&gt;-9999&lt;/code&gt;) policy ebuilds now have &lt;code&gt;USE&lt;/code&gt; support for
building in (or leaving out) SELinux policy statements.&lt;/p&gt;
&lt;p&gt;One of the "problems" I have been facing since I took on the maintenance
of SELinux policies within Gentoo Hardened is the (seeming) inability to
make a "least privilege" policy that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;tl;dr:&lt;/em&gt; Some (&lt;code&gt;-9999&lt;/code&gt;) policy ebuilds now have &lt;code&gt;USE&lt;/code&gt; support for
building in (or leaving out) SELinux policy statements.&lt;/p&gt;
&lt;p&gt;One of the "problems" I have been facing since I took on the maintenance
of SELinux policies within Gentoo Hardened is the (seeming) inability to
make a "least privilege" policy that suits the flexibility that Gentoo
offers. As a quick recap: SELinux policies describe the "acceptable
behavior" of an application (well, domain to be exact), often known as
the "normalized behavior" in the security world. When an application
(which runs within a SELinux domain) wants to perform some action which
is not part of the policy, then this action is denied.&lt;/p&gt;
&lt;p&gt;Some applications can have very broad acceptable behavior. A web server
for instance might need to connect to a database, but that is not the
case if the web server only serves static information, or dynamic
information that doesn't need a database. To support this, SELinux has
&lt;em&gt;booleans&lt;/em&gt; through which optional policy statements can be enabled or
disabled. So far so good.&lt;/p&gt;
&lt;p&gt;Let's look at a second example: ALSA. When ALSA enabled applications
want to access the sound devices, they use IPC resources to
"collaborate" around the sound subsystem (semaphores and shared memory
to be exact). Semaphores inherit the type of the domain that first
created the semaphore (so if &lt;strong&gt;mplayer&lt;/strong&gt; creates it, then the semaphore
has the &lt;code&gt;mplayer_t&lt;/code&gt; context) whereas shared memory usually gets the
tmpfs-related type (&lt;code&gt;mplayer_tmpfs_t&lt;/code&gt;). When a second application wants
to access the sound device as well, it needs access to the semaphore and
shared memory. Assuming this second application is the browser, then
&lt;code&gt;mozilla_t&lt;/code&gt; needs access to semaphores by &lt;code&gt;mplayer_t&lt;/code&gt;. And the same for
&lt;code&gt;chromium_t&lt;/code&gt;. Or &lt;code&gt;java_t&lt;/code&gt; applications that are ALSA-enabled. And
&lt;code&gt;alsa_t&lt;/code&gt;. And all other applications that are ALSA enabled.&lt;/p&gt;
&lt;p&gt;In Gentoo, ALSA support can be made optional through &lt;code&gt;USE="alsa"&lt;/code&gt;. If a
user decides not to use ALSA, then it doesn't make sense to allow all
those domains access to each others' semaphores and shared memory. And
although SELinux booleans can help, this would mean that for each
application domain, something like the following policy would need to
be, optionally, allowed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# For the mplayer_t domain:
optional_policy(`
  tunable_policy(`use_alsa&amp;#39;,`
    mozilla_rw_semaphores(mplayer_t)
    mozilla_rw_shm(mplayer_t)
    mozilla_tmpfs_rw_files(mplayer_t)
  &amp;#39;)
&amp;#39;)

optional_policy(`
  tunable_policy(`use_alsa&amp;#39;,`
    chromium_rw_semaphores(mplayer_t)
    chromium_rw_shm(mplayer_t)
    chromium_tmpfs_rw_files(mplayer_t)
  &amp;#39;)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this for all domains that are ALSA-enabled. Every time a new
application is added that knows ALSA, the same code needs to be added to
all policies. And this only uses a single SELinux boolean (whereas
Gentoo supports &lt;code&gt;USE="alsa"&lt;/code&gt; on per-package level), although we can
create separate booleans for each domain if we want to. Not that that
will make it more manageable.&lt;/p&gt;
&lt;p&gt;One way of dealing with this would be to use attributes. Say we have a
policy like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;attribute alsadomain;
attribute alsatmpfsfile;

allow alsadomain alsadomain:sem rw_sem_perms;
allow alsadomain alsadomain:shm rw_shm_perms;
allow alsadomain alsatmpfsfile:file rw_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By assigning the attribute to the proper domains whenever ALSA support
is needed, we can toggle this more easily:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# In alsa.if
interface(`alsa_domain&amp;#39;,`
  gen_require(`
    attribute alsadomain;
    attribute alsatmpfsfile;
  &amp;#39;)
  typeattribute $1 alsadomain;
  typeattribute $2 alsatmpfsfile;
&amp;#39;)


# In mplayer.te
optional_policy(`
  tunable_policy(`use_alsa&amp;#39;,`
    alsa_domain(mplayer_t, mplayer_tmpfs_t)
  &amp;#39;)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That would solve the problem of needlessly adding more calls in a policy
for every ALSA application. And hey, we can probably live with either a
global boolean (&lt;code&gt;use_alsa&lt;/code&gt;) or per-domain one (&lt;code&gt;mplayer_use_alsa&lt;/code&gt;) and
toggle this according to our needs.&lt;/p&gt;
&lt;p&gt;Sadly, the above is not possible: one cannot define &lt;code&gt;typeattribute&lt;/code&gt;
assignments inside a &lt;code&gt;tunable_policy&lt;/code&gt; code: attributes are part of the
non-conditional part of a SELinux policy. The solution would be to
create build-time conditionals (rather than run-time):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ifdef(`use_alsa&amp;#39;,`
  optional_policy(`
    alsa_domain(mplayer_t, mplayer_tmpfs_t)
  &amp;#39;)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This does mean that &lt;code&gt;use_alsa&lt;/code&gt; has to be known when the policy is built.
For Gentoo, that's not that bad, as policies are part of separate
packages, like &lt;code&gt;sec-policy/selinux-mplayer&lt;/code&gt;. So what I now added was
USE-enabled build-time decisions that trigger this code. The
&lt;code&gt;selinux-mplayer&lt;/code&gt; package has &lt;code&gt;IUSE="alsa"&lt;/code&gt; which will enable, if set,
the &lt;code&gt;use_alsa&lt;/code&gt; build-time conditional.&lt;/p&gt;
&lt;p&gt;As a result, we now support a better, fine-grained privilege setting
inside the SELinux policy which is triggered through the proper USE
flags.&lt;/p&gt;
&lt;p&gt;Is this a perfect solution? No, but it is manageable and known to Gentoo
users. It isn't perfect, because it listens to the USE flag setting for
the &lt;code&gt;selinux-mplayer&lt;/code&gt; package (and of course globally set USE flags) but
doesn't "detect" that the firefox application (for which the policy is
meant) is or isn't built with &lt;code&gt;USE="alsa"&lt;/code&gt;. So users/administrators will
need to keep this in mind when using package-local USE flag definitions.&lt;/p&gt;
&lt;p&gt;Also, this will make it a bit more troublesome for myself to manage the
SELinux policy for Gentoo (as upstream will not use this setup, and as
such patches from upstream might need a few manual corrections before
they apply to our tree). However, I gladly take that up if it means my
system will have somewhat better confinement.&lt;/p&gt;</content><category term="Gentoo"></category><category term="alsa"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Decoding the hex-coded path information in AVC denials</title><link href="https://blog.siphos.be/2014/03/decoding-the-hex-coded-path-information-in-avc-denials/" rel="alternate"></link><published>2014-03-30T16:37:00+02:00</published><updated>2014-03-30T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-30:/2014/03/decoding-the-hex-coded-path-information-in-avc-denials/</id><summary type="html">&lt;p&gt;When investigating AVC denials, some denials show a path that isn't
human readable, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1396189189.734:1913): avc:  denied  { execute } for  pid=17955 comm=&amp;quot;emerge&amp;quot; path=2F7661722F666669737A69596157202864656C6574656429 dev=&amp;quot;dm-3&amp;quot; ino=1838 scontext=staff_u:sysadm_r:portage_t tcontext=staff_u:object_r:var_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To know what this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When investigating AVC denials, some denials show a path that isn't
human readable, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1396189189.734:1913): avc:  denied  { execute } for  pid=17955 comm=&amp;quot;emerge&amp;quot; path=2F7661722F666669737A69596157202864656C6574656429 dev=&amp;quot;dm-3&amp;quot; ino=1838 scontext=staff_u:sysadm_r:portage_t tcontext=staff_u:object_r:var_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To know what this file is (or actually was, because such encoded paths
mean that the file ~~wasn't accessible anymore at the time that the
denial was shown~~ contains a space), you need to hex-decode the value.
For instance, with python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ python -c &amp;quot;import base64; print(base64.b16decode(\&amp;quot;2F7661722F666669737A69596157202864656C6574656429\&amp;quot;));&amp;quot;;
b&amp;#39;/var/ffisziYaW (deleted)&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, &lt;code&gt;/var/ffisziYaW&lt;/code&gt; was the path of the file (note
that, as it starts with ffi, it is caused by libffi which I've blogged
about before). The reason that the file was deleted at the time the
denial was generated is because what libffi does is create a file, get
the file descriptor and unlink the file (so it is deleted and only the
(open) file handle allows for accessing it) before it wants to execute
it. As a result, the execution (which is denied) triggers a denial for
the file whose path is no longer valid (as it is now appended with
"&lt;code&gt;(deleted)&lt;/code&gt;").&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit 1:&lt;/em&gt; Thanks to IooNag who pointed me to the truth that it is due to
a space in the file name, not because it was deleted. Having the file
deleted makes the patch be appended with "&lt;code&gt;(deleted)&lt;/code&gt;" which contains a
space.&lt;/p&gt;</content><category term="SELinux"></category><category term="avc"></category><category term="decode"></category><category term="path"></category><category term="selinux"></category></entry><entry><title>Managing Inter-Process Communication (IPC)</title><link href="https://blog.siphos.be/2014/03/managing-inter-process-communication-ipc/" rel="alternate"></link><published>2014-03-30T12:50:00+02:00</published><updated>2014-03-30T12:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-30:/2014/03/managing-inter-process-communication-ipc/</id><summary type="html">&lt;p&gt;As a Linux administrator, you'll eventually need to concern you about
&lt;em&gt;Inter-Process Communication (IPC)&lt;/em&gt;. The IPC primitives that most POSIX
operating systems provide are semaphores, shared memory and message
queues. On Linux, the first utility that helps you with those primitives
is &lt;strong&gt;ipcs&lt;/strong&gt;. Let's start with semaphores first.&lt;/p&gt;
&lt;p&gt;Semaphores in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a Linux administrator, you'll eventually need to concern you about
&lt;em&gt;Inter-Process Communication (IPC)&lt;/em&gt;. The IPC primitives that most POSIX
operating systems provide are semaphores, shared memory and message
queues. On Linux, the first utility that helps you with those primitives
is &lt;strong&gt;ipcs&lt;/strong&gt;. Let's start with semaphores first.&lt;/p&gt;
&lt;p&gt;Semaphores in general are integer variables that have a positive value,
and are accessible by multiple processes (users/tasks/whatever). The
idea behind a semaphore is that it is used to streamline access to a
shared resource. For instance, a device' control channel might be used
by multiple applications, but only one application at a time is allowed
to put something on the channel. Through semaphores, applications check
the semaphore value. If it is zero, they wait. If it is higher, they
attempt decrement the semaphore. If it fails (because another
application in the mean time has decremented the semaphore) then the
application waits, otherwise it continues as it has successfully
decremented the semaphore. In effect, it acts as a sort-of lock towards
a common resource.&lt;/p&gt;
&lt;p&gt;An example you can come across is with ALSA. Some of the ALSA plugins
(such as dmix) use IPC semaphores to allow multiple ALSA applications to
connect to and use the sound subsystem. When an ALSA-enabled application
is using the sound system, you'll see that a semaphore is active:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -s
------ Semaphore Arrays --------
key        semid      owner      perms      nsems     
0x0056a4d5 32768      swift      660        1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;More information about a particular semaphore can be obtained using
&lt;strong&gt;ipcs -s -i SEMID&lt;/strong&gt; where &lt;code&gt;SEMID&lt;/code&gt; is the value in the &lt;em&gt;semid&lt;/em&gt; column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -s -i 32768
Semaphore Array semid=32768
uid=1001         gid=18  cuid=1001       cgid=100
mode=0660, access_perms=0660
nsems = 1
otime = Sun Mar 30 12:33:46 2014  
ctime = Sun Mar 30 12:33:38 2014  
semnum     value      ncount     zcount     pid       
0          0          0          0          32061
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As with all IPC resources, we have information about the owner of the
semaphore (&lt;code&gt;uid&lt;/code&gt; and &lt;code&gt;gid&lt;/code&gt;), the creator of the semaphore (&lt;code&gt;cuid&lt;/code&gt; and
&lt;code&gt;cgid&lt;/code&gt;) as well as its access mask, similar to the file access mask on
Linux systems (&lt;code&gt;mode&lt;/code&gt; and &lt;code&gt;access_perms&lt;/code&gt;). Specific to the IPC
semaphore, you can also notice the &lt;code&gt;nsems = 1&lt;/code&gt;. Unlike the general
semaphores, IPC semaphores are actually a wrapper around one or more
"real" semaphores. The &lt;code&gt;nsems&lt;/code&gt; variable shows how many "real" semaphores
are handled by the IPC semaphore.&lt;/p&gt;
&lt;p&gt;Another very popular IPC resource is &lt;em&gt;shared memory&lt;/em&gt;. This is memory
that is accessible by multiple applications, and provides a very
versatile approach to sharing information and collaboration between
processes. Usually, a semaphore is also used to govern writes and reads
to the shared memory, so that one process that wants to update a part of
the shared memory takes a semaphore (a sort-of lock), makes the updates,
and then increments the semaphore again.&lt;/p&gt;
&lt;p&gt;You can see the currently defined shared memory using &lt;strong&gt;ipcs -m&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status      
0x00000000 655370     swift      600        393216     2          dest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Again, more information can be obtained through &lt;strong&gt;-i SHMID&lt;/strong&gt;. An
interesting value to look at as well is the creator PID (just in case
the process still runs, or through the audit logs) and the last PID used
to operate on the shared memory (which also might no longer exist, but
is still an important value to investigate).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -m -p
------ Shared Memory Creator/Last-op PIDs --------
shmid      owner      cpid       lpid      
655370     swift      6147       6017

~$ ps -ef | grep -E &amp;#39;(6147|6017)&amp;#39;
root      6017  6016  0 09:49 tty1     00:01:30 /usr/bin/X -nolisten tcp :0 -auth /home/swift/.serverauth.6000
swift     6147     1  2 09:50 tty1     00:05:10 firefox
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the shared memory is most likely used to handle the UI of
firefox towards the X server.&lt;/p&gt;
&lt;p&gt;A last IPC resource are message queues, through which processes can put
messages on a queue and remove messages (by reading them) from the
queue. I don't have an example at hand for the moment, but just like
semaphores and shared memory, queues can be looked at through &lt;strong&gt;ipcs
-q&lt;/strong&gt; with more information being available through &lt;strong&gt;ipcs -q -i MSQID&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now what if you need to operate these? For this, you can use &lt;strong&gt;ipcrm&lt;/strong&gt;
to remove an IPC resource whereas &lt;strong&gt;ipcmk&lt;/strong&gt; can be used to create one
(although the latter is not that often used for administrative purposes,
whereas &lt;strong&gt;ipcrm&lt;/strong&gt; can help you troubleshoot and fix issues without
having to reboot a system). Of course, removing IPC resources from the
system should only be done when there is a bug in the application(s)
that use it (for instance, a process decreased a semaphore and then
crashed - in that case, remove the semaphore and start one of the
application(s) that also operates on the semaphore as they usually
recreate it and continue happily).&lt;/p&gt;
&lt;p&gt;Now before finishing this post, I do need to tell you about the
difference between an IPC resource key and its identifier. The &lt;em&gt;key&lt;/em&gt; is
like a path or URL, and is a value used by the applications to find and
obtain existing IPC resources (something like, "give me the list of
semaphores that I can access with key 12345"). The &lt;em&gt;identifier&lt;/em&gt; is a
unique ID generated by the Linux kernel at the moment that the IPC
resource is created. Unlike the key, which can be used for multiple IPC
resources, the identifier is unique. This is why the identifier is used
in the &lt;strong&gt;ipcs -i&lt;/strong&gt; command rather than the key. Also, that means that if
applications would properly document their IPC usage then we would
easily know what an IPC resource is used for.&lt;/p&gt;</content><category term="Free-Software"></category><category term="ipc"></category><category term="ipcrm"></category><category term="ipcs"></category><category term="linux"></category><category term="msg"></category><category term="sem"></category><category term="shmem"></category></entry><entry><title>Querying SELinux policy for boolean information</title><link href="https://blog.siphos.be/2014/03/querying-selinux-policy-for-boolean-information/" rel="alternate"></link><published>2014-03-28T23:38:00+01:00</published><updated>2014-03-28T23:38:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-28:/2014/03/querying-selinux-policy-for-boolean-information/</id><summary type="html">&lt;p&gt;Within an SELinux policy, certain access vectors (permissions) can be
conditionally granted based on the value of a &lt;em&gt;SELinux boolean&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To find the list of SELinux booleans that are available on your system,
you can use the &lt;strong&gt;getsebool -a&lt;/strong&gt; method, or &lt;strong&gt;semanage boolean -l&lt;/strong&gt;. The
latter also displays the description …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Within an SELinux policy, certain access vectors (permissions) can be
conditionally granted based on the value of a &lt;em&gt;SELinux boolean&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To find the list of SELinux booleans that are available on your system,
you can use the &lt;strong&gt;getsebool -a&lt;/strong&gt; method, or &lt;strong&gt;semanage boolean -l&lt;/strong&gt;. The
latter also displays the description of the boolean:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage boolean -l | grep user_ping
user_ping                      (on   ,   on)  Control users use of ping and traceroute
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can easily query the SELinux policy to see what this boolean
triggers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# sesearch -b user_ping -A -C
Found 22 semantic av rules:
ET allow ping_t staff_t : process sigchld ; [ user_ping ]
ET allow ping_t staff_t : fd use ; [ user_ping ]
ET allow ping_t staff_t : fifo_file { ioctl read write getattr lock append open } ; [ user_ping ]
ET allow ping_t user_t : process sigchld ; [ user_ping ]
ET allow ping_t user_t : fd use ; [ user_ping ]
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, often you want to know if a particular access is allowed and,
if it is conditionally allowed, which boolean enables it. In the case of
user ping, we want to know if (and when) a user domain (&lt;code&gt;user_t&lt;/code&gt;) is
allowed to transition to the ping domain (&lt;code&gt;ping_t&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# sesearch -s user_t -t ping_t -c process -p transition -ACTS
Found 1 semantic av rules:
ET allow user_t ping_t : process transition ; [ user_ping ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So there you go - it is allowed if the &lt;code&gt;user_ping&lt;/code&gt; SELinux boolean is
enabled.&lt;/p&gt;</content><category term="SELinux"></category><category term="boolean"></category><category term="query"></category><category term="selinux"></category><category term="sesearch"></category></entry><entry><title>Online hardened meeting of March</title><link href="https://blog.siphos.be/2014/03/online-hardened-meeting-of-march/" rel="alternate"></link><published>2014-03-27T23:44:00+01:00</published><updated>2014-03-27T23:44:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-27:/2014/03/online-hardened-meeting-of-march/</id><summary type="html">&lt;p&gt;I'm back from the depths of the unknown, so time to pick up my usual
write-up of the online Gentoo Hardened meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.9 is being worked on, and might be released by end of April (based
on the amount of open bugs). You can find the &lt;a href="http://gcc.gnu.org/gcc-4.9/changes.html"&gt;changes …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm back from the depths of the unknown, so time to pick up my usual
write-up of the online Gentoo Hardened meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.9 is being worked on, and might be released by end of April (based
on the amount of open bugs). You can find the &lt;a href="http://gcc.gnu.org/gcc-4.9/changes.html"&gt;changes
online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speaking of GCC, pipacs asked if it is possible in the upcoming 4.8.2
ebuilds to disable the SSP protection for development purposes (such as
when you're developing GCC plugins that do similar protection measures
like SSP, but you don't want those to collide with each other). Recent
discussion on Gentoo development mailinglist had a consensus that the
SSP protection measures (&lt;code&gt;-fstack-protector&lt;/code&gt;) can be enabled by default,
but of course if people are developing new GCC plugins which might
interfere with SSP, disabling it is needed. One can use
&lt;code&gt;-fno-stack-protector&lt;/code&gt; for this, or build stuff with &lt;code&gt;-D__KERNEL__&lt;/code&gt; (as
for kernel builds the default SSP handling is disabled anyway, allowing
for kernel-specific implementations).&lt;/p&gt;
&lt;p&gt;Other than those, there is no direct method to make SSP generally
unavailable.&lt;/p&gt;
&lt;p&gt;Blueness is also working on &lt;a href="http://www.musl-libc.org/"&gt;musc-libc&lt;/a&gt; on
Gentoo, which would give a strong incentive for hardened embedded
devices. For desktops, well, don't hold your breath just yet.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel grSec/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It looks like kernel 3.13 will be Ubuntu's LTS kernel choice, which also
makes it the kernel version that grSecurity will put the long term
support for in. And with Linux 3.14 almost out, the grsec patches for it
are ready as well. Of the previous LTS kernels, 3.2 will probably finish
seeing grsec support somewhere this year.&lt;/p&gt;
&lt;p&gt;The C wrapper (called &lt;strong&gt;install-xattr&lt;/strong&gt;) used to preserve xattr
information during Portage builds has not been integrated in Portage
yet, but the development should be finished.&lt;/p&gt;
&lt;p&gt;During the chat session, we also discussed the
&lt;a href="https://lwn.net/Articles/274859/"&gt;gold&lt;/a&gt;
&lt;a href="https://wiki.gentoo.org/wiki/Gold"&gt;linker&lt;/a&gt; and how it might be used by
more and more packages (so not only by users that explicitly ask for
it). udev version 210 onwards is one example, but some others exist. But
other than its existence there's not much to say right here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The
&lt;a href="http://oss.tresys.com/projects/refpolicy/wiki/DownloadRelease"&gt;20140311&lt;/a&gt;
release of the reference policy is now in the Portage tree.&lt;/p&gt;
&lt;p&gt;Also, prometheanfire caught a vulnerability
(&lt;a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-1874"&gt;CVE-2014-1874&lt;/a&gt;)
in SELinux which has been fixed in the latest kernels.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I made a few updates to the &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/"&gt;gentoo hardening
guide&lt;/a&gt; in
XCCDF/OVAL format. Nothing major, and I still need to add in a lot of
other best practices (as well as automate the tests through OVAL), but I
do intend to update the files (at least the gentoo one and ssh as
OpenSSH 6 is now readily available) regularly in the next few weeks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A few minor changes have been made to &lt;code&gt;hardened/uclibc&lt;/code&gt; to support
multilib, but other than that nothing has been done (nor needed to be
done) to our profiles.&lt;/p&gt;
&lt;p&gt;That's it for this months hardened meeting write-up. See you next time!&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category></entry><entry><title>Fixing the busybox build failure</title><link href="https://blog.siphos.be/2014/03/fixing-the-busybox-build-failure/" rel="alternate"></link><published>2014-03-26T14:18:00+01:00</published><updated>2014-03-26T14:18:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-26:/2014/03/fixing-the-busybox-build-failure/</id><summary type="html">&lt;p&gt;Since a few months I have a build failure every time I try to generate
an initial ram file system (as my current primary workstation uses a
separate &lt;code&gt;/usr&lt;/code&gt; and LVM for everything except &lt;code&gt;/boot&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;* busybox: &amp;gt;&amp;gt; Compiling...
* ERROR: Failed to compile the &amp;quot;all&amp;quot; target...
* 
* -- Grepping log... --
* 
*           - busybox-1.7.4-signal-hack.patch …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Since a few months I have a build failure every time I try to generate
an initial ram file system (as my current primary workstation uses a
separate &lt;code&gt;/usr&lt;/code&gt; and LVM for everything except &lt;code&gt;/boot&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;* busybox: &amp;gt;&amp;gt; Compiling...
* ERROR: Failed to compile the &amp;quot;all&amp;quot; target...
* 
* -- Grepping log... --
* 
*           - busybox-1.7.4-signal-hack.patch
* busybox: &amp;gt;&amp;gt; Configuring...
*COMMAND: make -j2 CC=&amp;quot;gcc&amp;quot; LD=&amp;quot;ld&amp;quot; AS=&amp;quot;as&amp;quot;  
*  HOSTCC  scripts/basic/fixdep
*make: execvp: /var/tmp/genkernel/18562.2920.28766.17301/busybox-1.20.2/scripts/gen_build_files.sh: Permission denied
*make: *** [gen_build_files] Error 127
*make: *** Waiting for unfinished jobs....
*/bin/sh: scripts/basic/fixdep: Permission denied
*make[1]: *** [scripts/basic/fixdep] Error 1
*make: *** [scripts_basic] Error 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I know it isn't SELinux that is causing this, as I have no denial
messages and even putting SELinux in permissive mode doesn't help. Today
I found the time to look at it with more fresh eyes, and noticed that it
wants to execute a file (&lt;code&gt;gen_build_files.sh&lt;/code&gt;) situated in &lt;code&gt;/var/tmp&lt;/code&gt;
somewhere. That file system however is mounted with &lt;code&gt;noexec&lt;/code&gt; (amongst
other settings) so executing anything from within that file system is
not allowed.&lt;/p&gt;
&lt;p&gt;The solution? Update &lt;code&gt;/etc/genkernel.conf&lt;/code&gt; and have &lt;code&gt;TMPDIR&lt;/code&gt; point to a
location where executing &lt;em&gt;is&lt;/em&gt; allowed. Of course, this being a SELinux
system, the new location will need to be labeled as &lt;code&gt;tmp_t&lt;/code&gt; as well, but
that's just a simple thing to do.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage fcontext -a -t tmp_t /var/build/genkernel(/.*)?
~# restorecon -R /var/build/genkernel
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The new location is not world-writable (only for root as only root
builds initial ram file systems here) so not having &lt;code&gt;noexec&lt;/code&gt; here is ok.&lt;/p&gt;</content><category term="Gentoo"></category><category term="busybox"></category><category term="genkernel"></category><category term="Gentoo"></category><category term="initramfs"></category><category term="initrd"></category><category term="noexec"></category><category term="tmp"></category></entry><entry><title>Talk about SELinux on GSE Linux/Security</title><link href="https://blog.siphos.be/2014/03/talk-about-selinux-on-gse-linuxsecurity/" rel="alternate"></link><published>2014-03-25T23:11:00+01:00</published><updated>2014-03-25T23:11:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-25:/2014/03/talk-about-selinux-on-gse-linuxsecurity/</id><summary type="html">&lt;p&gt;On today's &lt;a href="http://www.gsebelux.com"&gt;GSE Linux / GSE Security&lt;/a&gt; meeting
(in cooperation with
&lt;a href="http://www.imug.be/events_be/IMUG_LinuxSecurity_Event.asp"&gt;IMUG&lt;/a&gt;) I
gave a small (30 minutes) presentation about what SELinux is. The
&lt;a href="http://dev.gentoo.org/~swift/blog/201403/20140325_GSE_SELinux.pdf"&gt;slides are
online&lt;/a&gt;
and cover two aspects of SELinux: some of its design principles, and
then a set of features provided by SELinux. The talk is directed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;On today's &lt;a href="http://www.gsebelux.com"&gt;GSE Linux / GSE Security&lt;/a&gt; meeting
(in cooperation with
&lt;a href="http://www.imug.be/events_be/IMUG_LinuxSecurity_Event.asp"&gt;IMUG&lt;/a&gt;) I
gave a small (30 minutes) presentation about what SELinux is. The
&lt;a href="http://dev.gentoo.org/~swift/blog/201403/20140325_GSE_SELinux.pdf"&gt;slides are
online&lt;/a&gt;
and cover two aspects of SELinux: some of its design principles, and
then a set of features provided by SELinux. The talk is directed towards
less technical folks - still IT of course, but not immediately involved
in daily operations - so no commands and example/output.&lt;/p&gt;
&lt;p&gt;SELinux came across the board a few times during the entire day. In the
talks about &lt;em&gt;Open Source Security&lt;/em&gt; and &lt;em&gt;Security Guidelines for z/VM and
Linux on System z&lt;/em&gt; SELinux came (of course) up as the technology of
choice for providing in-operating system mandatory access control (on
the zEnterprise' Z/VM level - the hypervisor - this is handled through
RACF Mandatory Access Control) and the &lt;em&gt;Security Enablement on Virtual
Machines&lt;/em&gt; had SELinux in the front line for the sVirt security
protection measures (which focuses on the segregation through MLS
categories).&lt;/p&gt;
&lt;p&gt;And during the talk about &lt;em&gt;A customer story about logging and audit&lt;/em&gt;,
well, you can guess which technology is also one of the many sources of
logging. Right. SELinux ;-)&lt;/p&gt;
&lt;p&gt;Anyway, if your company is interested in such GSE events, make sure to
follow the &lt;a href="http://www.gsebelux.com"&gt;gsebelux.com&lt;/a&gt; site for updates.
It's a great way for networking as well as sharing experiences.&lt;/p&gt;</content><category term="Security"></category><category term="gse"></category><category term="mainframe"></category><category term="s390x"></category><category term="security"></category><category term="selinux"></category><category term="zenterprise"></category></entry><entry><title>Create your own SELinux Gentoo profile</title><link href="https://blog.siphos.be/2014/03/create-your-own-selinux-gentoo-profile/" rel="alternate"></link><published>2014-03-24T21:51:00+01:00</published><updated>2014-03-24T21:51:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-24:/2014/03/create-your-own-selinux-gentoo-profile/</id><summary type="html">&lt;p&gt;Or any other profile for that matter ;-)&lt;/p&gt;
&lt;p&gt;A month or so ago we got the question how to enable SELinux on a Gentoo
profile that doesn't have a &lt;code&gt;&amp;lt;some profilename&amp;gt;/selinux&lt;/code&gt; equivalent.
Because we don't create SELinux profiles for all possible profiles out
there, having a way to do this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Or any other profile for that matter ;-)&lt;/p&gt;
&lt;p&gt;A month or so ago we got the question how to enable SELinux on a Gentoo
profile that doesn't have a &lt;code&gt;&amp;lt;some profilename&amp;gt;/selinux&lt;/code&gt; equivalent.
Because we don't create SELinux profiles for all possible profiles out
there, having a way to do this yourself is good to know.&lt;/p&gt;
&lt;p&gt;Sadly, the most efficient way to deal with this isn't supported by
Portage: creating a &lt;code&gt;parent&lt;/code&gt; file pointing to
&lt;code&gt;/usr/portage/profiles/features/selinux&lt;/code&gt; in &lt;code&gt;/etc/portage/profile&lt;/code&gt;, as
is done for all SELinux enabled profiles. The &lt;code&gt;/etc/portage/profile&lt;/code&gt;
location (where users can do local changes to the profile settings) does
not support a &lt;code&gt;parent&lt;/code&gt; file in there.&lt;/p&gt;
&lt;p&gt;Luckily, enabling SELinux is a matter of merging the files in
&lt;code&gt;/usr/portage/profiles/features/selinux&lt;/code&gt; into &lt;code&gt;/etc/portage/profile&lt;/code&gt;. If
you don't have any files in there, you can blindly copy over the files
from &lt;code&gt;features/selinux&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; &lt;em&gt;aballier&lt;/em&gt; on &lt;code&gt;#gentoo-dev&lt;/code&gt; mentioned that you can create a
&lt;code&gt;/etc/portage/make.profile&lt;/code&gt; directory (instead of having it be a symlink
managed by &lt;strong&gt;eselect profile&lt;/strong&gt;) which does support &lt;code&gt;parent&lt;/code&gt; files. In
that case, just create one with two entries: one path to the profile you
want, and one path to the &lt;code&gt;features/selinux&lt;/code&gt; location.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="profile"></category></entry><entry><title>Hidden symbols and dynamic linking</title><link href="https://blog.siphos.be/2014/03/hidden-symbols-and-dynamic-linking/" rel="alternate"></link><published>2014-03-24T21:14:00+01:00</published><updated>2014-03-24T21:14:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-24:/2014/03/hidden-symbols-and-dynamic-linking/</id><summary type="html">&lt;p&gt;A few weeks ago, we introduced an error in the (\~arch) &lt;code&gt;libselinux&lt;/code&gt;
ebuild which caused the following stacktrace to occur every time the
&lt;strong&gt;semanage&lt;/strong&gt; command was invoked:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~ # semanage
Traceback (most recent call last):
  File &amp;quot;/usr/lib/python-exec/python2.7/semanage&amp;quot;, line 27, in 
    import seobject
  File &amp;quot;/usr/lib64/python2.7 …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, we introduced an error in the (\~arch) &lt;code&gt;libselinux&lt;/code&gt;
ebuild which caused the following stacktrace to occur every time the
&lt;strong&gt;semanage&lt;/strong&gt; command was invoked:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~ # semanage
Traceback (most recent call last):
  File &amp;quot;/usr/lib/python-exec/python2.7/semanage&amp;quot;, line 27, in 
    import seobject
  File &amp;quot;/usr/lib64/python2.7/site-packages/seobject.py&amp;quot;, line 27, in 
    import sepolicy
  File &amp;quot;/usr/lib64/python2.7/site-packages/sepolicy/__init__.py&amp;quot;, line 11, in 
    import sepolgen.interfaces as interfaces
  File &amp;quot;/usr/lib64/python2.7/site-packages/sepolgen/interfaces.py&amp;quot;, line 24, in 
    import access
  File &amp;quot;/usr/lib64/python2.7/site-packages/sepolgen/access.py&amp;quot;, line 35, in 
    from selinux import audit2why
ImportError: /usr/lib64/python2.7/site-packages/selinux/audit2why.so: undefined symbol: sepol_set_policydb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Usually this error means that a needed library (a &lt;code&gt;.so&lt;/code&gt; file) is
missing, or is not part of the &lt;code&gt;/etc/ld.so.conf&lt;/code&gt; list of directories to
scan. And when SELinux is enabled, you might want to check the
permissions of that file as well (who knows). But that wasn't the case
here. After trying to figure things out (which includes switching Python
versions, grepping for &lt;em&gt;sepol_set_policydb&lt;/em&gt; in &lt;code&gt;libsepol.so&lt;/code&gt; and more)
I looked at the &lt;code&gt;audit2why.c&lt;/code&gt; code and see if/where
&lt;em&gt;sepol_set_policydb&lt;/em&gt; is needed, as well as at the &lt;code&gt;libsepol&lt;/code&gt; sources
to see where it is defined. And yes, the call is (of course) needed, but
the definition made me wonder if this wasn't a bug:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;int hidden sepol_set_policydb(policydb_t * p)
{
        policydb = p;
        return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Hidden? But, that means that the function symbol is not available for
dynamic linking... So if that is the case, shouldn't &lt;code&gt;audit2why.c&lt;/code&gt; not
call it? Turns out, this was due to a fix we introduced earlier on,
where &lt;code&gt;libsepol&lt;/code&gt; got linked dynamically instead of statically (i.e.
using &lt;code&gt;libsepol.a&lt;/code&gt;). Static linking of libraries still allows for the
(hidden) symbols to be used, whereas dynamic linking doesn't.&lt;/p&gt;
&lt;p&gt;So that part of the fix got reverted (and should fix the bug we
introduced), and I learned a bit more about symbols (and the &lt;em&gt;hidden&lt;/em&gt;
statement).&lt;/p&gt;
&lt;p&gt;Bonus: if you need to check what symbols are available in a binary /
shared library, use &lt;strong&gt;nm&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ nm -D /path/to/binary
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Gentoo"></category><category term="elf"></category><category term="hidden"></category><category term="selinux"></category><category term="symbols"></category></entry><entry><title>Closing week? No, starting week...</title><link href="https://blog.siphos.be/2014/03/closing-week-no-starting-week/" rel="alternate"></link><published>2014-03-16T21:36:00+01:00</published><updated>2014-03-16T21:36:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-03-16:/2014/03/closing-week-no-starting-week/</id><summary type="html">&lt;p&gt;I've been away for a while, and this week will (hopefully) be the last
week of all the effort that is causing this. And that means I'll get
back to blogging, documentation development, SELinux integration,
SELinux policy development and more. To be honest, I'm eagerly awaiting
this moment of getting …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been away for a while, and this week will (hopefully) be the last
week of all the effort that is causing this. And that means I'll get
back to blogging, documentation development, SELinux integration,
SELinux policy development and more. To be honest, I'm eagerly awaiting
this moment of getting back on my feet.&lt;/p&gt;
&lt;p&gt;That doesn't mean it's been silent the last few months. A few days ago,
the Reference Policy made a new release of the policy. For Gentoo, that
won't be a major update as we already follow the reference policy (git
repository) quite closely with our policy packages. I'll hopefully merge
the last few changes and bump the version on the Gentoo policy packages
as well soon.&lt;/p&gt;
&lt;p&gt;The Gentoo wiki has seen quite a few updates. I tried to approve most of
the translation-ready changes a few days ago. Those that I didn't do I
should get feedback on (through the Talk/Discussion pages) in the next
week.&lt;/p&gt;
&lt;p&gt;And then on to the huge backlog of issues on bugzie...&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Switching context depending on user code-wise</title><link href="https://blog.siphos.be/2014/01/switching-context-depending-on-user-code-wise/" rel="alternate"></link><published>2014-01-12T22:43:00+01:00</published><updated>2014-01-12T22:43:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-12:/2014/01/switching-context-depending-on-user-code-wise/</id><summary type="html">&lt;p&gt;I blogged about how SELinux decides what the context should be for a
particular Linux user; how it checks the default context(s) and tells
the SELinux-aware application on what the new context should be. Let's
look into the C code that does so, and how an application should behave …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I blogged about how SELinux decides what the context should be for a
particular Linux user; how it checks the default context(s) and tells
the SELinux-aware application on what the new context should be. Let's
look into the C code that does so, and how an application should behave
depending on the enforcing/permissive mode...&lt;/p&gt;
&lt;p&gt;I use the following, extremely simple C that fork()'s and executes
&lt;code&gt;id -Z&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/wait.h&amp;gt;
#include &amp;lt;stdarg.h&amp;gt;

#define DEBUG  7
#define INFO   6
#define NOTICE 5
#define WARN   4
#define ERR    3
#define CRIT   2
#define ALERT  1
#define EMERG  0

#ifndef LOGLEVEL
#define LOGLEVEL 4
#endif

/* out - Simple output */
void out(int level, char * msg, ...) {
  if (level &amp;lt;= LOGLEVEL) {
    va_list ap;
    printf(&amp;quot;%d - &amp;quot;, level);

    va_start(ap, msg);
    vprintf(msg, ap);
    va_end(ap);
  };
};
int main(int argc, char * argv[]) {
  int rc = 0;
  pid_t child;

  child = fork();
  if (child &amp;lt; 0) {
    out(WARN, &amp;quot;fork() failed\n&amp;quot;, NULL);
  };

  if (child == 0) {
    int pidrc;
    pidrc = execl(&amp;quot;/usr/bin/id&amp;quot;, &amp;quot;id&amp;quot;, &amp;quot;-Z&amp;quot;, NULL);
    if (pidrc != 0) {
      out(WARN, &amp;quot;Command failed with return code %d\n&amp;quot;, pidrc);
    };
    return(0);
  } else {
    int status;
    out(DEBUG, &amp;quot;Child is %d\n&amp;quot;, child);
    wait(&amp;amp;status);
    out(DEBUG, &amp;quot;Child exited with %d\n&amp;quot;, status);
  };
  return 0;
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code is ran as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ test myusername
staff_u:staff_r:staff_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, it shows the output of the &lt;code&gt;id -Z&lt;/code&gt; command. Let's
enhance this code with some SELinux specific functions. The purpose of
the application now is to ask SELinux what the context should be that
the command should run in, and switch to that context for the &lt;code&gt;id -Z&lt;/code&gt;
invocation.&lt;/p&gt;
&lt;p&gt;We will include the necessary SELinux code with &lt;code&gt;#ifdef SELINUX&lt;/code&gt;,
allowing the application to be build without SELinux code if wanted.&lt;/p&gt;
&lt;p&gt;First, add in the proper &lt;code&gt;#include&lt;/code&gt; directives.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#ifdef SELINUX
#include &amp;lt;selinux/selinux.h&amp;gt;
#include &amp;lt;selinux/flask.h&amp;gt;
#include &amp;lt;selinux/av_permissions.h&amp;gt;
#include &amp;lt;selinux/get_context_list.h&amp;gt;
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we create a function called &lt;em&gt;selinux_prepare_fork()&lt;/em&gt; which takes
one input variable: the Linux user name for which we are going to
transition (and thus run &lt;code&gt;id -Z&lt;/code&gt; for). This function can always be
called, even if SELinux is not built in. If that happens, we return 0
immediately.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/* selinux_prepare_fork - Initialize context switching
 *
 * Returns
 *  - 0 if everything is OK, 
 *  - +1 if the code should continue, even if SELinux wouldn&amp;#39;t allow
 *       (for instance due to permissive mode)
 *  - -1 if the code should not continue
 */
int selinux_prepare_fork(char * name) {
#ifndef SELINUX
  return 0;
#else
  // ... this is where the remainder goes
#endif
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We include this call in the application above, and take into account the
return codes passed on. As can be seen from the comment, if the
returncode is 0 (zero) then everything can go on as expected. A positive
return code means that there are some issues, but the application should
continue with its logic as SELinux is either in permissive, or the
domain in which the application runs is permissive - in either case, the
code will succeed. A returncode of -1 means that the code will most
likely fail and thus the application should log an error and exit or
break.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  pid_t child;

  rc = selinux_prepare_fork(argv[1]);
  if (rc &amp;lt; 0) {
    out(WARN, &amp;quot;The necessary context change will fail.\n&amp;quot;);
    // Continuing here would mean that the newly started process
    // runs in the wrong context (current context) which might
    // be either too privileged, or not privileged enough.
    return -1;
  } else if (rc &amp;gt; 0) {
    out(WARN, &amp;quot;The necessary context change will fail, but permissive mode is active.\n&amp;quot;);
  };

  child = fork();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now all we need to do is fill in the logic in &lt;em&gt;selinux_prepare_fork&lt;/em&gt;.
Let's start with the variable declarations (boring stuff):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#ifndef SELINUX
  return 0;
#else
  security_context_t newcon = 0;
  security_context_t curcon = 0;
  struct av_decision avd;
  int rc;
  int permissive = 0;
  int dom_permissive = 0;

  char * sename = 0;
  char * selevel = 0;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that out of the way, let's take our first step: we want to see if
SELinux is enabled or not. Applications that are SELinux-aware should
always check if SELinux itself is enabled and, if not, just continue
with the (application) logic.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * See if SELinux is enabled.
   * If not, then we can immediately tell the code
   * that everything is OK.
   */
  rc = is_selinux_enabled();
  if (rc == 0) {
    out(DEBUG, &amp;quot;SELinux is not enabled.\n&amp;quot;);
    return 0;
  } else if (rc == -1) {
    out(WARN, &amp;quot;Could not check SELinux state (is_selinux_enabled() failed)\n&amp;quot;);
    return 1;
  };
  out(DEBUG, &amp;quot;SELinux is enabled.\n&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, we use &lt;em&gt;is_selinux_enabled&lt;/em&gt; here to do just that. If
it returns 0, then it is not enabled. A returncode of 1 means it is
enabled, and -1 means something wicked happened. I recommend that
applications who are SELinux-aware enable info on these matters in
debugging output. Nothing is more annoying than having to debug
permission issues that might be SELinux-related, but are not enforced
through SELinux (and as such do not show up in any logs).&lt;/p&gt;
&lt;p&gt;Next, see if SELinux is in permissive mode and register this (as we need
this later for deciding to continue or not).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * See if SELinux is in enforcing mode
   * or permissive mode
   */
  rc = security_getenforce();
  if (rc == 0) {
    permissive = 1;
  } else if (rc == 1) {
    permissive = 0;
  } else {
    out(WARN, &amp;quot;Could not check SELinux mode (security_getenforce() failed)\n&amp;quot;);
  }
  out(DEBUG, &amp;quot;SELinux mode is %s\n&amp;quot;, permissive ? &amp;quot;permissive&amp;quot; : &amp;quot;enforcing&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;security_getenforce&lt;/em&gt; method will check the current SELinux mode
(enforcing or permissive). If SELinux is in permissive mode, then the
application logic should always go through - even if that means contexts
will go wrong and such. The end user marked the system in permissive
mode, meaning he does not want to have SELinux (or SELinux-aware
applications) to block things purely due to SELinux decisions, but log
when things are going wrong (for instance for policy development).&lt;/p&gt;
&lt;p&gt;Now, let's look up what the current context is (the context that the
process is running in). This will be used later for logging by the
SELinux-aware application in debugging mode. Often, applications that
fail run too short to find out if their context is correct or not, and
having it logged definitely helps. This step is not mandatory per se (as
you will see from the code later).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * Get the current SELinux context of the process.
   * Always interesting to log this for end users
   * trying to debug a possible issue.
   */
  rc = getcon(&amp;amp;curcon);
  if (rc) {
    out(WARN, &amp;quot;Could not get current SELinux context (getcon() failed)\n&amp;quot;);
    if (permissive)
      return +1;
    else
      return -1;
  };
  out(DEBUG, &amp;quot;Currently in SELinux context \&amp;quot;%s\&amp;quot;\n&amp;quot;, (char *) curcon);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;getcon()&lt;/em&gt; method places the current context in the &lt;em&gt;curcon&lt;/em&gt;
variable. Note that from this point onwards, we should always
&lt;em&gt;freecon()&lt;/em&gt; the context before exiting the &lt;em&gt;selinux_prepare_fork()&lt;/em&gt;
method.&lt;/p&gt;
&lt;p&gt;A second important note is that, if we have a failure, we now check the
permissive state and return a positive error (SELinux is in permissive
mode, so log but continue) or negative error (SELinux is in enforcing
mode). The negative error is needed so that the code itself does not go
run the &lt;em&gt;fork()&lt;/em&gt; as it will fail anyway (or, it might succeed, but run
in the parent context which is not what the application should do).&lt;/p&gt;
&lt;p&gt;Next, we try to find out what the SELinux user is for the given Linux
account name.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * Get the SELinux user given the Linux user
   * name passed on to this function.
   */
  rc = getseuserbyname(name, &amp;amp;sename, &amp;amp;selevel);
  if (rc) {
    out(WARN, &amp;quot;Could not find SELinux user for Linux user \&amp;quot;%s\&amp;quot; (getseuserbyname() failed)\n&amp;quot;, name);
    freecon(curcon);
    if (permissive)
      return +1;
    else
      return -1;
  };
  out(DEBUG, &amp;quot;SELinux user for Linux user \&amp;quot;%s\&amp;quot; is \&amp;quot;%s\&amp;quot;\n&amp;quot;, name, sename);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;getseuserbyname()&lt;/em&gt; method returns the SELinux name for the given
Linux user. It also returns the MLS level (but we're not going to use
that in the remainder of the code). Again, if it fails, we check the
permissive state to see how to bail out.&lt;/p&gt;
&lt;p&gt;Now get the context to which we should transition when calling &lt;code&gt;id -Z&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * Find out what the context is that this process should transition
   * to.
   */
  rc = get_default_context(sename, NULL, &amp;amp;newcon);
  if (rc) {
    out(WARN, &amp;quot;Could not deduce default context for SELinux user \&amp;quot;%s\&amp;quot; given our current context (\&amp;quot;%s\&amp;quot;)\n&amp;quot;, sename, (char *) curcon);
    freecon(curcon);
    if (permissive)
      return +1;
    else
      return -1;
  };
  out(DEBUG, &amp;quot;SELinux context to transition to is \&amp;quot;%s\&amp;quot;\n&amp;quot;, (char *) newcon);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;get_default_context()&lt;/em&gt; will do what I blogged about earlier.
It'll check what the contexts are in the user-specific context files or
the &lt;code&gt;default_contexts&lt;/code&gt; file, given the current context. You might notice
I don't pass on this context - the &lt;code&gt;NULL&lt;/code&gt; second argument means "use the
current context". This is why the &lt;em&gt;getcon()&lt;/em&gt; method earlier is not
strictly needed. But again, for logging (and thus debugging) this is
very much recommended.&lt;/p&gt;
&lt;p&gt;From this point onward, we also need to &lt;em&gt;freecon()&lt;/em&gt; the &lt;code&gt;newcon&lt;/code&gt;
variable before exiting the function.&lt;/p&gt;
&lt;p&gt;Now let's see if we are allowed to transition. We will query the SELinux
policy and see if a transition from the current context to the new
context is allowed (class &lt;code&gt;process&lt;/code&gt;, privilege &lt;code&gt;transition&lt;/code&gt;). I know, to
truly see if a transition is allowed, more steps should be checked, but
let's stick with this one permission.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * Now let&amp;#39;s look if we are allowed to transition to the new context.
   * We currently only check the transition access for the process class. However,
   * transitioning is a bit more complex (execute rights on target context, 
   * entrypoint of that context for the new domain, no constraints like target
   * domain not being a valid one, MLS constraints, etc.).
   */
  rc = security_compute_av_flags(curcon, newcon, SECCLASS_PROCESS, PROCESS__TRANSITION, &amp;amp;avd);
  if (rc) {
    out(WARN, &amp;quot;Could not deduce rights for transitioning \&amp;quot;%s\&amp;quot; -&amp;gt; \&amp;quot;%s\&amp;quot; (security_compute_av_flags() failed)\n&amp;quot;, (char *) curcon, (char *) newcon);
    freecon(curcon);
    freecon(newcon);
    if (permissive)
      return +1;
    else
      return -1;
  };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above code, I didn't yet check the result. This is done in two
steps.&lt;/p&gt;
&lt;p&gt;In the first step, I want to know if the current context is a permissive
domain. Since a few years, SELinux supports permissive domains, so that
a single domain is permissive even though the rest of the system is in
enforcing mode. Currently, we only know if the system is in permissive
mode or not.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /* Validate the response 
   *
   * We are interested in two things:
   * - Is the transition allowed, but also
   * - Is the permissive flag set
   *
   * If the permissive flag is set, then we
   * know the current domain is permissive
   * (even if the rest of the system is in
   * enforcing mode).
   */
  if (avd.flags &amp;amp; SELINUX_AVD_FLAGS_PERMISSIVE) {
    out(DEBUG, &amp;quot;The SELINUX_AVD_FLAGS_PERMISSIVE flag is set, so domain is permissive.\n&amp;quot;);
    dom_permissive = 1;
  };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We check the flags provided to us by the SELinux subsystem and check if
&lt;code&gt;SELINUX_AVD_FLAGS_PERMISSIVE&lt;/code&gt; is set. If it is, then the current domain
is permissive, and we register this (in the &lt;code&gt;dom_permissive&lt;/code&gt; variable).
From this point onwards, &lt;code&gt;permissive=1&lt;/code&gt; or &lt;code&gt;dom_permissive=1&lt;/code&gt; is enough
to tell the real application logic to continue (even if things would
fail SELinux-wise) - the actions are executed by a permissive domain (or
system) and thus should continue.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  if (!(avd.allowed &amp;amp; PROCESS__TRANSITION)) {
    // The transition is denied
    if (permissive) {
      out(DEBUG, &amp;quot;Transition is not allowed by SELinux, but permissive mode is enabled. Continuing.\n&amp;quot;);
    };
    if (dom_permissive) {
      out(DEBUG, &amp;quot;Transition is not allowed by SELinux, but domain is in permissive mode. Continuing.\n&amp;quot;);
    };
    if ((permissive == 0) &amp;amp;&amp;amp; (dom_permissive == 0)) {
      out(WARN, &amp;quot;The domain transition is not allowed and we are not in permissive mode.\n&amp;quot;);
      freecon(curcon);
      freecon(newcon);
      return -1;
    };
  };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the second step, we checked if the requested operation (transition)
is allowed or not. If denied, we log it, but do not break out of the
function if either &lt;code&gt;permissive&lt;/code&gt; (SELinux permissive mode) or
&lt;code&gt;dom_permissive&lt;/code&gt; (domain is permissive) is set.&lt;/p&gt;
&lt;p&gt;Finally, we set the (new) context, telling the SELinux subsystem that
the next &lt;em&gt;exec()&lt;/em&gt; done by the application should also switch the domain
of the process to the new context (i.e. a domain transition):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  /*
   * Set the context for the fork (process execution).
   */
  rc = setexeccon(newcon);
  if (rc) {
    out(WARN, &amp;quot;Could not set execution context (setexeccon() failed)\n&amp;quot;);
    freecon(curcon);
    freecon(newcon);
    if ((permissive) || (dom_permissive))
      return +1;
    else
      return -1;
  };

  freecon(newcon);
  freecon(curcon);

  return 0;
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's it - we free'd all our variables and can now have the application
continue (taking into account the return code of this function). As
mentioned before, a positive return code (0 or higher) means the logic
should continue; a strictly negative return code means that the
application should gracefully fail.&lt;/p&gt;</content><category term="SELinux"></category><category term="default_context"></category><category term="domain"></category><category term="libselinux"></category><category term="selinux"></category><category term="selinux-aware"></category><category term="transition"></category></entry><entry><title>Can Gentoo play a role in a RHEL-only environment?</title><link href="https://blog.siphos.be/2014/01/can-gentoo-play-a-role-in-a-rhel-only-environment/" rel="alternate"></link><published>2014-01-09T04:13:00+01:00</published><updated>2014-01-09T04:13:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-09:/2014/01/can-gentoo-play-a-role-in-a-rhel-only-environment/</id><summary type="html">&lt;p&gt;Sounds like a stupid question, as the answer is already in the title. If
a company has only RedHat Enterprise Linux as allowed / supported Linux
platform (be it for a support model requirement, ISV certification,
management tooling support or what not) how could or would Gentoo still
play a role …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sounds like a stupid question, as the answer is already in the title. If
a company has only RedHat Enterprise Linux as allowed / supported Linux
platform (be it for a support model requirement, ISV certification,
management tooling support or what not) how could or would Gentoo still
play a role in it.&lt;/p&gt;
&lt;p&gt;But the answer is, surprisingly, that Gentoo can still be made available
in the company architecture. One of the possible approaches is a
&lt;em&gt;virtual appliance&lt;/em&gt; role.&lt;/p&gt;
&lt;p&gt;Virtual appliances are entire operating systems, provided through VM
images (be it VMDK or in an
&lt;a href="http://en.wikipedia.org/wiki/Open_Virtualization_Format"&gt;OVF&lt;/a&gt; package),
which offer a well defined service to its consumers. More and more
products are being presented as virtual appliances. Why not - in the
past, they would be in sealed hardware appliances (but still running
some form of Linux on it) but nowadays the hypervisor and other
infrastructure is strong and powerful enough to handle even the most
intensive tasks in a virtual guest.&lt;/p&gt;
&lt;p&gt;Gentoo is extremely powerful as a meta-distribution. You can tweak,
update, enhance and tune a Gentoo Linux installation to fulfill whatever
requirement you have. And in the end, you can easily create a virtual
image from it, and have it run as a virtual appliance in the company.&lt;/p&gt;
&lt;p&gt;An example could be to offer a web-based password management suite. A
Gentoo Linux deployment could be created, fully
&lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;hardened&lt;/a&gt; of course,
with a MAC such as SELinux active. On it, a properly secured web server
with the password management suite, with underlying database (of course
only listening on localhost - don't want to expose the database to the
wider network). Through a simple menu, the various administrative
services needed to integrate the "appliance" in a larger environment can
be configured: downloading an SSL certificate request (and uploading the
signed one), (encrypted) backup/restore routines, SNMP configuration and
more.&lt;/p&gt;
&lt;p&gt;If properly designed, all configuration data could be easily exported
and imported (or provided through a secundary mount) so that updates on
the appliance are as simple as booting a new image and
uploading/mounting the configuration data.&lt;/p&gt;
&lt;p&gt;Building such a virtual appliance can be simplified with &lt;a href="http://www.gentoo.org/proj/en/gentoo-alt/prefix/"&gt;Gentoo
Prefix&lt;/a&gt;, multi-tenancy
on the web application level through the
&lt;a href="http://www.gentoo.org/proj/en/webapps/index.xml"&gt;webapp-config&lt;/a&gt; tool
while all necessary software is readily available in the Portage tree.&lt;/p&gt;
&lt;p&gt;All you need is some imagination...&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="linux"></category><category term="vappliance"></category><category term="virtual-appliance"></category></entry><entry><title>Linux protip: environment for a process</title><link href="https://blog.siphos.be/2014/01/linux-protip-environment-for-a-process/" rel="alternate"></link><published>2014-01-07T04:31:00+01:00</published><updated>2014-01-07T04:31:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-07:/2014/01/linux-protip-environment-for-a-process/</id><summary type="html">&lt;p&gt;Just a quick pro-tip: if you need to know the environment variables for
a process, you can see them in that process' &lt;code&gt;/proc/${PID}/environ&lt;/code&gt;
file. The file however shows the environment variables on one line, with
a null character as separator. With a simple &lt;strong&gt;sed&lt;/strong&gt; you can show it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just a quick pro-tip: if you need to know the environment variables for
a process, you can see them in that process' &lt;code&gt;/proc/${PID}/environ&lt;/code&gt;
file. The file however shows the environment variables on one line, with
a null character as separator. With a simple &lt;strong&gt;sed&lt;/strong&gt; you can show it
with newlines instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sed -e &amp;quot;s:\x0:\n:g&amp;quot; /proc/144320/environ
TERM=xterm
SHELL=/bin/bash
OLDPWD=/home/swift/docs
USER=root
SUDO_USER=swift
SUDO_UID=1001
USERNAME=root
MAIL=/var/mail/root
PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/opt/bin
PWD=/var/db/pkg/eix
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The trick is to use &lt;code&gt;\x0&lt;/code&gt; (hexcode 0) for the null character, which the
&lt;strong&gt;sed&lt;/strong&gt; command then replaces with a newline.&lt;/p&gt;</content><category term="Free-Software"></category><category term="environ"></category><category term="linux"></category><category term="protip"></category></entry><entry><title>How does foo_t get this privilege?</title><link href="https://blog.siphos.be/2014/01/how-does-foo_t-get-this-privilege/" rel="alternate"></link><published>2014-01-05T04:14:00+01:00</published><updated>2014-01-05T04:14:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-05:/2014/01/how-does-foo_t-get-this-privilege/</id><summary type="html">&lt;p&gt;Today a question was raised how the unprivileged user domain &lt;code&gt;user_t&lt;/code&gt;
was allowed to write to &lt;code&gt;cgroup_t&lt;/code&gt; files. There is nothing obvious about
that in the &lt;code&gt;roles/unprivuser.te&lt;/code&gt; file, so what gives?&lt;/p&gt;
&lt;p&gt;I used a simple script (which I've been using for a while already)
called &lt;strong&gt;seshowtree&lt;/strong&gt; which presents …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today a question was raised how the unprivileged user domain &lt;code&gt;user_t&lt;/code&gt;
was allowed to write to &lt;code&gt;cgroup_t&lt;/code&gt; files. There is nothing obvious about
that in the &lt;code&gt;roles/unprivuser.te&lt;/code&gt; file, so what gives?&lt;/p&gt;
&lt;p&gt;I used a simple script (which I've been using for a while already)
called &lt;strong&gt;seshowtree&lt;/strong&gt; which presents the SELinux rules for a particular
domain in a tree-like structure, expanding the interfaces as it finds
them. The script is far from perfect, but does enough to help me answer
such questions. If you're interested, the script is also available on my
&lt;a href="https://github.com/sjvermeu/small.coding/blob/master/se_scripts/seshowtree"&gt;github
small.coding&lt;/a&gt;
project.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# seshowtree user roles/unprivuser.te &amp;gt; output.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the resulting output, I search for the &lt;code&gt;cgroup_t&lt;/code&gt; and work my way up,
which gives:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;userdom_unpriv_user_template(user)
. userdom_common_user_template($1)
. . fs_rw_cgroup_files($1_t)
. . . rw_files_pattern($1, cgroup_t, cgroup_t)
. . . . allow $1 $3:file rw_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So in this case, the user forgot to look into
&lt;code&gt;userdom_common_user_template&lt;/code&gt;, which is called by
&lt;code&gt;userdom_unpriv_user_template&lt;/code&gt; to find the path to this privilege. Of
course, that still doesn't explain why the privileges are assigned in
the first place. As the policy file itself does not contain the
necessary comments to deduce this, I had to ask the git repository for
more information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ git annotate userdomain.if
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the end, it was a commit from 2010, informing me that "Common users
can read and write cgroup files (access governed by dac)". So the
privilege is by design, referring to the regular DAC permissions to
properly govern access to the files.&lt;/p&gt;</content><category term="SELinux"></category><category term="policy"></category><category term="selinux"></category><category term="seshowtree"></category></entry><entry><title>Oh it is cron again...</title><link href="https://blog.siphos.be/2014/01/oh-it-is-cron-again/" rel="alternate"></link><published>2014-01-03T21:05:00+01:00</published><updated>2014-01-03T21:05:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-03:/2014/01/oh-it-is-cron-again/</id><summary type="html">&lt;p&gt;Today I was pointed to the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test fcron[6722]: fcron[6722] 3.1.2 started
test fcron[6722]: Cannot bind socket to &amp;#39;/var/run/fcron.fifo&amp;#39;: Permission denied
test fcron[6722]:  &amp;quot;at&amp;quot; reboot jobs will only be run at computer&amp;#39;s startup.
test fcron[6722]: updating configuration from …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Today I was pointed to the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test fcron[6722]: fcron[6722] 3.1.2 started
test fcron[6722]: Cannot bind socket to &amp;#39;/var/run/fcron.fifo&amp;#39;: Permission denied
test fcron[6722]:  &amp;quot;at&amp;quot; reboot jobs will only be run at computer&amp;#39;s startup.
test fcron[6722]: updating configuration from /var/spool/fcron
test fcron[6722]: adding file systab Jan  3 17:51:19 test fcron[6722]: adding new file user
test fcron[6722]: NO CONTEXT for user &amp;quot;(null)&amp;quot;: Invalid argument
test fcron[6722]: ENTRYPOINT FAILED for user &amp;quot;user&amp;quot; (CONTEXT (null)) for file CONTEXT user_u:object_r:user_cron_spool_t:s0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;First of all, the moment I notice that it is cron, I know I'm up for a
few hours at it. Cron has been notoriously difficult to integrate with
SELinux, because it doesn't use the simpler "fork-execute" method (where
we can put in transitions). Instead, it often has to call
SELinux-specific methods to get the job done. Same was true here.&lt;/p&gt;
&lt;p&gt;Anyway, on to the issues. First of all, the &lt;em&gt;Cannot bind socket&lt;/em&gt; is a
simple SELinux policy thingie that one can easily ignore for now (I'll
patch and upstream that in a minute). The problem is the &lt;em&gt;NO CONTEXT&lt;/em&gt;
stuff.&lt;/p&gt;
&lt;p&gt;The code looks as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#ifdef SYSFCRONTAB
    if(!strcmp(cf-&amp;gt;cf_user, SYSFCRONTAB))
        user_name = &amp;quot;system_u&amp;quot;;
    else
#endif /* def SYSFCRONTAB */
        user_name = cf-&amp;gt;cf_user;
    if(flask_enabled)
    {
        if(get_default_context(user_name, NULL, &amp;amp;cf-&amp;gt;cf_user_context))
            error_e(&amp;quot;NO CONTEXT for user \&amp;quot;%s\&amp;quot;&amp;quot;, cf-&amp;gt;cf_user_context);
        retval = security_compute_av(cf-&amp;gt;cf_user_context, cf-&amp;gt;cf_file_context
                , SECCLASS_FILE, FILE__ENTRYPOINT, &amp;amp;avd);

        if(retval || ((FILE__ENTRYPOINT &amp;amp; avd.allowed) != FILE__ENTRYPOINT))
        {
            syslog(LOG_ERR, &amp;quot;ENTRYPOINT FAILED for user \&amp;quot;%s\&amp;quot; &amp;quot;
                   &amp;quot;(CONTEXT %s) for file CONTEXT %s&amp;quot;
                   , cf-&amp;gt;cf_user, cf-&amp;gt;cf_user_context, cf-&amp;gt;cf_file_context);
            goto err;
        }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It wasn't obvious to me either, but from a quick look through the
&lt;a href="http://userspace.selinuxproject.org/trac/browser/libselinux/include/selinux/selinux.h"&gt;selinux.h&lt;/a&gt;
code I found out that &lt;em&gt;get_default_context()&lt;/em&gt; requires the SELinux
user rather than Linux user.&lt;/p&gt;
&lt;p&gt;The purpose of the &lt;em&gt;get_default_context()&lt;/em&gt; method is to return the
SELinux context in which newly started tasks, originating from the
current context (if second argument is &lt;em&gt;NULL&lt;/em&gt;) or given context (second
argument), owned by the given user (first argument) should start in. In
case of cron, the code is asking SELinux what the context should be for
the cronjob itself, considering that it has to be executed for a given
user.&lt;/p&gt;
&lt;p&gt;Now the code currently passes on the owner (Linux user) of the crontab
file. As this owner usually is not a SELinux user (only when there is a
SELinux user named after the Linux user will this succeed), the method
returns &lt;em&gt;NULL&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The right call here would be to first look up the correct SELinux user
for the given Linux user, and then call the &lt;em&gt;get_default_context()&lt;/em&gt;
method. This will return a context to transition to.&lt;/p&gt;
&lt;p&gt;Now, cron systems usually do a second check - they see if the file in
which the cronjobs are mentioned is an &lt;em&gt;entrypoint&lt;/em&gt; for the context that
it should transition to. Even though the file itself will not be
directly executed, by checking if the &lt;em&gt;entrypoint&lt;/em&gt; permission is set
cron can be reasonably certain that it should proceed. So for cron, this
is like saying "Yes, the file with context &lt;code&gt;cron_spool_t&lt;/code&gt; is allowed to
contain job definitions for cron to execute".&lt;/p&gt;
&lt;p&gt;I've sent the
&lt;a href="http://thread.gmane.org/gmane.comp.sysutils.fcron.devel/89"&gt;patch&lt;/a&gt; for
this upstream and hopefully it gets added in - if I'm correct in the
deduction, that is.&lt;/p&gt;
&lt;p&gt;So when you get issues with cron, do the following checks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is the cron daemon running in the right domain? It should run in a
    &lt;code&gt;crond_t&lt;/code&gt; domain, otherwise it will not be able to get a proper
    default context.&lt;/li&gt;
&lt;li&gt;Assuming that cron uses the right arguments, make sure that a
    default context is set for the given SELinux user (check the
    &lt;code&gt;contexts/default_contexts&lt;/code&gt; and &lt;code&gt;contexts/users/*&lt;/code&gt; files) and that
    this context is valid&lt;/li&gt;
&lt;li&gt;Check the context of the file in which the definitions are stored
    and make sure it is mentioned as an entrypoint for the job domain&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Or, in some code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# ps -efZ | grep fcron | awk &amp;#39;{print $1}&amp;#39;
system_u:system_r:crond_t
# getseuser swift system_u:system_r:crond_t
seuser: user_u
Context 0     user_u:user_r:cronjob_t
# ls -lZ /var/spool/fcron/new.user
... user_u:object_r:user_cron_spool_t
# sesearch -s cronjob_t -t user_cron_spool_t -c file -p entrypoint -A
Found 1 semantic av rules:
  allow cronjob_t user_cron_spool_t : file entrypoint ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="cron"></category><category term="selinux"></category></entry><entry><title>Private key handling and SELinux protection</title><link href="https://blog.siphos.be/2014/01/private-key-handling-and-selinux-protection/" rel="alternate"></link><published>2014-01-02T04:00:00+01:00</published><updated>2014-01-02T04:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-01-02:/2014/01/private-key-handling-and-selinux-protection/</id><summary type="html">&lt;p&gt;In this post I'll give some insight in a &lt;em&gt;possible&lt;/em&gt; SELinux policy for a
script I wrote.&lt;/p&gt;
&lt;p&gt;The script is a certificate authority handling script, in which I can
generate a private key (and certificate assigned to it), sign the
certificate either by itself (for the root CA key) or …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post I'll give some insight in a &lt;em&gt;possible&lt;/em&gt; SELinux policy for a
script I wrote.&lt;/p&gt;
&lt;p&gt;The script is a certificate authority handling script, in which I can
generate a private key (and certificate assigned to it), sign the
certificate either by itself (for the root CA key) or by another
previously created key (for subkeys), create certificates (such as user
certificates or device certificates) and sign them, sign certificate
requests, and revoke certificates.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;export KEYLOC=&amp;quot;/var/db/ca&amp;quot;
# Create a root CA key for &amp;quot;genfic&amp;quot;
# (openssl questions and other output not shown)
certcli.sh -r genfic
# Create a subkey, signed by the &amp;quot;genfic&amp;quot; key, for users
certcli.sh -p genfic -c genfic-user
# Create a user certificate
certcli.sh -p genfic-user -R /var/db/ca/myUserId
# Sign a certificate
certcli.sh -p genfic-user -s /var/db/ca/requests/someUser.csr
# Revoke a certificate
certcli.sh -p genfic-user -x myuser@genfic.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From a security point of view, I currently focus on two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ca_private_key_t&lt;/code&gt; is for the private key and should not be
    accessible by anyone, anywhere, anytime, except through the
    management script itself (which will run as &lt;code&gt;ca_cli_t&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ca_misc_t&lt;/code&gt; is for the other related files, such as certificates,
    revocation lists, serial information, etc. If this would be "for
    real" I'd probably make a bit more types for this depending on the
    accesses, but for this post this suffices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to provide the necessary support policy-wise, the following
types also are declared:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ca_cli_exec_t&lt;/code&gt; as the entrypoint for the script&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ca_misc_tmp_t&lt;/code&gt; as the temporary file type used by OpenSSL when
    handling certificates (it is not used for the private key afaics,
    but it should still be sufficiently - and perhaps even equally
    well - protected like the private key&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So let's start with this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(ca, 1.0.0)

# CA management script and domain
type ca_cli_t;
type ca_cli_exec_t;
domain_base_type(ca_cli_t)
fs_associate(ca_cli_exec_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Above, I declared the two types &lt;code&gt;ca_cli_t&lt;/code&gt; and &lt;code&gt;ca_cli_exec_t&lt;/code&gt;. Then,
two non-standard approaches were followed.&lt;/p&gt;
&lt;p&gt;Normally, application domains are granted through &lt;code&gt;application_type()&lt;/code&gt;,
&lt;code&gt;application_domain()&lt;/code&gt; or even &lt;code&gt;userdom_user_application_domain()&lt;/code&gt;.
Which interface you use depends on the privileges you want to grant on
the domain, but also which existing privileges should also be applicable
to the domain. Make sure you review the interfaces. For instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# seshowif application_domain
interface(`application_type&amp;#39;,`
        gen_require(`
                attribute application_domain_type;
        &amp;#39;)

        typeattribute $1 application_domain_type;

        # start with basic domain
        domain_type($1)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This means that the assigned domain (&lt;code&gt;ca_cli_t&lt;/code&gt; in our example) would be
assigned the &lt;code&gt;application_domain_type&lt;/code&gt; attribute, as well as the
&lt;code&gt;domain&lt;/code&gt; attribute and other privileges. If we really want to prevent
any access to the &lt;code&gt;ca_cli_t&lt;/code&gt; domain for handling the certificates, we
need to make sure that the lowest possible privileges are assigned.&lt;/p&gt;
&lt;p&gt;The same is true for the file type &lt;code&gt;ca_cli_exec_t&lt;/code&gt;. Making it through
&lt;code&gt;files_type()&lt;/code&gt; interface would assign the &lt;code&gt;file_type&lt;/code&gt; attribute to it,
and other domains might have access to &lt;code&gt;file_type&lt;/code&gt;. So all I do here is
to allow the type &lt;code&gt;ca_cli_exec_t&lt;/code&gt; to be associated on a file system.&lt;/p&gt;
&lt;p&gt;Similarly, I define the remainder of file types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type ca_private_key_t;
fs_associate(ca_private_key_t)

type ca_misc_tmp_t;
fs_associate(ca_misc_tmp_t)
fs_associate_tmpfs(ca_misc_tmp_t)

type ca_misc_t;
fs_associate(ca_misc_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, grant the CA handling script (which will run as &lt;code&gt;ca_cli_t&lt;/code&gt;) the
proper access to these types.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow ca_cli_t ca_misc_t:dir create_dir_perms;
manage_files_pattern(ca_cli_t, ca_misc_t, ca_misc_t)

allow ca_cli_t ca_private_key_t:dir create_dir_perms;
manage_files_pattern(ca_cli_t, ca_private_key_t, ca_private_key_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This of course heavily depends on the script itself. Mine creates a
directory "private", so needs the proper rights on the
&lt;code&gt;ca_private_key_t&lt;/code&gt; type for directories as well. The "private" directory
is created in a generic directory (which is labeled as &lt;code&gt;ca_misc_t&lt;/code&gt;) so I
can also create a file transition. This means that the SELinux policy
will automatically assign the &lt;code&gt;ca_private_key_t&lt;/code&gt; type to a directory,
created in a directory with label &lt;code&gt;ca_misc_t&lt;/code&gt;, if created by the
&lt;code&gt;ca_cli_t&lt;/code&gt; domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;filetrans_pattern(ca_cli_t, ca_misc_t, ca_private_key_t, dir, &amp;quot;private&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, &lt;code&gt;ca_cli_t&lt;/code&gt; is a domain used for a shell script, which in my case
also requires the following permissions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Handling pipes between commands
allow ca_cli_t self:fifo_file rw_fifo_file_perms;
# Shell script...
corecmd_exec_shell(ca_cli_t)
# ...which invokes regular binaries
corecmd_exec_bin(ca_cli_t)
# Allow output on the screen
getty_use_fds(ca_cli_t)
userdom_use_user_terminals(ca_cli_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I still need to mark &lt;code&gt;ca_cli_exec_t&lt;/code&gt; as an entrypoint for
&lt;code&gt;ca_cli_t&lt;/code&gt;, meaning that the &lt;code&gt;ca_cli_t&lt;/code&gt; domain can only be accessed
(transitioned to) through the execution of a file with label
&lt;code&gt;ca_cli_exec_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow ca_cli_t ca_cli_exec_t:file entrypoint;
allow ca_cli_t ca_cli_exec_t:file { mmap_file_perms ioctl lock };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Normally, the above is granted through the invocation of the
&lt;code&gt;application_domain(ca_cli_t, ca_cli_exec_t)&lt;/code&gt; but as mentioned before,
this would also assign attributes that I explicitly want to prevent in
this example.&lt;/p&gt;
&lt;p&gt;Next, the &lt;code&gt;openssl&lt;/code&gt; application, which the script uses extensively, also
requires additional permissions. As the &lt;code&gt;openssl&lt;/code&gt; command just runs in
the &lt;code&gt;ca_cli_t&lt;/code&gt; domain, I extend the privileges for this domain more:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Read access on /proc files
kernel_read_system_state(ca_cli_t)
# Access to random devices
dev_read_rand(ca_cli_t)
dev_read_urand(ca_cli_t)
# Regular files
files_read_etc_files(ca_cli_t)
miscfiles_read_localization(ca_cli_t)
# /tmp access
fs_getattr_tmpfs(ca_cli_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Also, the following file transition is created: when OpenSSL creates a
temporary file in &lt;code&gt;/tmp&lt;/code&gt;, this file should immediately be assigned the
&lt;code&gt;ca_misc_tmp_t&lt;/code&gt; type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# File transition in /tmp to ca_misc_tmp_t
files_tmp_filetrans(ca_cli_t, ca_misc_tmp_t, file)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this in place, the application works just fine - all I need to do
is have an initial location marked as &lt;code&gt;ca_misc_t&lt;/code&gt;. For now, none of the
users have the rights to do so, so I create three additional interfaces
to be used against other user domains.&lt;/p&gt;
&lt;p&gt;The first one is to allow user domains to use the CA script. This is
handled by the &lt;code&gt;ca_role()&lt;/code&gt; interface. In order to support such an
interface, let's first create the &lt;code&gt;ca_roles&lt;/code&gt; role attribute in the &lt;code&gt;.te&lt;/code&gt;
file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;attribute_role ca_roles;
role ca_roles types ca_cli_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I can define the &lt;code&gt;ca_role()&lt;/code&gt; interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;interface(`ca_role&amp;#39;,`
  gen_require(`
    attribute_role ca_roles;
    type ca_cli_t, ca_cli_exec_t;
    type ca_misc_t;
  &amp;#39;)

  # Allow the user role (like sysadm_r) the types granted to ca_roles
  roleattribute $1 ca_roles;

  # Read the non-private key files and directories
  allow $2 ca_misc_t:dir list_dir_perms;
  allow $2 ca_misc_t:file read_file_perms;

  # Allow to transition to ca_cli_t by executing a ca_cli_exec_t file
  domtrans_pattern($2, ca_cli_exec_t, ca_cli_t)

  # Look at the process info
  ps_process_pattern($2, ca_cli_t)

  # Output (and redirect) handling
  allow ca_cli_t $2:fd use;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This role allows to run the command, but we still don't have the rights
to create a &lt;code&gt;ca_misc_t&lt;/code&gt; directory. So another interface is created,
which is granted to &lt;em&gt;regular&lt;/em&gt; system administrators (as the &lt;code&gt;ca_role()&lt;/code&gt;
might be granted to non-admins as well, who can invoke the script
through &lt;code&gt;sudo&lt;/code&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;interface(`ca_sysadmin&amp;#39;,`
  gen_require(`
    type ca_misc_t;
    type ca_private_key_t;
  &amp;#39;)

  # Allow the user relabel rights on ca_misc_t
  allow $1 ca_misc_t:dir relabel_dir_perms;
  allow $1 ca_misc_t:file relabel_file_perms;

  # Allow the user to label /to/ ca_private_key_t (but not vice versa)
  allow $1 ca_private_key_t:dir relabelto_dir_perms;
  allow $1 ca_private_key_t:file relabelto_file_perms;

  # Look at regular file/dir info
  allow $1 ca_misc_t:dir list_dir_perms;
  allow $1 ca_misc_t:file read_file_perms;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ca_sysadmin()&lt;/code&gt; interface can also be assigned to the &lt;code&gt;setfiles_t&lt;/code&gt;
command so that relabel operations (and file system relabeling) works
correctly.&lt;/p&gt;
&lt;p&gt;Finally, a real administrative interface is created that also has
relabel &lt;em&gt;from&lt;/em&gt; rights (so any domain granted this interface will be able
- if Linux allows it and the type the operation goes to/from is allowed
- to change the type of private keys to a regular file). This one should
&lt;em&gt;only&lt;/em&gt; be assigned to a rescue user (if any). Also, this interface is
allowed to label CA management scripts.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;interface(`ca_admin&amp;#39;,`
  gen_require(`
    type ca_misc_t, ca_private_key_t;
    type ca_cli_exec_t;
  &amp;#39;)

  allow $1 { ca_misc_t ca_private_key_t }:dir relabel_dir_perms;
  allow $1 { ca_misc_t ca_private_key_t }:file relabel_file_perms;

  allow $1 ca_cli_exec_t:file relabel_file_perms;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So regular system administrators would be assigned the &lt;code&gt;ca_sysadmin()&lt;/code&gt;
interface as well as the &lt;code&gt;setfiles_t&lt;/code&gt; domain; CA handling roles would be
granted the &lt;code&gt;ca_role()&lt;/code&gt; interface. The &lt;code&gt;ca_admin()&lt;/code&gt; interface would only
be granted on the rescue (or super-admin).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Regular system administrators
ca_sysadmin(sysadm_t)
ca_sysadmin(setfiles_t)
# Certificate administrator
ca_role(certadmin_r, certadmin_t)
# Security administrator
ca_admin(secadm_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="ca"></category><category term="certcli"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Limiting file access with SELinux alone?</title><link href="https://blog.siphos.be/2013/12/limiting-file-access-with-selinux-alone/" rel="alternate"></link><published>2013-12-31T21:18:00+01:00</published><updated>2013-12-31T21:18:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-31:/2013/12/limiting-file-access-with-selinux-alone/</id><summary type="html">&lt;p&gt;While writing a small script to handle simple certificate authority
activities using OpenSSL, I considered how to properly protect the files
that OpenSSL uses for these activities. As you are probably aware, a
system that hosts the necessary files for CA activities (like signing
certificate requests) should be very secure …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While writing a small script to handle simple certificate authority
activities using OpenSSL, I considered how to properly protect the files
that OpenSSL uses for these activities. As you are probably aware, a
system that hosts the necessary files for CA activities (like signing
certificate requests) should be very secure, and the private key used to
sign (and the private subkeys) should be very well protected.&lt;/p&gt;
&lt;p&gt;Without the help of an &lt;a href="https://en.wikipedia.org/wiki/Hardware_security_module"&gt;Hardware Security Module
(HSM)&lt;/a&gt; these
private keys are just plain text files on the file system. Access to
this file system should therefor be very well audited and protected.&lt;/p&gt;
&lt;p&gt;It of course starts with proper Discretionary Access Control (DAC)
protections on Linux. The private key should only be accessible by the
(technical) user used to operate the CA activities. Next, access to this
user should also be properly protected - if the CA activities are not
done through the root account, make sure that all users who can get root
access on the system and to the (technical) user used to perform the CA
activities are trusted.&lt;/p&gt;
&lt;p&gt;Sometimes however this isn't sufficient, or you want to protect it even
more. With SELinux, we can implement a Mandatory Access Control (MAC)
policy to further restrict access to the private key. The idea is to
only allow the application (in my case the script) that performs the CA
activities access to the private key, and nothing more. Even users who
can get root access, but do not have the privileges SELinux-wise to
execute the CA management script (with the proper domain transition)
should not have any access to the private key.&lt;/p&gt;
&lt;p&gt;I'll discuss a sample policy for that later, but for now I want to focus
on what that would mean - not allowing other users access.&lt;/p&gt;
&lt;p&gt;When users log on on a SELinux-enabled system, they (well, the process
that starts the user session) get assigned a security context. This
security context defines what the user is allowed to do on the system.
And although it is "easy" to disallow a domain read access to a
particular file, we must consider all other activities that the user can
perform.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;First risk: direct file access&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Assume that a user is logged on with the &lt;code&gt;unconfined_t&lt;/code&gt; domain (an
entire context is more than just the domain, but let's stick to this for
now). The &lt;code&gt;unconfined_t&lt;/code&gt; domain is an extremely powerful domain -
basically, SELinux will not prevent much. That doesn't mean that SELinux
is disabled, but rather that the &lt;code&gt;unconfined_t&lt;/code&gt; domain is granted many
privileges. So if a user is in the &lt;code&gt;unconfined_t&lt;/code&gt; domain &lt;em&gt;and&lt;/em&gt; is not
prevented by the standard Linux access controls (for instance because he
is root), he can do basically everything.&lt;/p&gt;
&lt;p&gt;SELinux-wise, we can still create a new type that &lt;code&gt;unconfined_t&lt;/code&gt; has no
immediate access to. By creating a new type (say &lt;code&gt;ca_private_key_t&lt;/code&gt;) and
not assign it any attributes that the &lt;code&gt;unconfined_t&lt;/code&gt; domain has
privileges towards, the user would still not be able to access the file
directly. The same is true for the &lt;code&gt;sysadm_t&lt;/code&gt; domain (a still
privileged, yet slightly more restricted user domain designed for system
administrators). However, such users could still access the file
indirectly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Second risk: indirect access through new SELinux policies&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important privilege that these users have is that they can load a new
SELinux policy, or disable SELinux (actually switch it to permissive
mode) if the Linux kernel has this enabled. By allowing users to load a
new policy, they can basically create a SELinux policy module that
grants them the necessary accesses towards the file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow sysadm_t ca_misc_file_t:file manage_file_perms;
allow sysadm_t ca_misc_file_t:dir manage_dir_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So in order to prevent this, we have to make sure that these users can
either not manipulate the SELinux policy - or make sure users on the
system do not get access to these domains to begin with. Preventing
loading new policies can be handled by the Linux kernel itself
(&lt;code&gt;CONFIG_SECURITY_SELINUX_DEVELOP&lt;/code&gt; should not be set then) and through
SELinux booleans (&lt;code&gt;secure_mode_policyload&lt;/code&gt; should be set to &lt;code&gt;on&lt;/code&gt; and
toggling the boolean off again should be prohibited). Still, it makes
more sense to restrict people with access to these roles - something
I'll definitely come back to at a later point.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Third risk: indirect access through attributes&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Another privilege that needs to be watched for is the ability to change
the context of a file. If the &lt;code&gt;ca_private_key_t&lt;/code&gt; type would not be
declared properly, then the type might be assigned an attribute that
domains have privileges against. Consider the &lt;code&gt;file_type&lt;/code&gt; attribute,
granted to file types (at least the name makes sense ;-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# seinfo -tca_private_key_t -x
  ca_private_key_t
    file_type
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The moment a domain has been granted read access to the &lt;code&gt;file_type&lt;/code&gt;
files, then it has read access to the &lt;code&gt;ca_private_key_t&lt;/code&gt; type. In other
words, while designing the policy, make sure that all granted
permissions are accounted for.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fourth risk: "raw" file system (or memory) access&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So if SELinux itself would not allow access to the file(s) anymore, are
we done yet? Of course not. In the end, the keys are stored on the file
system, which is located on partitions or disks, accessible by
privileged users. Someone with direct read access to the block devices
can still obtain the file directly, so even that should be properly
governed.&lt;/p&gt;
&lt;p&gt;This even extends towards memory access, because the private key might
be cached in buffers (by the kernel) or even mapped in memory (even for
a short while) and made accessible through direct memory access.&lt;/p&gt;
&lt;p&gt;Such accesses might not be available to many users, but don't forget
that on the system other applications are running as well. Some daemons
might have the necessary privileges to access file systems directly, or
the memory. Some users might have the rights to execute commands that
have direct file system access (or memory). Making sure that &lt;em&gt;all&lt;/em&gt;
domains that have these accesses are properly audited (including the
access to those domains) will already be quite a challenge.&lt;/p&gt;
&lt;p&gt;For a system that acts as a certificate handling system, it makes sense
to limit exposure to a bare minimum as that makes the above auditing a
bit less daunting.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
Ok ok, let's assume the security administrator has thought of all those
things and SELinux policy-wise properly prevents any access. That's
enough, right?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fifth risk: authentication and authorization access&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Of course not. Some processes or users might have access to the
authentication files on the system (and I'm not only talking about the
&lt;code&gt;/etc/shadow&lt;/code&gt; and &lt;code&gt;/etc/passwd&lt;/code&gt; files, but also the &lt;code&gt;/etc/pam.d/*&lt;/code&gt;
files, or the libraries used by the PAM modules in &lt;code&gt;/lib/security&lt;/code&gt;, or
modify rights on binaries likely executed by administrators who do have
rights we want - there might always be a "recovery user" enabled just in
case things really go wrong, but such "recovery users" imply that rights
are still granted somewhere.&lt;/p&gt;
&lt;p&gt;When modify access is granted to any of the authentication or
authorization services, then users can grant them privileges you don't
want to give them. So not only should access to &lt;code&gt;ca_private_key_t&lt;/code&gt;,
&lt;code&gt;memory_device_t&lt;/code&gt;, &lt;code&gt;device_node&lt;/code&gt; and &lt;code&gt;fixed_disk_device_t&lt;/code&gt; be properly
governed - also &lt;code&gt;etc_t&lt;/code&gt; (as this is the default for PAM files - this
really should be worked on) and &lt;code&gt;lib_t&lt;/code&gt; are important types, and these
are very, very open (many domains have write access to those by
default).&lt;/p&gt;
&lt;p&gt;Fine. Let's "hypothetically" consider that the security engineer has
thought about all SELinux accesses and made a perfect policy. Happy now?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sixth risk: system boot privileges&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Actually no... first of all, some users might be able to reboot the
system with updated boot parameters, or even with a different kernel.
With this at hand, they can disable SELinux and still access the file.
So make sure that rebooting the system still happens securely (you can
use secure boot, signed kernels, ... or at least focus on boot loader
password protection and console access). The
&lt;a href="https://wiki.gentoo.org/wiki/Project:Integrity"&gt;Integrity&lt;/a&gt; subproject
of the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened&lt;/a&gt;
project will focus on these matters more (just waiting for an
EFI-enabled system to properly start documenting things). In the mean
time, make sure that the Linux system itself is properly secured.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Seventh risk: direct system access&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yet having a secured boot also doesn't protect us completely. If there
is access to the system physically, users can still mount the disk on
their system and access the files. So one might want to consider
encrypting the partition on which the keys are stored. But the
encryption key for the partition should still be available somewhere,
because regular administration might need to reboot the system.&lt;/p&gt;
&lt;p&gt;A reasonably secure system would keep the encryption key in a HSM device
(which we don't have - see beginning of this post) or only have it
decrypted in a secure environment (like a TPM chip on more modern
systems). Handling TPM and securing keys is definitely also something to
focus on further in the Integrity subproject.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eighth risk: backups&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And if you take backups (which we of course all do) then those backups
should be properly protected as well. We might want to create encrypted
backups (meaning that the CA script should allow for encrypting and
decrypting the private key) and &lt;em&gt;not store the encryption key with the
backup&lt;/em&gt;. And before asking me why I've emphasized this - I've seen it
before, and I'll probably see it again in the future. Don't laugh.&lt;/p&gt;
&lt;p&gt;Almost starts sounding like a nightmare, right? There might even be more
risks that I haven't covered here (and I haven't even discussed
potential vulnerabilities in the CA management script itself, or even in
OpenSSL or other tools like the &lt;a href="http://it.slashdot.org/story/13/12/18/2122226/scientists-extract-rsa-key-from-gnupg-using-sound-of-cpu"&gt;let's hear the encryption
key&lt;/a&gt;
attack). Welcome to the world of security ;-)&lt;/p&gt;
&lt;p&gt;In any case, in the next post I'll focus on the SELinux policy I wrote
up for the simple script I created. But all the above is just to show
that SELinux is not the answer - it is merely a part in an entire
security architecture. A flexible, powerful part... but still a part.&lt;/p&gt;</content><category term="SELinux"></category><category term="access"></category><category term="acl"></category><category term="file-access"></category><category term="Gentoo"></category><category term="selinux"></category></entry><entry><title>Upgrading old Gentoo installations</title><link href="https://blog.siphos.be/2013/12/upgrading-old-gentoo-installations/" rel="alternate"></link><published>2013-12-29T14:18:00+01:00</published><updated>2013-12-29T14:18:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-29:/2013/12/upgrading-old-gentoo-installations/</id><summary type="html">&lt;p&gt;Today I got "pinged" on &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=463240"&gt;bug
#463240&lt;/a&gt; about the
difficulty of upgrading a Gentoo Linux deployment after a long time of
inactivity on the system. We already have an &lt;a href="https://wiki.gentoo.org/wiki/Upgrading_Gentoo"&gt;Upgrading
Gentoo&lt;/a&gt; article on the
Gentoo wiki that describes in great detail how upgrades can be
accomplished. But one of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I got "pinged" on &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=463240"&gt;bug
#463240&lt;/a&gt; about the
difficulty of upgrading a Gentoo Linux deployment after a long time of
inactivity on the system. We already have an &lt;a href="https://wiki.gentoo.org/wiki/Upgrading_Gentoo"&gt;Upgrading
Gentoo&lt;/a&gt; article on the
Gentoo wiki that describes in great detail how upgrades can be
accomplished. But one of the methods I personally suggest most is to do
small, incremental upgrades.&lt;/p&gt;
&lt;p&gt;Say you have a system from early 2009. Not too long ago, but also not
that recent anymore. If you would upgrade that system using the regular
approach, your system would probably be using a non-existing profile
(the &lt;code&gt;/etc/make.profile&lt;/code&gt; symlink would point to a non-existing
location), and if you switch the profile to an existing one, you might
have to deal with problems like the profile requiring certain features
(or EAPI version) that the software currently available on your system
doesn't support.&lt;/p&gt;
&lt;p&gt;This problem is mentioned in the upgrade guide through the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure your Portage is updated before performing any profile
changes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, it does not tell how to update Portage. In my opinion the best
way forward is to install an older Portage tree snapshot (somewhat more
recent than your own deployment) and upgrade &lt;em&gt;at least&lt;/em&gt; portage, perhaps
also the packages belonging to the &lt;em&gt;system&lt;/em&gt; set. So for a system that
has not been updated since January 2009, you might want to try the
portage tree snapshot of July 2009, then January 2010, then July 2010,
etc. until you have a recent deployment again.&lt;/p&gt;
&lt;p&gt;All that is left for you to do is to find such a snapshot (as the
Portage tree snapshots from the mirrors are cleaned out after a few
months). I try to keep a set of &lt;a href="http://dev.gentoo.org/~swift/snapshots/"&gt;Portage tree snapshots
available&lt;/a&gt; with a 2-month
period which should be sufficient for most users to gradually upgrade
their systems.&lt;/p&gt;
&lt;p&gt;Considering I've used this method a few times already I'm going to add
them to the &lt;a href="https://wiki.gentoo.org/wiki/Upgrading_Gentoo"&gt;upgrading
instructions&lt;/a&gt; as well.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="portage"></category><category term="snapshot"></category><category term="upgrade"></category></entry><entry><title>Giving weights to compliance rules</title><link href="https://blog.siphos.be/2013/12/giving-weights-to-compliance-rules/" rel="alternate"></link><published>2013-12-26T04:13:00+01:00</published><updated>2013-12-26T04:13:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-26:/2013/12/giving-weights-to-compliance-rules/</id><summary type="html">&lt;p&gt;Now that we wrote up a few OVAL statements and used those instead of SCE
driven checks (where possible), let's finish up and go back to the XCCDF
document and see how we can put weights in place.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;CVE (Common Vulnerability Exposure)&lt;/strong&gt; standard allows for
vulnerabilities to be given …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Now that we wrote up a few OVAL statements and used those instead of SCE
driven checks (where possible), let's finish up and go back to the XCCDF
document and see how we can put weights in place.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;CVE (Common Vulnerability Exposure)&lt;/strong&gt; standard allows for
vulnerabilities to be given weights through a scoring mechanism called
&lt;strong&gt;CVSS (Common Vulnerability Scoring System)&lt;/strong&gt;. The method for giving
weights to such vulnerabilities is based on several factors, which you
can play with through an &lt;a href="https://nvd.nist.gov/cvss.cfm?calculator&amp;amp;version=2"&gt;online CVSS
calculator&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Giving weights on a vulnerability based on these metrics is not that
difficult, but what about compliance misconfigurations?&lt;/p&gt;
&lt;p&gt;There is a suggested standard for this, &lt;strong&gt;CCSS (Common Configuration
Scoring System)&lt;/strong&gt; which is based on the CVSS scoring and CMSS scoring.
Especially the base scoring is tailored to the CVSS scoring, so let's
look at an example from the &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/guide-gentoo-xccdf.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
(still in draft):&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;The base scoring of a misconfiguration focuses on the following items:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Access Vector (AV)&lt;/dt&gt;
&lt;dd&gt;How can the misconfiguration be "reached" or exploited - Local (on
the system), Adjacent Network or Network&lt;/dd&gt;
&lt;dt&gt;Access Complexity (AC)&lt;/dt&gt;
&lt;dd&gt;How complex would it be to exploit the misconfiguration - High,
Medium or Low&lt;/dd&gt;
&lt;dt&gt;Authentication (Au)&lt;/dt&gt;
&lt;dd&gt;Does the attacker need to be authenticated in order to exploit the
misconfiguration - None, Single (one account) or Multiple (several
accounts or multi-factor authenticated)&lt;/dd&gt;
&lt;dt&gt;Confidentiality (C)&lt;/dt&gt;
&lt;dd&gt;Does a successful exploit have impact on the confidentiality of the
system or data (None, Partial or Complete)&lt;/dd&gt;
&lt;dt&gt;Integrity (I)&lt;/dt&gt;
&lt;dd&gt;Does a successful exploit have impact on the integrity of the system
or data (None, Partial or Complete)&lt;/dd&gt;
&lt;dt&gt;Availability (A)&lt;/dt&gt;
&lt;dd&gt;Does a successful exploit have impact on the availability of the
system or data (None, Partial or Complete)&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;In order to exploit that &lt;code&gt;/tmp&lt;/code&gt; is not on a separate file system, we can
think about dumping lots of information in &lt;code&gt;/tmp&lt;/code&gt;, flooding the root
file system. This is simple to accomplish locally and requires a single
authentication (you need to be authenticated on the system). Once
performed, this only impacts availability.&lt;/p&gt;
&lt;p&gt;The CCSS (and thus CVSS) base vector looks like so:
&lt;code&gt;AV:L/AC:L/Au:S/C:N/I:N/A:C&lt;/code&gt; and gives a base score of 4.6, which is
reflected in the XCCDF in the &lt;code&gt;weight="4.6"&lt;/code&gt; attribute.&lt;/p&gt;
&lt;p&gt;The severity I give in the XCCDF is "gut feeling". Basically, I use the
following description:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high constitutes a grave or critical problem. A rule with this
    severity MUST be tackled as it detected a misconfiguration that is
    easily exploitable and could lead to full system compromise.&lt;/li&gt;
&lt;li&gt;medium reflects a fairly serious problem. A rule with this severity
    SHOULD be tackled as it detected a misconfiguration that is
    easily exploitable.&lt;/li&gt;
&lt;li&gt;low reflects a non-serious problem. A rule with this severity has
    detected a misconfiguration but its influence on the overall system
    security is minor (if other compliance rules are followed).&lt;/li&gt;
&lt;li&gt;info reflects an informational rule. Failure to comply with this
    rule does not mean failure to comply with the document itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, you can put your own weights and severities in your XCCDF
documents. Important however is to make sure it is properly documented -
other people who read the document must be aware of the consequences of
the rules if they are not compliant.&lt;/p&gt;
&lt;p&gt;By introducing weights and severities, administrators of systems that
are not compliant (or of a large set of systems) can prioritize which
misconfigurations or vulnerabilities they will handle first. And it
reduces the amount of discussions as well, because without these, your
administrators will start debating what to tackle first, each with their
own vision and opinion. Which is great, but not when time is ticking.
Having a predefined priority list makes it clear how to react now.&lt;/p&gt;
&lt;p&gt;That's it for this post series. I hope you enjoyed it and learned from
it. Of course, this wont be the last post related to SCAP so stay tuned
for more ;-)&lt;/p&gt;
&lt;p&gt;This post is the final one in a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices – XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;Documenting a bit more than just
    descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;Running a bit with the XCCDF
    document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/remediation-through-scap/"&gt;Remediation through
    SCAP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/what-is-oval/"&gt;What is OVAL?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/doing-a-content-check-with-oval/"&gt;Doing a content check with
    OVAL&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="ccss"></category><category term="cvss"></category><category term="scap"></category><category term="xccdf"></category></entry><entry><title>Doing a content check with OVAL</title><link href="https://blog.siphos.be/2013/12/doing-a-content-check-with-oval/" rel="alternate"></link><published>2013-12-24T04:25:00+01:00</published><updated>2013-12-24T04:25:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-24:/2013/12/doing-a-content-check-with-oval/</id><summary type="html">&lt;p&gt;Let's create an OVAL check to see if &lt;code&gt;/etc/inittab&lt;/code&gt;'s single user
definitions only refer to &lt;code&gt;/sbin/sulogin&lt;/code&gt; or &lt;code&gt;/sbin/rc single&lt;/code&gt;. First,
the skeleton:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;The first thing we notice is that there are several namespaces defined
within OVAL. These namespaces refer to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's create an OVAL check to see if &lt;code&gt;/etc/inittab&lt;/code&gt;'s single user
definitions only refer to &lt;code&gt;/sbin/sulogin&lt;/code&gt; or &lt;code&gt;/sbin/rc single&lt;/code&gt;. First,
the skeleton:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;The first thing we notice is that there are several namespaces defined
within OVAL. These namespaces refer to the platforms on which the tests
can be executed. OVAL has independent definitions, unix-global
definitions or linux-specific definitions. You can find the overview of
&lt;a href="http://oval.mitre.org/language/version5.10.1/"&gt;all supported schemas and definitions
online&lt;/a&gt; - definitely
something to bookmark if you plan on developing your own OVAL checks.&lt;/p&gt;
&lt;p&gt;So let's create the definition:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;There is lots of information to be found in this simple snippet.&lt;/p&gt;
&lt;p&gt;First of all, notice the &lt;code&gt;class="compliance"&lt;/code&gt; part. OVAL definitions can
be given a class that informs the OVAL interpreter what kind of test it
is.&lt;/p&gt;
&lt;p&gt;Supported classes are:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;compliance&lt;/dt&gt;
&lt;dd&gt;Does the system adhere to a predefined wanted state&lt;/dd&gt;
&lt;dt&gt;inventory&lt;/dt&gt;
&lt;dd&gt;Is the given software or hardware available/installed on the system&lt;/dd&gt;
&lt;dt&gt;patch&lt;/dt&gt;
&lt;dd&gt;Is the selected patch installed on the system&lt;/dd&gt;
&lt;dt&gt;vulnerability&lt;/dt&gt;
&lt;dd&gt;Is the system vulnerable towards this particular exposure (CVE)&lt;/dd&gt;
&lt;dt&gt;miscellaneous&lt;/dt&gt;
&lt;dd&gt;Everything that doesn't fit the above&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Next, we see metadata that tells the OVAL interpreter that the
definition applies to Unix family systems, and more specifically a
Gentoo Linux system. However, this is not a CPE entry
(&lt;em&gt;cpe:/o:gentoo:linux&lt;/em&gt;). The idea is that the OVAL Interpreter should
interpret the information as it wants without focusing on CPE details -
I think (I might be mistaken though) because the SCAP standard does not
want to introduce loops - a CPE that refers to an OVAL to validate,
which in turn refers to the same CPE.&lt;/p&gt;
&lt;p&gt;Also, a reference is included in the OVAL. Remember that we also had
references in the XCCDF document? Well, the same is true for OVAL
statements - you can add in references that help administrators get more
information about a definition. In this case, it refers to a &lt;strong&gt;CCE
(Common Configuration Enumeration)&lt;/strong&gt; entry. You can find all official
CCE entries &lt;a href="https://nvd.nist.gov/cce/index.cfm"&gt;online as well&lt;/a&gt;. This
particular one, CCE-4241-6, sais:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;CCE-4241-6  Platform: rhel5     Date: (C)2011-10-07   (M)2013-11-28

The requirement for a password to boot into single-user mode should be configured correctly.

Parameter: enabled/disabled

Technical Mechanism: via /etc/inittab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By requiring &lt;strong&gt;sulogin&lt;/strong&gt; or &lt;strong&gt;rc single&lt;/strong&gt; in &lt;code&gt;inittab&lt;/code&gt;, Gentoo Linux
will ask for the root password before granting a shell, thereby
complying with the requirement to have a password before providing a
shell in single-user mode.&lt;/p&gt;
&lt;p&gt;Finally, the definition refers to a single test, which we will now look
into:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;This particular test is part of the &lt;em&gt;independent&lt;/em&gt; definitions. Checking
the content of a file is something all platforms support. Within this
independent definition set, a &lt;a href="http://oval.mitre.org/language/version5.10.1/ovaldefinition/documentation/independent-definitions-schema.html"&gt;large set of
tests&lt;/a&gt;
are supported, including file hash checking (does the checksum of a file
still match), environment variable test (verifying the existence and
content of an environment variable), LDAP tests and also text file
content tests.&lt;/p&gt;
&lt;p&gt;In the test, there are two important attributes to closely look into:
&lt;code&gt;check&lt;/code&gt; and &lt;code&gt;check_existence&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;check_existence&lt;/code&gt; attribute tells the OVAL interpreter how to deal
with the object definition. In our case, the object will refer to the
lines in the &lt;code&gt;/etc/inittab&lt;/code&gt; file that match a certain pattern. With
&lt;code&gt;check_existence="at_least_one_exists"&lt;/code&gt; the OVAL interpreter knows it
has to have at least one line that matches the pattern before it can
continue. If no line matches, then the test fails.&lt;/p&gt;
&lt;p&gt;Other values for &lt;code&gt;check_existence&lt;/code&gt; are "all_exist" (every object
described must exist), any_exist (doesn't matter if zero, one or more
exists), none_exist (no object described must exist) and
"only_one_exists" (one, and only one match for the described objects
must exist).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;check&lt;/code&gt; attributes tells the OVAL interpreter how to match the
object (if there is one) with the state. In our example, &lt;code&gt;check="all"&lt;/code&gt;
tells the OVAL interpreter that all lines that match the object
definition must also match the state definition.&lt;/p&gt;
&lt;p&gt;Other values for &lt;code&gt;check&lt;/code&gt; are "at least one", "none satisfy" and "only
one". These should be self-explanatory. Notice that there are no
underscores involved here (unlike with the &lt;code&gt;check_existence&lt;/code&gt; attribute).&lt;/p&gt;
&lt;p&gt;See the &lt;a href="https://oval.mitre.org/language/version5.10.1/ovaldefinition/documentation/oval-common-schema.html"&gt;common
schema&lt;/a&gt;
for more general OVAL attribute information.&lt;/p&gt;
&lt;p&gt;The test refers to the following object:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;The object represents lines in the &lt;code&gt;/etc/inittab&lt;/code&gt; file that match the
expression &lt;code&gt;^[\S]+:S:[\S]+:.*&lt;/code&gt;. The OVAL definition uses &lt;a href="http://oval.mitre.org/language/about/perlre.html"&gt;perl-style
regular expressions&lt;/a&gt;,
so this means that the lines must start with a non-whitespace string,
followed by a colon (:), followed by the letter "S", followed by a
colon, followed by non-whitespace string, followed by colon and then a
remainder string.&lt;/p&gt;
&lt;p&gt;Also, the object evaluates if at least one such line is found.&lt;/p&gt;
&lt;p&gt;The state, also referred to by the test, looks like so:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;Here again we see a regular expression; this time, the expression sais
that the line must start with "su" and that the fourth field equals
&lt;code&gt;/sbin/rc single&lt;/code&gt; or &lt;code&gt;/sbin/sulogin&lt;/code&gt;. In our example, if there is at
least one "single user" line that does not match this expression, then
the OVAL statement will return a failure and the system is
non-compliant.&lt;/p&gt;
&lt;p&gt;Now you could be wondering if this is the best approach. We can create
an object that refers to all single-user lines in &lt;code&gt;/etc/inittab&lt;/code&gt; that do
not comply with the expression just in the object definition. The
expression would be more complex by itself, but wouldn't need a state
anymore. True, but the advantage here is that the object itself matches
all single user lines, and can be reused later in other tests. Also, if
we later evaluate the OVAL statements, we will get an overview of all
lines that match the object (and then evaluate these lines against the
state) - similar to the script output we got with SCE tests.&lt;/p&gt;
&lt;p&gt;We can create other OVALs for all other tests. To refer to these OVAL
tests in an XCCDF document, take a look at the following example:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;Instead of referring to the SCE engine (with
&lt;code&gt;system="http://open-scap.org/page/SCE"&lt;/code&gt;) we refer to the OVAL with
&lt;code&gt;system="http://oval.mitre.org/XMLSchema/oval-definitions-5"&lt;/code&gt;, point the
XCCDF interpreter where the OVAL statements are stored in
&lt;code&gt;href="gentoo-oval.xml"&lt;/code&gt; and what definition we want to test
(&lt;code&gt;oval:org.gentoo.dev.swift:def:22&lt;/code&gt;). The XCCDF interpreter will then
pass this information on to the OVAL interpreter (in case of openscap,
this is the same tool, but it doesn't have to be) so it can evaluate the
right OVAL statement on the system.&lt;/p&gt;
&lt;p&gt;In the next post, I'll use the &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/guide-gentoo-xccdf.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
as a guide to explain how to further structure and document things in
XCCDF/OVAL.&lt;/p&gt;
&lt;p&gt;This post is part of a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices - XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;Documenting a bit more than just
    descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;Running a bit with the XCCDF
    document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/remediation-through-scap/"&gt;Remediation through
    SCAP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/what-is-oval/"&gt;What is OVAL?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="openscap"></category><category term="oval"></category><category term="scap"></category><category term="xccdf"></category></entry><entry><title>What is OVAL?</title><link href="https://blog.siphos.be/2013/12/what-is-oval/" rel="alternate"></link><published>2013-12-22T04:40:00+01:00</published><updated>2013-12-22T04:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-22:/2013/12/what-is-oval/</id><summary type="html">&lt;p&gt;Time to discuss &lt;strong&gt;OVAL (Open Vulnerability Assessment Language)&lt;/strong&gt;. In
all the &lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;previous
posts&lt;/a&gt;
I focused the checking of rules (does the system comply with the given
rule) on scripts, through the Script Check Engine supported by openscap.
The advantage of SCE is that most people can quickly provide automated
checks …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Time to discuss &lt;strong&gt;OVAL (Open Vulnerability Assessment Language)&lt;/strong&gt;. In
all the &lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;previous
posts&lt;/a&gt;
I focused the checking of rules (does the system comply with the given
rule) on scripts, through the Script Check Engine supported by openscap.
The advantage of SCE is that most people can quickly provide automated
checks to run in script format. But SCE has a few downsides.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You cannot guarantee that the scripts will do no harm on the system.
    A badly written script might manipulate system settings, get a huge
    amount of resources, leave stale result files on the system, flood
    file systems and more. If you get scripts from other parties, you'll
    need to review them thoroughly before running them against all
    your systems. Especially when you run the compliance validation tool
    (openscap in our example) as root.&lt;/li&gt;
&lt;li&gt;SCE support is only available for openscap (and perhaps one or
    two others) as it is not an international standard. If you use any
    of the &lt;a href="https://nvd.nist.gov/scapproducts.cfm"&gt;SCAP validated tools&lt;/a&gt;
    then you will not be able to benefit from the SCE scripts. And that
    would make the XCCDF document back to a purely documenting
    best practice.&lt;/li&gt;
&lt;li&gt;Every rule requires separate scripts, even though many of the rules
    will be very similar and thus reuse a lot of the scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OVAL on the other hand provides those advantages. An OVAL file is an XML
file that contains the tests to run, in an (I must say) somewhat complex
manner. Really, OVAL is not simple, but it does contain advantages that
SCE doesn't.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is a standard, part of the SCAP standards. OVAL files are
    reusable across multiple tools, allowing you to focus once on the
    rules rather than having to rewrite the rules for every time you
    change the tool.&lt;/li&gt;
&lt;li&gt;OVAL can be platform-agnostic. Of course, not all tests are
    platform-agnostic (validating registry keys is a Windows-only check)
    but many are.&lt;/li&gt;
&lt;li&gt;All rules can be mentioned in a single file (or spread across
    multiple files if that makes management easier), but more
    importantly rules will also reuse definitions from other rules. If
    you have three rules that pertain to a file (say &lt;code&gt;/etc/rc.conf&lt;/code&gt;)
    then the definition of that file is shared across all rules.&lt;/li&gt;
&lt;li&gt;The OVAL standard is designed to be non-intrusive. All declarations
    you do in an OVAL file are pure read-only statements. This gives
    more confidence to have OVAL statements from third parties ran
    across your organization. Of course, reviewing them never hurts, but
    you already know that they will not modify any setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like SCE, OVAL checks are individual checks that are executed and
returned. They too return a success or failure (or error) and can
deliver more detailed information as part of their result (like the SCE
results output we looked at before) so allow administrators to
investigate further why a rule failed (without needing to log on to the
system and look for themselves).&lt;/p&gt;
&lt;p&gt;A basic structure of OVAL is a &lt;em&gt;definition&lt;/em&gt; that describes what the rule
is for. The definition refers to one or more &lt;em&gt;tests&lt;/em&gt; that are evaluated
on a system. These tests refer to an &lt;em&gt;object&lt;/em&gt; that needs to be checked,
and optionally a &lt;em&gt;state&lt;/em&gt; to which the object should (or shouldn't)
match.&lt;/p&gt;
&lt;p&gt;Consider the test we made with SCE to see if a platform is a Gentoo
Linux system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#!/bin/sh

# If /etc/gentoo-release exists then the system is a Gentoo Linux system.
test -f /etc/gentoo-release &amp;amp;&amp;amp; exit ${XCCDF_RESULT_PASS};
exit ${XCCDF_RESULT_FAIL};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In OVAL, this would be structured as follows (pseudo-OVAL):&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;definition&lt;/dt&gt;
&lt;dd&gt;The system is a Gentoo Linux system&lt;/dd&gt;
&lt;dt&gt;test&lt;/dt&gt;
&lt;dd&gt;The object that represents /etc/gentoo-release must exist&lt;/dd&gt;
&lt;dt&gt;object&lt;/dt&gt;
&lt;dd&gt;The /etc/gentoo-release file&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;The resulting OVAL file is quite complex for a simple rule. I see the
OVAL complexity as part of a normalization (similar to database
normalization) process to allow higher reuse. If we later want to check
the content of the &lt;code&gt;gentoo-release&lt;/code&gt; file, we will reuse the definition
(object with id &lt;em&gt;oval:org.gentoo.dev.swift:obj:1&lt;/em&gt;) rather than making a
second object for it, and use that definition to create new tests.&lt;/p&gt;
&lt;p&gt;The structure of OVAL is the same everywhere. First define the
definitions, then the tests, then the objects and then, optionally, the
states. A very important aspect is to have the identifiers (&lt;code&gt;id="..."&lt;/code&gt;)
correct. The structure of OVAL identifiers is standardized as well:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;namespace&lt;/dt&gt;
&lt;dd&gt;Like the namespace used in XCCDF documents, this is the reverse
notation of a domainname. In the example above, this
is org.gentoo.dev.swift.&lt;/dd&gt;
&lt;dt&gt;type&lt;/dt&gt;
&lt;dd&gt;The type of the entry in OVAL. This can be def (definition), tst
(test), obj (object), ste (state) or var (variable).&lt;/dd&gt;
&lt;dt&gt;id&lt;/dt&gt;
&lt;dd&gt;The identifier of this particular entry. This identifier has to be a
positive integer.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;By standardizing the identifiers, you can create repositories within
your organization, and have other teams reuse your OVAL components when
needed. As the identifier remains the same (even when you update the
OVAL object itself to be more precise) those tests keep validating
correctly. For instance, say that Gentoo Linux would be changed in the
future not to provide a &lt;code&gt;gentoo-release&lt;/code&gt; file anymore, but
&lt;code&gt;gentoo-linux-release&lt;/code&gt; file instead (not that it is planning that, it is
just hypothetical), then you can update the test (with description
"Gentoo Linux is installed") to check if either of the two files exist:&lt;/p&gt;
&lt;p&gt;(XML content lost due to blog conversion)&lt;/p&gt;
&lt;p&gt;If we save all Gentoo releated OVAL statements in a file called
&lt;code&gt;gentoo-oval.xml&lt;/code&gt; then we can update the &lt;code&gt;gentoo-cpe.xml&lt;/code&gt; file (which we
discussed in the past) to the following:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;With this change, openscap (or any other XCCDF interpreter) will use the
OVAL definition to see if a platform is Gentoo Linux or not, and does
not need to execute the &lt;code&gt;gentoo-platform.sh&lt;/code&gt; script anymore, which is
now fully deprecated and superceded by the OVAL statement.&lt;/p&gt;
&lt;p&gt;In the next posts, I'll write up one of the other tests we had (which
checks the content of a file - one of the most used tests I think) in
OVAL, and have the XCCDF document updated to only use OVAL statements.&lt;/p&gt;
&lt;p&gt;This post is part of a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices - XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;Documenting a bit more than just
    descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;Running a bit with the XCCDF
    document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/remediation-through-scap/"&gt;Remediation through
    SCAP&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="openscap"></category><category term="oval"></category><category term="scap"></category><category term="sce"></category><category term="xccdf"></category></entry><entry><title>December hardened meeting</title><link href="https://blog.siphos.be/2013/12/december-hardened-meeting/" rel="alternate"></link><published>2013-12-20T10:20:00+01:00</published><updated>2013-12-20T10:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-20:/2013/12/december-hardened-meeting/</id><summary type="html">&lt;p&gt;Yesterday evening (UTC, that is) the members of the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project filled
the #gentoo-hardened IRC channel again - it was time for another online
follow-up meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A few patches on the toolchain need to be created to mark SSP as
default, but this is just a minor workload.&lt;/p&gt;
&lt;p&gt;And …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday evening (UTC, that is) the members of the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project filled
the #gentoo-hardened IRC channel again - it was time for another online
follow-up meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A few patches on the toolchain need to be created to mark SSP as
default, but this is just a minor workload.&lt;/p&gt;
&lt;p&gt;And on the &lt;a href="http://code.google.com/p/address-sanitizer/"&gt;ASAN (Address
Sanitizer)&lt;/a&gt; debacle;
well... still the same. Doesn't work with PaX. I think there is a
standstill on this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel, grsecurity/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is not clear yet what the next LTS (Long Term Stable) kernel will be
that the &lt;a href="https://grsecurity.net/"&gt;grSecurity&lt;/a&gt; team will support. This
depends on the Ubuntu LTS support, and as it is not clear which kernel
that distribution will use for LTS, the grSecurity team can also not say
what kernel it will be. (So please stop asking ;-)&lt;/p&gt;
&lt;p&gt;grsecurity 3.0 is out, with the following commit message:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;commit 4f48151d49f2697c3e2e108a50513a8d61fb150d
Author: Brad Spengler 
Date:   Sun Nov 24 17:47:14 2013 -0500

    Version bumped to 3.0 (we&amp;#39;d been on 2.9.1 for way too long and numerous
    features have been added since then)

    Introduce new atomic RBAC reload method, developed as part of sponsorship
    by EIG

    This is accompanied by an updated 3.0 gradm which will use the new reload
    method when -R is passed to gradm.  The old method will still be available
    via gradm -r (which is what a 2.9.1 gradm will continue to use).

    The new RBAC reload method is atomic in the sense that at no point in the
    reload process will the system not be covered by a coherent full policy.
    In contrast to previous reload behavior, it also preserves inherited subjects
    and special roles.

    The old RBAC reload method has also been made atomic.  Both methods have
    been updated to perform role_allowed_ip checks only against the IP tagged
    to the task at the time its role was first applied or changed.  This resolves
    long-standing usability problems with the use of role_allowed_ip and matches
    the policies created by learning.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The new version requires the use of &lt;code&gt;&amp;gt;=gradm-3.0&lt;/code&gt;. Some hardened-sources
packages already contain the 3.0 patchset (currently in testing). In a
few days, a final hardened-sources with a 2.9.1 patchset will be
stabilized; after that, only 3.0 patchset sources will be used.&lt;/p&gt;
&lt;p&gt;Another open issue (for a while) is the &lt;code&gt;install.py&lt;/code&gt; wrapper used to
properly pax-mark binaries during package building (instead of
post-merge changes). Although it works functionally well, it has serious
performance regressions when hundreds of files need to be merged and
marked. For each file, the Python interpreter is launched again, making
this a very time-consuming effort. Blueness is currently working on a
C-based wrapper which should load much faster.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The live repository of the Gentoo Hardened policies is well up to date
with the latest evolutions of the reference policy. If you want to use
these, use the &lt;code&gt;-9999&lt;/code&gt; ebuilds for the SELinux policy packages. This can
be accomplished with the following line in
&lt;path&gt;package.accept_keywords&lt;/path&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sec-policy/* **
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Recently, revision 5 of the SELinux policies has been pushed to the tree
(currently in testing). This one also contains a few required changes to
the policy for the new userspace utilities which are also now available
in the tree. An important update on the new userspace utilities is that
they contain many of the patches we needed in Gentoo and of course the
necessary updates, patches and improvements all-round. Once the SELinux
policies are stabilized, the userspace utilities will be too.&lt;/p&gt;
&lt;p&gt;After a few successful runs with SELinux on ARM devices, we will most
likely be tagging our SELinux packages (policies and userspace
utilities) as &lt;code&gt;~arm&lt;/code&gt;. Documentation will need to be updated on this as
well, not only to cater for the additional keyword, but also because of
one (perhaps a few) changes needed, like fixing the SELinux binary
policy (as most ARM systems come with a lower kernel version).&lt;/p&gt;
&lt;p&gt;SELinux support on ZFS also seems to work well, with the last patches in
(thanks to prometheanfire).&lt;/p&gt;
&lt;p&gt;As a final aspect, the SELinux eclass in Gentoo Linux now also supports
fetching the latest policy sources from git through HTTP (instead of
git/ssh).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Not much to discuss here; IMA/EVM and kernel signing all work well.&lt;/p&gt;
&lt;p&gt;In the next few days/weeks, I will be working on a &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
as a sort-of follow-up (improvement) of the current &lt;a href="http://www.gentoo.org/doc/en/security/"&gt;Gentoo Security
Handbook&lt;/a&gt;, now using SCAP
methods.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There has been discussion about supporting a &lt;a href="http://www.gentoo.org/doc/en/security/"&gt;hardened desktop
profile&lt;/a&gt; in Gentoo again. This
does come with the nasty surprise that these profiles don't play well
together, so a solution needs to be brought in place. This could be a
"hardened desktop" profile separate from the gentoo desktop one (and as
such manually synchronized), or an improved approach on profile
stacking.&lt;/p&gt;
&lt;p&gt;One idea was to support stacking with a maximum depth, allowing to use
changes made by a profile without inheriting the changes that that
profile inherited to a certain extend.&lt;/p&gt;
&lt;p&gt;Another idea is to use a more dependency-based syntax (similar to OpenRC
dependency system for init scripts), which not only allows for proper
stacking and the necessary flexibility, but also improves readability:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;before {
  hardened/linux/amd64
}
after {
  default/linux/amd64
}
depends {
  targets/desktop/gnome
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next few months will be interesting to see how this will evolve ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All our documents are on the &lt;a href="https://wiki.gentoo.org"&gt;wiki&lt;/a&gt;, so if you
notice gaps or see possibilities for improvement - by all means, do
them.&lt;/p&gt;
&lt;p&gt;All in all a good session. Thanks for the good work guys!&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="online"></category></entry><entry><title>Remediation through SCAP</title><link href="https://blog.siphos.be/2013/12/remediation-through-scap/" rel="alternate"></link><published>2013-12-20T04:47:00+01:00</published><updated>2013-12-20T04:47:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-20:/2013/12/remediation-through-scap/</id><summary type="html">&lt;p&gt;I promised in my &lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;previous
post&lt;/a&gt;
to give some information about remediation.&lt;/p&gt;
&lt;p&gt;Remediation is the process where you fix a system to become compliant
again after finding out there is a violation on the system. The easiest
form of remediation of course is to just notify the administrator and
give …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I promised in my &lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;previous
post&lt;/a&gt;
to give some information about remediation.&lt;/p&gt;
&lt;p&gt;Remediation is the process where you fix a system to become compliant
again after finding out there is a violation on the system. The easiest
form of remediation of course is to just notify the administrator and
give him the instructions to fix the problem - and in the majority of
cases, this is exactly what you will do, considering that automatically
fixing things might create more breakage if you are not careful.&lt;/p&gt;
&lt;p&gt;But suppose that you know, for a few rules, what the remediation really
should be, and you want to automate it. Well, in that case, you can
document the remediation (the commands or scripts) in the XCCDF
document. As you might have noticed in the previous example, this is
handled through a &lt;code&gt;&amp;lt;fix&amp;gt;&lt;/code&gt; entity.&lt;/p&gt;
&lt;p&gt;In the fix, we mention how the fix should be executed
(&lt;em&gt;urn:xccdf:fix:system:commands&lt;/em&gt; is to tell the XCCDF interpreter that
the remediation are commands executed on the command line (or verbatim
within a script). The platform attribute allows us to differentiate
based on the platform (or even version of the platform). The other
attributes, such as &lt;em&gt;complexity&lt;/em&gt;, &lt;em&gt;disruption&lt;/em&gt; and &lt;em&gt;reboot&lt;/em&gt; is metadata
that helps in deciding which auto-remediation you want to execute.&lt;/p&gt;
&lt;p&gt;With openscap, the remediation can be triggered online during the
evaluation, by adding &lt;code&gt;--remediate&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval --remediate --profile ${PROFILE} --cpe gentoo-cpe.xml --results results-test-xccdf.xml test-xccdf.xml
Title   There should be no /dev/ROOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-root
Result  pass

Title   There should be no /dev/BOOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot
Result  pass

Title   rc_sys should be defined in /etc/rc.conf
Rule    xccdf_org.gentoo.dev.swift_rule_installation-rc_sys
Result  fail

 --- Starting remediation ---
Title   rc_sys should be defined in /etc/rc.conf
Rule    xccdf_org.gentoo.dev.swift_rule_installation-rc_sys
Result  fixed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And indeed, the file has been changed and now complies with the rules
again.&lt;/p&gt;
&lt;p&gt;We can also generate the remediation scripts offline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval --profile ${PROFILE} --results results-test-xccdf.xml --cpe gentoo-cpe.xml test-xccdf.xml
~$ oscap xccdf generate fix --output remediate.sh results-test-xccdf.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting &lt;code&gt;remediate.sh&lt;/code&gt; script then contains the steps to remediate
the failures reported in the &lt;code&gt;results-test-xccdf.xml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;In general however, auto-remediation is not that recommended. The amount
of effort you put in creating remediation scripts that are safe to
execute is huge. If you do this for a single system, it is much easier
to just remediate manually. If you need to do it for a large set of
systems, it makes more sense to use a configuration management solution
like &lt;a href="http://www.ansibleworks.com/"&gt;ansible&lt;/a&gt; or
&lt;a href="http://puppetlabs.com/"&gt;puppet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, now that we have experience with documenting our best practices and
running validation, I'll talk about OVAL in the next post.&lt;/p&gt;
&lt;p&gt;This post is part of a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices - XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;Documenting a bit more than just
    descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/"&gt;Running a bit with the XCCDF
    document&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="openscap"></category><category term="remediation"></category><category term="scap"></category><category term="xccdf"></category></entry><entry><title>GPT or MBR in the Gentoo Handbook</title><link href="https://blog.siphos.be/2013/12/gpt-or-mbr-in-the-gentoo-handbook/" rel="alternate"></link><published>2013-12-18T12:25:00+01:00</published><updated>2013-12-18T12:25:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-18:/2013/12/gpt-or-mbr-in-the-gentoo-handbook/</id><summary type="html">&lt;p&gt;I just committed a set of changes against the Gentoo Handbook (x86 and
amd64) with the intent to have better instructions on GPT (GUID
Partition Table) layout versus MBR (Master Boot Record) or MSDOS-style
layout.&lt;/p&gt;
&lt;p&gt;The part on "Preparing the Disks" saw the most changes. It starts with
explaining the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just committed a set of changes against the Gentoo Handbook (x86 and
amd64) with the intent to have better instructions on GPT (GUID
Partition Table) layout versus MBR (Master Boot Record) or MSDOS-style
layout.&lt;/p&gt;
&lt;p&gt;The part on "Preparing the Disks" saw the most changes. It starts with
explaining the differences between the two layouts with advantages and
disadvantages. It then helps the user decide what layout is best for him
(or her).&lt;/p&gt;
&lt;p&gt;Second, the &lt;em&gt;example&lt;/em&gt; (and let me stress that one out again, because
many people have reported bugs on it: it is an &lt;em&gt;example&lt;/em&gt;) partition
layout now includes a BIOS boot partition in the beginning, 2 MB in
size. This is to support GRUB2 on GPT, but doesn't hurt for GRUB2 with
the MSDOS-style layout either. That means that the partition numbers now
move up one (the &lt;em&gt;example&lt;/em&gt; /boot is now at sda2, the swap at sda3 and
root on sda4).&lt;/p&gt;
&lt;p&gt;The partitioning instructions now also include the proper alignment
instructions (using MB alignment), and use parted as the default
partitioning method.&lt;/p&gt;
&lt;p&gt;The changes are submitted to CVS so will show up on the Gentoo site in
an hour or so (documentation on the sites is synchronized hourly if I'm
not mistaken). Please do give it a good read and report bugs on
&lt;a href="https://bugs.gentoo.org"&gt;bugs.gentoo.org&lt;/a&gt;. You might also want to ping
me on IRC if it is urgent, although no guarantees that I'm behind my
computer at any point.&lt;/p&gt;</content><category term="Gentoo"></category><category term="documentation"></category><category term="fdisk"></category><category term="gdp"></category><category term="Gentoo"></category><category term="gpt"></category><category term="handbook"></category><category term="mbr"></category><category term="parted"></category></entry><entry><title>Running a bit with the XCCDF document</title><link href="https://blog.siphos.be/2013/12/running-a-bit-with-the-xccdf-document/" rel="alternate"></link><published>2013-12-18T04:23:00+01:00</published><updated>2013-12-18T04:23:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-18:/2013/12/running-a-bit-with-the-xccdf-document/</id><summary type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;previous
post&lt;/a&gt;
I introduced automated checking of rules through &lt;em&gt;SCE (Script Check
Engine)&lt;/em&gt;. Let's focus a bit more now on running with an XCCDF document:
how to automatically check the system, read the results and find more
information of those results.&lt;/p&gt;
&lt;p&gt;To provide a usable example, you can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;previous
post&lt;/a&gt;
I introduced automated checking of rules through &lt;em&gt;SCE (Script Check
Engine)&lt;/em&gt;. Let's focus a bit more now on running with an XCCDF document:
how to automatically check the system, read the results and find more
information of those results.&lt;/p&gt;
&lt;p&gt;To provide a usable example, you can download the following files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/test-xccdf.txt"&gt;test-xccdf.xml&lt;/a&gt;,
    a sample XCCDF document that documents and tests a few settings
    (save it with the &lt;code&gt;.xml&lt;/code&gt; extension, I publish it as txt to make
    downloading and viewing easier).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-cpe.txt"&gt;gentoo-cpe.xml&lt;/a&gt;
    which defines when a system is a Gentoo system (I'll cover this in
    a minute).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-platform.sh"&gt;gentoo-platform.sh&lt;/a&gt;
    which is the script that tests if a system is a Gentoo system.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-fstab-noroot.sh"&gt;gentoo-fstab-noroot.sh&lt;/a&gt;
    which tests that &lt;code&gt;/dev/ROOT&lt;/code&gt; is not set in &lt;code&gt;/etc/fstab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-fstab-noboot.sh"&gt;gentoo-fstab-noboot.sh&lt;/a&gt;
    which tests that &lt;code&gt;/dev/BOOT&lt;/code&gt; is not set in &lt;code&gt;/etc/fstab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-rc.conf-rc_sys.sh"&gt;gentoo-rc.conf-rc_sys.sh&lt;/a&gt;
    which tests that the &lt;code&gt;rc_sys&lt;/code&gt; variable is declared in
    &lt;code&gt;/etc/rc.conf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Extract it to a directory of your choice, and let's get started.&lt;/p&gt;
&lt;p&gt;With &lt;a href="http://www.open-scap.org"&gt;openscap&lt;/a&gt; (available as
&lt;em&gt;app-forensics/openscap&lt;/em&gt; in Gentoo), we can generate a guide of the
XCCDF document as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf generate guide test-xccdf.xml &amp;gt; guide-test-xccdf.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The
&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/guide-test-xccdf.html"&gt;result&lt;/a&gt;
is an HTML guide that reflects the content of the XCCDF document. By
default, it contains all text and rules, but shows no information about
the profiles (if any). We can add in the &lt;code&gt;--profile ...&lt;/code&gt; tag to include
an overview of the checks that are selected if that profile is selected.
That would give a result similar to &lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/guide-test-xccdf-withprofile.html"&gt;this
one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;--format docbook&lt;/code&gt; arguments, the output can also be DocBook
instead of HTML. The advantage of DocBook is that it can generate a
multitude of other formats, including
&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/guide-test-xccdf-withprofile.pdf"&gt;PDF&lt;/a&gt;,
although I had to do some manual cleanups in the output to have it
render a PDF here using FOP (there are other methods to create PDFs too)
such as removing the &lt;code&gt;&amp;lt;preface&amp;gt;&lt;/code&gt; part.&lt;/p&gt;
&lt;p&gt;Let's try evaluating the XCCDF document on the system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval test-xccdf.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Nothing happened. That is because there are no rules that are by default
selected (all rules in the document have &lt;code&gt;selected="false"&lt;/code&gt;) and we have
not passed on a profile. I don't know if there is a way to automatically
make a particular profile default, so let's try it with the
&lt;em&gt;xccdf_org.gentoo.dev.swift_profile_default&lt;/em&gt; (which I always use as
the default profile name for all my XCCDF documents):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ export PROFILE=&amp;quot;xccdf_org.gentoo.dev.swift_profile_default&amp;quot;
~$ oscap xccdf eval --profile ${PROFILE} test-xccdf.xml

Title   There should be no /dev/ROOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-root
Result  notapplicable

Title   There should be no /dev/BOOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot
Result  notapplicable

Title   rc_sys should be defined in /etc/rc.conf
Rule    xccdf_org.gentoo.dev.swift_rule_installation-rc_sys
Result  notapplicable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At least we have output now, but still no results. In fact, all rules
have &lt;em&gt;notapplicable&lt;/em&gt; as a result. What gives?&lt;/p&gt;
&lt;p&gt;The reason is that the XCCDF interpreter (&lt;strong&gt;oscap&lt;/strong&gt;) does not know about
the Gentoo Linux platform, whereas the XCCDF document explicitly
mentions that it is applicable to a Gentoo Linux system.&lt;/p&gt;
&lt;p&gt;What we need to do is to provide the XCCDF interpreter with a test that
helps it evaluate if a system is a Gentoo Linux system or not. In other
words, a test that the XCCDF interpreter will run if the
&lt;em&gt;cpe:/o:gentoo:linux&lt;/em&gt; platform is mentioned. We do this with a &lt;em&gt;CPE
dictionary&lt;/em&gt; element which is saved as
&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/gentoo-cpe.txt"&gt;gentoo-cpe.xml&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the dictionary, the coupling between cpe:/o:gentoo:linux and a scripted
check called gentoo-platform.sh is made. Let's now give this info to oscap:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval --profile ${PROFILE} --cpe gentoo-cpe.xml test-xccdf.xml
Title   There should be no /dev/ROOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-root
Result  pass

Title   There should be no /dev/BOOT in /etc/fstab
Rule    xccdf_org.gentoo.dev.swift_rule_installation-fstab-boot
Result  pass

Title   rc_sys should be defined in /etc/rc.conf
Rule    xccdf_org.gentoo.dev.swift_rule_installation-rc_sys
Result  pass

OpenSCAP Error: Document is empty [./gentoo-platform.sh:1] [elements.c:207]
No definition with ID: (null) in definition model. [oval_probe.c:338]
No definition with ID: (null) in result model. [oval_agent.c:184]
No definition with ID: (null) in definition model. [oval_probe.c:338]
No definition with ID: (null) in result model. [oval_agent.c:184]
No definition with ID: (null) in definition model. [oval_probe.c:338]
No definition with ID: (null) in result model. [oval_agent.c:184]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great; we now see that openscap ran the tests and gave feedback. It also
gave a few errors. These can be ignored now - it is openscap that tries
to interpret the shell scripts as OVAL scripts (I'll talk about OVAL in
a later post). After changing my system to be non-compliant, I see that
openscap detects that as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Title   rc_sys should be defined in /etc/rc.conf
Rule    xccdf_org.gentoo.dev.swift_rule_installation-rc_sys
Result  fail
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, by itself the rule might give us enough clues as to what is wrong,
but sometimes you might want to get the output of the scripts as well.
You can enable this through the &lt;code&gt;--check-engine-results&lt;/code&gt; option. This
will leave the generated output of the scripts available as XML files.&lt;/p&gt;
&lt;p&gt;In it, we see the output (through &lt;code&gt;&amp;lt;sceres:stdout&amp;gt;&lt;/code&gt;) of the &lt;strong&gt;grep&lt;/strong&gt;
command we did in the script.&lt;/p&gt;
&lt;p&gt;Finally, by adding in a &lt;code&gt;--report report-test-xccdf.html&lt;/code&gt; to the
argument list, the results of the XCCDF evaluation is also saved as
&lt;a href="http://dev.gentoo.org/~swift/blog/201312/18/report-test-xccdf.html"&gt;HTML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The oscap command has many more options, which I will not discuss in
more detail now, but are important to know (for instance, you can save
the XCCDF results in XML format for future processing).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval -h
 oscap -&amp;gt; xccdf -&amp;gt; eval

Perform evaluation driven by XCCDF file and use OVAL as checking engine

Usage: oscap [options] xccdf eval [options] INPUT_FILE [oval-definitions-files]

INPUT_FILE - XCCDF file or a source data stream file

Options:
   --profile               - The name of Profile to be evaluated.
   --tailoring-file        - Use given XCCDF Tailoring file.
   --tailoring-id  - Use given DS component as XCCDF Tailoring file.
   --cpe                   - Use given CPE dictionary or language (autodetected)
                                   for applicability checks.
   --oval-results                - Save OVAL results as well.
   --sce-results                 - Save SCE results as well. (DEPRECATED! use --check-engine-results)
   --check-engine-results        - Save results from check engines loaded from plugins as well.
   --export-variables            - Export OVAL external variables provided by XCCDF.
   --results               - Write XCCDF Results into file.
   --results-arf           - Write ARF (result data stream) into file.
   --report                - Write HTML report into file.
   --skip-valid                  - Skip validation.
   --fetch-remote-resources      - Download remote content referenced by XCCDF.
   --progress                    - Switch to sparse output suitable for progress reporting.
                                   Format is &amp;quot;$rule_id:$result\n&amp;quot;.
   --datastream-id           - ID of the datastream in the collection to use.
                                   (only applicable for source datastreams)
   --xccdf-id                - ID of component-ref with XCCDF in the datastream that should be evaluated.
                                   (only applicable for source datastreams)
   --benchmark-id            - ID of XCCDF Benchmark in some component in the datastream that should be evaluated.
                                   (only applicable for source datastreams)
                                   (only applicable when datastream-id AND xccdf-id are not specified)
   --remediate                   - Automatically execute XCCDF fix elements for failed rules.
                                   Use of this option is always at your own risk.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In my next post, I'll talk a bit more about remediation.&lt;/p&gt;
&lt;p&gt;This post is part of a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices - XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/"&gt;Documenting a bit more than just
    descriptions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="openscap"></category><category term="scap"></category><category term="sce"></category><category term="xccdf"></category></entry><entry><title>Updated Linux Sea, now with viewport thingie</title><link href="https://blog.siphos.be/2013/12/updated-linux-sea-now-with-viewport-thingie/" rel="alternate"></link><published>2013-12-16T23:37:00+01:00</published><updated>2013-12-16T23:37:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-16:/2013/12/updated-linux-sea-now-with-viewport-thingie/</id><summary type="html">&lt;p&gt;I just pushed out an update to &lt;a href="http://swift.siphos.be/linux_sea/"&gt;Linux
Sea&lt;/a&gt; (an online resource to introduce
you to Linux, using Gentoo Linux as an example), including its PDF and
ePub versions. The changes are pretty small (see its
&lt;a href="https://github.com/sjvermeu/Linux-Sea/blob/master/ChangeLog"&gt;ChangeLog&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Together with the update, it now also includes a
&lt;code&gt;&amp;lt;meta name="viewport"...&amp;gt;&lt;/code&gt; so …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just pushed out an update to &lt;a href="http://swift.siphos.be/linux_sea/"&gt;Linux
Sea&lt;/a&gt; (an online resource to introduce
you to Linux, using Gentoo Linux as an example), including its PDF and
ePub versions. The changes are pretty small (see its
&lt;a href="https://github.com/sjvermeu/Linux-Sea/blob/master/ChangeLog"&gt;ChangeLog&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Together with the update, it now also includes a
&lt;code&gt;&amp;lt;meta name="viewport"...&amp;gt;&lt;/code&gt; so that the document renders better on a
mobile device. Not perfect, but at least it's not small text anymore
where you need to pinch-zoom and then scroll left/right all over the
place. Same with my blog btw.&lt;/p&gt;</content><category term="Documentation"></category><category term="css"></category><category term="documentation"></category><category term="Gentoo"></category><category term="linux-sea"></category><category term="mobile"></category></entry><entry><title>XCCDF - Documenting a bit more than just descriptions</title><link href="https://blog.siphos.be/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/" rel="alternate"></link><published>2013-12-16T04:58:00+01:00</published><updated>2013-12-16T04:58:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-16:/2013/12/xccdf-documenting-a-bit-more-than-just-descriptions/</id><summary type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;previous
post&lt;/a&gt; I
made a skeleton XCCDF document. By now, we can create a well documented
"baseline" (best practice) for our subject (say PostgreSQL). But for now
I only talked about &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; whereas XCCDF allows many other tags
as well.&lt;/p&gt;
&lt;p&gt;You can add &lt;em&gt;metadata&lt;/em&gt; information for a particular …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;previous
post&lt;/a&gt; I
made a skeleton XCCDF document. By now, we can create a well documented
"baseline" (best practice) for our subject (say PostgreSQL). But for now
I only talked about &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; whereas XCCDF allows many other tags
as well.&lt;/p&gt;
&lt;p&gt;You can add &lt;em&gt;metadata&lt;/em&gt; information for a particular &lt;code&gt;Group&lt;/code&gt;. It is
recommended to use the &lt;a href="http://dublincore.org/"&gt;dublin core&lt;/a&gt; notation:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)`&lt;/p&gt;
&lt;p&gt;If you use metadata information however, it should &lt;strong&gt;not&lt;/strong&gt; be used
&lt;em&gt;instead&lt;/em&gt; of XCCDF elements.&lt;/p&gt;
&lt;p&gt;Another set of elements that can be used are &lt;code&gt;warning&lt;/code&gt; elements:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;rationale&amp;gt;&lt;/code&gt; element can be used to explain in more detail why a
rule is important.&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;Some elements we saw before also apply on the specific &lt;code&gt;&amp;lt;Group&amp;gt;&lt;/code&gt;
elements, such as &lt;code&gt;&amp;lt;status&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt;. The combination of these
elements should allow for a pretty good explanation of the secure setup
we want to achieve.&lt;/p&gt;
&lt;p&gt;But documentation is one thing - how about checking something
automatically? Enter the XCCDF &lt;code&gt;Rule&lt;/code&gt; element.&lt;/p&gt;
&lt;p&gt;Rules are particular tests, checks if you wish, that you want to have
executed. To start off, let's look at a &lt;code&gt;Rule&lt;/code&gt; element that, as
automated approach, calls a script. To accomplish this, we use the &lt;strong&gt;SCE
(Script Check Engine)&lt;/strong&gt; method. This is &lt;em&gt;not&lt;/em&gt; part of the SCAP standard
by itself (SCAP uses OVAL for automated checks - I'll discuss OVAL
later) but XCCDF allows for other check systems to be used. And SCE is
supported by &lt;a href="http://www.open-scap.org"&gt;openscap&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;First of all, we have the &lt;code&gt;Rule&lt;/code&gt; element itself, with the specially
crafted &lt;code&gt;id&lt;/code&gt; attribute as seen before. There are three attributes used
in the example:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;selected="false"&lt;/code&gt; tells the XCCDF interpreter that the Rule should
    not be automatically selected. In other words, only if a &lt;code&gt;Profile&lt;/code&gt;
    refers to the rule will be rule be triggered (and the
    check executed).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;severity="low"&lt;/code&gt; is a matter of documenting how severe a
    non-compliant rule is.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight="0.0"&lt;/code&gt; gives a weight on the &lt;code&gt;Rule&lt;/code&gt;. In this case, the
    weight is 0, meaning that the rule might be recommended but by
    itself does not introduce a security vulnerability or mismatch. Of
    course, you are free to use whatever value suits you best.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also notice a &lt;code&gt;fixtext&lt;/code&gt; element. When the rule failed (the system is
not compliant to the rule) then the fixtext should assist administrators
in securing the system again. In other words, &lt;code&gt;fixtext&lt;/code&gt; are the
human-readable instructions on remediating the system.&lt;/p&gt;
&lt;p&gt;Finally, we have the &lt;code&gt;check&lt;/code&gt; element. This element tells the XCCDF
interpreter that an automated validation is defined. The type of
automated validation is provided by the &lt;code&gt;system&lt;/code&gt; attribute, which in
this case refers to the SCE system. The &lt;code&gt;check-content-ref&lt;/code&gt; element
refers to the script to be executed.&lt;/p&gt;
&lt;p&gt;Let's look at the contents of the script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#!/bin/sh

# Get CHOST value
echo &amp;quot;Getting CHOST variable content through portageq.&amp;quot;;
my_chost=$(portageq envvar CHOST);
if [ -z &amp;quot;${my_chost}&amp;quot; ];
then
  echo &amp;quot;-- The portageq command failed. Falling back to glibc build info.&amp;quot;;
  my_chost=$(cat /var/db/pkg/sys-libs/glibc-*/CHOST | tail -1);
fi
echo &amp;quot;-- Got CHOST=${my_chost}&amp;quot;;

# Get current GCC version
echo &amp;quot;Getting current GCC version through /etc/env.d/gcc/config-*&amp;quot;;
current_gcc=$(grep CURRENT /etc/env.d/gcc/config-* | sed -e &amp;quot;s:CURRENT=${my_chost}-::g&amp;quot; | sed -e &amp;quot;s:\([0-9\.-r]*\){$,-.*$}:\1:g&amp;quot; );
echo &amp;quot;-- Got version=${current_gcc}&amp;quot;;

# Get type
echo &amp;quot;Getting compiler type (profile/spec) through its CURRENT= value.&amp;quot;;
current_type=$(grep CURRENT /etc/env.d/gcc/config-* | sed -e &amp;quot;s:CURRENT=${my_chost}-${current_gcc}::g&amp;quot; | sed -e &amp;#39;s:^-::g&amp;#39;);
echo &amp;quot;-- Got type=${current_type}&amp;quot;;

echo &amp;quot;Checking USE flags of gcc-${current_gcc} for hardened USE flag.&amp;quot;;
grep -q hardened /var/db/pkg/sys-devel/gcc-${current_gcc}/USE;
current_hardened_use=$?;
if [ ${current_hardened_use} -ne 0 ];
then
  echo &amp;quot;!! GCC ${current_gcc} is not build with USE=hardened!&amp;quot;;
  echo &amp;quot;!! Please enable a hardened profile.&amp;quot;;
  exit ${XCCDF_RESULT_FAIL};
else
  echo &amp;quot;-- GCC ${current_gcc} is build with USE=hardened.&amp;quot;;
  if [ -z &amp;quot;${current_type}&amp;quot; ];
  then
    echo &amp;quot;-- The default type is used which is a hardened type.&amp;quot;;
    exit ${XCCDF_RESULT_PASS};
  else
    echo &amp;quot;!! A non-default type is used: ${current_type}&amp;quot;;
    echo &amp;quot;!! This means not all hardened toolchain measures are enabled.&amp;quot;;
    exit ${XCCDF_RESULT_FAIL};
  fi
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the script can give output when needed, but the most
important part is that it has to return a particular return value. This
return value is provided through environment variables
(&lt;code&gt;XCCDF_RESULT_*&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;All we need to do now is to include this &lt;code&gt;Rule&lt;/code&gt; in the &lt;code&gt;Profile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;We can now evaluate the XCCDF file on the system if we refer to the
right profile. By selecting the profile, the XCCDF interpreter will also
automatically check the rules referred to by the profile (and the rules
that do not have &lt;code&gt;selected="false"&lt;/code&gt; set too).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# oscap xccdf eval --profile ... gentoo-xccdf.xml

Title   Test if the hardened toolchain is used
Rule    xccdf_org.gentoo.dev.swift_rule_installation-toolchain-hardened
Result  pass

Title   Test if sulogin is used for single-user boot (/etc/inittab)
Rule    xccdf_org.gentoo.dev.swift_rule_inittab-sulogin
Result  fail
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, if you want to check this on several systems, you would need to
distribute not only the XCCDF file, but also all files referred to by
the XCCDF document. As this is counterproductive, SCAP supports &lt;em&gt;Data
Streams&lt;/em&gt;. A data stream is a single file that includes the content of
all files. With openscap, data streams can be made as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# oscap ds sds-compose postgresql-xccdf.xml postgresql-ds.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So now we have a document explaining the secure setup of a component,
and included automated checks to validate system compliance with the
document using scripts. In the next post, I'll go on with OVAL.&lt;/p&gt;
&lt;p&gt;This post is part of a series on SCAP content:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;Documenting security best practices - XCCDF
    introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/"&gt;An XCCDF skeleton for
    PostgreSQL&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Security"></category><category term="openscap"></category><category term="scap"></category><category term="sce"></category><category term="xccdf"></category></entry><entry><title>An XCCDF skeleton for PostgreSQL</title><link href="https://blog.siphos.be/2013/12/an-xccdf-skeleton-for-postgresql/" rel="alternate"></link><published>2013-12-14T04:00:00+01:00</published><updated>2013-12-14T04:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-14:/2013/12/an-xccdf-skeleton-for-postgresql/</id><summary type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;previous
post&lt;/a&gt;
I wrote about the documentation structure I have in mind for a
PostgreSQL security best practice. Considering what XCCDF can give us,
the idea is to have the following structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Hardening PostgreSQL
+- Basic setup
+- Instance level configuration
|  +- Pre-startup configuration
|  `- PostgreSQL internal configuration
+- Database recommendations
`- User definitions …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/"&gt;previous
post&lt;/a&gt;
I wrote about the documentation structure I have in mind for a
PostgreSQL security best practice. Considering what XCCDF can give us,
the idea is to have the following structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Hardening PostgreSQL
+- Basic setup
+- Instance level configuration
|  +- Pre-startup configuration
|  `- PostgreSQL internal configuration
+- Database recommendations
`- User definitions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For the profiles, I had:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;infrastructure
instance
user
+- administrator
+- end user
`- functional account
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's bring this into an XCCDF document.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;XCCDF (Extensible Configuration Checklist Description Format)&lt;/strong&gt;
format is an XML structure in which we can document whatever we want -
but it is primarily used for configuration checklists and best
practices. The &lt;em&gt;documenting&lt;/em&gt; aspect of a security best practice in XCCDF
is done through XHTML basic tags (do not use fancy things - limit
yourself to &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;pre&lt;/code&gt;, &lt;code&gt;em&lt;/code&gt;, &lt;code&gt;strong&lt;/code&gt;, ... tags), so some knowledge on
XHTML (next to XML in general) is quite important while developing XCCDF
guides. At least, if you don't use special editors for that.&lt;/p&gt;
&lt;p&gt;We start with the basics:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;Two things I want to focus on here: the &lt;code&gt;xmlns:h&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; attributes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;xmlns:h&lt;/code&gt; attribute is an XML requirement, telling whatever XML
    parser is used later that tags that use the &lt;code&gt;h:&lt;/code&gt; namespace is for
    XHTML tags. So later in the document, we'll use &lt;code&gt;&amp;lt;h:p&amp;gt;...&amp;lt;/h:p&amp;gt;&lt;/code&gt; for
    XHTML paragraphs.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;id&lt;/code&gt; attribute is XCCDF specific, and since XCCDF 1.2 also
    requires to be in this particular syntax:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &amp;lt;/p&amp;gt;
        xccdf_&amp;lt;namespace&amp;gt;_benchmark_&amp;lt;name&amp;gt;

    &amp;lt;p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;The `&amp;lt;namespace&amp;gt;` is recommended to be an inverted domain
name structure. I also added my nickname so there are no collisions
with namespaces provided by other developers in Gentoo. So *SwifT&amp;#39;s
dev.gentoo.org* becomes *org.gentoo.dev.swift*.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This id structure will be used in other tags as well. Instead of
&lt;code&gt;*_benchmark&lt;/code&gt; it would be &lt;code&gt;*_rule&lt;/code&gt; (for &lt;code&gt;Rule&lt;/code&gt; ids), &lt;code&gt;*_group&lt;/code&gt; (for
&lt;code&gt;Group&lt;/code&gt; ids), etc. You get the idea.&lt;/p&gt;
&lt;p&gt;Now we add in some metadata in the document (with &lt;code&gt;Benchmark&lt;/code&gt; as
parent):&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;So what is all this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;&amp;lt;status&amp;gt;&lt;/code&gt; tag helps in tracking the state of the document.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;&amp;lt;platform&amp;gt;&lt;/code&gt; tag is to tell the XCCDF interpreter when the
    document is applicable. It references a &lt;strong&gt;CPE (Common
    Platform Enumeration)&lt;/strong&gt; entity, in this case for PostgreSQL. Later,
    we will see that an automated test is assigned to the detection of
    this CPE. If the test succeeds, then PostgreSQL is installed and the
    XCCDF interpreter can continue testing and evaluating the system for
    PostgreSQL best practices. If not, then PostgreSQL is not installed
    and the XCCDF does not apply to the system.
    &lt;/p&gt;
    &lt;p&gt;
    There is a huge advantage to this: you can check all your systems
    for compliance with the PostgreSQL best practices (this
    XCCDF document) and on the systems that PostgreSQL is not installed,
    it will simply state that the document is not applicable (without
    actually trying to validate all the rules in the document). So there
    is no direct need to only check systems you know have PostgreSQL on
    (and thus potentially ignore systems that have PostgreSQL but that
    you don't know of - usually those systems are much less secure as
    well ;-).&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt; tag versions the document.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;&amp;lt;model&amp;gt;&lt;/code&gt; tags tell the XCCDF interpreter which &lt;em&gt;scoring system&lt;/em&gt;
    should be used.
    &lt;/p&gt;
    &lt;p&gt;
    Scoring will give points to rules, and the XCCDF interpreter will
    sum the scores of all rules to give a final score to the
    "compliance" state of the system. But scoring can be done on
    several levels. The default one uses the hierarchy of the document
    (nested &lt;code&gt;Group&lt;/code&gt;s and &lt;code&gt;Rule&lt;/code&gt;s) to give a final number whereas the
    flat one does not care about the structure. Finally, the
    flat-unweighted one does not take into account the scores given by
    the author - all rules get the value of 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we define the &lt;code&gt;Profile&lt;/code&gt;s to use. I will give the example for two:
&lt;em&gt;user&lt;/em&gt; and &lt;em&gt;administrator&lt;/em&gt;, you can fill in the other ones ;-)&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;Group&lt;/code&gt;s (still with &lt;code&gt;Benchmark&lt;/code&gt; as their parent, but below
the &lt;code&gt;Profile&lt;/code&gt;s) which define the documentation structure of the guide:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;With all this defined, our basic skeleton for the PostgreSQL best
practice document is ready. To create proper content, we can use the
XHTML code inside the &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; tags, like so:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;As said in the previous post though, just documenting various aspects is
not enough. It is recommended to add references. In XCCDF, this is done
through the &lt;code&gt;&amp;lt;reference&amp;gt;&lt;/code&gt; tag, which is within a &lt;code&gt;Group&lt;/code&gt; and usually
below the &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; information:&lt;/p&gt;
&lt;p&gt;(XML content lost during blog conversion)&lt;/p&gt;
&lt;p&gt;With this alone, it is already possible to write up an XCCDF guide
describing how to securely setup (in our case) PostgreSQL while keeping
track of the resources that helped define the secure setup. Tools like
&lt;a href="http://www.open-scap.org"&gt;openscap&lt;/a&gt; can generate HTML or even Docbook
(which in turn can be converted to manual pages, PDF, Word, RTF, ...)
from this information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# oscap xccdf generate guide --format docbook --output guide.docbook postgresql-xccdf.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the next post, I'll talk about the other documenting entities within
XCCDF (besides &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; and their meaning) and start with
enhancing the document with automated checks.&lt;/p&gt;</content><category term="Security"></category><category term="postgresql"></category><category term="scap"></category><category term="xccdf"></category></entry><entry><title>Documenting security best practices - XCCDF introduction</title><link href="https://blog.siphos.be/2013/12/documenting-security-best-practices-xccdf-introduction/" rel="alternate"></link><published>2013-12-12T16:04:00+01:00</published><updated>2013-12-12T16:04:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-12:/2013/12/documenting-security-best-practices-xccdf-introduction/</id><summary type="html">&lt;p&gt;When I have some free time, I try to work on a &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
which not only documents security best practices (loosely based on the
&lt;a href="http://www.gentoo.org/doc/en/security/security-handbook.xml"&gt;Gentoo Security
Handbook&lt;/a&gt;
which hasn't seen much updates in the last few years) but also uses the
SCAP protocols. This set of protocols allows …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I have some free time, I try to work on a &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
which not only documents security best practices (loosely based on the
&lt;a href="http://www.gentoo.org/doc/en/security/security-handbook.xml"&gt;Gentoo Security
Handbook&lt;/a&gt;
which hasn't seen much updates in the last few years) but also uses the
SCAP protocols. This set of protocols allows security administrators to
automate and document many of their tasks, and a security best practices
guide is almost a must-have in any organization. So I decided to do a
few write-ups about these SCAP protocols and how I hope to be using them
more in the future.&lt;/p&gt;
&lt;p&gt;In this post, I'm going to focus on a very simple matter: documenting.
SCAP goes much, much beyond documenting, but I'll discuss those various
features in subsequent posts. The end goal of the series is to have a
best practice document for PostgreSQL.&lt;/p&gt;
&lt;p&gt;To document the secure state of a component, it is important to first
have an idea about what you are going to document. Some people might
want to document best practices across many technologies so that there
is a coherent, single document explaining the security best practices
for the entire organization. In my opinion, that is not manageable in
the long term. We tried that with the Gentoo Security Handbook, but you
quickly start wrestling with the order of chapters, style concerns and
what not. Also, some technologies will be much more discussed in depth
than others, making the book look "unfinished".&lt;/p&gt;
&lt;p&gt;Personally, I rather focus on a specific technology. For instance:
&lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/openssh.html"&gt;Hardening
OpenSSH&lt;/a&gt;
(very much work in progress - the rules are generated automatically for
now and will be rewritten in the near future). It talks about a single
component (OpenSSH) allowing the freedom for the author to focus on what
matters. By providing security best practices on these component levels,
you'll create a set of security best practices that can often be reused.
This is what &lt;a href="http://www.cisecurity.org"&gt;the Center for Internet
Security&lt;/a&gt; is doing with its benchmarks:
popular technologies are described in detail on how to configure them to
be more secure.&lt;/p&gt;
&lt;p&gt;Once you know what technology you want to describe, we need to consider
how this technology is used. Some technologies are very flexible in
their setup, and might have different security setups depending on their
use. For instance, an OpenLDAP server can be used internally as a public
address book, or disclosed on the Internet in a multi-replicated setup
with authentication data in it. The security best practices for these
deployments will vary. The &lt;strong&gt;XCCDF (Extensible Configuration Checklist
Description Format)&lt;/strong&gt; standard allows authors to write a single guide,
while taking into account the different deployment approaches through
the use of &lt;code&gt;Profile&lt;/code&gt; settings.&lt;/p&gt;
&lt;p&gt;In XCCDF, &lt;code&gt;Profile&lt;/code&gt;s allow for selectively enabling or disabling
document fragments (called &lt;code&gt;Group&lt;/code&gt;s) and checks (called &lt;code&gt;Rule&lt;/code&gt;s - I will
post about checks later) or even change values (like the minimum
password length) depending on the profile. A document can then describe
settings with different profiles depending on the use and deployment of
the technology. Profiles can also inherit from each other, so you can
have a base (default) security conscious setup, and enhance it through
other profiles.&lt;/p&gt;
&lt;p&gt;Next to the "how", we also need to consider the structure we want for
such a best practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will have rules in place for the deployment of PostgreSQL itself.
    These rules range from making sure a stable, patched version is
    used, to the proper rights on the software files, partitioning and
    file system rules and operating system level requirements (such as
    particular kernel parameters).&lt;/li&gt;
&lt;li&gt;We will also have rules for each instance. We could plan on running
    multiple PostgreSQL instances next to each other, so these rules are
    distinct from the deployment rules. These rules include settings on
    instance level, process ownership (in case of running PostgreSQL as
    different service user), etc.&lt;/li&gt;
&lt;li&gt;We might even have rules for databases and users (roles) in
    the database.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It &lt;em&gt;might&lt;/em&gt; make sense to split the best practices in separate documents,
such as one for PostgreSQL infrastructure (which is database-agnostic)
and one for PostgreSQL databases (and users). I would start with one
document for the technology if I was responsible for the entire
definition, but if this responsibility is not with one person (or team),
it makes sense to use different documents. Also, as we will see later,
XCCDF documents can be "played" against a target. If the target is
different (for infrastructure, the target usually is the host on which
PostgreSQL is installed, whereas for the database settings the target is
probably the PostgreSQL instance itself) I would definitely have the
definitions through separate profiles, but that does not mean the
document needs to be split either.&lt;/p&gt;
&lt;p&gt;Finally, documenting a secure best practice also involves keeping track
of the references. It is not only about documenting something and why
&lt;em&gt;you&lt;/em&gt; think this is the best approach, but also about referring readers
to more information and other resources that collaborate your story.
These can be generic control objectives (such as those provided by the
&lt;a href="http://www.opensecurityarchitecture.org/cms/library/0802control-catalogue"&gt;open security
architecture&lt;/a&gt;)
or specific best practices of the vendor itself or third parties.&lt;/p&gt;
&lt;p&gt;At the end, for a PostgreSQL security guide, we would probably start
with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Hardening PostgreSQL
+- Basic setup
+- Instance level configuration
|  +- Pre-startup configuration
|  `- PostgreSQL internal configuration
+- Database recommendations
`- User definitions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Profile-wise, I probably would need an &lt;em&gt;infrastructure&lt;/em&gt; profile, an
&lt;em&gt;instance&lt;/em&gt; profile, a &lt;em&gt;user&lt;/em&gt; profile and a &lt;em&gt;database&lt;/em&gt; profile. I might
even have profiles for the different roles (&lt;em&gt;functional account&lt;/em&gt;,
&lt;em&gt;administrator&lt;/em&gt; and &lt;em&gt;end user&lt;/em&gt; profiles which inherit from the &lt;em&gt;user&lt;/em&gt;
profile) as they will have different rules assigned to them.&lt;/p&gt;
&lt;p&gt;In my next post, we'll create a skeleton XCCDF document and already talk
about some of the XCCDF features that we can benefit from immediately.&lt;/p&gt;</content><category term="Security"></category><category term="postgresql"></category><category term="scap"></category><category term="xccdf"></category></entry><entry><title>Gentoo SELinux policy release script</title><link href="https://blog.siphos.be/2013/12/gentoo-selinux-policy-release-script/" rel="alternate"></link><published>2013-12-11T18:37:00+01:00</published><updated>2013-12-11T18:37:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-11:/2013/12/gentoo-selinux-policy-release-script/</id><summary type="html">&lt;p&gt;A few months ago, I wrote a small script that aids in the creation of
new SELinux policy packages. The script is on the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-refpolicy.git;a=summary"&gt;repository&lt;/a&gt;
itself, in the &lt;code&gt;gentoo/&lt;/code&gt; subdirectory, and is called
&lt;code&gt;release-prepare.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The reason for the script is that there are a number of steps to
perform …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few months ago, I wrote a small script that aids in the creation of
new SELinux policy packages. The script is on the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-refpolicy.git;a=summary"&gt;repository&lt;/a&gt;
itself, in the &lt;code&gt;gentoo/&lt;/code&gt; subdirectory, and is called
&lt;code&gt;release-prepare.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The reason for the script is that there are a number of steps to
perform, one of which (tagging the release) I forgot to do too often. So
today I made a new release of the policy packages (2.20130424-r4) with
the script, and decided to blog about it as other developers in the
hardened team might one day be asked to make a release when I'm not
available.&lt;/p&gt;
&lt;p&gt;When the script is called, it spits out the usual help information.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sh release-prepare.sh -h
Usage: release-prepare.sh

Example: release-prepare.sh 2.20130424-r2 2.20130424-r3

The script will copy the ebuilds of the  towards the
 and update the string occurrences of that version
(mostly for the BASEPOL variable).

The following environment variables must be declared correctly for the script
to function properly:
  - GENTOOX86 should point to the gentoo-x86 checkout
    E.g. export GENTOOX86=&amp;quot;/home/user/dev/gentoo-x86&amp;quot;
  - HARDENEDREFPOL should point to the hardened-refpolicy.git checkout
    E.g. export HARDENEDREFPOL=&amp;quot;/home/user/dev/hardened-refpolicy&amp;quot;
  - REFPOLRELEASE should point to the current latest /release/ of the reference
    policy (so NOT to a checkout), extracted somewhere on the file system.
    E.g. export REFPOLRELEASE=&amp;quot;/home/user/local/refpolicy-20130424&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So first, we need to export three environment variables needed by the
script:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GENTOOX86&lt;/code&gt; points to the CVS checkout of the Portage tree. It is
    used to create new ebuilds.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HARDENEDREFPOL&lt;/code&gt; is the git checkout of the policy repository. This
    one is used to read the changes from to generate a patch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;REFPOLRELEASE&lt;/code&gt; is an extracted &lt;code&gt;refpolicy-20130424.tar.gz&lt;/code&gt; (the
    upstream release of the reference policy). This extracted location
    is needed to generate the patch (the difference between our
    repository and the upstream release).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After setting the variables, the script does its magic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sh release-prepare.sh 2.20130424-r3 2.20130424-r4
Creating patch 0001-full-patch-against-stable-release.patch... done
Creating patch bundle for 2.20130424-r4... done
Copying patch bundle into /usr/portage/distfiles and dev.g.o... done
Removing old patchbundle references in Manifest (in case of rebuild)... done
Creating new ebuilds based on old version... done
Marking ebuilds as ~arch... done
Creating tag 2.20130424-r4 in our repository... done
The release has now been prepared.

Please go do the following to finish up:
- In /home/swift/dev/gentoo-x86/sec-policy go &amp;quot;cvs add&amp;quot; all the new ebuilds
- In /home/swift/dev/gentoo-x86/sec-policy run &amp;quot;repoman manifest&amp;quot; and &amp;quot;repoman full&amp;quot;

Then, before finally committing - do a run yourself, ensuring that the right
version is deployed of course:
- &amp;quot;emerge -1 sec-policy/selinux-aide
sec-policy/selinux-alsa
...
Only then do a &amp;#39;repoman commit -m &amp;#39;Release of 2.20130424-r4&amp;#39;&amp;#39;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The script performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It creates the patch with the difference between the main refpolicy
    release and our repository. Our repository closely follows the
    upstream release, but still contains quite a few changes that have
    not been upstreamed yet (due to history loss of the changes, or the
    changes are very gentoo-specific, or the changes still need to
    be improved). In the past, we maintained all the patches separately,
    but this meant that the deployment of the policy ebuilds took too
    long (around 100 patches being applied takes a while, and took more
    than 80% of the total deployment time on a regular server system).
    By using a single patch file, the deployment time is
    reduced drastically.&lt;/li&gt;
&lt;li&gt;It then compresses this patch file and stores it in
    &lt;code&gt;/usr/portage/distfiles&lt;/code&gt; (so that later &lt;code&gt;repoman manifest&lt;/code&gt; can take
    the file into account) as well as on my dev.gentoo.org location
    (where it is referenced). If other developers create a release, they
    will need to change this location (and the pointer in the ebuilds).&lt;/li&gt;
&lt;li&gt;Previous file references in the &lt;code&gt;Manifest&lt;/code&gt; files are removed, so
    that &lt;code&gt;repoman&lt;/code&gt; does not think the digest can be skipped.&lt;/li&gt;
&lt;li&gt;New ebuilds are created, copied from the previous version. In these
    ebuilds, the &lt;code&gt;KEYWORDS&lt;/code&gt; variable is updated to only contain
    &lt;code&gt;~arch&lt;/code&gt; keywords.&lt;/li&gt;
&lt;li&gt;A release tag is created in the git repository.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then the script tells the user to add the new files to the repository,
run &lt;code&gt;repoman manifest&lt;/code&gt; and &lt;code&gt;repoman full&lt;/code&gt; to verify the quality of the
ebuilds and generate the necessary digests. Then, and also after
testing, the created ebuilds can be committed to the repository.&lt;/p&gt;
&lt;p&gt;The last few steps have explicitly not been automated so the developer
has the chance to update the ebuilds (in case more than just the policy
contents has changed between releases) or do dry runs without affecting
the &lt;code&gt;gentoo-x86&lt;/code&gt; repository.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="policy"></category><category term="release"></category><category term="selinux"></category></entry><entry><title>November online hardened meeting</title><link href="https://blog.siphos.be/2013/12/november-online-hardened-meeting/" rel="alternate"></link><published>2013-12-11T12:12:00+01:00</published><updated>2013-12-11T12:12:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-11:/2013/12/november-online-hardened-meeting/</id><summary type="html">&lt;p&gt;Later than usual, as I wasn't able to make the meeting myself (thus had
to wait for the meeting logs in order to draft up this summary), so here
it is. The next meeting is scheduled for next week, btw ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The 4.8.2 ebuild for GCC is available …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Later than usual, as I wasn't able to make the meeting myself (thus had
to wait for the meeting logs in order to draft up this summary), so here
it is. The next meeting is scheduled for next week, btw ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The 4.8.2 ebuild for GCC is available in the tree, currently still
masked.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grSecurity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The grSecurity project has made the patches for the 3.12 kernel
available; a hardened 3.12 kernel is available in the
&lt;code&gt;hardened-development&lt;/code&gt; overlay.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Matthew is working on &lt;a href="https://github.com/zfsonlinux/zfs/pull/1835"&gt;ZFSonLinux and
SELinux&lt;/a&gt; support.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Matthew and Steev have been working on SELinux and ARM support. It seems
to work, but kernel versions matter greatly. We might want to open up
ARM keywords.&lt;/p&gt;
&lt;p&gt;That's about it. As blueness wasn't there as well, the topics were
discussed quite fast. The full logs can be found &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.hardened/6117"&gt;on the gentoo-hardened
mailinglist&lt;/a&gt;.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="online"></category></entry><entry><title>Majority of GDP documents moved to Gentoo wiki</title><link href="https://blog.siphos.be/2013/12/majority-of-gdp-documents-moved-to-gentoo-wiki/" rel="alternate"></link><published>2013-12-10T16:03:00+01:00</published><updated>2013-12-10T16:03:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-12-10:/2013/12/majority-of-gdp-documents-moved-to-gentoo-wiki/</id><summary type="html">&lt;p&gt;The majority of the English gentoo documents that resided in
&lt;a href="http://www.gentoo.org/doc/en"&gt;www.gentoo.org/doc/en&lt;/a&gt; have now been
moved to the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;. All those documents
have been made available in the main namespace, meaning that
non-developers can continue to contribute on those articles and guides,
fully in the spirit …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The majority of the English gentoo documents that resided in
&lt;a href="http://www.gentoo.org/doc/en"&gt;www.gentoo.org/doc/en&lt;/a&gt; have now been
moved to the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;. All those documents
have been made available in the main namespace, meaning that
non-developers can continue to contribute on those articles and guides,
fully in the spirit of a wiki and community.&lt;/p&gt;
&lt;p&gt;We are now working on &lt;a href="https://wiki.gentoo.org/wiki/Project:Documentation/Overview"&gt;a method for displaying an overview of
documents&lt;/a&gt;
that have been moved, or have seen enough development on the wiki that
they are deemed of proper quality and stable (not too many changes
left). Currently, such documents are (requested to be) marked for
translation, allowing me or other developers to review if the guides are
indeed ok to move forward. If so, they are marked as translatable
(allowing the various translation members on the wiki to move forward
with translating the guides) and will be made part of the overview as
well.&lt;/p&gt;
&lt;p&gt;One of the few documents that has not been moved yet is the Gentoo
Handbook as it is of a different format than the guides, and includes
dynamic documentation generation features which I have yet to fully
investigate (wiki-wise). I currently focused mainly on the non-handbook
guides, so the handbook itself will get more attention later (after the
documentation overview is completed).&lt;/p&gt;
&lt;p&gt;Big thanks to &lt;a href="https://wiki.gentoo.org/wiki/User:A3li"&gt;Alex Legler&lt;/a&gt; for
assisting me and providing the necessary features and functionality in
the Gentoo wiki to make this all possible.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; added link to overview page.&lt;/p&gt;</content><category term="Documentation"></category><category term="documentation"></category><category term="documents"></category><category term="gdp"></category><category term="Gentoo"></category><category term="wiki"></category></entry><entry><title>New SELinux userspace release</title><link href="https://blog.siphos.be/2013/11/new-selinux-userspace-release-2/" rel="alternate"></link><published>2013-11-05T00:06:00+01:00</published><updated>2013-11-05T00:06:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-11-05:/2013/11/new-selinux-userspace-release-2/</id><summary type="html">&lt;p&gt;Between now and an hour, Gentoo users using the \~arch branch will
notice that new versions of the &lt;a href="http://userspace.selinuxproject.org/trac/wiki/Releases"&gt;SELinux userspace
applications&lt;/a&gt;
are now available. Released on October 30th, they contain many bug fixes
sent previously as well as a couple of interesting developments and
enhancements (more work on sepolicy, for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Between now and an hour, Gentoo users using the \~arch branch will
notice that new versions of the &lt;a href="http://userspace.selinuxproject.org/trac/wiki/Releases"&gt;SELinux userspace
applications&lt;/a&gt;
are now available. Released on October 30th, they contain many bug fixes
sent previously as well as a couple of interesting developments and
enhancements (more work on sepolicy, for instance).&lt;/p&gt;
&lt;p&gt;My tests revealed only a single issue that I still need to solve
(another issue, &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=490436"&gt;bug
490436&lt;/a&gt;, is hopefully
properly worked around), which is &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=490442"&gt;bug
490442&lt;/a&gt;, where
audit2allow does not want to generate refpolicy-style SELinux policy
modules. Other than that, most infrastructural tests were successful.&lt;/p&gt;
&lt;p&gt;Two SELinux policy updates were needed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;manage_lnk_files_pattern(semanage_t, semanage_store_t, semanage_store_t)
selinux_read_policy(sysadm_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first one is sent upstream as I think it is an obvious one (new
userspace now using symbolic links). The other one I'm not that sure of,
but for now it works. I made the above policy changes locally; if
approved I'll commit them to our tree asap.&lt;/p&gt;
&lt;p&gt;So, go play with it and report whatever you can on
&lt;a href="https://bugs.gentoo.org"&gt;bugzilla&lt;/a&gt; (SELinux component).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>The mix of libffi with other changes</title><link href="https://blog.siphos.be/2013/11/the-mix-of-libffi-with-other-changes/" rel="alternate"></link><published>2013-11-03T10:27:00+01:00</published><updated>2013-11-03T10:27:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-11-03:/2013/11/the-mix-of-libffi-with-other-changes/</id><summary type="html">&lt;p&gt;I &lt;a href="http://blog.siphos.be/2013/04/securely-handling-libffi/"&gt;once again&lt;/a&gt;
came across libffi. Not only does the libffi approach fight with SELinux
alone, it also triggers the TPE (Trusted Path Execution) protections in
grSecurity. And when I tried to reinstall Portage, Portage seemed to
create some sort of runtime environment in a temporary directory as
well, and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I &lt;a href="http://blog.siphos.be/2013/04/securely-handling-libffi/"&gt;once again&lt;/a&gt;
came across libffi. Not only does the libffi approach fight with SELinux
alone, it also triggers the TPE (Trusted Path Execution) protections in
grSecurity. And when I tried to reinstall Portage, Portage seemed to
create some sort of runtime environment in a temporary directory as
well, and SELinux wasn't up to allowing that either.&lt;/p&gt;
&lt;p&gt;Let's first talk about a quick workaround for the libffi-with-TPE issue.
Because libffi wants to create executable files in a world-writable
directory and then execute that file (try finding the potential security
issue here) TPE is prohibiting the execution. The easiest workaround is
to add the &lt;code&gt;portage&lt;/code&gt; Linux user, as well as the Linux accounts that you
use to run emerge with (even just things like &lt;strong&gt;emerge --info&lt;/strong&gt;) in the
&lt;code&gt;wheel&lt;/code&gt; group. This group is exempt from TPE protections (unless you
configured a different group in your kernel for this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# zgrep CONFIG_GRKERNSEC_TPE_TRUSTED_GID /proc/config.gz
CONFIG_GRKERNSEC_TPE_TRUSTED_GID=10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we also need to allow the &lt;code&gt;portage_t&lt;/code&gt; domain to execute files
labeled with &lt;code&gt;portage_tmpfs_t&lt;/code&gt;. You can do this by creating your own
SELinux policy module with the following content (or use selocal):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;can_exec(portage_t, portage_tmpfs_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This works around the libffi issue for now. A better solution still has
to be implemented (as discussed in the previous post).&lt;/p&gt;
&lt;p&gt;With regards to the portage installation failing, you'll notice this
quickly when you get an error like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge -1 portage
Calculating dependencies  ... done!
Traceback (most recent call last):
  File &amp;quot;/usr/bin/emerge&amp;quot;, line 50, in &amp;lt;module&amp;gt;
    retval = emerge_main()
  File &amp;quot;/usr/lib64/portage/pym/_emerge/main.py&amp;quot;, line 1031, in emerge_main
    return run_action(emerge_config)
  File &amp;quot;/usr/lib64/portage/pym/_emerge/actions.py&amp;quot;, line 4062, in run_action
    emerge_config.args, spinner)
  File &amp;quot;/usr/lib64/portage/pym/_emerge/actions.py&amp;quot;, line 453, in action_build
    retval = mergetask.merge()
  File &amp;quot;/usr/lib64/portage/pym/_emerge/Scheduler.py&amp;quot;, line 946, in merge
    rval = self._handle_self_update()
  File &amp;quot;/usr/lib64/portage/pym/_emerge/Scheduler.py&amp;quot;, line 316, in _handle_self_update
    _prepare_self_update(self.settings)
  File &amp;quot;/usr/lib64/portage/pym/portage/package/ebuild/doebuild.py&amp;quot;, line 2326, in _prepare_self_update
    shutil.copytree(orig_bin_path, portage._bin_path, symlinks=True)
  File &amp;quot;/usr/lib64/portage/pym/portage/__init__.py&amp;quot;, line 259, in __call__
    rval = self._func(*wrapped_args, **wrapped_kwargs)
  File &amp;quot;/usr/lib64/python3.3/shutil.py&amp;quot;, line 343, in copytree
    raise Error(errors)
shutil.Error: [(b&amp;#39;/usr/lib64/portage/bin/ebuild-helpers/prepalldocs&amp;#39;, 
b&amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepalldocs&amp;#39;, 
&amp;quot;[Errno 13] Permission denied: &amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepalldocs&amp;#39;&amp;quot;), 
(b&amp;#39;/usr/lib64/portage/bin/ebuild-helpers/prepinfo&amp;#39;, 
b&amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepinfo&amp;#39;, 
&amp;quot;[Errno 13] Permission denied: &amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/prepinfo&amp;#39;&amp;quot;), 
(b&amp;#39;/usr/lib64/portage/bin/ebuild-helpers/newlib.so&amp;#39;, 
b&amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/newlib.so&amp;#39;, 
&amp;quot;[Errno 13] Permission denied: &amp;#39;/var/tmp/portage/._portage_reinstall_.osj370/bin/ebuild-helpers/newlib.so&amp;#39;&amp;quot;), 
[...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And the errors go on and on and on.&lt;/p&gt;
&lt;p&gt;I've been able to get it working again by allowing the following SELinux
permissions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow portage_t portage_tmp_t:dir relabel_dir_perms;
allow portage_t portage_tmp_t:lnk_file relabel_lnk_file_perms;
allow portage_t bin_t:dir relabel_dir_perms;
allow portage_t bin_t:file relabel_file_perms;
allow portage_t bin_t:lnk_file relabel_lnk_file_perms;
allow portage_t portage_exec_t:file relabel_file_perms;
allow portage_t portage_fetch_exec_t:file relabel_file_perms;
allow portage_t lib_t:dir relabel_dir_perms;
allow portage_t lib_t:file relabel_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can somewhat shorten this by combining types (but this doesn't work
with selocal for now):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow portage_t { portage_tmp_t bin_t lib_t}:dir relabel_dir_perms;
allow portage_t { portage_tmp_t bin_t }:lnk_file relabel_lnk_file_perms;
allow portage_t { bin_t portage_exec_t portage_fetch_exec_t lib_t}:file relabel_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At the end of the emerge process (when the new portage is installed) you
might want to reset the labels of all files provided by the portage
package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rlpkg portage
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;These changes have not been passed into the policy yet as I first need
to find out why exactly they are needed, and as you can see from &lt;a href="http://dev.gentoo.org/devaway/"&gt;the
gentoo devaway&lt;/a&gt; page, time is not on my
side to do this. I'll try to free up some time in the next few days to
handle this as well as the &lt;a href="http://userspace.selinuxproject.org/trac/wiki/Releases"&gt;SELinux userspace
release&lt;/a&gt; but no
promises here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; I found why - it is the &lt;code&gt;_prepare_self_update&lt;/code&gt; in the
&lt;code&gt;doebuild.py&lt;/code&gt; script. It creates temporary copies of the Portage
binaries and Portage python libraries, which is why we need to support
relabel operations on the files. Support for this is now in the policy
repository.&lt;/p&gt;
&lt;/p&gt;</content><category term="Security"></category><category term="Gentoo"></category><category term="hardened"></category><category term="libffi"></category><category term="portage"></category><category term="selinux"></category></entry><entry><title>Gentoo Hardened meeting 201310</title><link href="https://blog.siphos.be/2013/10/gentoo-hardened-meeting-201310/" rel="alternate"></link><published>2013-10-24T23:25:00+02:00</published><updated>2013-10-24T23:25:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-10-24:/2013/10/gentoo-hardened-meeting-201310/</id><summary type="html">&lt;p&gt;We gathered online again to talk about the progress, changes and other
stuff related to the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;New Developer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We welcomed Zero_Chaos as a new addition to our team. Big welcome, with
the usual IRC kick in between, ensued.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8.x is unmasked and ready …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We gathered online again to talk about the progress, changes and other
stuff related to the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo
Hardened&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;New Developer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We welcomed Zero_Chaos as a new addition to our team. Big welcome, with
the usual IRC kick in between, ensued.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8.x is unmasked and ready for testing. The ASAN problem however is
not resolved and it doesn't look like upstream wants to provide the fix
for this. As a result, if you want to use ASAN, you will need to disable
UDEREF in the kernel.&lt;/p&gt;
&lt;p&gt;A difficult problem here is that, if a user forgets to disable UDEREF,
then building with ASAN will fail. If he disabled UDEREF and uses ASAN,
then booting into a UDEREF enabled kernel will fail. If he starts
building with ASAN on UDEREF kernels, things will break. Its ugly, and
unless the ASAN code is changed to support other technologies using the
address space layout information, it will remain mutually exclusive.&lt;/p&gt;
&lt;p&gt;For now, we'll properly document the situation.&lt;/p&gt;
&lt;p&gt;GCC 4.9 will end its stage1 development phase on November 21st.&lt;/p&gt;
&lt;p&gt;The uclibc stages are currently built with GCC-4.7.3-r1, except for the
MIPS architectures which use GCC 4.8.1-r1.&lt;br&gt;
The stages are built once every two months, give or take.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel grSec/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Standard maintenance, such as keeping up with upstream changes, has
taken place. The XATTR problems with the &lt;strong&gt;install&lt;/strong&gt; binary/phase has
not been resolved yet due to time constraints. First focus will be on
writing the C-based wrapper to see if this significantly speeds things
up.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;USE=pax_kernel&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is ambiguity of the meaning or use of the &lt;code&gt;pax_kernel&lt;/code&gt; USE flag.
It seems to be used for multiple, exclusive things, like limiting builds
on PaX kernels or limiting run-time behavior on PaX kernels. To build
Gentoo images, developers need to set &lt;code&gt;pax_kernel&lt;/code&gt; on some packages and
disable it on others in order for the image to build properly.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;@blueness&amp;gt; USE=pax_kernel in the case of userland should apply patches etc, that fix the code
            so that it won&amp;#39;t trip pax protection eg mprotect
&amp;lt;@blueness&amp;gt; in the case of kernel modules, it can&amp;#39;t mean that obviously!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;One of the mentioned problems is that some ebuilds only enable pax
markings when &lt;code&gt;pax_kernel&lt;/code&gt; is set. However, that shouldn't be done
conditionally. PaX markings can be done even without a PaX kernel.&lt;/p&gt;
&lt;p&gt;Zero_Chaos (new developers always have the necessary energy) will try
to update ebuilds where applicable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux and System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nothing worth mentioning here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profile&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many users were using hardened profiles on the desktop. In the past, we
had the desktop/hardened profiles removed (referring users to the
regular hardened profiles and asking them to update their system with
the USE flags (and other settings) they want for their desktop.
Apparently, this is giving some problems for some users, so the idea is
to reinstate the desktop/hardened profiles where the hardened settings
overrule the desktop settings.&lt;/p&gt;
&lt;p&gt;However, we should take care of tackling the issues we had in the past
(which is the reason why we removed the profiles in the first place). It
is recommended that we discuss this outside the IRC meeting to make sure
we don't reintroduce the issues again while using a flexible approach.&lt;/p&gt;
&lt;p&gt;That's it. A big thanks to the developers, contributors and thousands of
users!&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="online"></category></entry><entry><title>In-browser encryption for online password management</title><link href="https://blog.siphos.be/2013/10/in-browser-encryption-for-online-password-management/" rel="alternate"></link><published>2013-10-20T21:29:00+02:00</published><updated>2013-10-20T21:29:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-10-20:/2013/10/in-browser-encryption-for-online-password-management/</id><summary type="html">&lt;p&gt;Lately I've been trying to find a good free software project that uses
PHP or cgi-bin (one of the requirements for this particular
organization) that allows its users to store passwords centrally, but
uses encryption on the browser level before the passwords are sent to
the central server. I've found …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately I've been trying to find a good free software project that uses
PHP or cgi-bin (one of the requirements for this particular
organization) that allows its users to store passwords centrally, but
uses encryption on the browser level before the passwords are sent to
the central server. I've found one -
&lt;a href="https://www.clipperz.com"&gt;Clipperz&lt;/a&gt; - but was not able to get it to
build and install.&lt;/p&gt;
&lt;p&gt;With the continuous revelations regarding hacked sites and servers (and
even potential snooping into server data by governments) the requirement
isn't that weird: by using strong encryption (I currently still assume
that AES-256 is safe for use) on the browser level, no unencrypted
sensitive data (such as usernames and passwords) would be sent to the
server, let alone stored (in plain text) on the server database.&lt;/p&gt;
&lt;p&gt;I did a small test to see how difficult it would be to implement this in
a simple PHP password management tool called &lt;a href="http://onlinepasswords.sourceforge.net/"&gt;online
passwords&lt;/a&gt;. The PHP-based
application does not even use a database, relying on flat-files instead.
By design, the tool encrypts the data before storing on the file system,
but I wanted to go a bit further, implementing the in-browser
encryption. The Javascript AES is provided by
&lt;a href="http://www.movable-type.co.uk/scripts/aes.html"&gt;movable-type.co.uk&lt;/a&gt; and
for the hashing algorithm I found &lt;a href="http://pajhome.org.uk/crypt/md5"&gt;pajhome's
implementation&lt;/a&gt; often cited.&lt;/p&gt;
&lt;p&gt;The first thing I did was substitute the password information needed to
log on to the site (and which is also used as encryption key for the
back-end side encryption) with a hashed version of the password. For the
application, this hardly matters - it is still the encryption key it
will use on the backend, although most likely a bit stronger than most
passwords would be.&lt;/p&gt;
&lt;p&gt;Next, I keep the real password in a local session storage (which is
supported by most modern browsers nowadays) so that the user only has to
enter it once (when logging on to the site) and it is kept in memory
then, never leaving the browser. This is needed in order to decrypt the
data as we get it without having to ask the user for the password over
and over again. Of course, I don't want to keep this password in a
Cookie (or pass it on through the URL) because that would void the idea
of keeping the password (reasonably) secure.&lt;/p&gt;
&lt;p&gt;To accomplish this, I hide the password field of the PHP application
itself, and create a second input field (outside the &lt;code&gt;&amp;lt;form&amp;gt; &amp;lt;/form&amp;gt;&lt;/code&gt; to
make sure its value is never POSTed to the site) in which the user
enters his password. Upon submit of the data, the following javascript
code will create the hash of the password (and user name) to use as the
"site password" for the application, and put that in the (hidden) input
field. It then also stores the site password in the local session
storage in the browser. The code is triggered through the &lt;em&gt;onSubmit&lt;/em&gt;
handler of the form.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;function storeAppPassword() {
  var sitepw = document.getElementById(&amp;#39;password&amp;#39;);
  var siteus = document.getElementById(&amp;#39;login&amp;#39;);
  var userpw = document.getElementById(&amp;#39;userpassword&amp;#39;);

  sessionStorage.setItem(&amp;#39;userpassword&amp;#39;, userpw.value);
  sitepw.value = hex_sha1(siteus.value + userpw.value);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I need to make sure that the fields that need to be encrypted (the
various user ids and passwords that are stored on the site) are
encrypted before they are sent to the server, and decrypted after having
received them by the browser. For instance, if the fields are within a
form, the following javascript function could be triggered on the
&lt;em&gt;onSubmit&lt;/em&gt; handler again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;function encryptFields() {
  var useridFld = document.getElementById(&amp;#39;userid&amp;#39;);
  var passwordFld = document.getElementById(&amp;#39;password&amp;#39;);
  var notesFld = document.getElementById(&amp;#39;notes&amp;#39;);

  var pw = sessionStorage.getItem(&amp;#39;userpassword&amp;#39;);
  useridFld.value = Aes.Ctr.encrypt(useridFld.value, pw, 256);
  passwordFld.value = Aes.Ctr.encrypt(passwordFld.value, pw, 256);
  notesFld.value = Aes.Ctr.encrypt(notesFld.value, pw, 256);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly, to decrypt the fields (inside the same form), that part of
the code would become:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;useridFld.value = Aes.Ctr.decrypt(useridFld.value, pw, 256);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Decryption of the fields can be called by a simple javascript call at
the end of the page.&lt;/p&gt;
&lt;p&gt;If the data is within regular fields (non-form related), such as a
table, you'll need to find the right DOM element and call the decryption
function there.&lt;/p&gt;
&lt;p&gt;With those few changes, I was able to get it up and running quickly. I
don't think I'll use the PHP application itself in production though, as
it doesn't look like it sanitizes the field data in the PHP code and it
starts to show performance issues when called with only a few hundred
accounts, each having a few dozen passwords. But that hardly matters for
this post where I want to point out that it isn't that hard to put some
higher security on such sites.&lt;/p&gt;
&lt;p&gt;The big downside right now is that, if the user forgets his password, he
wont have access to all his data (similar to the Clipperz one). And
unlike Clipperz, the approach above does not allow for password changes
yet (although it doesn't look that hard to implement some logic
decrypting and re-encrypting the data with a different password if that
comes about). An approach to resolve that would be to encrypt all data
with a static key, and then encrypt that key with the password, storing
the encrypted key on the server. A password change only requires a
decrypt/encrypt of the key while all values remain encrypted with the
static key.&lt;/p&gt;
&lt;p&gt;Moral of the story: application managers of web password storage sites:
please add in-browser encryption for those of us that want to make
*really* sure that no sensitive data is sent over unencrypted (I don't
count SSL/TLS as that "ends" at the remote side while this one is full
end-to-end encryption).&lt;/p&gt;</content><category term="Security"></category><category term="aes"></category><category term="encryption"></category><category term="javascript"></category><category term="password"></category><category term="passwordmanagement"></category></entry><entry><title>A bug please...</title><link href="https://blog.siphos.be/2013/09/a-bug-please/" rel="alternate"></link><published>2013-09-30T21:53:00+02:00</published><updated>2013-09-30T21:53:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-30:/2013/09/a-bug-please/</id><summary type="html">&lt;p&gt;I know contacting me (or other developers) through IRC is often fast,
but having a bug report on our &lt;a href="https://bugs.gentoo.org"&gt;bugzilla&lt;/a&gt; is
very important to me and other developers. Allow me to explain a bit
why.&lt;/p&gt;
&lt;p&gt;First of all, &lt;em&gt;IRC is ephemeral&lt;/em&gt;. If we are not immediately on IRC
noticing it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I know contacting me (or other developers) through IRC is often fast,
but having a bug report on our &lt;a href="https://bugs.gentoo.org"&gt;bugzilla&lt;/a&gt; is
very important to me and other developers. Allow me to explain a bit
why.&lt;/p&gt;
&lt;p&gt;First of all, &lt;em&gt;IRC is ephemeral&lt;/em&gt;. If we are not immediately on IRC
noticing it, we might not notice it at all. Even with highlights on IRC
or in a separate &lt;code&gt;/QUERY&lt;/code&gt; window we might still miss it, because the IRC
client might disconnect (for instance because the server on which we run
our chat client reboots for a kernel update, or because of some weird
issue where we decide quit/restart is better). That doesn't make IRC the
wrong method - it can be perfect to ask for some attention on a bug, and
if you are on IRC while we are tackling it, it is very easy to ask for
some feedback.&lt;/p&gt;
&lt;p&gt;A second aspect is about the &lt;em&gt;completeness of the report&lt;/em&gt;. On a bugzilla
report, you can (well, should) add in the necessary information that
developers need to troubleshoot it. Gentoo supports many setups, so the
output of &lt;code&gt;emerge --info&lt;/code&gt; and other output that might be requested (like
build logs) are very important. On IRC, a "report" might seem as simple
as "package foo has a circular dependency", but without knowing with
which package the circular dependency is or with which versions, it
might be difficult to deduce. Dependencies can even be USE-driven, so
without knowing what the USE flags are, the circular dependency issue
might not be that obvious. Having a complete bugreport (with the
necessary attachments) makes for a much easier resolution development
lifecycle.&lt;/p&gt;
&lt;p&gt;Third, and in my opinion &lt;strong&gt;extremely important&lt;/strong&gt;, is that bug reports
are &lt;em&gt;searchable&lt;/em&gt;. Other users might have the same problem you are
facing. By having a bugreport they can chime in (if the problem has not
been resolved yet), providing faster feedback (for instance, other users
give feedback on additional questions you ask while you are sleeping and
when you are back up the bug is resolved). Or, if the bug has been
resolved (but the change is still in &lt;code&gt;~arch&lt;/code&gt;) they will find the
solution more easily. And often bugreports also document the workaround
(even if just as a way for the developer to confirm that the issue is
what it is) allowing other users to switch gears faster with the
problem.&lt;/p&gt;
&lt;p&gt;Also, the entire communication chain between developer and reporter(s)
also gives a lot of interesting information. Some developers give a
lengthy explanation as to why the issue occurred (which is always useful
to know) or explain in which circumstances it would or wouldn't be
visible. Most developers also link the bug in the commits and ChangeLog
entries so that people who read through the changes can quickly find
more information about the change.&lt;/p&gt;
&lt;p&gt;Bugreports also allow for tracking by people who are not on IRC (or
mailinglists) but that do want to help out. For instance, bugs assigned
to the SELinux alias on Gentoo's bugzilla are followed up by a handful
of non-developers who often give feedback much faster than I or another
developer can. If this would be reported on IRC only, you would miss the
opportunity to work with these excellent people (and thus get a faster
resolution).&lt;/p&gt;
&lt;p&gt;Another advantage of bugreports is that dependencies between bugs can be
placed. A bug might only be resolved if another bug is resolved as well
- this dependency information is made part of bug reports to give a
clear view on the situation.&lt;/p&gt;
&lt;p&gt;Bug reports on a bugzilla (or other bug tracking software) have other
advantages as well, but the above ones are my top reasons. So while I
don't mind if I can quickly fix things I notice on IRC (such as a
missing dependency), if you want to make sure I catch it, please use
bugzilla.&lt;/p&gt;</content><category term="Gentoo"></category><category term="bugreport"></category><category term="bugs"></category><category term="bugzilla"></category><category term="Gentoo"></category></entry><entry><title>It has finally arrived: SELinux System Administration</title><link href="https://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/" rel="alternate"></link><published>2013-09-27T15:10:00+02:00</published><updated>2013-09-27T15:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-27:/2013/09/it-has-finally-arrived-selinux-system-administration/</id><summary type="html">&lt;p&gt;Almost everyone has it - either physical or in their heads: a list of
things you want to do or achieve before you... well, stop existing. Mine
still has numerous things on it (I should get on it, I know) but one of
the items on that list has recently been …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Almost everyone has it - either physical or in their heads: a list of
things you want to do or achieve before you... well, stop existing. Mine
still has numerous things on it (I should get on it, I know) but one of
the items on that list has recently been removed: write and have a book
published. And the result is a book called &lt;a href="http://www.packtpub.com/selinux-system-administration/book"&gt;SELinux System
Administration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Somewhere in the second quarter of this year, &lt;a href="http://www.packtpub.com/"&gt;Packt
Publishing&lt;/a&gt; contacted me to see if I am
interested in authoring a book about SELinux, focusing on the usage of
SELinux (you know - handling booleans, dealing with file contexts, etc.)
in a short technical book (the aim was 100 pages). Considering that I'm
almost always busy with documentation and editing (for instance, I
&lt;a href="http://www.gentoo.org/news/en/gwn/20031110-newsletter.xml#doc_chap2"&gt;joined
Gentoo&lt;/a&gt;
as documentation translator and editor beginning of 2003 if I remember
correctly) and am busy keeping SELinux support within Gentoo on a good
level, I of course said yes to the request.&lt;/p&gt;
&lt;p&gt;Now, 100 pages is not a lot for a topic as complex and diverse as
SELinux so was really challenging, but I do think I managed to get
everything in it while keeping it practical. The book first starts with
the fundamentals of SELinux - concepts you really need to grasp before
diving into SELinux. Then, it goes on about switching SELinux state
(disabling, permissive, granular permissive, etc.), logging, managing
SELinux users and roles, handling process domains, etc. Just take a look
at the table of contents and you'll see what I mean ;-)&lt;/p&gt;
&lt;p&gt;Inside the book, examples are given based on Fedora (and thus also
RedHat Enterprise Linux) and Gentoo Hardened while ensuring that there
are few distribution specific sections in it, making it usable for Linux
administrators of systems with a different Linux distribution installed
to it. Take a look at the &lt;a href="http://www.packtpub.com/sites/default/files/9781783283170_Chapter-03.pdf?utm_source=packtpub&amp;amp;utm_medium=free&amp;amp;utm_campaign=pdf"&gt;sample
chapter&lt;/a&gt;
and, if you like it, put it on your wish list and let everyone know ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="administration"></category><category term="book"></category><category term="fedora"></category><category term="Gentoo"></category><category term="packt"></category><category term="packtpub"></category><category term="selinux"></category><category term="system"></category></entry><entry><title>Aaaand we're back - hardened monthly meeting</title><link href="https://blog.siphos.be/2013/09/aaaand-were-back-hardened-monthly-meeting/" rel="alternate"></link><published>2013-09-26T22:22:00+02:00</published><updated>2013-09-26T22:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-26:/2013/09/aaaand-were-back-hardened-monthly-meeting/</id><summary type="html">&lt;p&gt;It almost feels like we had our monthly online meeting just a week ago.
Below a small write-up of the highlights. If you want to know the gory
details, just wait a few hours/days until the IRC logs are sent out ;-)
Now remember, the project does more than what …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It almost feels like we had our monthly online meeting just a week ago.
Below a small write-up of the highlights. If you want to know the gory
details, just wait a few hours/days until the IRC logs are sent out ;-)
Now remember, the project does more than what the highlights tell you -
there is lots of maintenance done "under the hood", allowing everyone to
keep using the various
&lt;a href="https://wiki.gentoo.org/wiki/Hardened/Introduction_to_Hardened_Gentoo"&gt;technologies&lt;/a&gt;
supported through our project.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As per a discussion on the Gentoo development mailinglist, GCC 4.8 will
most likely have SSP enabled as default. Gentoo Hardened will still
enable full SSP (&lt;code&gt;-fstack-protector-all&lt;/code&gt;) whereas Gentoo by default will
probably work with &lt;code&gt;-fstack-protector&lt;/code&gt;). We will also still provide GCC
specifications that disable stack protection completely (the
&lt;code&gt;hardened-nossp&lt;/code&gt; specs) for when developers or users need it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grsec/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Blueness informed us about one issue with XATTR_PAX implementation,
being the &lt;code&gt;install.py&lt;/code&gt; wrapper that we need in order to set the right
attributes as early as possible. The problem is that it is very slow (as
it is invoked over and over again, so the overhead of it being an
interpreted script becomes huge). A solution for this still has to be
defined.&lt;/p&gt;
&lt;p&gt;Some ideas are to use a compiled version, but other possible solutions
such as hooking into Portage directly or using lists have been suggested
as well.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nothing big - standard maintenance stuff, such as pushing our own
patches upstream for others to benefit (and hopefully have the master
projects include the patches so we don't need to maintain them
ourselves). Also, revision 3 of the 2.20130424 policies are now in the
tree (\~arch'ed for now).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within Gentoo, we have a couple of SCAP scanners/software available,
such as open-scap and ovaldi. Recently, openscap-9999 is made available
(allowing users to directly use the latest openscap release) which is
used to validate and evaluate security benchmarks I am developing for
Gentoo and related services.&lt;/p&gt;
&lt;p&gt;Next to the SCAP-related developments, a
&lt;a href="https://wiki.gentoo.org/wiki/Signed_kernel_module_support"&gt;guide&lt;/a&gt; has
been put on the Gentoo wiki about using signed kernel modules.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The hardened profiles have been updated to use EAPI-5 so we can benefit
from its features, such as improved multilib support.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned before, I'm working a bit on a &lt;a href="http://dev.gentoo.org/~swift/docs/security_benchmarks/"&gt;Gentoo Security
Benchmark&lt;/a&gt; that
can be used with SCAP software. The sources are in the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-docs.git;a=tree;f=xml/SCAP;hb=HEAD"&gt;hardened-docs&lt;/a&gt;
repository for now.&lt;/p&gt;
&lt;p&gt;Also, most/all hardened documentation is moved from the
(developer-editable only) &lt;code&gt;Project:&lt;/code&gt; namespace to the general one,
allowing users and contributors to help us with the documents as well.&lt;/p&gt;</content><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category></entry><entry><title>Underestimated or underused: Portage (e)logging</title><link href="https://blog.siphos.be/2013/09/underestimated-or-underused-portage-elogging/" rel="alternate"></link><published>2013-09-25T10:09:00+02:00</published><updated>2013-09-25T10:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-25:/2013/09/underestimated-or-underused-portage-elogging/</id><summary type="html">&lt;p&gt;Within 30 minutes of each other, two people on the &lt;code&gt;#gentoo&lt;/code&gt; channel
asked if Portage kept logs of the messages displayed during the build
and installation of a package. Of course, the answer is a sounding "yes"
- and depending on your needs, you can even save more of the logging …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Within 30 minutes of each other, two people on the &lt;code&gt;#gentoo&lt;/code&gt; channel
asked if Portage kept logs of the messages displayed during the build
and installation of a package. Of course, the answer is a sounding "yes"
- and depending on your needs, you can even save more of the logging. If
you haven't read the &lt;a href="http://www.gentoo.org/doc/en/handbook"&gt;Gentoo
handbook&lt;/a&gt; yet, do so - it
contains a section on &lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-x86.xml?part=3&amp;amp;chap=1#doc_chap4"&gt;Logging
Features&lt;/a&gt;
that explains how and when logging is enabled.&lt;/p&gt;
&lt;p&gt;Let's start with the default &lt;em&gt;elog&lt;/em&gt; support. By default, Gentoo's
Portage will log messages that ebuild developers have put in the
packages through the use of the &lt;code&gt;elog&lt;/code&gt;, &lt;code&gt;ewarn&lt;/code&gt; and &lt;code&gt;eerror&lt;/code&gt; functions.
These methods are used by the developers to notify the administrator
about something useful or important (usually &lt;code&gt;elog&lt;/code&gt;), a potential issue
that might occur (&lt;code&gt;ewarn&lt;/code&gt;) or even a problem that was found (&lt;code&gt;eerror&lt;/code&gt;).
Let's look at a snippet of the &lt;code&gt;app-admin/eselect-postgresql-1.0.10&lt;/code&gt;
package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pkg_postinst() {
    ewarn &amp;quot;If you are updating from app-admin/eselect-postgresql-0.4 or older, run:&amp;quot;
    ewarn
    ewarn &amp;quot;    eselect postgresql update&amp;quot;
    ewarn
    ewarn &amp;quot;To get your system in a proper state.&amp;quot;
    elog &amp;quot;You should set your desired PostgreSQL slot:&amp;quot;
    elog &amp;quot;    eselect postgresql list&amp;quot;
    elog &amp;quot;    eselect postgresql set &amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, the developer warns the administrator that, if an upgrade of the
package from version 0.4 or older occurred, an additional action needs
to be taken. For others, he informs the administrators how to mark the
proper active PostgreSQL slot (using the &lt;code&gt;elog&lt;/code&gt; methods).&lt;/p&gt;
&lt;p&gt;Portage will save this information by default in &lt;code&gt;/var/log/portage/elog&lt;/code&gt;
as separate files for each deployment:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ls /var/log/portage/elog/*eselect-postgres*
/var/log/portage/elog/app-admin:eselect-postgresql-1.0.10:20130818-184756.log
/var/log/portage/elog/app-admin:eselect-postgresql-1.2.0:20130915-131631.log
/var/log/portage/elog/app-admin:eselect-postgresql-1.2.0:20130915-200851.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If &lt;code&gt;FEATURES="split-elog"&lt;/code&gt; is set in your &lt;code&gt;make.conf&lt;/code&gt; file, then the
elog files will be stored in separate category subdirectories. Below is
the content of such a single file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ cat /var/log/portage/elog/app-admin\:eselect-postgresql-1.0.10\:20130818-184756.log
INFO: setup
Package:    app-admin/eselect-postgresql-1.0.10
Repository: gentoo
Maintainer: titanofold@gentoo.org postgresql
USE:        abi_x86_64 amd64 elibc_glibc kernel_linux multilib selinux userland_GNU
FEATURES:   preserve-libs sandbox selinux sesandbox
WARN: postinst
If you are updating from app-admin/eselect-postgresql-0.4 or older, run:

    eselect postgresql update

To get your system in a proper state.
LOG: postinst
You should set your desired PostgreSQL slot:
    eselect postgresql list
    eselect postgresql set &amp;lt;slot&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the file, package information is provided, and each set of logging
paragraphs is accompanied with information where in the build process
the logging came up. For instance, &lt;code&gt;WARN: postinst&lt;/code&gt; sais that the
following text is through the &lt;code&gt;ewarn&lt;/code&gt; method in the post install phase
of the package installation process.&lt;/p&gt;
&lt;p&gt;By default Portage logs this, but you can ask it to mail the logs as
well. For more information about that, check the handbook link earlier
and look for the &lt;code&gt;PORTAGE_ELOG_SYSTEM&lt;/code&gt; and &lt;code&gt;PORTAGE_ELOG_MAIL*&lt;/code&gt;
variables. Or look at &lt;strong&gt;man make.conf&lt;/strong&gt; which also contains all the
information needed.&lt;/p&gt;
&lt;p&gt;Now, this only pertains to the specific logging that ebuild maintainers
added in the packages. But what if you want to keep track of all the
build logs? In that case, you need to define &lt;code&gt;PORT_LOGDIR&lt;/code&gt; in your
&lt;code&gt;make.conf&lt;/code&gt; file. This variable has to point to a location where Portage
(which usually runs as the &lt;code&gt;portage&lt;/code&gt; Linux user) has write access to and
where it is allowed to store the build logs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ grep PORT_LOGDIR /etc/portage/make.conf 
PORT_LOGDIR=&amp;quot;/var/log/portage&amp;quot;
~$ ls -ldZ /var/log/portage/
drwxrwsr-x. 3 portage portage system_u:object_r:portage_log_t 679936 Sep 24 13:12 /var/log/portage/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the variable set, build logs will be provided for each emerge (and
unmerge) operation of the package. These logs contain everything shown
on the terminal during a build process. Of course, this directory will
grow considerably in size so it is wise to properly handle old log
files. You can use logrotate for this, or just a single cronjob that
cleans up log files older than say 6 months. But Portage also provides
this functionality. If you set &lt;code&gt;FEATURES="clean-logs"&lt;/code&gt; in your
&lt;code&gt;make.conf&lt;/code&gt; file, then all log files older than 7 days will be removed
from the system. You can fine-tune this by setting &lt;code&gt;PORT_LOGDIR_CLEAN&lt;/code&gt;
to the command you want executed. Its default value can be found in the
&lt;code&gt;/usr/share/portage/config/make.globals&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;If you set &lt;code&gt;PORT_LOGDIR&lt;/code&gt;, be aware that the elog files (described at the
beginning) will be stored in &lt;code&gt;${PORT_LOGDIR}/elog&lt;/code&gt;. Similar to the
elogging, build logging can also be done in category subdirectories. If
&lt;code&gt;FEATURES="split-log"&lt;/code&gt; is set, the build logs will be stored in
&lt;code&gt;${PORT_LOGDIR}/build/&amp;lt;category&amp;gt;&lt;/code&gt; instead of &lt;code&gt;${PORT_LOGDIR}&lt;/code&gt; directly.&lt;/p&gt;
&lt;p&gt;Hopefully this post brings some users closer to this nice feature of
Portage.&lt;/p&gt;</content><category term="Gentoo"></category><category term="elog"></category><category term="Gentoo"></category><category term="logging"></category><category term="portage"></category></entry><entry><title>Creating a poor man central SCAP system</title><link href="https://blog.siphos.be/2013/09/creating-a-poor-man-central-scap-system/" rel="alternate"></link><published>2013-09-24T13:35:00+02:00</published><updated>2013-09-24T13:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-24:/2013/09/creating-a-poor-man-central-scap-system/</id><summary type="html">&lt;p&gt;A few weeks ago, I was asked to give some explanation about how SCAP
content can be used in companies to improve their infrastructure
knowledge. The focus back then was to look at benchmarks (secure states)
and violations, but other functionality should not be ignored. I'm not
going to talk …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, I was asked to give some explanation about how SCAP
content can be used in companies to improve their infrastructure
knowledge. The focus back then was to look at benchmarks (secure states)
and violations, but other functionality should not be ignored. I'm not
going to talk about how SCAP can be used in various cases - that'll be
for later - but one of the remarks came how SCAP can be used in larger
environments. The question was not all that weird, as I explained the
functionality through &lt;a href="http://www.open-scap.org"&gt;Open-SCAP&lt;/a&gt; which
currently uses a local scanning approach, and for larger environments
you want to have remote scanning capabilities.&lt;/p&gt;
&lt;p&gt;There are many commercial products available that provide remote
scanning (so you centrally manage the SCAP content and it is "played"
against remote systems), but as a free software advocate I want to see
how this can be achieved in free software. There is
&lt;a href="http://spacewalk.redhat.com/"&gt;spacewalk&lt;/a&gt;, the upstream project of the
RedHat Network Sattelite project, but that looked a bit too complex for
"just" handling SCAP content on remote systems. That, and I'm wondering
if it would be that usable for non-RedHat systems.&lt;/p&gt;
&lt;p&gt;So I decided to make a quick prototype of how I would approach handling
SCAP content in a larger environment. I pushed the result to a
&lt;a href="https://github.com/sjvermeu/pmcs"&gt;github&lt;/a&gt; project, called &lt;em&gt;poor man
central SCAP&lt;/em&gt;. Or, in abbreviated form, &lt;em&gt;pmcs&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The idea is simple: have a central configuration repository that defines
the SCAP content to be played on the remote systems, and have the remote
systems pull this information, evaluating the SCAP content and sending
it to a central reporting repository (which of course can be the same
target where the central configuration is stored). After a few hours of
coding and writing documentation, I have a workable solution with some
nice features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pmcs supports a local-configuration-less approach on the
    target systems. The only thing you need to do is schedule the pmcs
    agent (currently only available as a shell script, but I'll be
    working on a perl or python equivalent soon so that I can support
    other platforms than Unix) with one URL - which is where the
    configuration is stored. No need for local configuration files.&lt;/li&gt;
&lt;li&gt;pmcs uses local SCAP scanning software (that needs to be available
    on the target platforms) but has no strict requirements to that
    software other than that it has to be triggered command-line. This
    allows us to leverage the existing knowledge and features available
    in tools like open-scap or
    &lt;a href="http://sourceforge.net/projects/ovaldi/"&gt;ovaldi&lt;/a&gt; or
    &lt;a href="http://joval.org/"&gt;jOVAL&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;By using a somewhat hierarchical configuration structure and
    keywording support, administrators can fine-tune which SCAP content
    is evaluated on which systems&lt;/li&gt;
&lt;li&gt;pmcs also supports a "daemonized" approach where administrators can
    ask for the immediate evaluation of some SCAP content. This allows
    admins to quickly obtain system information using OVAL/XCCDF.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information, consult the
&lt;a href="https://github.com/sjvermeu/pmcs/blob/master/README.md"&gt;README&lt;/a&gt; or
&lt;a href="https://github.com/sjvermeu/pmcs/blob/master/docs/DESIGN.md"&gt;DESIGN&lt;/a&gt;
document. I'm working on a &lt;a href="https://github.com/sjvermeu/pmcs/blob/master/docs/USES.md"&gt;use
case&lt;/a&gt;
document as well. The tool hardly contains much coding (thanks to the
available KISS tools on many Unix/Linux systems) and is GPL-3.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Switching gpg key to 0x2EDD52403B68AF47</title><link href="https://blog.siphos.be/2013/09/switching-gpg-key-to-0x2edd52403b68af47/" rel="alternate"></link><published>2013-09-19T21:17:00+02:00</published><updated>2013-09-19T21:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-19:/2013/09/switching-gpg-key-to-0x2edd52403b68af47/</id><summary type="html">&lt;p&gt;I recently switched my GnuPG key. The previous key - which is still in
place for now (no revocation send out yet) - was 0x5DFAB3ECCDBA2FDB and
was a 1024 bit DSA key. The new one, 0x2EDD52403B68AF47, is a 4096 bit
RSA key. It also has the following preferences:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gpg&amp;gt; showpref
[ultimate] (1 …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I recently switched my GnuPG key. The previous key - which is still in
place for now (no revocation send out yet) - was 0x5DFAB3ECCDBA2FDB and
was a 1024 bit DSA key. The new one, 0x2EDD52403B68AF47, is a 4096 bit
RSA key. It also has the following preferences:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gpg&amp;gt; showpref
[ultimate] (1). Sven Vermeulen &amp;lt;sven.vermeulen@siphos.be&amp;gt;
[ultimate] (2)  Sven Vermeulen &amp;lt;swift@gentoo.org&amp;gt;
     Cipher: AES256, AES192, AES, CAST5, 3DES
     Digest: SHA512, SHA384, SHA256, SHA224, SHA1
     Compression: BZIP2, ZLIB, ZIP, Uncompressed
     Features: MDC, Keyserver no-modify
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The new key's fingerprint is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gpg&amp;gt; fpr
pub   4096R/0x2EDD52403B68AF47 2013-09-16 Sven Vermeulen 
 Primary key fingerprint: 7264 9F85 E8F1 6F6A 4B68  1102 2EDD 5240 3B68 AF47
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A signed &lt;a href="http://dev.gentoo.org/~swift/key-transition.txt.asc"&gt;key transition
statement&lt;/a&gt; can be
found on my Gentoo development page; the document is signed with both of
my keys (the old one and new one).&lt;/p&gt;</content><category term="Security"></category><category term="gpg"></category><category term="key"></category></entry><entry><title>cvechecker 3.3 released</title><link href="https://blog.siphos.be/2013/09/cvechecker-3-3-released/" rel="alternate"></link><published>2013-09-16T16:06:00+02:00</published><updated>2013-09-16T16:06:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-09-16:/2013/09/cvechecker-3-3-released/</id><summary type="html">&lt;p&gt;I just uploaded a new release of &lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt;
to the project files. The release is a (long overdue) bugfix release,
but includes two small enhancements: support standard input for the
binary list (so you can pipe the output of one command to cvechecker)
and the introduction of the &lt;code&gt;CVECHECKER_CONFFILE&lt;/code&gt; variable …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just uploaded a new release of &lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt;
to the project files. The release is a (long overdue) bugfix release,
but includes two small enhancements: support standard input for the
binary list (so you can pipe the output of one command to cvechecker)
and the introduction of the &lt;code&gt;CVECHECKER_CONFFILE&lt;/code&gt; variable to refer to
another location for the configuration file.&lt;/p&gt;
&lt;p&gt;Big thanks to &lt;a href="http://mulhern-at-yocto.dreamwidth.org/"&gt;Anne Mulhern&lt;/a&gt;
for the various patches submitted!&lt;/p&gt;</content><category term="Security"></category><category term="cvechecker"></category><category term="release"></category></entry><entry><title>Gentoo Hardened progress report</title><link href="https://blog.siphos.be/2013/08/gentoo-hardened-progress-report/" rel="alternate"></link><published>2013-08-29T20:27:00+02:00</published><updated>2013-08-29T20:27:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-29:/2013/08/gentoo-hardened-progress-report/</id><summary type="html">&lt;p&gt;Today, we had our monthly online meeting to discuss the progress amongst
the various Gentoo Hardened projects. As usual, here is a small
write-up.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lead election&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As every year, we also reviewed the current project leads. No surprises
here, everybody is happy with the current leads so they are re-elected …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, we had our monthly online meeting to discuss the progress amongst
the various Gentoo Hardened projects. As usual, here is a small
write-up.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lead election&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As every year, we also reviewed the current project leads. No surprises
here, everybody is happy with the current leads so they are re-elected
for another term. We did have two candidates for the lead position, but
even the other candidate vote for Zorry - so we had a unanimous vote ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC version 4.8.1 will be unmasked pretty soon, and the &lt;code&gt;hardenedno*&lt;/code&gt;
specs on it will work. However, there is still no progress on the asan
(Address Sanitizer) support together with UDEREF. As mentioned in a
previous post, UDEREF "reduces" the address space a bit which doesn't
play well with asan. Still, it isn't inevitable, since PowerPC also has
a reduced address space and so does Windows. So perhaps we can use the
same model for UDEREF enabled kernels? We'll send the suggestion and the
already-existing fixes upstream and hope for the best.&lt;/p&gt;
&lt;p&gt;In GCC 4.8.1, the &lt;code&gt;-fstack-check&lt;/code&gt; option might be enabled by default,
but the question is for which architectures and platforms. We know a few
packages, such as &lt;em&gt;ffmpeg&lt;/em&gt; and &lt;em&gt;libav&lt;/em&gt; have problems with it. In those
cases, the ebuild will be modified to use &lt;code&gt;-fno-stack-check&lt;/code&gt; (if
hardened). We opt to enable it for all as we don't really expect much
(if any) breakage that can't be dealt with swiftly.&lt;/p&gt;
&lt;p&gt;Support for hardened uClibc is still going steadily. Blueness is heating
his room a bit with it, seeing that mips32r2 takes about 2 weeks to
build hardened and vanilla stages - he is using an Ubiquity router
station for this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and Grsecurity/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Due to some boot freezes, as explained in bugs
&lt;a href="https://bugs.gentoo.org/482010"&gt;482010&lt;/a&gt; and
&lt;a href="https://bugs.gentoo.org/481790"&gt;481790&lt;/a&gt;, we don't have a stable 3.10.x
kernel yet. However, most of the issues should be resolved and we're
waiting for confirmation, so we can be looking at a stable 3.10.x kernel
soon.&lt;/p&gt;
&lt;p&gt;The 3.10 kernel will probably not be a long-term support kernel for PaX
and Grsecurity - such LTS kernel will be picked next year, most likely
the same kernel version that Ubuntu LTS settles on.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A small update on &lt;code&gt;policycoreutils&lt;/code&gt; has been made that updates &lt;strong&gt;rlpkg&lt;/strong&gt;
and &lt;strong&gt;selocal&lt;/strong&gt;. Other than that, our policies are in nice shape. A new
revision will be pushed to the tree soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;On the Integrity side, recent kernels support custom IMA policies
(again) so our documentation is accurate again. Next to IMA/EVM, I'll be
working on documentation for cryptographically signed kernel module
support soon as well as SCAP-based security baselines for Gentoo.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Blueness added a MUSL-based Gentoo profile (&lt;code&gt;hardened/linux/musl&lt;/code&gt;). Musl
is an even more slim libc and its profile is extremely experimental for
now. The profile structure is still a bit off though, a reorganization
will be suggested soon so that the profile inheritance is clear and
predictable, starting off with a non-hardened one
(&lt;code&gt;default/linux/{uclibc,musl}&lt;/code&gt;) and then a hardened specific one that
inherits from the default.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened
project&lt;/a&gt; now has its main
project page on the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;, and all
(most) documentation is moved to there as well for the Gentoo Hardened
subprojects.&lt;/p&gt;
&lt;p&gt;I also explained to the folks that I have authored a book on SELinux
System Administration (for Packt Publishing), which was why I was less
active the last few months. However, that is now done so I'm back on
track. More information about the book follows later on my blog ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Media&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And as usual, klondike has been tweeting the entire meeting through our
&lt;code&gt;@GentooHardened&lt;/code&gt; twitter account ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="minutes"></category><category term="progress_report"></category><category term="report"></category></entry><entry><title>Umounting IPv6 NFS(v4) mounts</title><link href="https://blog.siphos.be/2013/08/umounting-ipv6-nfsv4-mounts/" rel="alternate"></link><published>2013-08-23T13:46:00+02:00</published><updated>2013-08-23T13:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-23:/2013/08/umounting-ipv6-nfsv4-mounts/</id><summary type="html">&lt;p&gt;I had issues umounting my NFSv4 shares on an IPv6-only network. When
trying to umount the share, it said that it couldn't find the mount in
&lt;code&gt;/proc/mounts&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# umount /mnt/nfs/portage
/mnt/nfs/portage was not found in /proc/mounts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The solution: copy &lt;code&gt;/proc/mounts&lt;/code&gt; to &lt;code&gt;/etc/mtab&lt;/code&gt;, and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I had issues umounting my NFSv4 shares on an IPv6-only network. When
trying to umount the share, it said that it couldn't find the mount in
&lt;code&gt;/proc/mounts&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# umount /mnt/nfs/portage
/mnt/nfs/portage was not found in /proc/mounts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The solution: copy &lt;code&gt;/proc/mounts&lt;/code&gt; to &lt;code&gt;/etc/mtab&lt;/code&gt;, and the umount works
correctly again.&lt;/p&gt;</content><category term="Misc"></category><category term="ip6"></category><category term="ipv6"></category><category term="linux"></category><category term="nfs4"></category><category term="nfsv4"></category><category term="umount"></category></entry><entry><title>Why our policies don't like emerge --config</title><link href="https://blog.siphos.be/2013/08/why-our-policies-dont-like-emerge-config/" rel="alternate"></link><published>2013-08-23T11:53:00+02:00</published><updated>2013-08-23T11:53:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-23:/2013/08/why-our-policies-dont-like-emerge-config/</id><summary type="html">&lt;p&gt;One of the features that Portage provides is to have post-processing
done on request of the administrator for certain packages. For instance,
for the &lt;code&gt;dev-db/postgresql-server&lt;/code&gt; package we can call its
&lt;code&gt;pkg_config()&lt;/code&gt; phase to create the PostgreSQL instance and configure it
so that the configuration of the database is stored …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the features that Portage provides is to have post-processing
done on request of the administrator for certain packages. For instance,
for the &lt;code&gt;dev-db/postgresql-server&lt;/code&gt; package we can call its
&lt;code&gt;pkg_config()&lt;/code&gt; phase to create the PostgreSQL instance and configure it
so that the configuration of the database is stored in
&lt;code&gt;/etc/postgresql-9.2&lt;/code&gt; rather than together with the data files.&lt;/p&gt;
&lt;p&gt;When you run Gentoo with SELinux however, you might already have noticed
that this doesn't work. The reason is that, whenever an administrator
calls &lt;strong&gt;emerge&lt;/strong&gt;, the process (and by default its child processes) will
run in a confined domain called &lt;code&gt;portage_t&lt;/code&gt;. The domain is still quite
privileged, but not as privileged as the administrator domain
&lt;code&gt;sysadm_t&lt;/code&gt;. It holds the rights to build software and install files,
directories and other things on the file system. But it does not support
switching users for instance, which is what the PostgreSQL
&lt;code&gt;pkg_config()&lt;/code&gt; does: it wants to run a certain command as the &lt;code&gt;postgres&lt;/code&gt;
user, which is prohibited by SELinux.&lt;/p&gt;
&lt;p&gt;I'm not sure yet how to tackle this properly. One thing is that I might
update Portage to run in the user domain by default, and transition
dynamically towards the proper domains according to the task(s) it is
executing. We already do this for building software (where we transition
to a &lt;code&gt;portage_sandbox_t&lt;/code&gt; confined domain), perhaps it can be expanded to
support transitioning to &lt;code&gt;portage_t&lt;/code&gt; when it isn't running the
&lt;code&gt;pkg_config()&lt;/code&gt; phase. But that means injecting (more) SELinux-specific
code in Portage, something I'd rather not do (introduces additional
complexity and maintenance).&lt;/p&gt;
&lt;p&gt;Another possibility would be to have administrators explicitly state
that no transition should occur, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# runcon -t sysadm_t emerge --config =dev-db/postgresql-server-9.2.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With a minor addition to the policy, this gave me a good hope... until I
noticed that &lt;strong&gt;emerge&lt;/strong&gt; underlyingly calls &lt;strong&gt;ebuild&lt;/strong&gt; and &lt;strong&gt;ebuild.sh&lt;/strong&gt;,
both labeled as &lt;code&gt;portage_exec_t&lt;/code&gt;, so these calls transition to
&lt;code&gt;portage_t&lt;/code&gt; again.&lt;/p&gt;
&lt;p&gt;I'm going to look further into this - there are quite a few options
still open.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="pkg_config"></category><category term="portage"></category><category term="selinux"></category></entry><entry><title>Network routing based on SELinux?</title><link href="https://blog.siphos.be/2013/08/network-routing-based-on-selinux/" rel="alternate"></link><published>2013-08-21T19:43:00+02:00</published><updated>2013-08-21T19:43:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-21:/2013/08/network-routing-based-on-selinux/</id><summary type="html">&lt;p&gt;Today we had a question on #selinux if it was possible to route traffic
of a specific process using SELinux. The answer to this is "no",
although it has to be explained a bit in more detail.&lt;/p&gt;
&lt;p&gt;SELinux does not route traffic. SELinux is a local mandatory access
control system …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today we had a question on #selinux if it was possible to route traffic
of a specific process using SELinux. The answer to this is "no",
although it has to be explained a bit in more detail.&lt;/p&gt;
&lt;p&gt;SELinux does not route traffic. SELinux is a local mandatory access
control system; its purpose is to allow or deny certain actions, not
route traffic. However, Linux' NetFilter does support security markings
(SECMARK). I've &lt;a href="http://blog.siphos.be/2013/05/secmark-and-selinux/"&gt;blogged about
it&lt;/a&gt; in the past, and
there are good tutorials elsewhere on the Internet, such as Dan Walsh'
&lt;a href="http://www.linux.com/learn/tutorials/421152-using-selinux-and-iptables-together"&gt;Using SELinux and iptables
together&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Linux support for security marking does allow for routing decisions -
but it does not "use" SELinux in this regard. To mark traffic with a
certain label, the administrator has to put in the rules himself using
&lt;strong&gt;iptables&lt;/strong&gt; or &lt;strong&gt;ip6tables&lt;/strong&gt; commands. And if you have to do that, then
you are already working with the routing commands so why not just route
immediately? Of course, the advantage of labeling the traffic is that
you can then use SELinux to allow or deny processes to send or receive
those packets - but SELinux is not involved with routing.&lt;/p&gt;
&lt;p&gt;Another possibility is to use labeled networking, such as Labeled IPSec
or NetLabel/CIPSO support.&lt;/p&gt;
&lt;p&gt;With labeled networking, all hosts that participate in the network need
to support the labeled networking technology. If that is the case, then
SELinux policy can be used to deny traffic from one host or another -
but again, not to route traffic. You can use SELinux to deny one process
to send out data to one set of hosts and allow it to send data to
another, but that is not routing. The advantage of Labeled IPSec is that
contexts are retained across the network - decisions that SELinux has to
take on one system can be made based on the context of the process on
the other system.&lt;/p&gt;
&lt;p&gt;So no, SELinux cannot be used to route traffic, but it plays very nicely
with various networking controls to make proper decisions on its access
control enforcement.&lt;/p&gt;</content><category term="SELinux"></category><category term="ipsec"></category><category term="netlabel"></category><category term="networking"></category><category term="secmark"></category><category term="selinux"></category></entry><entry><title>Using CUSTOM_BUILDOPT in refpolicy for USE flag-alike functionality?</title><link href="https://blog.siphos.be/2013/08/using-custom_buildopt-in-refpolicy-for-use-flag-alike-functionality/" rel="alternate"></link><published>2013-08-16T09:17:00+02:00</published><updated>2013-08-16T09:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-16:/2013/08/using-custom_buildopt-in-refpolicy-for-use-flag-alike-functionality/</id><summary type="html">&lt;p&gt;As you are probably aware, Gentoo uses the &lt;a href="http://oss.tresys.com/projects/refpolicy/"&gt;reference
policy&lt;/a&gt; as its base for
SELinux policies. Yes, we do customize it and not everything is already
pushed upstream (for instance, our approach to use &lt;code&gt;xdg_*_home_t&lt;/code&gt;
customizable types to further restrict user application access has been
sent up for comments …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As you are probably aware, Gentoo uses the &lt;a href="http://oss.tresys.com/projects/refpolicy/"&gt;reference
policy&lt;/a&gt; as its base for
SELinux policies. Yes, we do customize it and not everything is already
pushed upstream (for instance, our approach to use &lt;code&gt;xdg_*_home_t&lt;/code&gt;
customizable types to further restrict user application access has been
sent up for comments a few times but we still need to iron it out
further) but all in all, we're pretty close to the upstream releases.
This is also visible when there are changes upstream as we very easily
integrate them back in our repository.&lt;/p&gt;
&lt;p&gt;But there are still a few things that I want to implement further, and
one of these things is perhaps too specific for Gentoo but would benefit
us (security-wise) in great detail: enabling domain privileges based on
USE flags. Allow me to quickly use an example to make this more
tangible.&lt;/p&gt;
&lt;p&gt;Consider the MPlayer application. As a media application, it of course
offers support for ALSA and PulseAudio (amongst other things). In the
SELinux policy, support for (and thus privileges related to) ALSA and
PulseAudio is handled through &lt;em&gt;optional_policy&lt;/em&gt; statements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;optional_policy(`
  pulseaudio_tmpfs_content(mplayer_tmpfs_t)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This means that the rules defined in &lt;em&gt;pulseaudio_tmpfs_content&lt;/em&gt; are
executed if the dependencies match:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;interface(`pulseaudio_tmpfs_content&amp;#39;,`
        gen_require(`
                attribute pulseaudio_tmpfsfile;
        &amp;#39;)

        typeattribute $1 pulseaudio_tmpfsfile;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If the &lt;code&gt;pulseaudio_tmpfsfile&lt;/code&gt; attribute exists, then the
&lt;code&gt;mplayer_tmpfs_t&lt;/code&gt; type gets the &lt;code&gt;pulseaudio_tmpfsfile&lt;/code&gt; attribute
assigned to it.&lt;/p&gt;
&lt;p&gt;This is flexible, because if the server/workstation does not use
PulseAudio, then in Gentoo, no pulseaudio SELinux module will be loaded
and thus the attribute will not exist. However, Gentoo tries to be a bit
more flexible in this - it is very well possible to have PulseAudio
installed, but disable PulseAudio support in MPlayer (build mplayer with
USE="-pulseaudio"). The current definitions in the policy do not support
this flexibility: if the pulseaudio module is loaded, then the
privileges become active.&lt;/p&gt;
&lt;p&gt;One way SELinux supports a more flexible approach is to use conditionals
in the policy. One could create booleans that administrators can toggle
to enable / disable SELinux rules. For instance, in the mplayer policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tunable_policy(`allow_mplayer_execstack&amp;#39;,`
        allow mencoder_t self:process { execmem execstack };
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If an administrator toggles the &lt;code&gt;allow_mplayer_execstack&lt;/code&gt; boolean to
"on", then the mentioned &lt;code&gt;allow&lt;/code&gt; rule becomes active.&lt;/p&gt;
&lt;p&gt;Sadly, this approach is not fully usable for USE driven decisions. Not
all rules can be enclosed in &lt;code&gt;tunable_policy&lt;/code&gt; statements, and &lt;a href="http://oss.tresys.com/pipermail/refpolicy/2013-July/006452.html"&gt;assigning
attributes to a
type&lt;/a&gt;
is one of them (cfr our pulseaudio example). A recent discussion on the
reference policy mailinglist gave me two ideas to investigate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See if we can support CIL (a new language for SELinux
    policy definitions) where such an approach would be easier&lt;/li&gt;
&lt;li&gt;Use build-time decisions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I want to go through the &lt;em&gt;build-time decisions&lt;/em&gt; idea. The
reference policy supports build-time options using &lt;em&gt;ifdef&lt;/em&gt; constructs.
These look at parameters provided by the build system (M4/Makefile
based) to see if rules need to be activated or not. For type attribute
declarations, this is a valid approach. So one idea would be to
transform USE flags, if they are set, into &lt;code&gt;use_${USEFLAG}&lt;/code&gt;, and make
decisions based on this in the policy code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ifdef(`use_pulseaudio&amp;#39;,`
  optional_policy(`
    pulseaudio_tmpfs_content(mplayer_tmpfs_t)
  &amp;#39;)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can add in the USE flags, if set, through the &lt;code&gt;CUSTOM_BUILDOPT&lt;/code&gt;
parameter that the reference policy provides. So introducing this is not
that difficult. The only thing I'm currently a bit weary about is the
impact on the policy files themselves (which is why I haven't done this
already) and the fact that USE flags on the "real" package are not know
to policy packages. In other words, if a user explicitly marks
&lt;code&gt;USE="-pulseaudio"&lt;/code&gt; on mplayer, but has &lt;code&gt;USE="pulseaudio"&lt;/code&gt; set as
general value, then the &lt;code&gt;selinux-mplayer&lt;/code&gt; package will still have
pulseaudio enabled.&lt;/p&gt;
&lt;p&gt;Still, I do like the idea. It would make it more consistent with what
Gentoo aims to do: if you do not want a certain support/feature in the
code, then why would the policy still have to allow it? With the proper
documentation towards administrators, I think this would be a good
approach.&lt;/p&gt;</content><category term="Gentoo"></category><category term="boolean"></category><category term="Gentoo"></category><category term="policy"></category><category term="selinux"></category><category term="use"></category><category term="useflag"></category></entry><entry><title>Today was a productive day</title><link href="https://blog.siphos.be/2013/08/today-was-a-productive-day/" rel="alternate"></link><published>2013-08-15T20:58:00+02:00</published><updated>2013-08-15T20:58:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-15:/2013/08/today-was-a-productive-day/</id><summary type="html">&lt;p&gt;Fixed 14 bugs today, with a few more pending (those for packages only
get marked as FIXED if it is moved to the stable state). One of the
changes is the
&lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml?part=1&amp;amp;chap=10#grub2"&gt;GRUB2&lt;/a&gt;
support in the Gentoo Handbook (yes, finally, sorry about that). That
opens up the road for the stabilization …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Fixed 14 bugs today, with a few more pending (those for packages only
get marked as FIXED if it is moved to the stable state). One of the
changes is the
&lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml?part=1&amp;amp;chap=10#grub2"&gt;GRUB2&lt;/a&gt;
support in the Gentoo Handbook (yes, finally, sorry about that). That
opens up the road for the stabilization of GRUB2.&lt;/p&gt;
&lt;p&gt;I also added a dozen fixes to the Gentoo SELinux policy repository and
sent a few others upstream that have been lingering in our policy for
quite some time. But the highlight for me was that I got to play with my
Wacom Bamboo pen and touch which, I must say, integrated easily with my
system. I only had to build the &lt;code&gt;wacom.ko&lt;/code&gt; kernel module and install the
&lt;code&gt;xf86-input-wacom&lt;/code&gt; package, but that's all - no reboot needed. Even GIMP
immediately detected the new device and I can start drawing and fixing
pictures more easily (I work on pictures often and a mouse just isn't
that obvious).&lt;/p&gt;
&lt;p&gt;One of the huge benefits is that the pressure you put on the drawing pad
is related (and configurable) to a drawing function in GIMP, something
you don't have with mice. For instance, if you need to Dodge or Burn on
layers, you don't need to continuously switch between the amount - just
adjust the pressure.&lt;/p&gt;</content><category term="Misc"></category><category term="gimp"></category><category term="tablet"></category><category term="wacom"></category></entry><entry><title>Some things sound more scary than they are</title><link href="https://blog.siphos.be/2013/08/some-things-sound-more-scary-than-they-are/" rel="alternate"></link><published>2013-08-15T10:02:00+02:00</published><updated>2013-08-15T10:02:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-15:/2013/08/some-things-sound-more-scary-than-they-are/</id><summary type="html">&lt;p&gt;A few days ago I finally got to the next thing on my &lt;em&gt;Want to do this
year&lt;/em&gt; list: put a new android
(&lt;a href="http://www.cyanogenmod.org/"&gt;Cyanogenmod&lt;/a&gt;) on my tablet, which was
still running the stock Android - but hasn't seen any updates in more
than a year. Considering the (in)security of Android …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few days ago I finally got to the next thing on my &lt;em&gt;Want to do this
year&lt;/em&gt; list: put a new android
(&lt;a href="http://www.cyanogenmod.org/"&gt;Cyanogenmod&lt;/a&gt;) on my tablet, which was
still running the stock Android - but hasn't seen any updates in more
than a year. Considering the (in)security of Android this was long
overdue for me. But the fear of getting an unbootable tablet ("bricked"
as it is often called) was keeping me from doing so.&lt;/p&gt;
&lt;p&gt;So when I finally got the nerves, I first had to run around screaming
for hours because the first step in the instructions didn't work. The
next day I read that it might have to do with the cable - and indeed,
tried with a different cable and the instructions just went along just
fine. So today I'm happily running with a more up-to-date Android again
on my tablet.&lt;/p&gt;
&lt;p&gt;Because my systems run Gentoo Hardened with SELinux, I did had to do
some small magic tricks to get the
&lt;a href="http://www.clockworkmod.com/"&gt;Clockworkmod&lt;/a&gt; recovery on the tablet: the
&lt;strong&gt;wheelie&lt;/strong&gt; binary (yes, I couldn't find the sources - if they are even
available) that I had to run required me to disable size overflow
detection in the kernel (a PaX countermeasure), allowed executable
memory (both through &lt;strong&gt;paxctl-ng&lt;/strong&gt; as well as in SELinux using the
&lt;code&gt;allow_execmem&lt;/code&gt; boolean) and had to temporarily add in the
&lt;code&gt;dev_rw_generic_usb_dev&lt;/code&gt; right (refpolicy macro) to my user.&lt;/p&gt;
&lt;p&gt;Also &lt;strong&gt;adb&lt;/strong&gt; had to be pax-marked, although I now know I don't need
&lt;strong&gt;adb&lt;/strong&gt; at all - I can just download the latest Android ZIP file from
the phone itself and refer to it from the recovery manager.&lt;/p&gt;
&lt;p&gt;All in all nothing to worry about - everything worked like a charm.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; (so I remember next time), if the system is stuck in CMR
(recovery), reboot with VolDown+Pwr, but don't select recovery. After 5
seconds, it will ask if you want a cold boot. Select it, and things work
again ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="android"></category><category term="grsecurity"></category><category term="pax"></category><category term="selinux"></category><category term="tablet"></category></entry><entry><title>And now, 31 days later...</title><link href="https://blog.siphos.be/2013/08/and-now-31-days-later/" rel="alternate"></link><published>2013-08-01T22:43:00+02:00</published><updated>2013-08-01T22:43:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-01:/2013/08/and-now-31-days-later/</id><summary type="html">&lt;p&gt;... the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo Hardened&lt;/a&gt; team
had its monthly online meeting again ;-)&lt;/p&gt;
&lt;p&gt;On the agenda were the usual suspects, such as the &lt;em&gt;toolchain&lt;/em&gt;. In this
category, Zorry mentioned that he has a fix for GCC 4.8.1 for the
&lt;code&gt;hardenedno*&lt;/code&gt; and vanilla &lt;code&gt;gcc-config&lt;/code&gt; options which will be added to
the tree …&lt;/p&gt;</summary><content type="html">&lt;p&gt;... the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo Hardened&lt;/a&gt; team
had its monthly online meeting again ;-)&lt;/p&gt;
&lt;p&gt;On the agenda were the usual suspects, such as the &lt;em&gt;toolchain&lt;/em&gt;. In this
category, Zorry mentioned that he has a fix for GCC 4.8.1 for the
&lt;code&gt;hardenedno*&lt;/code&gt; and vanilla &lt;code&gt;gcc-config&lt;/code&gt; options which will be added to
the tree after some more testing. The problem is that with GCC 4.8,
certain settings need to be set sooner than before (in the code path),
which is what the fix focuses on. The ASAN issue is still unresolved,
but otherwise GCC 4.8 is working fine.&lt;/p&gt;
&lt;p&gt;On &lt;em&gt;SELinux&lt;/em&gt;, the policycoreutils package has been bumped to include
support for &lt;code&gt;mcstrans&lt;/code&gt;, a translation daemon that visualizes humanly
readable strings instead of the standard sensitivity/category sets in
case of MCS/MLS policies.&lt;/p&gt;
&lt;p&gt;Regarding documentation, the wiki team (most notably a3li) is working
hard to support project pages on the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo
Wiki&lt;/a&gt;. Once we can, we will be moving our
project page with all related documentation to the wiki, allowing for
easier documentation development and a more modern look. To support
this, an XML-to-wiki stylesheet is available that translates ProjectXML
and GuideXML to the wiki language.&lt;/p&gt;
&lt;p&gt;During the meeting, we also mentioned the stabilization policy (or at
least, no-longer-stabilization) of the vanilla sources (plain kernel.org
Linux kernel sources). This doesn't immediately effect the hardened
project, but is important to know nonetheless, especially for users of
hardened technologies that are in the main kernel already (like SELinux
or IMA/EVM) as they have to be aware to either use the latest
(regardless of the keywords in use) or switch to gentoo-sources or
(preferably) hardened-sources.&lt;/p&gt;
&lt;p&gt;For uclibc support, the stages will be provided every 2 months rather
than every month as this is a resource-intensive process that isn't
fully automated yet (except for amd64 and x86 which are automated).&lt;/p&gt;
&lt;p&gt;Finally, on PaX and grSecurity support, the XATTR patch for tmpfs is now
in the kernel, and the problem about loosing PaX markings during
installation is fixed as Portage (2.1.12.9 and higher) now preserves the
flags during installation (a wrapper on &lt;code&gt;install&lt;/code&gt; is used that preserves
&lt;code&gt;usr.pax.flags&lt;/code&gt;).&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="grsecurity"></category><category term="hardened"></category><category term="irc"></category><category term="irl"></category><category term="meeting"></category><category term="minutes"></category><category term="pax"></category><category term="project"></category><category term="selinux"></category><category term="toolchain"></category></entry><entry><title>Putting OVAL at work</title><link href="https://blog.siphos.be/2013/08/putting-oval-at-work/" rel="alternate"></link><published>2013-08-01T15:01:00+02:00</published><updated>2013-08-01T15:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-08-01:/2013/08/putting-oval-at-work/</id><summary type="html">&lt;p&gt;When we look at the &lt;a href="http://scap.nist.gov/"&gt;SCAP security standards&lt;/a&gt;,
you might get the feeling of "How does this work". The underlying
interfaces, like OVAL and XCCDF, might seem a bit daunting to implement.&lt;/p&gt;
&lt;p&gt;This is correct, but you need to remember that the standards are
protocols, agreements that can be made …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When we look at the &lt;a href="http://scap.nist.gov/"&gt;SCAP security standards&lt;/a&gt;,
you might get the feeling of "How does this work". The underlying
interfaces, like OVAL and XCCDF, might seem a bit daunting to implement.&lt;/p&gt;
&lt;p&gt;This is correct, but you need to remember that the standards are
protocols, agreements that can be made across products so that several
products, each with their own expertise, can work together easily. It is
a matter of interoperability between components.&lt;/p&gt;
&lt;p&gt;Let's look at the following diagram to see how OVAL and XCCDF &lt;em&gt;can&lt;/em&gt; be
used. I'm not saying this is the only way forward, but it is a possible
approach.&lt;/p&gt;
&lt;p&gt;(Diagram lost during blog conversion)&lt;/p&gt;
&lt;p&gt;On the local side (and local here doesn't mean a single server, but
rather an organization or company) a list of checks is maintained. These
checks are OVAL checks, which can be downloaded from reputable sites
like NVD or are given to you by vendors (some vendors provide OVAL as
part of vulnerability reports). Do not expect this list to be hundreds
of checks - start small, the local database of checks will grow anyhow.&lt;/p&gt;
&lt;p&gt;The advantage is that the downloaded checks (OVALs) already have a
unique identifier (the OVAL ID). For instance, the check "Disable Java
in Firefox" for Windows is &lt;code&gt;oval:org.mitre.oval:def:12609&lt;/code&gt;. If
additional Windows operating systems are added, this ID remains the same
(it is updated) because the check (and purpose) remains the same.&lt;/p&gt;
&lt;p&gt;Locally, the OVAL checks are ran against targets by an OVAL interpreter.
Usually, you will have multiple interpreters in the organization, some
of them focused on desktops, some on servers, some perhaps on network
equipment, etc. By itself that doesn't matter, as long as they interpret
the OVAL checks. The list of targets to check against are usually
managed in a configuration management database.&lt;/p&gt;
&lt;p&gt;Targets can be of various granularity. The "Disable Java in Firefox"
will be against an operating system (where the check then sees if the
installed Firefox indeed has the setting disabled), but a check that
validates the permissions (rights) of a user will be against this user
account.&lt;/p&gt;
&lt;p&gt;The results of the OVAL checks are stored in a database that maps the
result against the target. By itself this result database does not
contain much more logic than "This rule is OK against this target and
that rule isn't" (well, there is some granularity, but not much more)
and the time stamp when this was done.&lt;/p&gt;
&lt;p&gt;Next comes the XCCDF. XCCDF defines the state that you want the system
to be in. It is a benchmark, a document describing how the system /
target should be configured. XCCDF documents usually contain the whole
shebang of configuration settings, and then differentiate them based on
profiles. For instance, a web server attached to the Internet might have
a different profile than a web server used internally or for development
purposes.&lt;/p&gt;
&lt;p&gt;The XCCDF document refers to OVAL checks, and thus uses the results from
the OVAL result database to see if a target is fully aligned with the
requirements or not. The XCCDF results themselves are also stored, often
together with exceptions (if any) that are approved (for instance, you
want to keep track of the workstations where Java &lt;em&gt;is&lt;/em&gt; enabled in
Firefox and only report for those systems where it is enabled by the
user without approval). Based on these results, reports can be generated
on the state of your park.&lt;/p&gt;
&lt;p&gt;Not all checks are already available as OVAL checks. Of course you can
write them yourself, but there are also other possibilities. Next to
OVAL, there are (less standardized) methods for doing checks which
integrate with XCCDF as well. The idea you'll need to focus on then is
the same as with OVAL: what is your source, how do you store it, you
need interpreters that can "play" it, and on the reporting side you'll
need to store the results so you can combine them later in your
reporting.&lt;/p&gt;</content><category term="Security"></category><category term="baseline"></category><category term="benchmark"></category><category term="oval"></category><category term="security"></category><category term="xccdf"></category></entry><entry><title>Moving Gentoo docs to the wiki</title><link href="https://blog.siphos.be/2013/07/moving-gentoo-docs-to-the-wiki/" rel="alternate"></link><published>2013-07-28T11:22:00+02:00</published><updated>2013-07-28T11:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-07-28:/2013/07/moving-gentoo-docs-to-the-wiki/</id><summary type="html">&lt;p&gt;Slowly but surely Gentoo documentation guides are being moved to the
&lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;. Thanks to the translation
support provided by the infrastructure, all "reasons" not to go forward
with this have been resolved. At first, I'm focusing on documentation
with open bugs that have not been picked up (usually due …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Slowly but surely Gentoo documentation guides are being moved to the
&lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;. Thanks to the translation
support provided by the infrastructure, all "reasons" not to go forward
with this have been resolved. At first, I'm focusing on documentation
with open bugs that have not been picked up (usually due to (human)
resource limits), but other documents will follow.&lt;/p&gt;
&lt;p&gt;Examples of already moved documents are the &lt;a href="https://wiki.gentoo.org/wiki/Xorg/Configuration"&gt;Xorg configuration
guide&lt;/a&gt;, the &lt;a href="https://wiki.gentoo.org/wiki/GCC_optimization"&gt;GCC
optimization guide&lt;/a&gt;,
&lt;a href="https://wiki.gentoo.org/wiki/UTF-8"&gt;UTF-8&lt;/a&gt; and &lt;a href="https://wiki.gentoo.org/wiki/User-mode_Linux/System_testing_with_UML"&gt;System testing with
UML&lt;/a&gt;.
Many more have been moved as well.&lt;/p&gt;
&lt;p&gt;The migrations are assisted by a conversion script that translates
GuideXML into wiki style content, although manual corrections remain
needed. For instance, all &lt;code&gt;&amp;lt;pre caption="..."&amp;gt;&lt;/code&gt; stuff is changed into
&lt;code&gt;{{Code|...}}&lt;/code&gt; even though the Wiki has several templates for code, such
as &lt;code&gt;{{Kernel|...}}&lt;/code&gt; for kernel configurations, &lt;code&gt;{{RootCmd|...}}&lt;/code&gt; for
commands ran as a privileged user and &lt;code&gt;{{Cmd|...}}&lt;/code&gt; for unprivileged
user commands.&lt;/p&gt;
&lt;p&gt;I updated the &lt;a href="http://www.gentoo.org/doc/en/list.xml"&gt;documentation
list&lt;/a&gt; on the main Gentoo site to
reflect the movement of documents as well, as this list will be slowly
shrinking.&lt;/p&gt;</content><category term="Documentation"></category><category term="docs"></category><category term="documentation"></category><category term="gdp"></category><category term="Gentoo"></category><category term="wiki"></category></entry><entry><title>Rebuilding SELinux contexts with sefcontext_compile</title><link href="https://blog.siphos.be/2013/07/rebuilding-selinux-contexts-with-sefcontext_compile/" rel="alternate"></link><published>2013-07-08T20:55:00+02:00</published><updated>2013-07-08T20:55:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-07-08:/2013/07/rebuilding-selinux-contexts-with-sefcontext_compile/</id><summary type="html">&lt;p&gt;A recent update of &lt;em&gt;libpcre&lt;/em&gt; caused the binary precompiled regular
expression files of SELinux to become outdated (and even blatantly
wrong). The details are in bug &lt;a href="https://bugs.gentoo.org/471718"&gt;471718&lt;/a&gt;
but that doesn't help the users that are already facing the problem, nor
have we found a good place to put the fix …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A recent update of &lt;em&gt;libpcre&lt;/em&gt; caused the binary precompiled regular
expression files of SELinux to become outdated (and even blatantly
wrong). The details are in bug &lt;a href="https://bugs.gentoo.org/471718"&gt;471718&lt;/a&gt;
but that doesn't help the users that are already facing the problem, nor
have we found a good place to put the fix in.&lt;/p&gt;
&lt;p&gt;Anyway, if you are facing issues with SELinux labeling (having files
being labeled as &lt;em&gt;portage_tmp_t&lt;/em&gt; instead of the proper label), check
with &lt;strong&gt;matchpathcon&lt;/strong&gt; if the label is correct. If &lt;strong&gt;matchpathcon&lt;/strong&gt; sais
that the label should be &lt;code&gt;&amp;lt;&amp;lt;none&amp;gt;&amp;gt;&lt;/code&gt; then you need to rebuild the SELinux
context files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cd /etc/selinux/strict/contexts/files
# for n in *.bin; do sefcontext_compile ${n%%.bin}; done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;sefcontext_compile&lt;/strong&gt; command will rebuild the SELinux context
files. When that has been done, &lt;strong&gt;matchpathcon&lt;/strong&gt; should show the right
context again, and Portage will relabel files correctly. Until then, you
will need to relabel the packages that have been built since (and
including) the &lt;em&gt;libpcre&lt;/em&gt; build.&lt;/p&gt;
&lt;p&gt;If someone has a good suggestion where to put these rebuilds in, please
do drop a note in the bug. Although the proper one might be &lt;em&gt;libpcre&lt;/em&gt;
itself, I'd rather not put too much SELinux logic in the ebuild unless
it is pretty safeguarded...&lt;/p&gt;
&lt;p&gt;In any case, it has also been documented in the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/FAQ#File_labels_do_not_seem_to_be_set_anymore.2C_and_matchpathcon_sais_.3C.3Cnone.3E.3E"&gt;Gentoo SELinux
FAQ&lt;/a&gt;
on the Gentoo wiki.&lt;/p&gt;</content><category term="SELinux"></category><category term="hardened"></category><category term="pcre"></category><category term="selinux"></category></entry><entry><title>Adding mcstrans to Gentoo</title><link href="https://blog.siphos.be/2013/07/adding-mcstrans-to-gentoo/" rel="alternate"></link><published>2013-07-07T20:38:00+02:00</published><updated>2013-07-07T20:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-07-07:/2013/07/adding-mcstrans-to-gentoo/</id><summary type="html">&lt;p&gt;If you use SELinux, you might be using an MLS-enabled policy. These are
policies that support sensitivity labels on resources and domains. In
Gentoo, these are supported in the &lt;code&gt;mcs&lt;/code&gt; and &lt;code&gt;mls&lt;/code&gt; policy stores. Now
sensitivity ranges are fun to work with, but the moment you have several
sensitivity levels …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you use SELinux, you might be using an MLS-enabled policy. These are
policies that support sensitivity labels on resources and domains. In
Gentoo, these are supported in the &lt;code&gt;mcs&lt;/code&gt; and &lt;code&gt;mls&lt;/code&gt; policy stores. Now
sensitivity ranges are fun to work with, but the moment you have several
sensitivity levels, or you have several dozen categories (sets or tags
that can be used in conjunction with pure sensitivity levels) these can
become a burden to maintain.&lt;/p&gt;
&lt;p&gt;The SELinux developers have had the same issue, so they wrote a tool
called &lt;em&gt;mcstransd&lt;/em&gt;, a translation daemon that reads the sensitivity
labels from the SELinux context (such as &lt;code&gt;s0-s0:c0.c1023&lt;/code&gt; or &lt;code&gt;s0:c12&lt;/code&gt;)
and displays a more human readable string for this (such as
&lt;code&gt;SystemLow-SystemHigh&lt;/code&gt; or &lt;code&gt;SalesTeam&lt;/code&gt;). The daemon is not a super
intelligent one - we just configure it by creating a mapping file in
&lt;code&gt;/etc/selinux/mcs&lt;/code&gt; called &lt;code&gt;setrans.conf&lt;/code&gt; which contains the mappings:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;## setrans.conf ## s0-s0:c0.c1023=SystemLow-SystemHigh s0:c12=SalesTeam&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The SELinux libraries (libselinux and libsemanage) use a socket to
communicate with the daemon to see if "translated" values need to be
displayed. If not (because the daemon is missing) the libraries keep the
SELinux syntax displayed. If it is, then the translated labels are
displayed.&lt;/p&gt;
&lt;p&gt;Support for categories and sensitivity labels is handled through the
&lt;strong&gt;chcat&lt;/strong&gt; tool, so you can list the current categories (and their
translated values) as well as assign them to files (or even logins).&lt;/p&gt;
&lt;p&gt;Although we supported categories for a while already, a recent update on
the &lt;code&gt;policycoreutils&lt;/code&gt; package now includes the mcstrans daemon as well.
Documentation is available, currently in the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml?part=2&amp;amp;chap=7#doc_chap3"&gt;pending changes
section&lt;/a&gt;
of the SELinux handbook (as this is not in the stable package yet) and
it will be moved to the main document when the package has stabilized.&lt;/p&gt;</content><category term="Gentoo"></category><category term="categories"></category><category term="mcs"></category><category term="mcstrans"></category><category term="mls"></category><category term="selinux"></category><category term="sensitivity"></category></entry><entry><title>Hardening is our business... new monthly report ;-)</title><link href="https://blog.siphos.be/2013/06/hardening-is-our-business-new-monthly-report/" rel="alternate"></link><published>2013-06-27T23:03:00+02:00</published><updated>2013-06-27T23:03:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-27:/2013/06/hardening-is-our-business-new-monthly-report/</id><summary type="html">&lt;p&gt;We're back with another report on the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; project. Please excuse
my brevity, as you've noticed I'm not that active (yet) due to work on
an external project - I'll be back mid-July though. I promise.&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;Toolchain&lt;/em&gt; side, GCC 4.8.1 is in the tree and has …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We're back with another report on the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; project. Please excuse
my brevity, as you've noticed I'm not that active (yet) due to work on
an external project - I'll be back mid-July though. I promise.&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;Toolchain&lt;/em&gt; side, GCC 4.8.1 is in the tree and has the GCC plugin
header fix. Also, for IA64 and ARM, the necessary PIE patches are
available as well to make this work on those architectures too.&lt;/p&gt;
&lt;p&gt;For uclibc, blueness is continuing the necessary support for everything
so far. He has also added mips64 n32 uclibc because new router boards
use this.&lt;/p&gt;
&lt;p&gt;In his time, blueness is also playing with a uclibc-powered desktop and
another C library called &lt;a href="http://www.musl-libc.org/"&gt;musl&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;Kernel&lt;/em&gt;, &lt;em&gt;grSecurity&lt;/em&gt; and &lt;em&gt;PaX&lt;/em&gt; side, we are having troubles
with the 3.8+ kernels and UEFI machines when the machines have ltitle
memory available (for instance when it is limited with &lt;code&gt;mem=&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The PaX extended attribute support is still on-going, mainly because we
need to have support for the &lt;code&gt;user.pax&lt;/code&gt; attributes in common tools like
&lt;strong&gt;install&lt;/strong&gt;, which is heavily used in Gentoo's ebuilds. The merge phase,
where the data is moved from the image location to the root, has been
supporting xattr moves for a while thanks to zmedico and arfrever, but
other installation phases still needed to be fixed or worked around. We
tried with a common patch on this, but there was little interest in this
approach, so we settled with a wrapper around &lt;strong&gt;install&lt;/strong&gt; inside of
Portage. This will be soon released and we again have full end-to-end
xattr pax flag support.&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;SELinux&lt;/em&gt; support, the latest userland and policy releases have
been stabilized in the Gentoo tree.&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;Profiles&lt;/em&gt;, blueness added a musl subprofile for testing of the
musl C library.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="progress"></category></entry><entry><title>My application base: graphviz</title><link href="https://blog.siphos.be/2013/06/my-application-base-graphviz/" rel="alternate"></link><published>2013-06-09T03:50:00+02:00</published><updated>2013-06-09T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-09:/2013/06/my-application-base-graphviz/</id><summary type="html">&lt;p&gt;Visualization of data is often needed in order to understand what the
data means. When data needs to be visualized automatically, I often use
the &lt;a href="http://www.graphviz.org/"&gt;graphviz&lt;/a&gt; tools. Not that they are
extremely pretty, but it works very well and is made to be automated.&lt;/p&gt;
&lt;p&gt;Let me give a few examples …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Visualization of data is often needed in order to understand what the
data means. When data needs to be visualized automatically, I often use
the &lt;a href="http://www.graphviz.org/"&gt;graphviz&lt;/a&gt; tools. Not that they are
extremely pretty, but it works very well and is made to be automated.&lt;/p&gt;
&lt;p&gt;Let me give a few examples of when visualization helps...&lt;/p&gt;
&lt;p&gt;In SELinux, there is the notion of domain transitions: security contexts
that can transition to another security context (and thus change the
permissions that the application/process has). Knowing where domains can
transition to (and how) as well as how domains can be transitioned to
(so input/output, if you may) is an important aspect to validate the
security of a system. The information can be obtained from tools such as
&lt;strong&gt;sesearch&lt;/strong&gt;, but even on a small system you easily find hundreds of
transitions that can occur. Visualizing the transitions in a graph
(using &lt;strong&gt;dot&lt;/strong&gt; or &lt;strong&gt;neato&lt;/strong&gt;) shows how a starting point can move (or
cannot move - equally important to know ;-) to another domain. So a
simple &lt;strong&gt;sesearch&lt;/strong&gt; with a few &lt;strong&gt;awk&lt;/strong&gt; statements in the middle and a
&lt;strong&gt;dot&lt;/strong&gt; at the end produces a nice graph in PNG format to analyze
further.&lt;/p&gt;
&lt;p&gt;A second visualization is about dependencies. Be it package dependencies
or library dependencies, or even architectural dependencies (in IT
architecturing, abstraction of assets and such also provides a
dependency-like structure), with the Graphviz tools the generation of
dependency graphs can be done automatically. At work, I sometimes use a
simple home-brew web-based API to generate the data (similar to
&lt;a href="http://ashitani.jp/gv/"&gt;Ashitani's Ajax/Graphviz&lt;/a&gt;) since the
workstations don't allow installation of your own software - and they're
windows.&lt;/p&gt;
&lt;p&gt;Another purpose I use graphviz for is to quickly visualize processes
during the design. Of course, this can be done using Visio or Draw.io
easily as well, but these have the disadvantage that you already require
some idea on how the process will evolve. With the dot language, I can
just start writing processes in a simple way, combining steps into
clusters (or in scheduling terms: streams or applications ;-) and let
Graphviz visualize it for me. When the process is almost finished, I can
either copy the result in Draw.io to generate a nicer drawing or use the
Graphviz result (especially when the purpose was just rapid
prototyping).&lt;/p&gt;
&lt;p&gt;And sometimes it is just fun to generate graphs based on data. For
instance, I can take the IRC logs of #gentoo or #gentoo-hardened to
generate graphs showing interactions between people (who speaks to who
and how frequently) or to find out the strength of topics (get the
keywords and generate communication graphs based on those keywords).&lt;/p&gt;</content><category term="Free-Software"></category><category term="dependencies"></category><category term="dot"></category><category term="graphviz"></category><category term="mab"></category><category term="neato"></category><category term="scheduling"></category><category term="visualization"></category><category term="visualize"></category></entry><entry><title>My application base: LibreOffice</title><link href="https://blog.siphos.be/2013/06/my-application-base-libreoffice/" rel="alternate"></link><published>2013-06-08T03:50:00+02:00</published><updated>2013-06-08T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-08:/2013/06/my-application-base-libreoffice/</id><summary type="html">&lt;p&gt;Of course, working with a Linux desktop eventually requires you to work
with an office suite. Although I have used alternatives like AbiWord and
Calligra in the past, and although I do think that Google Docs might
eventually become powerful enough to use instead, I'm currently using
&lt;a href="https://www.libreoffice.org/"&gt;LibreOffice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The use …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Of course, working with a Linux desktop eventually requires you to work
with an office suite. Although I have used alternatives like AbiWord and
Calligra in the past, and although I do think that Google Docs might
eventually become powerful enough to use instead, I'm currently using
&lt;a href="https://www.libreoffice.org/"&gt;LibreOffice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The use of LibreOffice for Linux users is well known: it has decent
Microsoft Office support (although I hardly ever need it; most users
don't mind exporting the files in an open document format and publishers
often support OpenOffice/LibreOffice formats themselves) and its
features are becoming more and more powerful, such as the CMIS support
(for online collaboration through content management systems). It also
has a huge community, sharing templates and other documents that make
life with LibreOffice even much prettier. Don't forget to check out its
&lt;a href="https://www.libreoffice.org/get-help/documentation/"&gt;extensive
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The aspects of LibreOffice I use the most are of course its writer (word
processor) and calc (spreadsheet application). The writer-part is for
when I do technical writing, whereas the spreadsheet application is for
generating simple management sheets for startups and households that
want to keep track of things (such as budgets, creating invoices, data
for mail-merge, etc.). At my work, Excel is one of the most used "end
user computing" tools, so I happen to get acquainted with quite a few
spreadsheet tips and tricks that are beneficial for small companies or
organizations ;-) Also, Calc has support for macro-like enhancements,
which makes it a good start for fast application development (until the
requests of the user/client has been stabilized, after which I usually
suggest a &lt;em&gt;real&lt;/em&gt; application development ;-)&lt;/p&gt;
&lt;p&gt;I generally don't use its presentation part much though - if I get a
powerpoint, I first see if Google Docs doesn't show it sufficiently
well. If not, then I try it out in LibreOffice. But usually, if someone
sends me a presentation, I tend to ask for a PDF version.&lt;/p&gt;</content><category term="Free-Software"></category><category term="excel"></category><category term="libreoffice"></category><category term="mab"></category><category term="openoffice"></category><category term="word"></category></entry><entry><title>My application base: firefox</title><link href="https://blog.siphos.be/2013/06/my-application-base-firefox/" rel="alternate"></link><published>2013-06-07T03:50:00+02:00</published><updated>2013-06-07T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-07:/2013/06/my-application-base-firefox/</id><summary type="html">&lt;p&gt;Browsers are becoming application disclosure frameworks rather than the
visualization tools they were in the past. More and more services, like
the
&lt;a href="http://blog.siphos.be/2013/06/my-application-base-draw-io/"&gt;Draw.io&lt;/a&gt;
one I discussed not that long ago, are using browsers are their client
side while retaining the full capabilities of end clients (such as drag
and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Browsers are becoming application disclosure frameworks rather than the
visualization tools they were in the past. More and more services, like
the
&lt;a href="http://blog.siphos.be/2013/06/my-application-base-draw-io/"&gt;Draw.io&lt;/a&gt;
one I discussed not that long ago, are using browsers are their client
side while retaining the full capabilities of end clients (such as drag
and drop, file management, editing capabilities and more).&lt;/p&gt;
&lt;p&gt;The browser I use consistently is
&lt;a href="https://www.mozilla.org/en-US/firefox/fx/"&gt;Firefox&lt;/a&gt;. I do think I will
move to Chromium (or at least use it more actively) sooner or later, but
firefox at this point in time covers all my needs. It isn't just the
browser itself though, but also the wide support in add-ons that I am
relying upon. This did make me push out SELinux policies to restrict the
actions that firefox can do, because it has become almost an entire
operating system by itself (like ChromeOS versus Chrome/Chromium). With
a few tunable settings (SELinux booleans) I can enable/disable access to
system devices (such as webcams), often vulnerable plugins (flash,
java), access to sensitive user information (I don't allow firefox
access to regular user files, only to the downloaded content) and more.&lt;/p&gt;
&lt;p&gt;One of the add-ons that is keeping me with Firefox for now is
&lt;a href="http://noscript.net/"&gt;NoScript&lt;/a&gt;. Being a security-conscious guy, being
able to limit the exposure of my surfing habits to advertisement
companies (and others) is very important to me. The NoScript add-on does
this perfectly. The add-on is very extensible (although I don't use that
- just the temporary/permanent allow) and easy to work with: on a site
where you notice some functionality isn't working, right-click and seek
the proper domain to allow methods from. Try-out a few of them
temporarily until you find the "sweet spot" and then allow those for
future reference.&lt;/p&gt;
&lt;p&gt;Another extension I use often (not often enough) is the spelling checker
capabilities. On multi-line fields, this gives me enough feedback about
what I am typing and if it doesn't use a mixture of American English and
British English. But with a simple javascript bookmarklet, I can even
enable spell check on a rendered page (simple javascript that sets the
designMode variable and the contentEditable variable to true), which is
perfect for the Gorg integration while developing Gentoo documentation.&lt;/p&gt;
&lt;p&gt;The abilities of a browser are endless: I have extensions that offer
ePub reading capabilities, full web development capabilities (to
edit/verify CSS and HTML changes), HTTPS Everywhere (to enforce SSL when
the site supports it), SQLite manager, Tamper Data (to track and
manipulate HTTP headers) and more. With the GoogleTalk plugins, doing
video chats and such is all done through the browser.&lt;/p&gt;
&lt;p&gt;This entire eco-system of plugins and extensions make the browser a big
but powerful interface, but also an important resource to properly
manage: keep it up-to-date, backup your settings (including auto-stored
passwords if you enable that), verify its integrity and ensure it runs
in its confined SELinux domain.&lt;/p&gt;</content><category term="Free-Software"></category><category term="browser"></category><category term="firefox"></category><category term="mab"></category></entry><entry><title>My application base: bash and kiss tools</title><link href="https://blog.siphos.be/2013/06/my-application-base-bash-and-kiss-tools/" rel="alternate"></link><published>2013-06-06T03:50:00+02:00</published><updated>2013-06-06T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-06:/2013/06/my-application-base-bash-and-kiss-tools/</id><summary type="html">&lt;p&gt;Okay, this just had to be here. I'm an automation guy - partially
because of my job in which I'm responsible for the long-term strategy
behind batch, scheduling and workload automation, but also because I
believe proper automation makes life just that much easier. And for
personal work, why not automate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Okay, this just had to be here. I'm an automation guy - partially
because of my job in which I'm responsible for the long-term strategy
behind batch, scheduling and workload automation, but also because I
believe proper automation makes life just that much easier. And for
personal work, why not automate the majority of stuff as well? For most
of the automation I use, I use bash scripts (or POSIX sh scripts that I
try out with the dash shell if I need to export the scripts to non-bash
users).&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://tiswww.case.edu/php/chet/bash/bashtop.html"&gt;Bourne-Again
SHell&lt;/a&gt; (or &lt;strong&gt;bash&lt;/strong&gt;)
is the default shell on Gentoo Linux systems, and is a powerful shell in
features as well. There are numerous resources available on bash
scripting, such as the &lt;a href="http://tldp.org/LDP/abs/html/"&gt;Advanced Bash-Scripting
Guide&lt;/a&gt; or the
&lt;a href="http://www.commandlinefu.com/commands/browse"&gt;commandlinefu.com&lt;/a&gt; (not
purely bash), and specific features of Bash have several posts and
articles all over the web.&lt;/p&gt;
&lt;p&gt;Shell scripts are easy to write, but their power comes from the various
tools that a Linux system contains (including the often forgotten
GNU-provided ones, of which bash is one of them). My system is filled
with scripts, some small, some large, all with a specific function that
I imagined I would need to use again later. I prefix almost all my
scripts with &lt;code&gt;sw&lt;/code&gt; (first letters of SwifT) or &lt;code&gt;mgl&lt;/code&gt; (in case the scripts
have the potential to be used by others) so I can easily find them (if
they are within my &lt;code&gt;${PATH}&lt;/code&gt; of course, not all of them are): just type
the first letters followed by two tabs and bash shows me the list of
scripts I have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sw\t\t
swbackup               swdocbook2html      swsandboxfirefox    swletter      swpics
swstartvm              swstripcomment      swvmconsole         swgenpdf      swcheckmistakes
swdoctransaccuracy     swhardened-secmerge swmailman2mbox      swmassrename  swmassstable
swmovepics             swbumpmod           swsecontribmerge    swseduplicate swfindbracket
swmergeoverlay         swshowtree          swsetvid            swfileprocmon swlocalize
swgendigest            swgenmkbase         swgenfinoverview    swmatchcve

$ mgl\t\t
mglshow                mglverify         mglgxml2docbook       mglautogenif  mgltellabout
mgltellhowto           mgltellwhynot     mglgenmodoverview     mglgenoval    mglgensetup
mglcertcli             mglcleannode      mglwaitforfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the proper basic template, I can keep the scripts sane and well
documented. None of the scripts execute something without arguments, and
"-h" and "--help" are always mapped to the help information. Those that
(re)move files often have a "-p" (or "--pretend") flag that instead of
executing the logic, echo's it to the screen.&lt;/p&gt;
&lt;p&gt;A simple example is the swpics script. It mounts the SD card, moves the
images to a first location (&lt;code&gt;Pictures/local/raw&lt;/code&gt;), unmounts the SD card,
renames the pictures based on the metadata information, finds duplicates
based on two checksums (in case I forgot to wipe the SD card afterwards
- I don't wipe it from the script) and removes the duplicates, converts
the raws into JPEGs and moves these to a minidlna-served location so I
can review the images from DLNA-compliant devices when I want and then
starts the Geeqie application. When the Geeqie application has finished,
it searches for the removed raws and removes those from the
minidlna-served location as well. It's simple, nothing fancy, and saves
me a few minutes of work every time.&lt;/p&gt;
&lt;p&gt;The kiss tools are not really a toolset that is called kiss, but rather
a set of commands that are simple in their use. Examples are exiv2 (to
manage JPEG EXIF information, including renaming them based on the EXIF
timestamp), inotifywait (passive waiting for file modification/writes),
sipcalc (calculating IP addresses and subnetwork ranges), socat (network
socket "cat" tool), screen (or tmux, to implement virtual sessions), git
(okay, not that KISS, but perfect for what it does - versioning stuff)
and more. Because these applications just do what they are supposed to,
without too many bells and whistles, it makes it easy to "glue" them
together to get an automated flow.&lt;/p&gt;
&lt;p&gt;Automation saves you from performing repetitive steps manually, so is a
real time-saver. And bash is a perfect scripting language for it.&lt;/p&gt;</content><category term="Free-Software"></category><category term="bash"></category><category term="dash"></category><category term="mab"></category><category term="scripting"></category></entry><entry><title>My application base: geekie</title><link href="https://blog.siphos.be/2013/06/my-application-base-geekie/" rel="alternate"></link><published>2013-06-05T03:50:00+02:00</published><updated>2013-06-05T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-05:/2013/06/my-application-base-geekie/</id><summary type="html">&lt;p&gt;In the past, when I had to manage my images (pictures) I used
&lt;a href="http://gqview.sourceforge.net/"&gt;GQview&lt;/a&gt; (which started back in
&lt;a href="http://blog.siphos.be/2008/08/playing-with-gqview/"&gt;2008&lt;/a&gt;). But the
application doesn't get many updates, and if an application does not get
many updates, it either means it is no longer maintained or that it does
its job perfectly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the past, when I had to manage my images (pictures) I used
&lt;a href="http://gqview.sourceforge.net/"&gt;GQview&lt;/a&gt; (which started back in
&lt;a href="http://blog.siphos.be/2008/08/playing-with-gqview/"&gt;2008&lt;/a&gt;). But the
application doesn't get many updates, and if an application does not get
many updates, it either means it is no longer maintained or that it does
its job perfectly. Sadly, for GQview, it is the unmaintained reason
(even though the application seems to work pretty well for most tasks).
Enter Geeqie, a fork of GQview to keep evolution on the application up
to speed.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://geeqie.sourceforge.net/"&gt;Geeqie&lt;/a&gt; image viewer is a simple
viewer that allows to easily manipulate images (like rotation). I launch
it the moment I insert my camera's SD card into my laptop for image
processing. It quickly shows the thumbnails of all images and I start
processing them to see which ones are eligible for manipulations later
on (or are just perfect - not that that occurs frequently) and which can
be deleted immediately. You can also quickly set Exif information (to
annotate the image further) and view some basic aspects of the picture
(such as histogram information).&lt;/p&gt;
&lt;p&gt;Two features however are what is keeping me with this image viewer:
finding duplicates, and side-by-side comparison.&lt;/p&gt;
&lt;p&gt;With the duplicate feature, geekie can compare images by name, size,
date, dimensions, checksum, path and - most interestingly, similarity.
If you start working on images, you often create intermediate snapshots
or tryouts. Or, when you start taking pictures, you take several ones in
a short time-frame. With the "find duplicate" feature, you can search
through the images to find all images that had the same base (or are
taking quickly after each other) and see them all simultaneously. That
allows you to remove those you don't need anymore and keep the good
ones. I also use this feature often when people come with their external
hard drive filled with images - none of them having any exif information
anymore and not in any way structured - and ask to see if there are any
duplicates on it. A simple checksum might reveal the obvious ones, but
the similarity search of geeqie goes much, much further.&lt;/p&gt;
&lt;p&gt;The side-by-side comparison creates a split view of the application, in
which each pane has another image. This feature I use when I have two
pictures that are taken closely after another (so very, very similar in
nature) and I need to see which one is better. With the side-by-side
comparison, I can look at artifacts in the image or the consequences of
the different aperture, ISO and shutter speed.&lt;/p&gt;
&lt;p&gt;And the moment I start working on images, Gimp and Darktable are just a
single click away.&lt;/p&gt;</content><category term="Free-Software"></category><category term="geeqie"></category><category term="gimp"></category><category term="gqview"></category><category term="images"></category><category term="mab"></category></entry><entry><title>My application base: freemind</title><link href="https://blog.siphos.be/2013/06/my-application-base-freemind/" rel="alternate"></link><published>2013-06-04T03:50:00+02:00</published><updated>2013-06-04T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-04:/2013/06/my-application-base-freemind/</id><summary type="html">&lt;p&gt;Anyone who is even remotely busy with innovation will know what mindmaps
are. They are a means to visualize information, ideas or tasks in
whatever structure you like. By using graphical annotations the
information is easier to look through, even when the mindmap becomes
very large. In the commercial world …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Anyone who is even remotely busy with innovation will know what mindmaps
are. They are a means to visualize information, ideas or tasks in
whatever structure you like. By using graphical annotations the
information is easier to look through, even when the mindmap becomes
very large. In the commercial world, mindmapping software such as
&lt;a href="http://www.xmind.net/"&gt;XMind&lt;/a&gt; and
&lt;a href="http://www.mindjet.com/products/mindmanager/"&gt;Mindmanager&lt;/a&gt; are often
used. But these companies should really start looking into Freemind.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://freemind.sourceforge.net/wiki/index.php/Main_Page"&gt;Freemind&lt;/a&gt;
software is a java-based mind map software, running perfectly on
Windows, Linux or other platforms. Installation is a breeze (if you are
allowed to on your work, you can just launch it from a USB drive if you
want, so no installation hassles whatsoever) and its interface is very
intuitive. For all the whistles and bells that the commercial ones
provide, I just want to create my mindmaps and export them into a format
that others can easily use and view.&lt;/p&gt;
&lt;p&gt;At my real-time job, we (have to) use XMind. If someone shares a mindmap
("their mind" map as I often see it - I seem to have a different mind
than most others in how I structure things, except for one colleague who
imo does not structure things at all) they just share the XMind file and
hope that the recipients can read it. Although XMind can export mindmaps
just fine, I do like the freemind approach where a simple java applet
can show the entire mindmap as interactively as you would navigate
through the application itself. This makes it perfect for discussing
ideas because you can close and open branches easily.&lt;/p&gt;
&lt;p&gt;The
&lt;a href="http://freemind.sourceforge.net/wiki/index.php/Import_and_export"&gt;export/import&lt;/a&gt;
capabilities of freemind are also interesting. Before being forced to
use XMind, we were using Mindmanager and I could just easily import the
mindmaps into freemind. The file format that freemind uses is an
XML-based one, so translating those onto other formats is not that
difficult if you know some XSLT.&lt;/p&gt;
&lt;p&gt;I personally use freemind when I embark on a new project, to structure
the approach, centralize all information, keep track of problems (and
their solutions), etc. The only thing I am missing is a nice interface
for mobile devices though.&lt;/p&gt;</content><category term="Free-Software"></category><category term="freemind"></category><category term="java"></category><category term="mab"></category><category term="mindmanager"></category><category term="mindmap"></category><category term="structure"></category><category term="xmind"></category></entry><entry><title>My application base: draw.io</title><link href="https://blog.siphos.be/2013/06/my-application-base-draw-io/" rel="alternate"></link><published>2013-06-03T03:50:00+02:00</published><updated>2013-06-03T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-03:/2013/06/my-application-base-draw-io/</id><summary type="html">&lt;p&gt;The next few weeks (months even) will be challenging my free time as I'm
working on (too many) projects simultaneously (sadly, only a few of
those are free software related, most are house renovations). But that
shouldn't stop me from starting a new set of posts, being &lt;em&gt;my
application base …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;The next few weeks (months even) will be challenging my free time as I'm
working on (too many) projects simultaneously (sadly, only a few of
those are free software related, most are house renovations). But that
shouldn't stop me from starting a new set of posts, being &lt;em&gt;my
application base&lt;/em&gt;. In this series, I'll cover a few applications (or
websites) that I either use often or that I should use more. In either
case, the application does its job very well so why not give some input
on it?&lt;/p&gt;
&lt;p&gt;The first on the agenda is the &lt;a href="http://www.draw.io"&gt;Draw.io&lt;/a&gt; website.&lt;/p&gt;
&lt;p&gt;With Draw.io, you get a web-browser based drawing application for
diagrams, flowcharts, UML, BPMN etc. I came across this application
while looking for an alternative to &lt;a href="https://live.gnome.org/Dia"&gt;Dia&lt;/a&gt;,
which by itself was supposed to be an alternative to &lt;a href="https://office.microsoft.com/en-us/visio/"&gt;Microsoft
Visio&lt;/a&gt; (err, no). Don't get
me wrong, Dia is nice, but it lacks evolution and just doesn't feel
easy. Draw.io on the other hand is evolving constantly, and it is also
active on &lt;a href="https://plus.google.com/100634082864796769666/"&gt;Google Plus&lt;/a&gt;
where you can follow up on all recent developments and thoughts (I hope
I get the G+ link correctly, it's not that I don't like numbers, just
not in URLs).&lt;/p&gt;
&lt;p&gt;I started using Draw.io while documenting free software IT architectures
(such as implementations of BIND, PostgreSQL, etc.) for which I needed
some diagrams. Although Draw.io is an online application (and its
underlying engine is not completely free software) you can easily work
with it from different locations. It integrates with &lt;a href="https://drive.google.com"&gt;Google
Drive&lt;/a&gt; to store the diagrams on if you want -
and if you don't, you can always save the diagrams in their native XML
format on your system and open them later again.&lt;/p&gt;
&lt;p&gt;The interface is very easy to use, and I recently found out that it now
also supports mobile devices, which is perfect for tablets (the mobile
device support is recent afaik and still undergoing updates). The site
also works well in various browsers (tried IExplorer 10 at work, Firefox
and Google Chrome and they all seem to work nicely) - eat that stupid
commercial vendors that force me into using IExplorer 8 or Firefox 10 -
you know who you are!&lt;/p&gt;
&lt;p&gt;A site/service to keep a close eye on. The service itself is free (and
doesn't seem too limited due to it), but Draw.io also has commercial
support if you want through Google Apps and Confluence integration. I
don't have much experience with those yet but that might change in the
near future (projects, projects).&lt;/p&gt;</content><category term="Documentation"></category><category term="appbase"></category><category term="architecturing"></category><category term="dia"></category><category term="draw"></category><category term="draw.io"></category><category term="mab"></category><category term="visio"></category></entry><entry><title>Using extended attributes for custom information</title><link href="https://blog.siphos.be/2013/06/using-extended-attributes-for-custom-information/" rel="alternate"></link><published>2013-06-02T03:50:00+02:00</published><updated>2013-06-02T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-02:/2013/06/using-extended-attributes-for-custom-information/</id><summary type="html">&lt;p&gt;One of the things I have been meaning to implement on my system is a way
to properly "remove" old files from the system. Currently, I do this
through frequently listing all files, going through them and deleting
those I feel I no longer need (in any case, I can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the things I have been meaning to implement on my system is a way
to properly "remove" old files from the system. Currently, I do this
through frequently listing all files, going through them and deleting
those I feel I no longer need (in any case, I can retrieve them back
from the backup within 60 days). But this isn't always easy since it
requires me to reopen the files and consider what I want to do with
them... again.&lt;/p&gt;
&lt;p&gt;Most of the time, when files are created, you generally know how long
they are needed on the system. For instance, an attachment you download
from an e-mail to view usually has a very short lifespan (you can always
re-retrieve it from the e-mail as long as the e-mail itself isn't
removed). Same with output you captured from a shell command, a strace
logfile, etc. So I'm wondering if I can't create a simple method for
keeping track of &lt;em&gt;expiration dates&lt;/em&gt; on files, similar to the expiration
dates supported for z/OS data sets. And to implement this, I am
considering to use extended attributes.&lt;/p&gt;
&lt;p&gt;The idea is simple: when working with a file, I want to be able to
immediately set an expiration date to it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ strace -o strace.log ...
$ expdate +7d strace.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This would set an extended attribute named &lt;code&gt;user.expiration&lt;/code&gt; with the
value being the number of seconds since epoch (which you can obtain
through &lt;strong&gt;date +%s&lt;/strong&gt; if you want) on which the file can be expired (and
thus deleted from the system). A system cronjob can then regularly scan
the system for files with the extended attribute set and, if the
expiration date is beyond the current date, the file can be removed from
the system (perhaps first into a specific area where it lingers for an
additional while just in case).&lt;/p&gt;
&lt;p&gt;It is just an example of course. The idea is that the extended
attributes keep information about the file close to the file itself. I'm
probably going to have an additional layer on top if it, checking
SELinux contexts and automatically identifying expiration dates based on
their last modification time. Setting the expiration dates manually
after creating the files is prone to be forgotten after a while. And
perhaps introduce the flexibility of setting an &lt;code&gt;user.expire_after&lt;/code&gt;
attribute is well, telling that the file can be removed if it hasn't
been touched (modification time) in at least XX number of days.&lt;/p&gt;</content><category term="Free-Software"></category><category term="attributes"></category><category term="expiration"></category><category term="extended-attributes"></category><category term="linux"></category><category term="xattr"></category></entry><entry><title>Hacking java bytecode with dhex</title><link href="https://blog.siphos.be/2013/06/hacking-java-bytecode-with-dhex/" rel="alternate"></link><published>2013-06-01T03:50:00+02:00</published><updated>2013-06-01T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-06-01:/2013/06/hacking-java-bytecode-with-dhex/</id><summary type="html">&lt;p&gt;I found myself in a weird situation: a long long time ago, I wrote a
java application that I didn't touch nor ran for a few years. Today, I
found it on a backup and wanted to run it again (its a graphical
application for generating HTML pages). However, it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I found myself in a weird situation: a long long time ago, I wrote a
java application that I didn't touch nor ran for a few years. Today, I
found it on a backup and wanted to run it again (its a graphical
application for generating HTML pages). However, it failed in a
particular feature. Not with an exception or stack trace, just
functionally. Now, I have the source code at hand, so I look into the
code and find the logical error. Below is a snippet of it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;if (myHandler != null) {
  int i = startValue + maxRange;
  for (int j = endValue; j &amp;gt; i; j--) {
    ... (do some logic)
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It doesn't matter what the code is supposed to do, but from what I can
remember, I shouldn't be adding &lt;code&gt;maxRange&lt;/code&gt; to the &lt;code&gt;i&lt;/code&gt; variable (yet - as
I do that later in the code). But instead of setting up the java
development environment, emerging the IDE etc. I decided to just edit
the class file directly using &lt;strong&gt;dhex&lt;/strong&gt; (a wonderful utility I recently
discovered) because doing things the hard way is sometimes fun as well.
So I ran &lt;strong&gt;javap -c MyClass&lt;/strong&gt; to get some java bytecode information from
the method, which gives me:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;   8:   ifnull  116
   11:  iload_2
   12:  iload_3
   13:  iadd
   14:  istore  5
   16:  iload_2
   17:  istore  6
   19:  iload   6
   21:  iload   5
   23:  if_icmpge       106
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I know lines 11 and 12 is about pushing the 2nd and 3rd arguments of the
function (which are &lt;code&gt;startValue&lt;/code&gt; and &lt;code&gt;maxRange&lt;/code&gt;) to the stack to add
them (line 13). To remove the third argument, I can change this opcode
from &lt;code&gt;1d&lt;/code&gt; (iload_3) to &lt;code&gt;03&lt;/code&gt; (iconst_0). This way, zero is added and
the code itself just continues as needed. And for some reason, that
seems to be the only mistake I made then because the application now
works flawlessly.&lt;/p&gt;
&lt;p&gt;Hacking is fun.&lt;/p&gt;</content><category term="Misc"></category><category term="bytecode"></category><category term="dhex"></category><category term="java"></category></entry><entry><title>A SELinux policy for incron: finishing up</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-finishing-up/" rel="alternate"></link><published>2013-05-31T03:50:00+02:00</published><updated>2013-05-31T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-31:/2013/05/a-selinux-policy-for-incron-finishing-up/</id><summary type="html">&lt;p&gt;After 9 posts, it's time to wrap things up. You can review the final
results online
(&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.te.txt"&gt;incron.te&lt;/a&gt;,
&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.if.txt"&gt;incron.if&lt;/a&gt; and
&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.fc.txt"&gt;incron.fc&lt;/a&gt;) and
adapt to your own needs if you want. But we should also review what we
have accomplished so far...&lt;/p&gt;
&lt;p&gt;We built the start of an entire …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After 9 posts, it's time to wrap things up. You can review the final
results online
(&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.te.txt"&gt;incron.te&lt;/a&gt;,
&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.if.txt"&gt;incron.if&lt;/a&gt; and
&lt;a href="http://dev.gentoo.org/~swift/blog/01/incron.fc.txt"&gt;incron.fc&lt;/a&gt;) and
adapt to your own needs if you want. But we should also review what we
have accomplished so far...&lt;/p&gt;
&lt;p&gt;We built the start of an entire policy for a daemon (the inotify cron
daemon) for two main types: the daemon itself, and its management
application &lt;strong&gt;incrontab&lt;/strong&gt;. We defined new types and contexts, we used
attributes, declared a boolean and worked with interfaces. That's a lot
to digest, and yet it is only a part of the various capabilities that
SELinux offers.&lt;/p&gt;
&lt;p&gt;The policy isn't complete though. We defined a type called
&lt;code&gt;incron_initrc_exec_t&lt;/code&gt; but don't really use it further. In practice, we
would need to define an additional interface (probably named
&lt;em&gt;incron_admin&lt;/em&gt;) that allows users and roles to manage &lt;em&gt;incron&lt;/em&gt; without
needing to grant this user/role &lt;code&gt;sysadm_r&lt;/code&gt; privileges. I leave that up
to you as an exercise for now, but I'll post more about admin interfaces
and how to work with them on a system in the near future.&lt;/p&gt;
&lt;p&gt;We also made a few assumptions and decisions while building the policy
that might not be how you yourself would want to build the policy.
SELinux is a MAC system, but the policy language is very flexible. You
can use an entirely different approach in policies if you want. For
instance, &lt;em&gt;incron&lt;/em&gt; supports launching the &lt;strong&gt;incrond&lt;/strong&gt; as a command-line,
foreground process. This could help users run &lt;strong&gt;incrond&lt;/strong&gt; under their
privileges for their own files - we did not consider this case in our
design. Although most policies try to capture all use cases of an
application, there will be cases when a policy developer did either not
consider the use case or found that it infringed with his own principles
on policy development (and allowed activities on a system).&lt;/p&gt;
&lt;p&gt;In Gentoo Hardened, I try to write down the principles and policies that
we follow in a &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-policy.xml"&gt;Gentoo Hardened SELinux Development
Policy&lt;/a&gt;
document. As decisions need to be taken, such a document might help find
common consensus on how to approach SELinux policy development further,
and I seriously recommend that you consider writing up a similar
document yourself, especially if you are going to develop policies for a
larger organization.&lt;/p&gt;
&lt;p&gt;One of the deficiencies of the current policy is that it worked with the
unmodified &lt;em&gt;incron&lt;/em&gt; version. If we would patch &lt;em&gt;incron&lt;/em&gt; so that it could
change context on executing the incrontab files of a user, then we can
start making use of the default context approach (and perhaps even
enhance with PAM services). In that case, user incrontabs could be
launched entirely from the users' context (like &lt;code&gt;user_u:user_r:user_t&lt;/code&gt;)
instead of the &lt;code&gt;system_u:system_r:incrond_t&lt;/code&gt; or transitioned
&lt;code&gt;system_u:system_r:whatever_t&lt;/code&gt; contexts. Having user provided commands
executed in the system context is a security risk, so in our policy we
would &lt;em&gt;not&lt;/em&gt; grant the &lt;em&gt;incron_role&lt;/em&gt; to untrusted users - probably only
to &lt;code&gt;sysadm_t&lt;/code&gt; and even then he probably would be better with using the
&lt;code&gt;/etc/incron.d&lt;/code&gt; anyway.&lt;/p&gt;
&lt;p&gt;The downside of patching code however is that this is only viable if
upstream wants to support this - otherwise we would need to maintain the
patches ourselves for a long time, creating delays in releases (upstream
released a new version and we still need to reapply and refactor
patches) and removing precious (human) resources from other, Gentoo
Hardened/SELinux specific tasks (like bugfixing and documentation
writing ;-)&lt;/p&gt;
&lt;p&gt;Still, the policy returned a fairly good view on how policies &lt;em&gt;can&lt;/em&gt; be
developed. And as I said, there are still other things that weren't
discussed, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build-time decisions, which can change policies based on build
    options of the policy. In the reference policy, this is most often
    used for distribution-specific choices: if Gentoo would use one
    approach and Redhat another, then the differences would be separated
    through &lt;code&gt;ifdef(`distro_gentoo',`...')&lt;/code&gt; and
    &lt;code&gt;ifdef(`distro_redhat',`...')&lt;/code&gt; calls.&lt;/li&gt;
&lt;li&gt;Some calls might only be needed if another policy is loaded. I think
    all calls made currently are part of base modules, so can be
    expected to be available at all times. But if we would need
    something like &lt;em&gt;icecast_signal(incrond_t)&lt;/em&gt;, then we would need to
    put that call inside a &lt;code&gt;optional_policy(`...')&lt;/code&gt; statement.
    Otherwise, our policy would fail to load because the &lt;em&gt;icecast&lt;/em&gt;
    SELinux policy isn't loaded.&lt;/li&gt;
&lt;li&gt;We could even introduce specific statements like &lt;em&gt;dontaudit&lt;/em&gt; or
    &lt;em&gt;neverallow&lt;/em&gt; to fine-tune the policy. Note though that &lt;em&gt;neverallow&lt;/em&gt;
    is a compile-time statement: it is not a way to negate &lt;em&gt;allow&lt;/em&gt;
    rules: if there is one &lt;em&gt;allow&lt;/em&gt; that would violate the &lt;em&gt;neverallow&lt;/em&gt;,
    then that module just refuses to build.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, if you want to create policies to be pushed upstream to the
reference policy project, you will need to look into the
&lt;a href="http://oss.tresys.com/projects/refpolicy/wiki/StyleGuide"&gt;StyleGuide&lt;/a&gt;
and
&lt;a href="http://oss.tresys.com/projects/refpolicy/wiki/InterfaceNaming"&gt;InterfaceNaming&lt;/a&gt;
documents as those define the order that rules should be placed and the
name syntax for interfaces. I have been contributing a lot to the
reference policy and I still miss a few of these, so for me they are not
that obvious. But using a common style is important as it allows for
simple patching, code comparison and even allows us to easily read
through complex policies.&lt;/p&gt;
&lt;p&gt;If you don't want to contribute it, but still use it on your Gentoo
system, you can use a simple ebuild to install the files. Create an
ebuild (for instance &lt;code&gt;selinux-incron&lt;/code&gt;), put the three files in the
&lt;code&gt;files/&lt;/code&gt; subdirectory, and use the following ebuild code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Copyright 1999-2013 Gentoo Foundation
# Distributed under the terms of the GNU General Public License v2
# $Header$
EAPI=&amp;quot;4&amp;quot;

IUSE=&amp;quot;&amp;quot;
MODS=&amp;quot;incron&amp;quot;
BASEPOL=&amp;quot;2.20130424-r1&amp;quot;
POLICY_FILES=&amp;quot;incron.te incron.fc incron.if&amp;quot;

inherit selinux-policy-2

DESCRIPTION=&amp;quot;SELinux policy for incron, the inotify cron daemon&amp;quot;

KEYWORDS=&amp;quot;~amd64 ~x86&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When installed, the interface files will be published as well and can
then be used by other modules (something we couldn't do in the past few
posts) or by the &lt;strong&gt;selocal&lt;/strong&gt; tool.&lt;/p&gt;</content><category term="SELinux"></category><category term="incron"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: using booleans</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-using-booleans/" rel="alternate"></link><published>2013-05-30T03:50:00+02:00</published><updated>2013-05-30T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-30:/2013/05/a-selinux-policy-for-incron-using-booleans/</id><summary type="html">&lt;p&gt;After using a default set of directories to watch, and &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/"&gt;allowing admins
to mark other
types&lt;/a&gt;
as such as well, let's consider another approach for making the policy
more flexible: booleans. The idea now is that a boolean called
&lt;em&gt;incron_notify_non_security_files&lt;/em&gt; enables &lt;strong&gt;incrond&lt;/strong&gt; to be
notified on changes on all possible …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After using a default set of directories to watch, and &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/"&gt;allowing admins
to mark other
types&lt;/a&gt;
as such as well, let's consider another approach for making the policy
more flexible: booleans. The idea now is that a boolean called
&lt;em&gt;incron_notify_non_security_files&lt;/em&gt; enables &lt;strong&gt;incrond&lt;/strong&gt; to be
notified on changes on all possible non-security related files (the
latter is merely an approach, you can define other sets as well if you
want, including all possible files).&lt;/p&gt;
&lt;p&gt;Booleans in SELinux policy can be generated in the &lt;code&gt;incron.te&lt;/code&gt; file as
follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## &amp;lt;desc&amp;gt;
## &amp;lt;p&amp;gt;
##      Determine whether incron can watch all non-security
##      file types
## &amp;lt;/p&amp;gt;
## &amp;lt;/desc&amp;gt;
gen_tunable(incron_notify_non_security_files, false)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this boolean in place, the policy can be enhanced with code like
the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tunable_policy(`incron_notify_non_security_files&amp;#39;,`
        files_read_non_security_files(incrond_t)
        files_read_all_dirs_except(incrond_t)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This code tells SELinux that, &lt;em&gt;if&lt;/em&gt; the
&lt;em&gt;incron_notify_non_security_files&lt;/em&gt; boolean is set (which by default
is not the case), then &lt;code&gt;incrond_t&lt;/code&gt; is able to read non security files.&lt;/p&gt;
&lt;p&gt;Let's try to watch for changes in the AIDE log directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail audit.log
type=AVC msg=audit(1368777675.597:28611): avc:  denied  { search } for  pid=11704 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;log&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=13 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_log_t tclass=dir
type=AVC msg=audit(1368777675.597:28612): avc:  denied  { search } for  pid=11704 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;log&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=13 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_log_t tclass=dir

# tail cron.log
May 17 10:01:15 test incrond[11704]: access denied on /var/log/aide - events will be discarded silently

# getsebool incron_notify_non_security_files
incron_notify_non_security_files --&amp;gt; off
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's enable the boolean and try again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# setsebool incron_notify_non_security_files on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Reloading the incrontab tables now works, and the notifications work as
well.&lt;/p&gt;
&lt;p&gt;As you can see, once a policy is somewhat working, policy developers are
considering the various "use cases" of an application, trying to write
down policies that can be used by the majority of users, without
granting too many rights automatically.&lt;/p&gt;</content><category term="SELinux"></category><category term="boolean"></category><category term="incron"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: marking types eligible for watching</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/" rel="alternate"></link><published>2013-05-29T03:50:00+02:00</published><updated>2013-05-29T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-29:/2013/05/a-selinux-policy-for-incron-marking-types-eligible-for-watching/</id><summary type="html">&lt;p&gt;In the
&lt;a herf="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-default-set/"&gt;previous
post&lt;/a&gt; we made &lt;strong&gt;incrond&lt;/strong&gt; able to watch &lt;code&gt;public_content_t&lt;/code&gt; and
&lt;code&gt;public_content_rw_t&lt;/code&gt; types. However, this is not scalable, so we might
want to be able to update the policy more dynamically with additional
types. To accomplish this, we will make types eligible for watching
through an attribute.&lt;/p&gt;
&lt;p&gt;So how …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the
&lt;a herf="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-default-set/"&gt;previous
post&lt;/a&gt; we made &lt;strong&gt;incrond&lt;/strong&gt; able to watch &lt;code&gt;public_content_t&lt;/code&gt; and
&lt;code&gt;public_content_rw_t&lt;/code&gt; types. However, this is not scalable, so we might
want to be able to update the policy more dynamically with additional
types. To accomplish this, we will make types eligible for watching
through an attribute.&lt;/p&gt;
&lt;p&gt;So how does this work? First, we create an attribute called
&lt;code&gt;incron_notify_type&lt;/code&gt; (we can choose the name we want of course) and
grant &lt;code&gt;incrond_t&lt;/code&gt; the proper rights on all types that have been assigned
the &lt;code&gt;incron_notify_type&lt;/code&gt; attribute. Then, we create an interface that
other modules (or admins) can use to mark specific types eligible for
watching, called &lt;em&gt;incron_notify_file&lt;/em&gt;. This interface will assign the
&lt;code&gt;incron_notify_type&lt;/code&gt; attribute to the provided type.&lt;/p&gt;
&lt;p&gt;First, the attribute and its associated privileges:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;attribute incron_notify_type;
...
allow incrond_t incron_notify_type:dir list_dir_perms;
allow incrond_t incron_notify_type:file read_file_perms;
allow incrond_t incron_notify_type:lnk_file read_lnk_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's it. For now, this won't do much as there are no types associated
with the &lt;code&gt;incron_notify_type&lt;/code&gt; attribute, so let's change that by
introducing the interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;########################################
## &amp;lt;summary&amp;gt;
##      Make the specified type a file or directory
##      that incrond can watch on.
## &amp;lt;/summary&amp;gt;
## &amp;lt;param name=&amp;quot;file_type&amp;quot;&amp;gt;
##      &amp;lt;summary&amp;gt;
##      Type of the file to be allowed to watch
##      &amp;lt;/summary&amp;gt;
## &amp;lt;/param&amp;gt;
#
interface(`incron_notify_file&amp;#39;,`
        gen_require(`
                attribute incron_notify_type;
        &amp;#39;)

        typeattribute $1 incron_notify_type;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's it! If you want &lt;strong&gt;incrond&lt;/strong&gt; to watch user content, one can now do
something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;incron_notify_file(home_root_t)
incron_notify_file(user_home_dir_t)
incron_notify_file(user_home_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Moreover, we can now also easily check what additional types have been
marked as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ seinfo -aincron_notify_type -x
   incron_notify_type
      user_home_dir_t
      user_home_t
      home_root_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This attribute approach is commonly used for such setups and is becoming
more and more a "standard" approach.&lt;/p&gt;
&lt;p&gt;In the next post, we'll cover a boolean-triggered approach where
&lt;strong&gt;incrond&lt;/strong&gt; will be eligible for watching all (non-security) content.&lt;/p&gt;</content><category term="SELinux"></category><category term="attribute"></category><category term="incrond"></category><category term="selinux"></category><category term="watch"></category></entry><entry><title>A SELinux policy for incron: default set</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-default-set/" rel="alternate"></link><published>2013-05-28T03:50:00+02:00</published><updated>2013-05-28T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-28:/2013/05/a-selinux-policy-for-incron-default-set/</id><summary type="html">&lt;p&gt;I finished the last post a bit with a
&lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/"&gt;cliffhanger&lt;/a&gt;
as &lt;strong&gt;incrond&lt;/strong&gt; is still not working properly, and we got a few denials
that needed to be resolved; here they are again for your convenience:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368734110.912:28353): avc:  denied  { getattr } for  pid=9716 comm=&amp;quot;incrond …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I finished the last post a bit with a
&lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/"&gt;cliffhanger&lt;/a&gt;
as &lt;strong&gt;incrond&lt;/strong&gt; is still not working properly, and we got a few denials
that needed to be resolved; here they are again for your convenience:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368734110.912:28353): avc:  denied  { getattr } for  pid=9716 comm=&amp;quot;incrond&amp;quot; path=&amp;quot;/home/user/test2&amp;quot; dev=&amp;quot;dm-0&amp;quot; ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir
type=AVC msg=audit(1368734110.913:28354): avc:  denied  { read } for  pid=9716 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;test2&amp;quot; dev=&amp;quot;dm-0&amp;quot; ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The permission we are looking for here is
&lt;em&gt;userdom_list_user_home_content&lt;/em&gt;, but this is only for when we want
to watch a user home directory. What if we want to watch a server upload
directory? Or a cache directory? We might need to have &lt;strong&gt;incrond&lt;/strong&gt; have
the proper accesses on all directories. But then again, &lt;em&gt;all&lt;/em&gt; does sound
a bit... much, doesn't it? So let's split it up in three waves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;incrond_t&lt;/code&gt; domain will support a minimal set of types that it
    can watch, based on common approaches&lt;/li&gt;
&lt;li&gt;I will introduce an interface that allows other modules to mark
    specific types as being "watch-worthy"&lt;/li&gt;
&lt;li&gt;A boolean will be set to allow &lt;code&gt;incrond_t&lt;/code&gt; to watch a very large set
    of types (just in case the admin trusts it sufficiently)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's first consider a decent minimal set. Within most SELinux policies,
two types are often used for public access (or for uploading of data).
These types are &lt;code&gt;public_content_t&lt;/code&gt; and &lt;code&gt;public_content_rw_t&lt;/code&gt;, and is
used for instance for FTP definitions (upload folders), HTTP servers and
such. So we introduce the proper rights to watch that data. There is an
interface available called &lt;em&gt;miscfiles_read_public_files&lt;/em&gt; but let's
first see if that interface isn't too broad (after all, watching might
not be the same as reading).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# This is only to temporarily check if the rights of the interface are too broad or not
# You can set this using &amp;quot;selocal&amp;quot; or in a module (in which case you&amp;#39;ll need to &amp;#39;require&amp;#39;
# the two types)
allow incrond_t public_content_t:dir { read getattr };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After editing the incrontab to watch a directory labeled with
&lt;code&gt;public_content_t&lt;/code&gt;, we now get the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail cron.log
May 17 08:46:12 test incrond[9716]: (user) CMD (/usr/local/bin/test)
May 17 08:46:12 test incrond[11281]: cannot exec process: Operation not permitted
May 17 08:46:12 test incrond[9716]: cannot send SIGCHLD token to notification pipe

# tail audit.log
type=AVC msg=audit(1368773172.313:28386): avc:  denied  { setgid } for  pid=11281 comm=&amp;quot;incrond&amp;quot; capability=6  scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=capability
type=AVC msg=audit(1368773172.314:28387): avc:  denied  { read } for  pid=9716 comm=&amp;quot;incrond&amp;quot; path=&amp;quot;pipe:[14027]&amp;quot; dev=&amp;quot;pipefs&amp;quot; ino=14027 scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=fifo_file
type=AVC msg=audit(1368773172.315:28388): avc:  denied  { write } for  pid=9716 comm=&amp;quot;incrond&amp;quot; path=&amp;quot;pipe:[14027]&amp;quot; dev=&amp;quot;pipefs&amp;quot; ino=14027 scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=fifo_file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As the incrontab is a user incrontab, we can expect &lt;code&gt;incrond_t&lt;/code&gt; to
require setuid and setgid privileges. Also, the &lt;em&gt;fifo_file&lt;/em&gt; access is
after forking (notice the difference in PID values) and most likely to
communicate to the master process. So let's allow those:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrond_t self:capability { setuid setgid };
allow incrond_t self:fifo_file { read write };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that set, we get the following upon triggering a file write:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail cron.log
May 17 08:52:46 test incrond[9716]: (user) CMD (/usr/local/bin/test)
May 17 08:52:46 test incrond[11338]: cannot exec process: Permission denied

# tail audit.log
type=AVC msg=audit(1368773566.606:28394): avc:  denied  { read } for  pid=11338 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;ngroups_max&amp;quot; dev=&amp;quot;proc&amp;quot; ino=5711 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:sysctl_kernel_t tclass=file
type=AVC msg=audit(1368773566.607:28395): avc:  denied  { search } for  pid=11338 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;bin&amp;quot; dev=&amp;quot;dm-3&amp;quot; ino=1048578 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:bin_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ngroups_max&lt;/code&gt; pseudo-file (in &lt;code&gt;/proc/sys/kernel&lt;/code&gt;) returns the
maximum number of supplementary group IDs per process, and is consulted
through the &lt;em&gt;initgroups()&lt;/em&gt; method provided by a system library, so it
&lt;em&gt;might&lt;/em&gt; make sense to allow it. For now though, I will not enable it (as
reading &lt;code&gt;sysctl_kernel_t&lt;/code&gt; exposes a lot of other system information) but
I might be forced to do so later if things don't work out well. The
&lt;em&gt;search&lt;/em&gt; privilege on &lt;code&gt;bin_t&lt;/code&gt; is needed to find the script that I have
prepared (&lt;code&gt;/usr/local/bin/test&lt;/code&gt;) to be executed, so I add in a
&lt;em&gt;corecmd_search_bin&lt;/em&gt; and retry.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail cron.log
May 17 09:02:55 test incrond[9716]: (user) CMD (/usr/local/bin/test)
May 17 09:02:55 test incrond[11427]: cannot exec process: Permission denied

# tail audit.log
type=AVC msg=audit(1368774175.646:28441): avc:  denied  { read } for  pid=11427 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;sh&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=131454 scontext=system_u:system_r:incrond_t tcontext=root:object_r:bin_t tclass=lnk_file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Still not there yet apparently. The &lt;strong&gt;incrond&lt;/strong&gt; forked process wants to
execute the script, but to do so it has to follow a symbolic link
labeled &lt;code&gt;bin_t&lt;/code&gt;. This is because the script points to &lt;code&gt;#!/bin/sh&lt;/code&gt; which
is a symlink to the system shell. We need to follow this link before the
execution can occur; only after execution will the transition from
&lt;code&gt;incrond_t&lt;/code&gt; to &lt;code&gt;system_cronjob_t&lt;/code&gt; be done.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corecmd_read_bin_symlinks(incrond_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that set in the policy, the watch works, &lt;strong&gt;incrond&lt;/strong&gt; properly
launches the command and the command properly transitions into
&lt;code&gt;system_cronjob_t&lt;/code&gt; as we defined earlier (I check this by echo'ing the
output of &lt;strong&gt;id -Z&lt;/strong&gt; into a temporary file).&lt;/p&gt;
&lt;p&gt;So we are left with the (temporary) rights we granted on
&lt;code&gt;public_content_t&lt;/code&gt;. Consider the rules we had versus the rules applied
with &lt;em&gt;miscfiles_read_public_files&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrond_t public_content_t:dir { read getattr };

# miscfiles_read_public_files
allow $1 { public_content_t public_content_rw_t }:dir list_dir_perms;
read_files_pattern($1, { public_content_t public_content_rw_t }, { public_content_t public_content_rw_t })
read_lnk_files_pattern($1, { public_content_t public_content_rw_t }, { public_content_t public_content_rw_t })
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The rights here seem to bemore than what we need. Playing around a bit
with the directories reveals that &lt;strong&gt;incrond&lt;/strong&gt; requires a bit more. For
instance, when you create additional directories (subdirectories) and
want to match multiple ones:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail cron.log
May 17 09:16:08 test incrond[11704]: access denied on /var/www/test/* - events will be discarded silently
May 17 09:16:08 test incrond[11704]: cannot create watch for user user: (13) Permission denied

# tail audit.log
type=AVC msg=audit(1368774968.416:28504): avc:  denied  { search } for  pid=11704 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;test&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=1488 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=dir
type=AVC msg=audit(1368774968.416:28505): avc:  denied  { search } for  pid=11704 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;test&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=1488 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly if you want to watch on a particular file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368775274.655:28552): avc:  denied  { getattr } for  pid=11704 comm=&amp;quot;incrond&amp;quot; path=&amp;quot;/var/www/test/testfile&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=1709 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=file
type=AVC msg=audit(1368775274.655:28553): avc:  denied  { read } for  pid=11704 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;testfile&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=1709 scontext=system_u:system_r:incrond_t tcontext=root:object_r:public_content_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So it looks like &lt;em&gt;miscfiles_read_public_files&lt;/em&gt; isn't that bad after
all.&lt;/p&gt;
&lt;p&gt;All we are left with is the access to &lt;code&gt;ngroups_max&lt;/code&gt;. We can ignore the
calls and make sure they don't show up in standard auditing using
&lt;em&gt;kernel_dontaudit_read_kernel_sysctls&lt;/em&gt; or we can allow it with
&lt;em&gt;kernel_read_kernel_sysctls&lt;/em&gt;. I'm going to take the former approach
for my system, but your own idea might be different.&lt;/p&gt;
&lt;p&gt;I tested all this with user incrontabs (as those are the "most"
advanced) but one can easily test with system incrontabs as well
(placing one in &lt;code&gt;/etc/incron.d&lt;/code&gt;). Just be aware that &lt;em&gt;incrond&lt;/em&gt; will take
the first match and will not seek other matches. So if a system
incrontab watches &lt;code&gt;/var/www&lt;/code&gt; and another line (or user incrontab)
watches &lt;code&gt;/var/www/localhost/upload&lt;/code&gt; it is very well possible that only
the &lt;code&gt;/var/www&lt;/code&gt; watch is triggered.&lt;/p&gt;
&lt;p&gt;So right now, our &lt;code&gt;incrond_t&lt;/code&gt; policy looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;###########################################
#
# incrond policy
#

allow incrond_t self:capability { setgid setuid };

allow incrond_t incron_spool_t:dir list_dir_perms;
allow incrond_t incron_spool_t:file read_file_perms;

allow incrond_t self:fifo_file { read write };

allow incrond_t incrond_var_run_t:file manage_file_perms;
files_pid_filetrans(incrond_t, incrond_var_run_t, file)

kernel_dontaudit_read_kernel_sysctls(incrond_t)

corecmd_read_bin_symlinks(incrond_t)
corecmd_search_bin(incrond_t)

files_search_spool(incrond_t)

logging_send_syslog_msg(incrond_t)

auth_use_nsswitch(incrond_t)

miscfiles_read_localization(incrond_t)
miscfiles_read_public_files(incrond_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next on the agenda is another interface to make other types
"watch-worthy".&lt;/p&gt;</content><category term="SELinux"></category><category term="booleans"></category><category term="incrond"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: the incrond daemon</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/" rel="alternate"></link><published>2013-05-27T03:50:00+02:00</published><updated>2013-05-27T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-27:/2013/05/a-selinux-policy-for-incron-the-incrond-daemon/</id><summary type="html">&lt;p&gt;With &lt;code&gt;incrontab_t&lt;/code&gt; (hopefully) complete, let's look at the &lt;code&gt;incrond_t&lt;/code&gt;
domain. As this domain will also be used to execute the user (and
system) commands provided through the incrontabs, we need to consider
how we are going to deal with this wide range of possible permissions
that it might take. One …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With &lt;code&gt;incrontab_t&lt;/code&gt; (hopefully) complete, let's look at the &lt;code&gt;incrond_t&lt;/code&gt;
domain. As this domain will also be used to execute the user (and
system) commands provided through the incrontabs, we need to consider
how we are going to deal with this wide range of possible permissions
that it might take. One would be to make &lt;code&gt;incrond_t&lt;/code&gt; quite powerful, and
extend its privileges as we go further. But in my opinion, that's not a
good way to deal with it.&lt;/p&gt;
&lt;p&gt;Another would be to support a small set of permissions, and introduce an
interface that other modules can use to create a transition when
&lt;code&gt;incrond_t&lt;/code&gt; executes a script properly labeled for a transition. For
instance, a domain &lt;code&gt;foo_t&lt;/code&gt; might have an executable type &lt;code&gt;foo_exec_t&lt;/code&gt;.
Most modules support an interface similar to &lt;em&gt;foo_domtrans&lt;/em&gt; (and
&lt;em&gt;foo_role&lt;/em&gt; if roles are applicable as well), but that assumes that the
&lt;em&gt;incron&lt;/em&gt; policy is modified every time a new target module is made
available (since we then need to add the proper &lt;em&gt;*_domtrans&lt;/em&gt; rules to
the &lt;em&gt;incron&lt;/em&gt; policy. Instead, we might want to make this something that
the &lt;em&gt;foo&lt;/em&gt; SELinux module can decide.&lt;/p&gt;
&lt;p&gt;It is that approach that we are going to take here. To do so, we will
create a new interface called &lt;em&gt;incron_entry&lt;/em&gt;, taken a bit from the
&lt;em&gt;cron_system_entry&lt;/em&gt; interface already in place for the regular &lt;em&gt;cron&lt;/em&gt;
domain (the following comes in &lt;code&gt;incron.if&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## &amp;lt;summary&amp;gt;
##      Make the specified program domain
##      accessible from the incrond job.
## &amp;lt;/summary&amp;gt;
## &amp;lt;param name=&amp;quot;domain&amp;quot;&amp;gt;
##      &amp;lt;summary&amp;gt;
##      The type of the process to transition to
##      &amp;lt;/summary&amp;gt;
## &amp;lt;/param&amp;gt;
## &amp;lt;param name=&amp;quot;entrypoint&amp;quot;&amp;gt;
##      &amp;lt;summary&amp;gt;
##      The type of the file used as an entrypoint to this domain
##      &amp;lt;/summary&amp;gt;
## &amp;lt;/param&amp;gt;
#
interface(`incron_entry&amp;#39;,`
        gen_require(`
                type incrond_t;
        &amp;#39;)

        domtrans_pattern(incrond_t, $2, $1)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this in place, the &lt;em&gt;foo&lt;/em&gt; SELinux module can call
&lt;em&gt;incron_entry(foo_t, foo_exec_t)&lt;/em&gt; so that, the moment &lt;code&gt;incrond_t&lt;/code&gt;
executes a file with label &lt;code&gt;foo_exec_t&lt;/code&gt;, the resulting process will run
in &lt;code&gt;foo_t&lt;/code&gt;. I am going to &lt;em&gt;test&lt;/em&gt; (and I stress that it is only for
&lt;em&gt;testing&lt;/em&gt;) this by assigning &lt;em&gt;incron_entry(system_cronjob_t,
shell_exec_t)&lt;/em&gt;, making every shell script being called run in
&lt;code&gt;system_cronjob_t&lt;/code&gt; domain (for instance in the &lt;code&gt;localuser.te&lt;/code&gt; file that
already assigned &lt;em&gt;incron_role&lt;/em&gt; to the &lt;code&gt;user_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;With that in place, it's time to start our iterations again.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# run_init rc-service incrond start
* start-stop-daemon: failed to start &amp;#39;/usr/sbin/incrond&amp;#39; [ !! ]
* ERROR: incrond failed to start

# tail audit.log
type=AVC msg=audit(1368732494.275:28319): avc:  denied  { read } for  pid=9282 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;localtime&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=393663 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:locale_t tclass=file
type=AVC msg=audit(1368732494.275:28320): avc:  denied  { create } for  pid=9282 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368732494.276:28321): avc:  denied  { read } for  pid=9282 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;incron.d&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=394140 scontext=system_u:system_r:incrond_t tcontext=root:object_r:incron_spool_t tclass=dir
type=AVC msg=audit(1368732494.276:28322): avc:  denied  { create } for  pid=9282 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368732494.276:28323): avc:  denied  { create } for  pid=9282 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Ignoring the &lt;em&gt;unix_dgram_socket&lt;/em&gt; for now, we need to allow &lt;code&gt;incrond_t&lt;/code&gt;
to read locale information, and to read the files in the
&lt;code&gt;/var/spool/incron&lt;/code&gt; location (this goes in &lt;code&gt;incron.te&lt;/code&gt; again):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;###########################################
#
# incrond policy
#

read_files_pattern(incrond_t, incron_spool_t, incron_spool_t)

files_search_spool(incrond_t)

miscfiles_read_localization(incrond_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next run fails again, with the following denials:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368732806.757:28328): avc:  denied  { create } for  pid=9419 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368732806.757:28329): avc:  denied  { read } for  pid=9419 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;incron.d&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=394140 scontext=system_u:system_r:incrond_t tcontext=root:object_r:incron_spool_t tclass=dir
type=AVC msg=audit(1368732806.757:28330): avc:  denied  { create } for  pid=9419 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368732806.757:28331): avc:  denied  { create } for  pid=9419 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So although &lt;code&gt;incrond_t&lt;/code&gt; has search rights on the &lt;code&gt;incron_spool_t&lt;/code&gt;
directories (through the &lt;code&gt;read_files_pattern&lt;/code&gt;), we need to grant it
&lt;em&gt;list_dir_perms&lt;/em&gt; as well (which contains the &lt;em&gt;read&lt;/em&gt; permission). As
&lt;em&gt;list_dir_perms&lt;/em&gt; contains search anyhow, we can just update the line
with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrond_t incron_spool_t:dir list_dir_perms;
allow incrond_t incron_spool_t:file read_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the startup seems to work, but we still get denials:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# run_init rc-service incrond start
* Starting incrond... [ ok ]

# ps -eZ | grep incrond
# tail /var/log/cron.log
(nothing)

# tail audit.log
type=AVC msg=audit(1368733443.799:28340): avc:  denied  { create } for  pid=9551 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368733443.802:28341): avc:  denied  { write } for  pid=9552 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;/&amp;quot; dev=&amp;quot;tmpfs&amp;quot; ino=1970 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:var_run_t tclass=dir
type=AVC msg=audit(1368733443.806:28342): avc:  denied  { create } for  pid=9552 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
type=AVC msg=audit(1368733443.806:28343): avc:  denied  { create } for  pid=9552 comm=&amp;quot;incrond&amp;quot; scontext=system_u:system_r:incrond_t tcontext=system_u:system_r:incrond_t tclass=unix_dgram_socket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Those &lt;em&gt;unix_dgram_sockets&lt;/em&gt; are here again. But seeing that &lt;code&gt;cron.log&lt;/code&gt;
is empty, and &lt;em&gt;logging_send_syslog_msg&lt;/em&gt; is one of the interfaces that
would enable it, we might want to do just that so that we get more
information about why &lt;strong&gt;incrond&lt;/strong&gt; doesn't properly start. Also, it tries
to write into &lt;code&gt;var_run_t&lt;/code&gt; labeled directories, probably for its PID
file, so add in a proper file transition as well as manage rights:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type incrond_var_run_t;
files_pid_file(incrond_var_run_t)
...
allow incrond_t incrond_var_run_t:file manage_file_perms;
files_pid_filetrans(incrond_t, incrond_var_run_t, file)
...
logging_send_syslog_msg(incrond_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that in place:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# run_init rc-service incrond start
* Starting incrond... [ ok ]

# ps -eZ | grep incron
system_u:system_r:incrond_t      9648 ?        00:00:00 incrond

# tail /var/log/cron.log 
May 16 21:51:34 test incrond[9647]: starting service (version 0.5.10, built on May 16 2013 12:11:29)
May 16 21:51:34 test incrond[9648]: loading system tables
May 16 21:51:34 test incrond[9648]: loading user tables
May 16 21:51:34 test incrond[9648]: table for invalid user user found (ignored)
May 16 21:51:34 test incrond[9648]: ready to process filesystem events

# tail audit.log
type=AVC msg=audit(1368733894.641:28347): avc:  denied  { read } for  pid=9648 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;nsswitch.conf&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=393768 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:etc_t tclass=file
type=AVC msg=audit(1368733894.645:28349): avc:  denied  { read } for  pid=9648 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;passwd&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=394223 scontext=system_u:system_r:incrond_t tcontext=system_u:object_r:etc_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It looks like we're getting there. Similar as with &lt;strong&gt;incrontab&lt;/strong&gt; we
allow &lt;em&gt;auth_use_nsswitch&lt;/em&gt; as well, and then get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# tail cron.log
May 16 21:55:10 test incrond[9715]: starting service (version 0.5.10, built on May 16 2013 12:11:29)
May 16 21:55:10 test incrond[9716]: loading system tables
May 16 21:55:10 test incrond[9716]: loading user tables
May 16 21:55:10 test incrond[9716]: loading table for user user
May 16 21:55:10 test incrond[9716]: access denied on /home/user/test2 - events will be discarded silently
May 16 21:55:10 test incrond[9716]: cannot create watch for user user: (13) Permission denied
May 16 21:55:10 test incrond[9716]: ready to process filesystem events

# tail audit.log
type=AVC msg=audit(1368734110.912:28353): avc:  denied  { getattr } for  pid=9716 comm=&amp;quot;incrond&amp;quot; path=&amp;quot;/home/user/test2&amp;quot; dev=&amp;quot;dm-0&amp;quot; ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir
type=AVC msg=audit(1368734110.913:28354): avc:  denied  { read } for  pid=9716 comm=&amp;quot;incrond&amp;quot; name=&amp;quot;test2&amp;quot; dev=&amp;quot;dm-0&amp;quot; ino=16 scontext=system_u:system_r:incrond_t tcontext=user_u:object_r:user_home_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What happens is that &lt;strong&gt;incrond&lt;/strong&gt; read the (user) crontab, found that it
had to "watch" &lt;code&gt;/home/user/test2&lt;/code&gt; but fails because SELinux doesn't
allow it to do so. We could just allow that, but we might do it a bit
better by looking into what we want it to do in a flexible manner...
next time ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="incrond"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: new types and transitions</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-new-types-and-transitions/" rel="alternate"></link><published>2013-05-26T03:50:00+02:00</published><updated>2013-05-26T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-26:/2013/05/a-selinux-policy-for-incron-new-types-and-transitions/</id><summary type="html">&lt;p&gt;So I've shown the &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/"&gt;iterative approach
used&lt;/a&gt;
to develop policies. Again, please be aware that this is my way of
developing policies, other policy developers might have a different
approach. We were working on the &lt;strong&gt;incrontab&lt;/strong&gt; command, so let's
continue with trying to create a new user incrontab:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -e …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;So I've shown the &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/"&gt;iterative approach
used&lt;/a&gt;
to develop policies. Again, please be aware that this is my way of
developing policies, other policy developers might have a different
approach. We were working on the &lt;strong&gt;incrontab&lt;/strong&gt; command, so let's
continue with trying to create a new user incrontab:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -e
cannot create temporary file: Permission denied

# tail audit.log
type=AVC msg=audit(1368709633.285:28211): avc:  denied  { setgid } for  pid=8159 comm=&amp;quot;incrontab&amp;quot; capability=6  scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability
type=AVC msg=audit(1368709633.285:28212): avc:  denied  { setuid } for  pid=8159 comm=&amp;quot;incrontab&amp;quot; capability=7  scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability
type=AVC msg=audit(1368709633.287:28213): avc:  denied  { search } for  pid=8159 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;/&amp;quot; dev=&amp;quot;tmpfs&amp;quot; ino=3927 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:tmp_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The requests for the setuid and setgid capabilities are needed for the
application to safely handle the user incrontabs. Note that SELinux does
not "remove" the setuid bit on the binary itself, but does govern the
related capabilities. Since this is required, we will add these
capabilities to the policy. We also notice that &lt;strong&gt;incrontab&lt;/strong&gt; searched
in the &lt;code&gt;/tmp&lt;/code&gt; location.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrontab_t self:capability { setuid setgid };
...
files_search_tmp(incrontab_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the next round of iteration, we notice the same error message with
the following denial:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368728433.521:28215): avc:  denied  { write } for  pid=8913 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;/&amp;quot; dev=&amp;quot;tmpfs&amp;quot; ino=3927 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:tmp_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is safe to assume here that the process wants to create a temporary
file (if it is a directory, we will find out later and can adapt). But
when temporary files are created, we better make those files a specific
type, like &lt;code&gt;incrontab_tmp_t&lt;/code&gt;. So we define that on top of the policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type incrontab_tmp_t;
files_tmp_file(incrontab_tmp_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Also, we need to allow the &lt;code&gt;incrontab_t&lt;/code&gt; domain write privileges into
the &lt;code&gt;tmp_t&lt;/code&gt; labeled directory, but with an automatic file transition
towards &lt;code&gt;incrontab_tmp_t&lt;/code&gt; for every file written. This is done through
the &lt;em&gt;files_tmp_filetrans&lt;/em&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;files_tmp_filetrans(incrontab_t, incrontab_tmp_t, file)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What this sais is that, if a domain &lt;code&gt;incrontab_t&lt;/code&gt; wants to create a
&lt;code&gt;file&lt;/code&gt; inside &lt;code&gt;tmp_t&lt;/code&gt;, then this file is automatically labeled
&lt;code&gt;incrontab_tmp_t&lt;/code&gt;. With SELinux, you can make this more precise: if you
know what the file name would be, then you can add that as a fourth
argument. However, this does not seem necessary now since we definitely
want all files created in &lt;code&gt;tmp_t&lt;/code&gt; to become &lt;code&gt;incrontab_tmp_t&lt;/code&gt;. All that
rests us is to allow &lt;strong&gt;incrontab&lt;/strong&gt; to actually manage those files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrontab_t incrontab_tmp_t:file manage_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With those in place, let's look at the outcome:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -e
editor finished with error: No such file or directory

# tail audit.log
type=AVC msg=audit(1368729268.465:28217): avc:  denied  { search } for  pid=8981 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;bin&amp;quot; dev=&amp;quot;dm-3&amp;quot; ino=524289 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:bin_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Considering that here, &lt;strong&gt;incrontab&lt;/strong&gt; is going to launch the users
&lt;code&gt;$EDITOR&lt;/code&gt; application to allow him (or her) to create an incrontab, we
need to allow &lt;code&gt;incrontab_t&lt;/code&gt; not only search privileges inside &lt;code&gt;bin_t&lt;/code&gt;
directories, but also execute rights:
&lt;em&gt;corecmd_exec_bin(incrontab_t)&lt;/em&gt;. We choose here to execute the editor
inside the existing domain (&lt;code&gt;incrontab_t&lt;/code&gt;) instead of creating a
different domain for the editor for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we would create a separate domain for the editor, the editor
    would eventually need to have major permissions, depending on when
    it is used. Editors can be used to modify the sudoers files, passwd
    files, the &lt;code&gt;/etc/selinux/config&lt;/code&gt; file, etc. Instead, it makes much
    more sense to just be able to launch the editor in the current
    domain (which is much more confined to its specific purpose)&lt;/li&gt;
&lt;li&gt;The additional privileges needed to launch the editor are usually
    very slim, or even nonexistent. It generally only makes sense if, by
    executing it, the existing domain would need many more privileges,
    because then a new (confined) domain keeps the privileges for the
    current domain low.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's see if things work now:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -e
(Editor opened, so I added in an incrontab line. Upon closing:)
cannot move temporary table: Permission denied

# tail audit.log
type=AVC msg=audit(1368729825.673:28237): avc:  denied  { dac_read_search } for  pid=9030 comm=&amp;quot;incrontab&amp;quot; capability=2  scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability
type=AVC msg=audit(1368729825.673:28237): avc:  denied  { dac_override } for  pid=9030 comm=&amp;quot;incrontab&amp;quot; capability=1  scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=capability
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From a quick look through &lt;strong&gt;ps&lt;/strong&gt;, I notice that the application runs as
the user (luckily, otherwise I could use the editor to escape and get a
root shell) after which it tries to do something. Of course, it makes
sense that it wants to move this newly created incrontab file somewhere
in &lt;code&gt;/var/spool/incron&lt;/code&gt; so we grant it the permission to
&lt;code&gt;dac_read_search&lt;/code&gt; (which is lower than &lt;code&gt;dac_override&lt;/code&gt; as &lt;a href="http://blog.siphos.be/2013/05/the-weird-audit_access-permission/"&gt;explained
before&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrontab_t self:capability { dac_read_search setuid setgid };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On to the next failure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -e 
cannot move temporary table: Permission denied

# tail audit.log
type=AVC msg=audit(1368730155.706:28296): avc:  denied  { write } for  pid=9088 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;incron&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=19725 scontext=user_u:user_r:incrontab_t tcontext=root:object_r:incron_spool_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the application wants to write this file there. Now remember we
already have &lt;code&gt;search_dir_perms&lt;/code&gt; permissions into &lt;code&gt;incron_spool_t&lt;/code&gt;? We
need to expand those with read/write permissions into the directory, and
manage permissions on files (manage because users should be able to
create, modify and delete their files). These two permissions are
combined in the &lt;em&gt;manage_files_pattern&lt;/em&gt; interface, and makes the search
one obsolete:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;manage_files_pattern(incrontab_t, incron_spool_t, incron_spool_t)

$ incrontab -e
...
table updated
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally! And looking at the other options in &lt;strong&gt;incrontab&lt;/strong&gt;, it seems
that the policy for &lt;code&gt;incrontab_t&lt;/code&gt; is finally complete, and looks like
so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;###########################################
#
# incrontab policy
#

allow incrontab_t self:capability { setuid setgid dac_read_search };

manage_files_pattern(incrontab_t, incron_spool_t, incron_spool_t)

allow incrontab_t incrontab_tmp_t:file manage_file_perms;
files_tmp_filetrans(incrontab_t, incrontab_tmp_t, file)

corecmd_exec_bin(incrontab_t)

domain_use_interactive_fds(incrontab_t)

files_search_spool(incrontab_t)
files_search_tmp(incrontab_t)

auth_use_nsswitch(incrontab_t)

userdom_use_user_terminals(incrontab_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next on the agenda: the &lt;code&gt;incrond_t&lt;/code&gt; domain.&lt;/p&gt;</content><category term="SELinux"></category><category term="incron"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: basic set for incrontab</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/" rel="alternate"></link><published>2013-05-25T03:50:00+02:00</published><updated>2013-05-25T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-25:/2013/05/a-selinux-policy-for-incron-basic-set-for-incrontab/</id><summary type="html">&lt;p&gt;Now that our &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-our-first-interface/"&gt;regular user is
allowed&lt;/a&gt;
to execute &lt;strong&gt;incrontab&lt;/strong&gt;, let's fire it up and look at the denials to
build up the policy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab --help
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That doesn't show much does it? Well, if you look into the &lt;code&gt;audit.log&lt;/code&gt;
(or &lt;code&gt;avc.log&lt;/code&gt;) file, you'll notice a lot of denials …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Now that our &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-our-first-interface/"&gt;regular user is
allowed&lt;/a&gt;
to execute &lt;strong&gt;incrontab&lt;/strong&gt;, let's fire it up and look at the denials to
build up the policy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab --help
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That doesn't show much does it? Well, if you look into the &lt;code&gt;audit.log&lt;/code&gt;
(or &lt;code&gt;avc.log&lt;/code&gt;) file, you'll notice a lot of denials. If you are
developing a policy, it is wise to clear the entire log and reproduce
the "situation" so you get a proper idea of the scope.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cd /var/log/audit
# &amp;gt; audit.log
# tail -f audit.log | grep AVC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now let's run &lt;strong&gt;incrontab --help&lt;/strong&gt; again and look at the denials:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368707274.429:28180): avc:  denied  { read write } for  pid=7742 comm=&amp;quot;incrontab&amp;quot; path=&amp;quot;/dev/tty2&amp;quot; dev=&amp;quot;devtmpfs&amp;quot; ino=1042 scontext=user_u:user_r:incrontab_t tcontext=user_u:object_r:user_tty_device_t tclass=chr_file
type=AVC msg=audit(1368707274.429:28180): avc:  denied  { use } for  pid=7742 comm=&amp;quot;incrontab&amp;quot; path=&amp;quot;/dev/tty2&amp;quot; dev=&amp;quot;devtmpfs&amp;quot; ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd
type=AVC msg=audit(1368707274.429:28180): avc:  denied  { use } for  pid=7742 comm=&amp;quot;incrontab&amp;quot; path=&amp;quot;/dev/tty2&amp;quot; dev=&amp;quot;devtmpfs&amp;quot; ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd
type=AVC msg=audit(1368707274.429:28180): avc:  denied  { use } for  pid=7742 comm=&amp;quot;incrontab&amp;quot; path=&amp;quot;/dev/tty2&amp;quot; dev=&amp;quot;devtmpfs&amp;quot; ino=1042 scontext=user_u:user_r:incrontab_t tcontext=system_u:system_r:getty_t tclass=fd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can start piping this information into &lt;strong&gt;audit2allow&lt;/strong&gt; to generate
policy statements, but I personally prefer not to use &lt;strong&gt;audit2allow&lt;/strong&gt;
for building new policies. For one, it is not intelligent enough to
deduce if a denial should be fixed by allowing it, or by relabeling or
even by creating a new type. Instead, it always grants it. Second, it
does not know if a denial is cosmetic (and thus can be ignored) or not.&lt;/p&gt;
&lt;p&gt;This latter is also why I don't run domains in permissive mode to see
the majority of denials first and to build from those: you might see
denials that are actually never triggered when running in enforcing
mode. So let's look at the access to &lt;code&gt;/dev/tty2&lt;/code&gt;. Given that this is a
user application where we expect output to the screen, we want to grant
it the proper access. With &lt;strong&gt;sefindif&lt;/strong&gt; as
&lt;a href="http://blog.siphos.be/2013/05/commandline-selinux-policy-helper-functions/"&gt;documented&lt;/a&gt;
before, we can look for the proper interfaces we need. I look for
&lt;code&gt;user_tty_device_t&lt;/code&gt; with &lt;code&gt;rw&lt;/code&gt; (commonly used for read-write):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sefindif user_tty_device_t.*rw
system/userdomain.if: template(`userdom_base_user_template&amp;#39;,`
system/userdomain.if:   allow $1_t user_tty_device_t:chr_file { setattr rw_chr_file_perms };
system/userdomain.if: interface(`userdom_use_user_ttys&amp;#39;,`
system/userdomain.if:   allow $1 user_tty_device_t:chr_file rw_term_perms;
system/userdomain.if: interface(`userdom_use_user_terminals&amp;#39;,`
system/userdomain.if:   allow $1 user_tty_device_t:chr_file rw_term_perms;
system/userdomain.if: interface(`userdom_dontaudit_use_user_terminals&amp;#39;,`
system/userdomain.if:   dontaudit $1 user_tty_device_t:chr_file rw_term_perms;
system/userdomain.if: interface(`userdom_dontaudit_use_user_ttys&amp;#39;,`
system/userdomain.if:   dontaudit $1 user_tty_device_t:chr_file rw_file_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Two of these look interesting: &lt;em&gt;userdom_use_user_ttys&lt;/em&gt; and
&lt;em&gt;userdom_use_user_terminals&lt;/em&gt;. Looking at the API documentation (or
the rules defined therein using &lt;strong&gt;seshowif&lt;/strong&gt;) reveals that
&lt;em&gt;userdom_use_user_terminals&lt;/em&gt; is needed if you also want the
application to work when invoked through a devpts terminal, which is
probably also something our user(s) want to do, so we'll add that. The
second one - using the file descriptor that has the &lt;code&gt;getty_t&lt;/code&gt; context -
is related to this, but not granted through the
&lt;em&gt;userdom_use_user_ttys&lt;/em&gt;. We could grant &lt;em&gt;getty_use_fds&lt;/em&gt; but my
experience tells me that &lt;em&gt;domain_use_interactive_fds&lt;/em&gt; is more likely
to be needed: the application inherits and uses a file descriptor
currently owned by &lt;code&gt;getty_t&lt;/code&gt; but it could be from any of the other
domains that has such file descriptors. For instance, if you grant the
&lt;em&gt;incron_role&lt;/em&gt; to &lt;code&gt;sysadm_r&lt;/code&gt;, then a user that switched roles through
&lt;strong&gt;newrole&lt;/strong&gt; will see denials for using a file descriptor owned by
&lt;code&gt;newrole_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Experience is an important aspect in developing policies. If you would
go through with &lt;em&gt;getty_use_fds&lt;/em&gt; it would work as well, and you'll
probably hit the above mentioned experience later when you try the
application through a few different paths (such as within a screen
session or so). When you &lt;em&gt;think&lt;/em&gt; that the target context (in this case
&lt;code&gt;getty_t&lt;/code&gt;) could be a placeholder (so other types are likely to be
needed as well), make sure you check which attributes are assigned to
the type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# seinfo -tgetty_t -x
   getty_t
      privfd
      mcssetcats
      mlsfileread
      mlsfilewrite
      application_domain_type
      domain
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of the above ones, &lt;code&gt;privfd&lt;/code&gt; is the important one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sefindif privfd.*use
kernel/domain.if: interface(`domain_use_interactive_fds&amp;#39;,`
kernel/domain.if:       allow $1 privfd:fd use;
kernel/domain.if: interface(`domain_dontaudit_use_interactive_fds&amp;#39;,`
kernel/domain.if:       dontaudit $1 privfd:fd use;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So let's update &lt;code&gt;incron.te&lt;/code&gt; accordingly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;...
type incron_spool_t;
files_type(incron_spool_t)

###########################################
#
# incrontab policy
#

userdom_use_user_terminals(incrontab_t)
domain_use_interactive_fds(incrontab_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Rebuild the policy and load it in memory.&lt;/p&gt;
&lt;p&gt;If we now run &lt;strong&gt;incrontab&lt;/strong&gt; we get the online help as we expected. Let's
now look at the currently installed incrontabs (there shouldn't be any
of course):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -l
cannot determine current user
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the denials, we notice:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1368708632.060:28192): avc:  denied  { create } for  pid=7968 comm=&amp;quot;incrontab&amp;quot; scontext=user_u:user_r:incrontab_t tcontext=user_u:user_r:incrontab_t tclass=unix_stream_socket
type=AVC msg=audit(1368708632.060:28194): avc:  denied  { read } for  pid=7968 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;nsswitch.conf&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=393768 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:etc_t tclass=file
type=AVC msg=audit(1368708632.062:28196): avc:  denied  { read } for  pid=7968 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;passwd&amp;quot; dev=&amp;quot;dm-2&amp;quot; ino=394223 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:etc_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's first focus on &lt;code&gt;nsswitch.conf&lt;/code&gt; and &lt;code&gt;passwd&lt;/code&gt;. Although both require
read access to &lt;code&gt;etc_t&lt;/code&gt; files, it might be wrong to just add in
&lt;em&gt;files_read_etc&lt;/em&gt; (which is what &lt;strong&gt;audit2allow&lt;/strong&gt; is probably going to
suggest). For nsswitch, there is a special interface available:
&lt;em&gt;auth_use_nsswitch&lt;/em&gt;. It is very, very likely that you'll need this
one, especially if you want to share the policy with others who might
not have all of the system databases in local files (as &lt;code&gt;etc_t&lt;/code&gt; files).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;...
domain_use_interactive_fds(incrontab_t)
auth_use_nsswitch(incrontab_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's retry:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -l
cannot read table for &amp;#39;user&amp;#39;: Permission denied

# tail audit.log
type=AVC msg=audit(1368708893.260:28199): avc:  denied  { search } for  pid=7997 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;spool&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=20 scontext=user_u:user_r:incrontab_t tcontext=system_u:object_r:var_spool_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So we need to grant search privileges on &lt;code&gt;var_spool_t&lt;/code&gt;. This is offered
through &lt;em&gt;files_search_spool&lt;/em&gt;. Add it to the policy, rebuild and retry.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -l
cannot read table for &amp;#39;user&amp;#39;: Permission denied

# tail audit.log
type=AVC msg=audit(1368709146.426:28201): avc:  denied  { search } for  pid=8046 comm=&amp;quot;incrontab&amp;quot; name=&amp;quot;incron&amp;quot; dev=&amp;quot;dm-4&amp;quot; ino=19725 scontext=user_u:user_r:incrontab_t tcontext=root:object_r:incron_spool_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For this one, no interface exists yet. We might be able to create one
for ourselves, but as long as other domains don't need it, we can just
add it locally in our policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow incrontab_t incron_spool_t:dir search_dir_perms;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Adding raw allow rules in a policy is, according to the &lt;a href="http://oss.tresys.com/projects/refpolicy/wiki/StyleGuide"&gt;refpolicy
styleguide&lt;/a&gt;,
only allowed if the policy module defines both the source and the
destination type of the rule. If you look into other policies you might
also find that you can use the &lt;em&gt;search_dirs_patter&lt;/em&gt; call. However,
that one only makes sense if you need to do this on top of another
directory - just look at the definition of &lt;em&gt;search_dirs_pattern&lt;/em&gt;. So
with this permission set, let's retry.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ incrontab -l
no table for user
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great, we have successfully updated the policy until the commands
worked. In the next post, we'll enhance it even further while creating
new incrontabs.&lt;/p&gt;</content><category term="SELinux"></category><category term="incron"></category><category term="incrontab"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>A SELinux policy for incron: our first interface</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-our-first-interface/" rel="alternate"></link><published>2013-05-24T03:50:00+02:00</published><updated>2013-05-24T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-24:/2013/05/a-selinux-policy-for-incron-our-first-interface/</id><summary type="html">&lt;p&gt;The next step after having &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/"&gt;a basic
skeleton&lt;/a&gt;
is to get &lt;strong&gt;incrontab&lt;/strong&gt; running. We know however that everything invoked
from the main daemon will be running with the rights of the daemon
context (unless we would patch the source code, but that is beyond the
scope of this set of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The next step after having &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/"&gt;a basic
skeleton&lt;/a&gt;
is to get &lt;strong&gt;incrontab&lt;/strong&gt; running. We know however that everything invoked
from the main daemon will be running with the rights of the daemon
context (unless we would patch the source code, but that is beyond the
scope of this set of posts). As a result, we probably do not want
everyone to be able to launch commands through this application.&lt;/p&gt;
&lt;p&gt;What we want to do is to limit who can invoke &lt;strong&gt;incrontab&lt;/strong&gt; and, as
such, limit who can decide what is invoked through &lt;strong&gt;incrond&lt;/strong&gt;. First of
all, we define a &lt;em&gt;role attribute&lt;/em&gt; called &lt;code&gt;incrontab_roles&lt;/code&gt;. Every role
that gets this attribute assigned will be able to transition to the
&lt;code&gt;incrontab_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;We can accomplish this by editing the &lt;code&gt;incron.te&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(incron, 0.2)

# Declare the incrontab_roles attribute
attribute_role incrontab_roles;

...
type incrontab_t;
type incrontab_exec_t;
application_domain(incrontab_t, incrontab_exec_t)
# Allow incrontab_t for all incrontab_roles 
role incrontab_roles types incrontab_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we need something where we can allow user domains to call
incrontab. This will be done through an interface. Let's look at
&lt;code&gt;incron.if&lt;/code&gt; with one such interface in it: the &lt;em&gt;incron_role&lt;/em&gt; interface.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## inotify-based cron-like daemon

#########################################
## &amp;lt;summary&amp;gt;
##      Role access for incrontab
## &amp;lt;/summary&amp;gt;
## &amp;lt;param name=&amp;quot;role&amp;quot;&amp;gt;
##      &amp;lt;summary&amp;gt;
##      Role allowed access.
##      &amp;lt;/summary&amp;gt;
## &amp;lt;/param&amp;gt;
## &amp;lt;param name=&amp;quot;domain&amp;quot;&amp;gt;
##      &amp;lt;summary&amp;gt;
##      User domain for the role.
##      &amp;lt;/summary&amp;gt;
## &amp;lt;/param&amp;gt;
#
interface(`incron_role&amp;#39;,`
        gen_require(`
                attribute_role incrontab_roles;
                type incrontab_exec_t, incrontab_t;
        &amp;#39;)

        roleattribute $1 incrontab_roles;

        domtrans_pattern($2, incrontab_exec_t, incrontab_t)

        ps_process_pattern($2, incrontab_t)
        allow $2 incrontab_t:process signal;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The comments in the file are somewhat special: if the comments start
with two hashes (&lt;code&gt;##&lt;/code&gt;) then it is taken into account while building the
policy documentation in &lt;code&gt;/usr/share/doc/selinux-base-*&lt;/code&gt;. The interface
itself, &lt;em&gt;incron_role&lt;/em&gt;, grants a user role and domain the necessary
privileges to transition to the &lt;code&gt;incrontab_t&lt;/code&gt; domain as well as read
process information (as used through &lt;strong&gt;ps&lt;/strong&gt;, hence the name of the
pattern being &lt;code&gt;ps_process_pattern&lt;/code&gt;) and send a standard signal to it.
Most of the time, you can use &lt;code&gt;signal_perms&lt;/code&gt; here but from looking at
the application we see that the application is setuid root, so we don't
want to grant too many privileges by default if they are not needed.&lt;/p&gt;
&lt;p&gt;With this interface file created, we can rebuild the module and load it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# make -f /usr/share/selinux/strict/include/Makefile incron.pp
# semodule -i incron.pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But how to assign this interface to users? Well, what we want to do is
something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;incron_role(user_r, user_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When interfaces are part of the policy provided by the distribution, the
definitions of it are stored in the proper location and you can easily
add it. For instance, in Gentoo, if you want to allow the &lt;code&gt;user_r&lt;/code&gt; role
and &lt;code&gt;user_t&lt;/code&gt; domain the &lt;em&gt;cron_role&lt;/em&gt; access (and assuming it doesn't
have so already), then you can call &lt;strong&gt;selocal&lt;/strong&gt; as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal -a &amp;quot;cron_role(user_r, user_t)&amp;quot; -c &amp;quot;Granting user_t cron access&amp;quot; -Lb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, because the interface is currently not known yet, we need to
create a second small policy that does this. Create a file (called
&lt;code&gt;localuser.te&lt;/code&gt; or so) with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(localuser, 0.1)

gen_require(`
        type user_t;
        role user_r;
&amp;#39;)

incron_role(user_r, user_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now build the policies and load them. We'll now just build and load all
the policies in the current directory (which will be the incron and
localuser ones):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# make -f /usr/share/selinux/strict/include/Makefile
# semodule -i *.pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can now verify that the user is allowed to transition to the
&lt;code&gt;incrontab_t&lt;/code&gt; domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# seinfo -ruser_r -x | grep incron
         incrontab_t
# sesearch -s user_t -t incrontab_exec_t -AdCTS
Found 1 semantic av rules:
   allow user_t incrontab_exec_t : file { read getattr execute open } ;

Found 1 semantic te rules:
   type_transition user_t incrontab_exec_t : process incrontab_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great, let's get to our first failure to resolve... in the next post ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="incron"></category><category term="interface"></category><category term="policy"></category></entry><entry><title>A SELinux policy for incron: the basic skeleton</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/" rel="alternate"></link><published>2013-05-23T03:50:00+02:00</published><updated>2013-05-23T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-23:/2013/05/a-selinux-policy-for-incron-the-basic-skeleton/</id><summary type="html">&lt;p&gt;So, in the &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-what-does-it-do/"&gt;previous
post&lt;/a&gt;
I talked about &lt;em&gt;incron&lt;/em&gt; and why I think moving it into the existing cron
policy would not be a good idea. It works, somewhat, but is probably not
that future-proof. So we're going to create our own policy for it.&lt;/p&gt;
&lt;p&gt;In SELinux, policies are generally …&lt;/p&gt;</summary><content type="html">&lt;p&gt;So, in the &lt;a href="http://blog.siphos.be/2013/05/a-selinux-policy-for-incron-what-does-it-do/"&gt;previous
post&lt;/a&gt;
I talked about &lt;em&gt;incron&lt;/em&gt; and why I think moving it into the existing cron
policy would not be a good idea. It works, somewhat, but is probably not
that future-proof. So we're going to create our own policy for it.&lt;/p&gt;
&lt;p&gt;In SELinux, policies are generally written through 3 files:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a &lt;em&gt;type enforcement&lt;/em&gt; file that contains the SELinux rules applicable
    to the domain(s) related to the application (in our example,
    &lt;em&gt;incron&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;a &lt;em&gt;file context&lt;/em&gt; file that tells the SELinux utilities how the files
    and directories offered by the application should be labeled&lt;/li&gt;
&lt;li&gt;an &lt;em&gt;interface definition&lt;/em&gt; file that allows other SELinux policy
    modules to gain rights offered through the (to be written) &lt;em&gt;incron&lt;/em&gt;
    policy&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We now need to create a skeleton for the policy. This skeleton will
define the types related to the application. Such types can be the
domains for the processes (the context of the &lt;strong&gt;incrond&lt;/strong&gt; and perhaps
also &lt;strong&gt;incrontab&lt;/strong&gt; applications), the contexts for the directories (if
any) and files, etc.&lt;/p&gt;
&lt;p&gt;So let's take a look at the content of the &lt;em&gt;incron&lt;/em&gt; package. On Gentoo,
we can use &lt;strong&gt;qlist incron&lt;/strong&gt; for this. In the output of &lt;strong&gt;qlist&lt;/strong&gt;, I
added comments to show you how contexts can be (easily) deduced.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Application binary for managing user crontabs. We want to give this a specific
# context because we want the application (which will manage the incrontabs in
# /var/spool/incron) in a specific domain
/usr/bin/incrontab  ## incrontab_exec_t

# General application information files, do not need specific attention
# (the default context is fine)
/usr/share/doc/incron-0.5.10/README.bz2
/usr/share/doc/incron-0.5.10/TODO.bz2
/usr/share/doc/incron-0.5.10/incron.conf.example.bz2
/usr/share/doc/incron-0.5.10/CHANGELOG.bz2
/usr/share/man/man8/incrond.8.bz2
/usr/share/man/man5/incron.conf.5.bz2
/usr/share/man/man5/incrontab.5.bz2
/usr/share/man/man1/incrontab.1.bz2

# Binary for the incrond daemon. This definitely needs its own context, since
# it will be launched from an init script and we do not want it to run in the
# initrc_t domain.
/usr/sbin/incrond ## incrond_exec_t

# This is the init script for the incrond daemon. If we want to allow 
# some users the rights to administer incrond without needing to grant
# those users the sysadm_r role, we need to give this file a different
# context as well.
/etc/init.d/incrond ## incrond_initrc_exec_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this information at hand, and the behavior of the application we
know from the previous post, can lead to the following &lt;code&gt;incron.fc&lt;/code&gt; file,
which defines the file contexts for the application.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/etc/incron.d(/.*)?     gen_context(system_u:object_r:incron_spool_t,s0)

/etc/rc\.d/init\.d/incrond      --      gen_context(system_u:object_r:incrond_initrc_exec_t,s0)

/usr/bin/incrontab      --      gen_context(system_u:object_r:incrontab_exec_t,s0)

/usr/sbin/incrond       --      gen_context(system_u:object_r:incrond_exec_t,s0)

/var/spool/incron(/.*)?         gen_context(system_u:object_r:incron_spool_t,s0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The syntax of this file closely follows the syntax that &lt;strong&gt;semanage
fcontext&lt;/strong&gt; takes - at least for the regular expressions in the
beginning. The last column is specifically for policy development to
generate a context based on the policies' requirements: an MCS/MLS
enabled policy will get the trailing sensitivity with it, but when
MCS/MLS is disabled then it is dropped. The middle column is to specify
if the label should only be set on regular files (&lt;code&gt;--&lt;/code&gt;), directories
(&lt;code&gt;-d&lt;/code&gt;), sockets (&lt;code&gt;-s&lt;/code&gt;), symlinks (&lt;code&gt;-l&lt;/code&gt;), etc. If it is omitted, it
matches whatever class the path matches.&lt;/p&gt;
&lt;p&gt;The second file needed for the skeleton is the &lt;code&gt;incron.te&lt;/code&gt; file, which
would look like so. I added in inline comments here to explain why
certain lines are prepared, but generally this is omitted when the
policy is &lt;a href="http://oss.tresys.com/projects/refpolicy/"&gt;upstreamed&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(incron, 0.1)
# The above line declares that this file is a SELinux policy file. Its name
# is incron, so the file should saved as incron.te

# First, we declare the incrond_t domain, used for the &amp;quot;incrond&amp;quot; process.
# Because it is launched from an init script, we tell the policy that
# incrond_exec_t (the context of incrond), when launched from init, should
# transition to incrond_t.
#
# Basically, the syntax here is:
# type 
# type 
# 
type incrond_t;
type incrond_exec_t;
init_daemon_domain(incrond_t, incrond_exec_t)

# Next we declare that the incrond_initrc_exec_t is an init script context
# so that init can execute it (remember, SELinux is a mandatory access control
# system, so if we do not tell that init can execute it, it won&amp;#39;t).
type incrond_initrc_exec_t;
init_script_file(incrond_initrc_exec_t)

# We also create the incrontab_t domain (for the &amp;quot;incrontab&amp;quot; application), which
# is triggered through the incrontab_exec_t labeled file. This again follows a bit
# the syntax as we used above, but now the interface call is &amp;quot;application_domain&amp;quot;.
type incrontab_t;
type incrontab_exec_t;
application_domain(incrontab_t, incrontab_exec_t)

# Finally we declare the spool type as well (incron_spool_t) and tell SELinux that
# it will be used for regular files.
type incron_spool_t;
files_type(incron_spool_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Knowing which interface calls, like &lt;em&gt;init_daemon_domain&lt;/em&gt; and
&lt;em&gt;application_domain&lt;/em&gt;, we should use is not obvious at first. Most of
this can be gathered from existing policies. Other frequently occurring
interfaces to be used immediately at the skeleton side are (examples for
a &lt;code&gt;foo_t&lt;/code&gt; domain):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;logging_log_file(foo_log_t)&lt;/em&gt; to inform SELinux that the context
    is used for logging purposes. This allows generic log-related
    daemons to do "their thing" with the file.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;files_tmp_file(foo_tmp_t)&lt;/em&gt; to identify the context as being
    used for temporary files&lt;/li&gt;
&lt;li&gt;&lt;em&gt;files_tmpfs_file(foo_tmpfs_t)&lt;/em&gt; for tmpfs files (which could be
    shared memory)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;files_pid_file(foo_var_run_t)&lt;/em&gt; for PID files (and other run
    metadata files)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;files_config_file(foo_conf_t)&lt;/em&gt; for configuration files (often
    within &lt;code&gt;/etc&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;files_lock_file(foo_lock_t)&lt;/em&gt; for lock files (often within
    &lt;code&gt;/run/lock&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We might be using these later as we progress with the policy (for
instance, the PID file is a very high candidate for needing to be
included). However, with the information currently at hand, we have our
first policy module ready for building. Save the type enforcement rules
in &lt;code&gt;incron.te&lt;/code&gt; and the file contexts in &lt;code&gt;incron.fc&lt;/code&gt; and you can then
build the SELinux policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# make -f /usr/share/selinux/strict/include/Makefile incron.pp
# semodule -i incron.pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On Gentoo, you can then relabel the files and directories offered
through the package using &lt;strong&gt;rlpkg&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# rlpkg incron
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next is to start looking at the &lt;strong&gt;incrontab&lt;/strong&gt; application.&lt;/p&gt;</content><category term="SELinux"></category><category term="fc"></category><category term="incron"></category><category term="policy"></category><category term="selinux"></category><category term="skeleton"></category><category term="te"></category></entry><entry><title>A SELinux policy for incron: what does it do?</title><link href="https://blog.siphos.be/2013/05/a-selinux-policy-for-incron-what-does-it-do/" rel="alternate"></link><published>2013-05-22T03:50:00+02:00</published><updated>2013-05-22T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-22:/2013/05/a-selinux-policy-for-incron-what-does-it-do/</id><summary type="html">&lt;p&gt;In this series of posts, we'll go through the creation of a SELinux
policy for
&lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=doc⟨=en"&gt;incron&lt;/a&gt;, a simple
inotify based cron-like application. I will talk about the various steps
that I would take in the creation of this policy, and give feedback when
certain decisions are taken and why. At …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this series of posts, we'll go through the creation of a SELinux
policy for
&lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=doc⟨=en"&gt;incron&lt;/a&gt;, a simple
inotify based cron-like application. I will talk about the various steps
that I would take in the creation of this policy, and give feedback when
certain decisions are taken and why. At the end of the series, we'll
have a hopefully well working policy.&lt;/p&gt;
&lt;p&gt;The first step in developing a policy is to know what the application
does and how/where it works. This allows us to check if its behavior
matches an existing policy (and as such might be best just added to this
policy) or if a new policy needs to be written. So, what does incron do?&lt;/p&gt;
&lt;p&gt;From the documentation, we know that &lt;em&gt;incron&lt;/em&gt; is a cron-like application
that, unlike cron, works with file system notification events instead of
time-related events. Other than that, it uses a similar way of working:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A daemon called &lt;strong&gt;incrond&lt;/strong&gt; is the run-time application that reads
    in the &lt;em&gt;incrontab&lt;/em&gt; files and creates the proper inotify watches.
    When a watch is triggered, it will execute the matching rule.&lt;/li&gt;
&lt;li&gt;The daemon looks at two definitions (incrontabs): one system-wide
    (in &lt;code&gt;/etc/incron.d&lt;/code&gt;) and one for users (in &lt;code&gt;/var/spool/incron&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;The user tabfiles are managed through &lt;strong&gt;incrontab&lt;/strong&gt; (the command)&lt;/li&gt;
&lt;li&gt;Logging is done through syslog&lt;/li&gt;
&lt;li&gt;User commands are executed with the users' privileges (so the
    application calls &lt;em&gt;setuid()&lt;/em&gt; and &lt;em&gt;setgid()&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this, one can create a script to be executed when a file is
uploaded (or deleted) to/from a file server, or when a process coredump
occurred, or whatever automation you want to trigger when some file
system event occurred. Events are plenty and can be found in
&lt;code&gt;/usr/include/sys/inotify.h&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, with this information, it is safe to assume that we might be able to
push incron in the existing &lt;em&gt;cron&lt;/em&gt; policy. After all, it defines the
contexts for all these and probably doesn't need any additional
tweaking. And this seems to work at first, but a few tests reveal that
the behavior is not that optimal.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# chcon -t crond_exec_t /usr/sbin/incrond
# chcon -t crontab_exec_t /usr/bin/incrontab
# chcon -R -t system_cron_spool_t /etc/incron.d
# chcon -t cron_log_t /var/log/cron.log
# chcon -R -t cron_spool_t /var/spool/incron
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;System tables work somewhat, but all commands are executed in the
&lt;code&gt;crond_t&lt;/code&gt; domain, not in a &lt;code&gt;system_cronjob_t&lt;/code&gt; or related domain.&lt;br&gt;
User tables fail when dealing with files in the users directories,
since these too run in &lt;code&gt;crond_t&lt;/code&gt; and thus have no read access to the
user home directories.&lt;/p&gt;
&lt;p&gt;The problems we notice come from the fact that the application is very
simple in its code: it is not SELinux-aware (so it doesn't change the
runtime context) as most cron daemons are, and when it changes the user
id it does not call PAM, so we cannot trigger &lt;code&gt;pam_selinux.so&lt;/code&gt; to handle
context changes either. As a result, the entire daemon keeps running in
&lt;code&gt;crond_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is one reason why a separate domain could be interesting: we might
want to extend the rights of the daemon domain a bit, but don't want to
extend these rights to the other cron daemons (who also run in
&lt;code&gt;crond_t&lt;/code&gt;). Another reason is that the cron policy has a few booleans
that would not affect the behavior at all, making it less obvious for
users to troubleshoot. As a result, we'll go for the separate policy
instead - which will be for the next post.&lt;/p&gt;</content><category term="SELinux"></category><category term="incron"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Why oh why does a process run in unlabeled_t?</title><link href="https://blog.siphos.be/2013/05/why-oh-why-does-a-process-run-in-unlabeled_t/" rel="alternate"></link><published>2013-05-21T03:50:00+02:00</published><updated>2013-05-21T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-21:/2013/05/why-oh-why-does-a-process-run-in-unlabeled_t/</id><summary type="html">&lt;p&gt;If you notice that a process is running in the &lt;code&gt;unlabeled_t&lt;/code&gt; domain, the
first question to ask is how it got there.&lt;/p&gt;
&lt;p&gt;Well, one way is to have a process running in a known domain, like
&lt;code&gt;screen_t&lt;/code&gt;, after which the SELinux policy module that provides this
domain is removed from …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you notice that a process is running in the &lt;code&gt;unlabeled_t&lt;/code&gt; domain, the
first question to ask is how it got there.&lt;/p&gt;
&lt;p&gt;Well, one way is to have a process running in a known domain, like
&lt;code&gt;screen_t&lt;/code&gt;, after which the SELinux policy module that provides this
domain is removed from the system (or updated and the update does not
contain the &lt;code&gt;screen_t&lt;/code&gt; definition anymore):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # ps -eZ | grep screen
root:sysadm_r:sysadm_screen_t    5047 ?        00:00:00 screen
test ~ # semodule -r screen
test ~ # ps -eZ | grep screen
system_u:object_r:unlabeled_t    5047 ?        00:00:00 screen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In permissive mode, this will be visible easily; in enforcing mode, the
domains you are running in might not be allowed to do anything with
&lt;code&gt;unlabeled_t&lt;/code&gt; files, directories and processes, so &lt;strong&gt;ps&lt;/strong&gt; might not show
it even though it still exists:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test audit # ps -eZ | grep 5047
test audit # ls -dZ /proc/5047
ls: cannot access /proc/5047: Permission denied
test audit # tail audit.log | grep unlabeled
type=AVC msg=audit(1368698097.494:27806): avc:  denied  { getattr } for  pid=4137 comm=&amp;quot;bash&amp;quot; path=&amp;quot;/proc/5047&amp;quot; dev=&amp;quot;proc&amp;quot; ino=6677 scontext=root:sysadm_r:sysadm_t tcontext=system_u:object_r:unlabeled_t tclass=dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice that, if you reload the module, the process becomes visible
again. That is because the process context itself (&lt;code&gt;screen_t&lt;/code&gt;) is
retained, but because the policy doesn't know it anymore, it shows it as
&lt;code&gt;unlabeled_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Basically, the moment the policy doesn't know how a label would be
(should be), it uses &lt;code&gt;unlabeled_t&lt;/code&gt;. The SELinux policy then defines how
this &lt;code&gt;unlabeled_t&lt;/code&gt; domain is handled. Processes getting into
&lt;code&gt;unlabeled_t&lt;/code&gt; is not that common though as there is no supported
transition to it. The above one is one way that this still can occur.&lt;/p&gt;</content><category term="SELinux"></category><category term="policy"></category><category term="selinux"></category><category term="unlabeled"></category></entry><entry><title>A simple IPv6 setup</title><link href="https://blog.siphos.be/2013/05/a-simple-ipv6-setup/" rel="alternate"></link><published>2013-05-20T03:50:00+02:00</published><updated>2013-05-20T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-20:/2013/05/a-simple-ipv6-setup/</id><summary type="html">&lt;p&gt;For internal communication between guests on my workstation, I use IPv6
which is set up using the &lt;em&gt;Router Advertisement&lt;/em&gt; "feature" of IPv6. The
tools I use are &lt;a href="http://www.thekelleys.org.uk/dnsmasq/doc.html"&gt;dnsmasq&lt;/a&gt;
for DNS/DHCP and router advertisement support, and
&lt;a href="http://roy.marples.name/projects/dhcpcd"&gt;dhcpcd&lt;/a&gt; as client. It might be
a total mess (grew almost organically until it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For internal communication between guests on my workstation, I use IPv6
which is set up using the &lt;em&gt;Router Advertisement&lt;/em&gt; "feature" of IPv6. The
tools I use are &lt;a href="http://www.thekelleys.org.uk/dnsmasq/doc.html"&gt;dnsmasq&lt;/a&gt;
for DNS/DHCP and router advertisement support, and
&lt;a href="http://roy.marples.name/projects/dhcpcd"&gt;dhcpcd&lt;/a&gt; as client. It might be
a total mess (grew almost organically until it worked), but as far as
I'm concerned, it is working... and that is all that matters (for now).
I'll have to look deeper into the IPv6 stuff to understand it all better
though.&lt;/p&gt;
&lt;p&gt;On the client side, &lt;strong&gt;dhcpcd&lt;/strong&gt; is ran with the following options:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dhcpcd_eth0=&amp;quot;-t 5 -L --ipv6ra_own&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I had to enable &lt;code&gt;--ipv6ra_own&lt;/code&gt; to get it to obtain its global address,
otherwise it only got its link local one (&lt;code&gt;fe80::&lt;/code&gt; something). I also
added a hook into &lt;code&gt;/lib/dhcpcd/dhcpcd-hooks&lt;/code&gt; to get it to trigger a
hostname update for IPv6.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cat 28-set-ip6-address 
if $ifup; then export new_ip_address=${ra1_prefix%%/64}; fi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;SELinux-policy wise, I had to enable &lt;code&gt;dhcpc_t&lt;/code&gt; to write to the hostname
proc file and set the system hostname. The first one (21) is needed
because of the &lt;code&gt;--ipv6ra_own&lt;/code&gt; parameter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal -l | grep dhcpc_t
21: allow dhcpc_t self:rawip_socket create_socket_perms; # dhcpclient
22: kernel_rw_kernel_sysctl(dhcpc_t) # set hostname
23: allow dhcpc_t self:capability sys_admin; # set hostname
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, in &lt;code&gt;/etc/dhcpcd.conf&lt;/code&gt;, I removed the &lt;em&gt;nohook lookup-hostname&lt;/em&gt;
and set the &lt;em&gt;force_hostname&lt;/em&gt; one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#nohook lookup-hostname
env force_hostname=YES
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On the server side, I use the following configuration of dnsmasq
(snippet):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dhcp-range=2001:db8:81:e2::,ra-only
enable-ra
dhcp-option=option6:dns-server,[2001:db8:81:e2::26b5:365b:5072]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, I use the documentation prefix for now (since it is
meant for internal communication only, and makes it easier to copy/paste
into documentation ;-) but when I am going to use full IPv6 access to
the Internet, this prefix will of course change.&lt;/p&gt;
&lt;p&gt;Finally, I enabled IPv6 forwarding on the &lt;code&gt;tap0&lt;/code&gt; interface because
otherwise I continuously got the following messages on the clients:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;May 12 18:43:07 test dhcpcd[3869]: eth0: adding default route via fe80::d848:19ff:fe0d:55c2
May 12 18:43:07 test dhcpcd[3869]: eth0: fe80::d848:19ff:fe0d:55c2 is no longer a router
May 12 18:43:07 test dhcpcd[3869]: eth0: deleting default route via fe80::d848:19ff:fe0d:55c2
May 12 18:43:13 test dhcpcd[3869]: eth0: fe80::d848:19ff:fe0d:55c2 is unreachable, expiring it
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To enable IPv6 forwarding, you can use sysctl but I added it in the
script that sets up the tap0 interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tunctl -b -u swift -t tap0
ifconfig tap0 add 2001:db8:81:e2::26b5:365b:5072/64
vde_switch --numports 16 --mod 777 --group users --tap tap0 -d
echo 1 &amp;gt; /proc/sys/net/ipv6/conf/tap0/forwarding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Documentation"></category><category term="dhcpcd"></category><category term="dnsmasq"></category><category term="ip6"></category><category term="ipv6"></category><category term="ra"></category></entry><entry><title>The weird "audit_access" permission</title><link href="https://blog.siphos.be/2013/05/the-weird-audit_access-permission/" rel="alternate"></link><published>2013-05-19T03:50:00+02:00</published><updated>2013-05-19T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-19:/2013/05/the-weird-audit_access-permission/</id><summary type="html">&lt;p&gt;While writing up the posts on capabilities, one thing I had in my mind
was to give some additional information on frequently occurring denials,
such as the &lt;em&gt;dac_override&lt;/em&gt; and &lt;em&gt;dac_read_search&lt;/em&gt; capabilities, and
when they are triggered. For the DAC-related capabilities, policy
developers often notice that these capabilities are triggered without …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While writing up the posts on capabilities, one thing I had in my mind
was to give some additional information on frequently occurring denials,
such as the &lt;em&gt;dac_override&lt;/em&gt; and &lt;em&gt;dac_read_search&lt;/em&gt; capabilities, and
when they are triggered. For the DAC-related capabilities, policy
developers often notice that these capabilities are triggered without a
real need for them. So in the majority of cases, the policy developer
wants to disable auditing of this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dontaudit &amp;lt;somedomain&amp;gt; self:capability { dac_read_search dac_override };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When applications wants to search through directories not owned by the
user as which the application runs, &lt;em&gt;both&lt;/em&gt; capabilities will be checked
- first the &lt;em&gt;dac_read_search&lt;/em&gt; one and, if that is denied (it will be
audited though) then &lt;em&gt;dac_override&lt;/em&gt; is checked. If that one is denied
as well, it too will be audited. That is why many developers
automatically &lt;em&gt;dontaudit&lt;/em&gt; both capability calls if the application
itself doesn't really need the permission.&lt;/p&gt;
&lt;p&gt;Let's say you allow this because the application needs it. But then
another issue comes up when the application checks file attributes or
access permissions (which is a second occurring denial that developers
come across with). Such applications use &lt;em&gt;access()&lt;/em&gt; or &lt;em&gt;faccessat()&lt;/em&gt; to
get information about files, but other than that don't do anything with
the files. When this occurs and the domain does not have read, write or
execute permissions on the target, then the denial is shown even when
the application doesn't really read, write or execute the file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char ** argv) {
  printf(&amp;quot;%s: Exists (%d), Readable (%d), Writeable (%d), Executable (%d)\n&amp;quot;, argv[1],
    access(argv[1], F_OK), access(argv[1], R_OK),
    access(argv[1], W_OK), access(argv[1], X_OK));
}

$ check /var/lib/logrotate.status
/var/lib/logrotate.status: Exists (0), Readable (-1), Writeable (-1), Executable (-1)

$ tail -1 /var/log/audit.log
...
type=AVC msg=audit(1367400559.273:5224): avc:  denied  { read } for  pid=12270 comm=&amp;quot;test&amp;quot; name=&amp;quot;logrotate.status&amp;quot; dev=&amp;quot;dm-3&amp;quot; ino=2849 scontext=staff_u:staff_r:staff_t tcontext=system_u:object_r:logrotate_var_lib_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This gives the impression that the application is doing nasty stuff,
even when it is merely checking permissions. One way would be to
dontaudit read as well, but if the application does the check against
several files of various types, that might mean you need to include
dontaudit statements for various domains. That by itself isn't wrong,
but perhaps you do not want to audit such checks but do want to audit
real read attempts. This is what the &lt;em&gt;audit_access&lt;/em&gt; permission is for.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://marc.info/?l=selinux&amp;amp;m=125349740623497&amp;amp;w=2"&gt;audit_access&lt;/a&gt;
&lt;a href="http://marc.info/?l=selinux&amp;amp;m=127239846604513"&gt;permission&lt;/a&gt; is meant to
be used only for &lt;em&gt;dontaudit&lt;/em&gt; statements: it has no effect on the
security of the system itself, so using it in &lt;em&gt;allow&lt;/em&gt; statements has no
effect. The purpose of the permission is to allow policy developers to
not audit access checks without really dontauditing other, possibly
malicious, attempts. In other words, checking the access can be
dontaudited while actually attempting to use the access (reading,
writing or executing the file) will still result in the proper denial.&lt;/p&gt;</content><category term="SELinux"></category><category term="access"></category><category term="audit"></category><category term="audit_access"></category><category term="selinux"></category></entry><entry><title>Commandline SELinux policy helper functions</title><link href="https://blog.siphos.be/2013/05/commandline-selinux-policy-helper-functions/" rel="alternate"></link><published>2013-05-18T03:50:00+02:00</published><updated>2013-05-18T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-18:/2013/05/commandline-selinux-policy-helper-functions/</id><summary type="html">&lt;p&gt;To work on SELinux policies, I use a couple of functions that I can call
on the shell (command line): &lt;strong&gt;seshowif&lt;/strong&gt;, &lt;strong&gt;sefindif&lt;/strong&gt;, &lt;strong&gt;seshowdef&lt;/strong&gt;
and &lt;strong&gt;sefinddef&lt;/strong&gt;. The idea behind the methods is that I want to search
(&lt;em&gt;find&lt;/em&gt;) for an interface (&lt;em&gt;if&lt;/em&gt;) or definition (&lt;em&gt;def&lt;/em&gt;) that contains a
particular method or …&lt;/p&gt;</summary><content type="html">&lt;p&gt;To work on SELinux policies, I use a couple of functions that I can call
on the shell (command line): &lt;strong&gt;seshowif&lt;/strong&gt;, &lt;strong&gt;sefindif&lt;/strong&gt;, &lt;strong&gt;seshowdef&lt;/strong&gt;
and &lt;strong&gt;sefinddef&lt;/strong&gt;. The idea behind the methods is that I want to search
(&lt;em&gt;find&lt;/em&gt;) for an interface (&lt;em&gt;if&lt;/em&gt;) or definition (&lt;em&gt;def&lt;/em&gt;) that contains a
particular method or call. Or, if I know what the interface or
definition is, I want to see it (&lt;em&gt;show&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;For instance, to find the name of the interface that allows us to define
file transitions from the &lt;code&gt;postfix_etc_t&lt;/code&gt; label:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sefindif filetrans.*postfix_etc
contrib/postfix.if: interface(`postfix_config_filetrans&amp;#39;,`
contrib/postfix.if:     filetrans_pattern($1, postfix_etc_t, $2, $3, $4)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or to show the content of the &lt;em&gt;corenet_tcp_bind_http_port&lt;/em&gt;
interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ seshowif corenet_tcp_bind_http_port
interface(`corenet_tcp_bind_http_port&amp;#39;,`
        gen_require(`
                type http_port_t;
        &amp;#39;)

        allow $1 http_port_t:tcp_socket name_bind;
        allow $1 self:capability net_bind_service;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For the definitions, this is quite similar:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sefinddef socket.*create
obj_perm_sets.spt:define(`create_socket_perms&amp;#39;, `{ create rw_socket_perms }&amp;#39;)
obj_perm_sets.spt:define(`create_stream_socket_perms&amp;#39;, `{ create_socket_perms listen accept }&amp;#39;)
obj_perm_sets.spt:define(`connected_socket_perms&amp;#39;, `{ create ioctl read getattr write setattr append bind getopt setopt shutdown }&amp;#39;)
obj_perm_sets.spt:define(`create_netlink_socket_perms&amp;#39;, `{ create_socket_perms nlmsg_read nlmsg_write }&amp;#39;)
obj_perm_sets.spt:define(`rw_netlink_socket_perms&amp;#39;, `{ create_socket_perms nlmsg_read nlmsg_write }&amp;#39;)
obj_perm_sets.spt:define(`r_netlink_socket_perms&amp;#39;, `{ create_socket_perms nlmsg_read }&amp;#39;)
obj_perm_sets.spt:define(`client_stream_socket_perms&amp;#39;, `{ create ioctl read getattr write setattr append bind getopt setopt shutdown }&amp;#39;)

$ seshowdef manage_files_pattern
define(`manage_files_pattern&amp;#39;,`
        allow $1 $2:dir rw_dir_perms;
        allow $1 $3:file manage_file_perms;
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I have these defined in my &lt;code&gt;~/.bashrc&lt;/code&gt; (they are simple
&lt;a href="http://dev.gentoo.org/~swift/blog/01/selinux-funcs.txt"&gt;functions&lt;/a&gt;) and
are used on a daily basis here ;-) If you want to learn a bit more on
developing SELinux policies for Gentoo, make sure you read the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-development.xml"&gt;Gentoo
Hardened SELinux
Development&lt;/a&gt;
guide.&lt;/p&gt;</content><category term="SELinux"></category><category term="bash"></category><category term="definition"></category><category term="functions"></category><category term="interface"></category><category term="policy"></category><category term="selinux"></category><category term="support"></category></entry><entry><title>Looking at the local Linux kernel privilege escalation</title><link href="https://blog.siphos.be/2013/05/looking-at-the-local-linux-kernel-privilege-escalation/" rel="alternate"></link><published>2013-05-17T03:50:00+02:00</published><updated>2013-05-17T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-17:/2013/05/looking-at-the-local-linux-kernel-privilege-escalation/</id><summary type="html">&lt;p&gt;There has been a few posts already on the local Linux kernel privilege
escalation, which has received the
&lt;a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2013-2094"&gt;CVE-2013-2094&lt;/a&gt;
ID.
&lt;a href="http://arstechnica.com/security/2013/05/critical-linux-vulnerability-imperils-users-even-after-silent-fix/"&gt;arstechnica&lt;/a&gt;
has a write-up with links to good resources on the Internet, but I
definitely want to point readers to the
&lt;a href="http://www.reddit.com/r/netsec/comments/1eb9iw/sdfucksheeporgs_semtexc_local_linux_root_exploit/c9ykrck"&gt;explanation&lt;/a&gt;
that Brad Spengler made on the vulnerability.&lt;/p&gt;
&lt;p&gt;In …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There has been a few posts already on the local Linux kernel privilege
escalation, which has received the
&lt;a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2013-2094"&gt;CVE-2013-2094&lt;/a&gt;
ID.
&lt;a href="http://arstechnica.com/security/2013/05/critical-linux-vulnerability-imperils-users-even-after-silent-fix/"&gt;arstechnica&lt;/a&gt;
has a write-up with links to good resources on the Internet, but I
definitely want to point readers to the
&lt;a href="http://www.reddit.com/r/netsec/comments/1eb9iw/sdfucksheeporgs_semtexc_local_linux_root_exploit/c9ykrck"&gt;explanation&lt;/a&gt;
that Brad Spengler made on the vulnerability.&lt;/p&gt;
&lt;p&gt;In short, the vulnerability is an &lt;em&gt;out-of-bound&lt;/em&gt; access to an array
within the Linux perf code (which is a performance measuring subsystem
enabled when &lt;code&gt;CONFIG_PERF_EVENTS&lt;/code&gt; is enabled). This subsystem is often
enabled as it offers a wide range of performance measurement techniques
(see &lt;a href="https://perf.wiki.kernel.org/index.php/Main_Page"&gt;its wiki&lt;/a&gt; for
more information). You can check on your own system through the kernel
configuration (&lt;strong&gt;zgrep CONFIG_PERF_EVENTS /proc/config.gz&lt;/strong&gt; if you
have the latter pseudo-file available - it is made available through
&lt;code&gt;CONFIG_IKCONFIG_PROC&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The public exploit maps memory in userland, fills it with known data,
then triggers an out-of-bound decrement that tricks the kernel into
decrementing this data (mapped in userland). By looking at where the
decrement occurred, the exploit now knows the base address of the array.
Next, it targets (through the same vulnerability) the IDT base
(Interrupt Descriptor Table) and targets the overflow interrupt vector.
It increments the top part of the address that the vector points to
(which is 0xffffffff, becoming 0x00000000 thus pointing to the
userland), maps this memory region itself with shellcode, and then
triggers the overflow. The shell code used in the public exploit
modifies the credentials of the current task, sets uid/gid with root and
gives full capabilities, and then executes a shell.&lt;/p&gt;
&lt;p&gt;As Brad mentions, &lt;a href="https://grsecurity.net/~spender/uderef.txt"&gt;UDEREF&lt;/a&gt;
(an option in a grSecurity enabled kernel) should mitigate the attempt
to get to the userland. On my system, the exploit fails with the
following (start of) oops (without affecting the system further) when it
tries to close the file descriptor returned from the syscall that
invokes the decrement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 1926.226678] PAX: please report this to pageexec@freemail.hu
[ 1926.227019] BUG: unable to handle kernel paging request at 0000000381f5815c
[ 1926.227019] IP: [] sw_perf_event_destroy+0x1a/0xa0
[ 1926.227019] PGD 58a7c000 
[ 1926.227019] Thread overran stack, or stack corrupted
[ 1926.227019] Oops: 0002 [#4] PREEMPT SMP 
[ 1926.227019] Modules linked in: libcrc32c
[ 1926.227019] CPU 0 
[ 1926.227019] Pid: 4267, comm: test Tainted: G      D      3.8.7-hardened #1 Bochs Bochs
[ 1926.227019] RIP: 0010:[]  [] sw_perf_event_destroy+0x1a/0xa0
[ 1926.227019] RSP: 0018:ffff880058a03e08  EFLAGS: 00010246
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The exploit also finds that the decrement didn't succeed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test: semtex.c:76: main: Assertion &amp;#39;i&amp;lt;0x0100000000/4&amp;#39; failed.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A second mitigation is that
&lt;a href="http://pax.grsecurity.net/docs/PaXTeam-H2HC12-PaX-kernel-self-protection.pdf"&gt;KERNEXEC&lt;/a&gt;
(also offered through grSecurity) which prevents the kernel from
executing data that is writable (including userland data). So modifying
the IDT would be mitigated as well.&lt;/p&gt;
&lt;p&gt;Another important mitigation is TPE - &lt;em&gt;Trusted Path Execution&lt;/em&gt;. This
feature prevents the execution of binaries that are not located in a
root-owned directory and owned by a trusted group (which on my system is
10 = wheel). So users attempting to execute such code will fail with a
&lt;em&gt;Permission denied&lt;/em&gt; error, and the following is shown in the logs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ 3152.165780] grsec: denied untrusted exec (due to not being in trusted group and file in non-root-owned directory) of /home/user/test by /home/user/test[bash:4382] uid/euid:1000/1000 gid/egid:100/100, parent /bin/bash[bash:4352] uid/euid:1000/1000 gid/egid:100/100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, even though a nicely hardened system should be fairly immune
against the currently circling public exploit, it should be noted that
it is not immune against the vulnerability itself. The methods above
mentioned make it so that that particular way of gaining root access is
not possible, but it still allows an attacker to decrement and increment
memory in specific locations so other exploits might be found to modify
the system.&lt;/p&gt;
&lt;p&gt;Now out-of-bound vulnerabilities are not new. Recently (february this
year), a
&lt;a href="http://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTMxMTg"&gt;vulnerability&lt;/a&gt;
in the networking code also provided an attack vector to get a local
privilege escalation. A mandatory access control system like SELinux has
little impact on such vulnerabilities if you allow users to execute
their own code. Even confined users can modify the exploit to disable
SELinux (since the shell code is ran with ring0 privileges it can access
and modify the SELinux state information in the kernel).&lt;/p&gt;
&lt;p&gt;Many thanks to Brad for the excellent write-up, and to the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; team for providing the
grSecurity PaX/TPE protections in its &lt;code&gt;hardened-sources&lt;/code&gt; kernel.&lt;/p&gt;</content><category term="Security"></category><category term="event"></category><category term="grsecurity"></category><category term="kernexec"></category><category term="linux"></category><category term="pax"></category><category term="perf"></category><category term="selinux"></category><category term="uderef"></category><category term="vulnerability"></category></entry><entry><title>Gentoo Hardened spring notes</title><link href="https://blog.siphos.be/2013/05/gentoo-hardened-spring-notes/" rel="alternate"></link><published>2013-05-16T22:54:00+02:00</published><updated>2013-05-16T22:54:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-16:/2013/05/gentoo-hardened-spring-notes/</id><summary type="html">&lt;p&gt;We got back together on the &lt;code&gt;#gentoo-hardened&lt;/code&gt; chat channel to discuss
the progress of &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt;, so it's time for
another write-up of what was said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8.1 will be out soon, although nothing major has occurred with it
since the last meeting. There is a plugin …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We got back together on the &lt;code&gt;#gentoo-hardened&lt;/code&gt; chat channel to discuss
the progress of &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt;, so it's time for
another write-up of what was said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8.1 will be out soon, although nothing major has occurred with it
since the last meeting. There is a plugin header install problem in 4.8
and its not certain that the (trivial) fix is in 4.8.1, but it certainly
is inside Gentoo's release.&lt;/p&gt;
&lt;p&gt;Blueness is also (still, and hopefully for a long time ;-) maintaining
the uclibc hardened related toolchain aspects.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grSecurity/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The further progress on the XATTR_PAX migration was put on a lower
level the past few weeks due to busy, busy... very busy weeks (but this
was announced and known in advance). We still need to do XATTR copying
in &lt;em&gt;install&lt;/em&gt; for packages that do pax markings before &lt;em&gt;src_install()&lt;/em&gt;
and include the &lt;code&gt;user.pax&lt;/code&gt; XATTR patch in the gentoo-sources kernel.
This will silence the errors for non-hardened users and fix the loss of
XATTR markings for those packages that do pax-mark before install.&lt;/p&gt;
&lt;p&gt;The set then needs to be documented further and tested on vanilla and
hardened systems.&lt;/p&gt;
&lt;p&gt;Zorry asked if a separate script can be provided for those ebuilds that
directly call &lt;strong&gt;paxctl&lt;/strong&gt;. These ebuilds might want to switch to the
eclass, but if they need to call &lt;strong&gt;paxctl&lt;/strong&gt; or similar directly (for
instance because the result is immediately used for further building), a
separate script or tool should be made available. Blueness will look
into this.&lt;/p&gt;
&lt;p&gt;On &lt;code&gt;hardened-sources&lt;/code&gt;, we are now with stable 2.6.32-r160, 3.2.42-r1 and
3.8.6 due to some vulnerabilities in earlier versions (in networking
code). There is still some bug (nfs-related) that is fixed in 3.2.44 so
that part might need a bump as well soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The
&lt;a href="http://blog.siphos.be/2013/04/introducing-selocal-for-small-selinux-policy-enhancements/"&gt;selocal&lt;/a&gt;
command is now available for Gentoo SELinux users, allowing them to
easily enhance the policy without having to maintain their own SELinux
policy modules (the script is a wrapper that does all that).&lt;/p&gt;
&lt;p&gt;The setools package now also uses the &lt;a href="http://blog.siphos.be/2013/04/sloting-the-old-swig-1/"&gt;SLOT'ed
swig&lt;/a&gt;, so no more
dependency breakage.&lt;/p&gt;
&lt;p&gt;On SELinux userspace and policy, both have seen a new release last
month, and both are already in the Gentoo portage tree.&lt;/p&gt;
&lt;p&gt;Finally, the SELinux policy ebuilds now also call
&lt;a href="http://blog.siphos.be/2013/05/overriding-the-default-selinux-policies/"&gt;epatch_user&lt;/a&gt;
so users can customize the policies even further without having to copy
ebuilds to their overlay.&lt;/p&gt;
&lt;p&gt;Now that &lt;strong&gt;tar&lt;/strong&gt; supports XATTR well, we might want to look into SELinux
stages again. Jmbsvicetto did some work on that, but the builds failed
during stage1. We'll look into that later.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nothing much to say, we're waiting a bit until the patches proposed by
the IMA team are merged in the main kernel.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Two no-multilib fixes have been applied to the
&lt;code&gt;hardened/amd64/no-multilib&lt;/code&gt; profiles. One was a QA issue and quickly
resolved, the other is due to the profile stacking within Gentoo
profiles, where we missed a profile and thus were missing a few masks
defined in that (missed) profile. But including the profile creates a
lot of duplicates again, so we are going to copy the masks across until
the duplicates are resolved in the other profiles.&lt;/p&gt;
&lt;p&gt;Blueness will also clean up the experimental &lt;code&gt;13.0&lt;/code&gt; directory since all
hardened profiles now follow 13.0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Docs&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The latest changes on SELinux have been added to the Gentoo SELinux
handbook. Also, I've been slowly (but surely) adding topics to the
&lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials"&gt;SELinux tutorials
listing&lt;/a&gt; on the Gentoo
wiki.&lt;/p&gt;
&lt;p&gt;The grSecurity 2 document is very much out of date, blueness hopes to
put some time in fixing that soon.&lt;/p&gt;
&lt;p&gt;So that's about it for the short write-up. Zorry will surely post the
log later on the appropriate channels. Good work done (again) by all
team members!&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="irc"></category><category term="meeting"></category><category term="monthly"></category><category term="online"></category></entry><entry><title>Public support channels: irc</title><link href="https://blog.siphos.be/2013/05/public-support-channels-irc/" rel="alternate"></link><published>2013-05-16T03:50:00+02:00</published><updated>2013-05-16T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-16:/2013/05/public-support-channels-irc/</id><summary type="html">&lt;p&gt;I've &lt;a href="http://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/"&gt;said
it&lt;/a&gt;
before - support channels for free software are often (imo) superior to
the commercial support that you might get with vendors. And although
those vendors often try to use "modern" techniques, I fail to see why
the old, but proven/stable methods would be wrong.&lt;/p&gt;
&lt;p&gt;Consider the "Chat …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've &lt;a href="http://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/"&gt;said
it&lt;/a&gt;
before - support channels for free software are often (imo) superior to
the commercial support that you might get with vendors. And although
those vendors often try to use "modern" techniques, I fail to see why
the old, but proven/stable methods would be wrong.&lt;/p&gt;
&lt;p&gt;Consider the "Chat with Support" feature that many vendors have on their
site. Often, these services use a webbrowser, AJAX-driven method for
talking with support engineers. The problem with this that I see is that
it is difficult to keep track of the feedback you got over time (unless
you manually copy/paste the information), and again that it isn't
public. With free software communities, we still often redirect such
"online" support requests to IRC.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Internet Relay Chat&lt;/em&gt; has been around for ages
(&lt;a href="https://en.wikipedia.org/wiki/IRC"&gt;1988&lt;/a&gt; according to wikipedia) and
still quite active. Gentoo has all of its support channels on the
&lt;a href="http://www.freenode.net"&gt;freenode&lt;/a&gt; IRC network: a community-driven,
active &lt;code&gt;#gentoo&lt;/code&gt; channel with often crosses the 1000 users, a
&lt;code&gt;#gentoo-dev&lt;/code&gt; development-related channel where many developers
communicate, the &lt;code&gt;#gentoo-hardened&lt;/code&gt; channel for all questions and
support regarding Gentoo Hardened specifics, etc.&lt;/p&gt;
&lt;p&gt;Using IRC has many advantages. One is that logs can be kept (either
individually or by the project itself) that can be queried later by the
people who want to provide support (to see if questions have already
been popping up, see what the common questions are for the last few
days, etc.) or get support (to see if their question was already
answered in the past). Of course, these logs can be made public through
web interfaces quite easily. For users, such log functionality is
offered through the IRC client. Another very simple, yet interesting
feature is &lt;em&gt;highlighting&lt;/em&gt;: give the set of terms for which you want to
be notified (usually through a highlight and a specific notification in
the client), making it easier to be on multiple channels without having
to constantly follow-up on all discussions.&lt;/p&gt;
&lt;p&gt;Another advantage is that there is such a thing like "bots". Most Gentoo
related channels do not allow active bots on the channels except for the
project-approved ones (such as &lt;em&gt;willikens&lt;/em&gt;). These bots can provide
project-specific help to users and developers alike:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give one-line information about bugs reported on bugzilla (id,
    assignee, status, but also the URL where the user/developer can view
    the bug etc.)&lt;/li&gt;
&lt;li&gt;Give meta information about a package (maintainer, herd, etc.), herd
    (members), GLSA details, dependency information, etc.&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to query if a developer is
    &lt;a href="https://dev.gentoo.org/devaway/"&gt;away&lt;/a&gt; or not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create notes (messages) for users that are not online yet but for
    which you know they come online later (and know their nickname or
    registered username)&lt;/li&gt;
&lt;li&gt;Notify when commits are made, or when tweets are sent that match a
    particular expression, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, the IRC protocol has many features that are very
interesting to use in free software communities as well. You can still
do private chats (when potentially confidential data is exchanged) for
instance, or even exchange files (although that is less common to use in
free software communities). There is also still some hierarchy in case
of abuse (channel operators can remove users from the chat or even ban
them for a while) and one can even quiet a channel when for instance
online team meetings are held (although using a different channel for
that might be an alternative).&lt;/p&gt;
&lt;p&gt;IRC also has the advantage that connecting to the IRC channels has a
very low requirement (software-wise): one can use console-only chat
clients (in case users cannot get their graphical environment to work -
example is irssi) or even &lt;a href="http://webchat.freenode.net/"&gt;webbrowser&lt;/a&gt;
based ones (if one wants to chat from other systems). Even smartphones
have good IRC applications, like &lt;a href="http://www.andchat.net/"&gt;AndChat&lt;/a&gt; for
Android.&lt;/p&gt;
&lt;p&gt;IRC is also distributed: an IRC network consists of many interconnected
servers who pass on all IRC traffic. If one node goes down, users can
access a different node and continue. That makes IRC quite
high-available. IRC network operators do need to try and keep the
network from splitting ("netsplit") which occurs when one part of the
distributed network gets segregated from the other part and thus two
"independent" IRC networks are formed. When that occurs, IRC operators
will try to join them back as fast as possible. I'm not going to explain
the details on this - it suffices to understand that IRC is a
distributed manner and thus often much more available than the "support
chat" sites that vendors provide.&lt;/p&gt;
&lt;p&gt;So although IRC looks archaic, it is a very good match for support
channel requirements.&lt;/p&gt;</content><category term="Free-Software"></category><category term="chat"></category><category term="irc"></category><category term="support"></category></entry><entry><title>Overriding the default SELinux policies</title><link href="https://blog.siphos.be/2013/05/overriding-the-default-selinux-policies/" rel="alternate"></link><published>2013-05-15T03:50:00+02:00</published><updated>2013-05-15T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-15:/2013/05/overriding-the-default-selinux-policies/</id><summary type="html">&lt;p&gt;Extending SELinux policies with additional rules is easy. As SELinux
uses a &lt;em&gt;deny by default&lt;/em&gt; approach, all you need to do is to &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;create a
policy
module&lt;/a&gt;
that contains the additional (allow) rules, load that and you're all
set. But what if you want to remove some rules?&lt;/p&gt;
&lt;p&gt;Well, sadly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Extending SELinux policies with additional rules is easy. As SELinux
uses a &lt;em&gt;deny by default&lt;/em&gt; approach, all you need to do is to &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;create a
policy
module&lt;/a&gt;
that contains the additional (allow) rules, load that and you're all
set. But what if you want to remove some rules?&lt;/p&gt;
&lt;p&gt;Well, sadly, SELinux does not support deny rules. Once an allow rule is
loaded in memory, it cannot be overturned anymore. Yes, you can disable
the module itself that provides the rules, but you cannot selectively
disable rules. So what to do?&lt;/p&gt;
&lt;p&gt;Generally, you can disable the module that contains the rules you want
to disable, and load a custom module that defines everything the
original module did, except for those rules you don't like. For
instance, if you do not want the &lt;code&gt;skype_t&lt;/code&gt; domain to be able to
read/write to the video device, create your own skype-providing module
(&lt;em&gt;myskype&lt;/em&gt;) with the exact same content (except for the module name at
the first line) as the original skype module, except for the video
device:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dev_read_sound(skype_t)
# dev_read_video_dev(skype_t)
dev_write_sound(skype_t)
# dev_write_video_dev(skype_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Load in this policy, and you now have the &lt;code&gt;skype_t&lt;/code&gt; domain without the
video access. You will get post-install failures when Gentoo pushes out
an update to the policy though, since it will attempt to reload the
&lt;code&gt;skype.pp&lt;/code&gt; file (through the &lt;code&gt;selinux-skype&lt;/code&gt; package) and fail because
it declares types and attributes already provided (by &lt;em&gt;myskype&lt;/em&gt;). You
can &lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-x86.xml?part=3&amp;amp;chap=5#doc_chap1"&gt;exclude the
package&lt;/a&gt;
from being updated, which works as long as no packages depend on it. Or
live with the post-install failure ;-) But there might be a simpler
approach: &lt;em&gt;epatch_user&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Recently, I added in support for &lt;em&gt;epatch_user&lt;/em&gt; in the policy ebuilds.
This allows users to create patches against the policy source code that
we use and put them in &lt;code&gt;/etc/portage/patches&lt;/code&gt; in the directory of the
right category/package. For module patches, the working directory used
is within the &lt;code&gt;policy/modules&lt;/code&gt; directory of the policy checkout. For
base, it is below the policy checkout (in other words, the patch will
need to use the &lt;code&gt;refpolicy/&lt;/code&gt; directory base). But because of how
&lt;em&gt;epatch_user&lt;/em&gt; works, any patch taken from the base will work as it will
start stripping directories up to the fourth one.&lt;/p&gt;
&lt;p&gt;This approach is also needed if you want to exclude rules from
interfaces rather than from the &lt;code&gt;.te&lt;/code&gt; file: create a small patch and put
it in &lt;code&gt;/etc/portage/patches&lt;/code&gt; for the &lt;code&gt;sec-policy/selinux-base&lt;/code&gt; package
(as this provides the interfaces).&lt;/p&gt;</content><category term="Gentoo"></category><category term="ebuild"></category><category term="epatch_user"></category><category term="Gentoo"></category><category term="override"></category><category term="patch"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Highlevel assessment of Cdorked and Gentoo Hardened/SELinux</title><link href="https://blog.siphos.be/2013/05/highlevel-assessment-of-cdorked-and-gentoo-hardenedselinux/" rel="alternate"></link><published>2013-05-14T03:50:00+02:00</published><updated>2013-05-14T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-14:/2013/05/highlevel-assessment-of-cdorked-and-gentoo-hardenedselinux/</id><summary type="html">&lt;p&gt;With all the
&lt;a href="http://www.welivesecurity.com/2013/05/07/linuxcdorked-malware-lighttpd-and-nginx-web-servers-also-affected/"&gt;reports&lt;/a&gt;
surrounding
&lt;a href="https://threatpost.com/attack-using-backdoored-apache-binaries-to-lead-to-blackhole-kit/"&gt;Cdorked&lt;/a&gt;,
I took a look at if SELinux and/or other Gentoo Hardened technologies
could reduce the likelihood that this infection occurs on your system.&lt;/p&gt;
&lt;p&gt;First of all, we don't know yet how the malware gets installed on the
server. We do know that the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With all the
&lt;a href="http://www.welivesecurity.com/2013/05/07/linuxcdorked-malware-lighttpd-and-nginx-web-servers-also-affected/"&gt;reports&lt;/a&gt;
surrounding
&lt;a href="https://threatpost.com/attack-using-backdoored-apache-binaries-to-lead-to-blackhole-kit/"&gt;Cdorked&lt;/a&gt;,
I took a look at if SELinux and/or other Gentoo Hardened technologies
could reduce the likelihood that this infection occurs on your system.&lt;/p&gt;
&lt;p&gt;First of all, we don't know yet how the malware gets installed on the
server. We do know that the Apache binaries themselves are modified, so
the first thing to look at is to see if this risk can be reduced. Of
course, using an intrusion detection system like
&lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;AIDE&lt;/a&gt; helps, but even with Gentoo's
&lt;strong&gt;qcheck&lt;/strong&gt; command you can test the integrity of the files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# qcheck www-servers/apache
Checking www-servers/apache-2.2.24 ...
  * 424 out of 424 files are good
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If the binary is modified, this would result in something equivalent to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Checking www-servers/apache-2.2.24 ...
 MD5-DIGEST: /usr/sbin/apache2
  * 423 out of 424 files are good
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I don't know if the modified binary would otherwise work just fine, I
have not been able to find exact details on the infected binary to (in a
sandbox environment of course) analyze this further. Also, because we
don't know how they are installed, it is not easy to know if binaries
that you built yourself are equally likely to be modified/substituted or
if the attack checks checksums of the binaries against a known list.&lt;/p&gt;
&lt;p&gt;Assuming that it would run, then the infecting malware would need to set
the proper SELinux context on the file (if it overwrites the existing
binary, then the context is retained, otherwise it gets the default
context of &lt;code&gt;bin_t&lt;/code&gt;). If the context is wrong, then starting Apache
results in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apache2: Syntax error on line 61 of /etc/apache2/httpd.conf: Cannot load /usr/lib64/apache2/modules/mod_actions.so into server: /usr/lib64/apache2/modules/mod_actions.so: cannot open shared object file: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because the modified binary stays in the calling domain context
(&lt;code&gt;initrc_t&lt;/code&gt;). If you use a targeted policy, then this will not present
itself as &lt;code&gt;initrc_t&lt;/code&gt; is an unconfined domain. But with strict policies,
&lt;code&gt;initrc_t&lt;/code&gt; is not allowed to read &lt;code&gt;httpd_modules_t&lt;/code&gt;. Even worse, the
remainder of SELinux protections don't apply anymore, since with
unconfined domains, all bets are off. That is why Gentoo focuses this
hard on using a strict policy.&lt;/p&gt;
&lt;p&gt;So, what if the binary runs in the proper domain? Well then, from the
articles I read, the malware can do a reverse connect. That means that
the domain will attempt to connect to an IP address provided by the
attacker (in a specifically crafted URL). For SELinux, this means that
the &lt;em&gt;name_connect&lt;/em&gt; permission is checked:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# sesearch -s httpd_t -c tcp_socket -p name_connect -ACTS
Found 20 semantic av rules:
   allow nsswitch_domain dns_port_t : tcp_socket { name_connect } ; 
DT allow httpd_t port_type : tcp_socket { name_connect } ; [ httpd_can_network_connect ]
DT allow httpd_t ftp_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ]
DT allow httpd_t smtp_port_t : tcp_socket { name_connect } ; [ httpd_can_sendmail ]
DT allow httpd_t postgresql_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ]
DT allow httpd_t oracledb_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ]
DT allow httpd_t squid_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ]
DT allow httpd_t mssql_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ]
DT allow httpd_t kerberos_port_t : tcp_socket { name_connect } ; [ allow_kerberos ]
DT allow nsswitch_domain ldap_port_t : tcp_socket { name_connect } ; [ authlogin_nsswitch_use_ldap ]
DT allow httpd_t http_cache_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ]
DT allow httpd_t http_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ]
DT allow httpd_t http_port_t : tcp_socket { name_connect } ; [ httpd_graceful_shutdown ]
DT allow httpd_t mysqld_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ]
DT allow httpd_t ocsp_port_t : tcp_socket { name_connect } ; [ allow_kerberos ]
DT allow nsswitch_domain kerberos_port_t : tcp_socket { name_connect } ; [ allow_kerberos ]
DT allow httpd_t pop_port_t : tcp_socket { name_connect } ; [ httpd_can_sendmail ]
DT allow nsswitch_domain ocsp_port_t : tcp_socket { name_connect } ; [ allow_kerberos ]
DT allow httpd_t gds_db_port_t : tcp_socket { name_connect } ; [ httpd_can_network_connect_db ]
DT allow httpd_t gopher_port_t : tcp_socket { name_connect } ; [ httpd_can_network_relay ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So by default, the Apache (&lt;code&gt;httpd_t&lt;/code&gt;) domain is allowed to connect to
DNS port (to resolve hostnames). All other &lt;em&gt;name_connect&lt;/em&gt; calls depend
on SELinux booleans (mentioned after it) that are by default disabled
(at least on Gentoo). Disabling hostname resolving is not really
feasible, so if the attacker uses a DNS port as port that the malware
needs to connect to, SELinux will not deny it (unless you use additional
networking constraints).&lt;/p&gt;
&lt;p&gt;Now, the reverse connect is an interesting feature of the malware, but
not the main one. The main focus of the malware is to redirect customers
to particular sites that can trick the user in downloading additional
(client) malware. Because this is done internally within Apache, SELinux
cannot deal with this. As a user, make sure you configure your browser
not to trust non-local iframes and such (always do this, not just
because there is a possible threat right now). The configuration of
Cdorked is a shared memory segment of Apache itself. Of course, since
Apache uses shared memory, the malware embedded within will also have
access to the shared memory. However, if this shared memory would need
to be accessed by third party applications (the malware seems to grant
read/write rights on everybody to this segment) SELinux will prevent
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# sesearch -t httpd_t -c shm -ACTS
Found 2 semantic av rules:
   allow unconfined_domain_type domain : shm { create destroy getattr setattr read write associate unix_read unix_write lock } ; 
   allow httpd_t httpd_t : shm { create destroy getattr setattr read write associate unix_read unix_write lock } ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Only unconfined domains and the &lt;code&gt;httpd_t&lt;/code&gt; domain itself have access to
&lt;code&gt;httpd_t&lt;/code&gt; labeled shared memory.&lt;/p&gt;
&lt;p&gt;So what about IMA/EVM? Well, those will not help here since IMA checks
for integrity of files that were modified &lt;em&gt;offline&lt;/em&gt;. As the modification
of the Apache binaries is most likely done online, IMA would just accept
this.&lt;/p&gt;
&lt;p&gt;For now, it seems that a good system integrity approach is the most
effective until we know more about how the malware-infected binary is
written to the system in the first place (as this is better protected by
MAC controls like SELinux).&lt;/p&gt;</content><category term="Security"></category><category term="apache"></category><category term="cdorked"></category><category term="Gentoo"></category><category term="hardened"></category><category term="ima"></category><category term="selinux"></category></entry><entry><title>SECMARK and SELinux</title><link href="https://blog.siphos.be/2013/05/secmark-and-selinux/" rel="alternate"></link><published>2013-05-13T03:50:00+02:00</published><updated>2013-05-13T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-13:/2013/05/secmark-and-selinux/</id><summary type="html">&lt;p&gt;When using SECMARK, the administrator configures the &lt;strong&gt;iptables&lt;/strong&gt; or
&lt;strong&gt;netfilter&lt;/strong&gt; rules to add a label to the packet data structure (on the
host itself) that can be governed through SELinux policies. Unlike peer
labeling, here the labels assigned to the network traffic is completely
locally defined. Consider the following command …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When using SECMARK, the administrator configures the &lt;strong&gt;iptables&lt;/strong&gt; or
&lt;strong&gt;netfilter&lt;/strong&gt; rules to add a label to the packet data structure (on the
host itself) that can be governed through SELinux policies. Unlike peer
labeling, here the labels assigned to the network traffic is completely
locally defined. Consider the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# iptables -t mangle -A INPUT -p tcp --src 192.168.1.2 --dport 443
  -j SECMARK --selctx system_u:object_r:myauth_packet_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this command, packets that originate from the &lt;em&gt;192.168.1.2&lt;/em&gt; host
and arrive on port 443 (typically used for HTTPS traffic) are marked as
&lt;code&gt;myauth_packet_t&lt;/code&gt;. SELinux policy writers can then allow domains to
receive this type of packets (or send) through the &lt;em&gt;packet&lt;/em&gt; class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Allow sockets with mydomain_t context to receive packets labeled myauth_packet_t
allow mydomain_t myauth_packet_t:packet recv;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The SELinux policy modules enable this through the
&lt;em&gt;corenet_sendrecv_&amp;lt;type&amp;gt;_{client,server}_packets&lt;/em&gt; interfaces:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_sendrecv_http_client_packets(mybrowser_t)
# allow mybrowser_t http_client_packet_t:packet { send recv };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As a common rule, packets are marked as client packets or server
packets, depending on the role of the &lt;em&gt;domain&lt;/em&gt;. In the above example,
the domain is a browser, so acts as a web client. So, it needs to send
and receive &lt;code&gt;http_client_packet_t&lt;/code&gt;. A web server on the other hand would
need to send and receive &lt;code&gt;http_server_packet_t&lt;/code&gt;. Note that the packets
that are sent over the wire do not have any labels assigned to them -
this is all local to the system. So even when the source and destination
use SELinux with SECMARK, on the source server the packets might be
labeled as &lt;code&gt;http_client_packet_t&lt;/code&gt; whereas on the target they are seen as
&lt;code&gt;http_server_packet_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As far as I know, when you want to use SECMARK, you will need to set the
contexts with &lt;strong&gt;iptables&lt;/strong&gt; yourself (there is no default labeling), so
knowing about the above convention is important.&lt;/p&gt;
&lt;p&gt;Again, Paul Moore has &lt;a href="http://paulmoore.livejournal.com/4281.html"&gt;more
information&lt;/a&gt; about this.&lt;/p&gt;</content><category term="SELinux"></category><category term="policy"></category><category term="secmark"></category><category term="selinux"></category></entry><entry><title>Peer labeling in SELinux policy</title><link href="https://blog.siphos.be/2013/05/peer-labeling-in-selinux-policy/" rel="alternate"></link><published>2013-05-12T03:50:00+02:00</published><updated>2013-05-12T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-12:/2013/05/peer-labeling-in-selinux-policy/</id><summary type="html">&lt;p&gt;Allow me to start with an important warning: I don't have much hands-on
experience with the remainder of this post. Its based on the few
resources I found on the Internet and a few tests done locally which
I've investigated in my attempt to understand SELinux policy writing for
networking …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Allow me to start with an important warning: I don't have much hands-on
experience with the remainder of this post. Its based on the few
resources I found on the Internet and a few tests done locally which
I've investigated in my attempt to understand SELinux policy writing for
networking stuff.&lt;/p&gt;
&lt;p&gt;So, with that out of the way, let's look into &lt;em&gt;peer labeling&lt;/em&gt;. As
mentioned in my &lt;a href="http://blog.siphos.be/2013/05/selinux-policy-and-network-controls/"&gt;previous
post&lt;/a&gt;,
SELinux supports some more advanced networking security features than
the default socket restrictions. I mentioned SECMARK and NetLabel
before, but NetLabel is actually part of the family of &lt;em&gt;peer&lt;/em&gt; labeling
technologies.&lt;/p&gt;
&lt;p&gt;With this technology approach, all participating systems in the network
must support the same labeling method. NetLabel supports CIPSO
(&lt;a href="https://tools.ietf.org/html/draft-ietf-cipso-ipsecurity-01"&gt;Commerial IP Security
Option&lt;/a&gt;)
where hosts label their network traffic to be part of a particular
"Domain of Interpretation". The labels are used by the hosts to identify
where a packet should be for. NetLabel, within Linux, is then used to
translate those CIPSO labels. SELinux itself labels the incoming sockets
based on the NetLabel information and the context of the listening
socket, resulting in a context that is governed policy-wise through the
&lt;em&gt;peer&lt;/em&gt; class. Since this is based on the information in the packet
instead of defined on the system itself, this allows remote systems to
have a say in how the packets are labeled.&lt;/p&gt;
&lt;p&gt;Another peer technology is the &lt;em&gt;Labeled IPSec&lt;/em&gt; one. In this case the
labels are fully provided by the remote system. I think they are based
on the security association within the IPSec setup.&lt;/p&gt;
&lt;p&gt;In both cases, in the SELinux policies, three definitions are important
to keep an eye out on: &lt;em&gt;interface&lt;/em&gt; definitions, &lt;em&gt;node&lt;/em&gt; definitions and
&lt;em&gt;peer&lt;/em&gt; definitions.&lt;/p&gt;
&lt;p&gt;Interface definitions allow users to (mainly) set the sensitivity that
is allowed to pass the interface. Using &lt;strong&gt;semanage interface&lt;/strong&gt; this can
be controlled by the user. One can also assign a different context to
the interface - by default, this is &lt;code&gt;netif_t&lt;/code&gt;. The permissions that are
checked on the traffic is &lt;em&gt;ingress&lt;/em&gt; (incoming) and &lt;em&gt;egress&lt;/em&gt; (outgoing)
traffic, and most policies set this through the following call (comment
shows the underlying SELinux rules, where &lt;em&gt;tcp_send&lt;/em&gt; and &lt;em&gt;tcp_recv&lt;/em&gt;
are - I think - obsolete):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_sendrecv_generic_if(something_t)
# allow something_t netif_t:netif { tcp_send tcp_recv egress ingress };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Node definitions define which targets (nodes, which can be IP addresses
or subnets) traffic meant for a particular socket is allow to originate
from (&lt;em&gt;recvfrom&lt;/em&gt;) or sent to (&lt;em&gt;sendto&lt;/em&gt;). Again, users can define their
own node types and manage them using &lt;strong&gt;semanage node&lt;/strong&gt;. The default node
I already covered in the previous post (&lt;code&gt;node_t&lt;/code&gt;) and is allowed by most
policies by default through the following call (where the &lt;em&gt;tcp_send&lt;/em&gt;
and &lt;em&gt;tcp_recv&lt;/em&gt; are probably deprecated as well):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_sendrecv_generic_node(something_t)
# allow something_t node_t:node { tcp_send tcp_recv sendto recvfrom };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, peer definitions are based on the labels from the traffic. If
the system uses NetLabel, then the target label will always be
&lt;code&gt;netlabel_peer_t&lt;/code&gt; since the workings of CIPSO are mainly (only?) mapped
towards sensitivity labels (in MLS policy). As a result, SELinux always
displays the peer as being &lt;code&gt;netlabel_peer_t&lt;/code&gt;. In case of Labeled IPSec,
this isn't the case as the peer label is transmitted by the peer itself.&lt;/p&gt;
&lt;p&gt;For NetLabel support, policies generally include two methods - one is to
support unlabeled traffic (only needed the moment you have support for
labeled traffic) and one is to allow the NetLabel'ed traffic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_all_recvfrom_unlabeled(something_t)
# allow something_t unlabeled_t:peer recv;
corenet_all_recvfrom_netlabel(something_t)
# allow something_t netlabel_peer_t:peer recv;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In case of IPSec for instance, the peer will have a provided label, as
is shown by the call for accepting hadoop traffic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;hadoop_recvfrom(something_t)
# allow something_t hadoop_t:peer recv;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, this alone is not sufficient for labeled IPSec. We also need to
allow the domain to be allowed to send anything towards an IPSec
security association. There is an interface called
&lt;em&gt;corenet_tcp_recvfrom_labeled&lt;/em&gt; that takes two arguments which,
amongst other things, enables &lt;em&gt;sendto&lt;/em&gt; towards its association.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_recvfrom_labeled(some_t, thing_t)
# allow { some_t thing_t} self:association sendto;
# allow some_t thing_t:peer recv;
# allow thing_t some_t:peer recv;
# corenet_tcp_recvfrom_netlabel(some_t)
# corenet_tcp_recvfrom_netlabel(thing_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This interface is usually called within a &lt;em&gt;*_tcp_connect()&lt;/em&gt; interface
for a particular domain, like with the &lt;em&gt;mysql_tcp_connect&lt;/em&gt; example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;interface(`mysql_tcp_connect&amp;#39;,`
        gen_require(`
                type mysqld_t;
        &amp;#39;)

        corenet_tcp_recvfrom_labeled($1, mysqld_t)
        corenet_tcp_sendrecv_mysqld_port($1) # deprecated
        corenet_tcp_connect_mysqld_port($1)
        corenet_sendrecv_mysqld_client_packets($1)
&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When using peer labeling, the domain that is allowed something is based
on the socket context of the application. Also, the rules when using
peer labeling are &lt;em&gt;in addition to&lt;/em&gt; the rules mentioned before
("standard" networking control): &lt;em&gt;name_bind&lt;/em&gt; and &lt;em&gt;name_connect&lt;/em&gt; are
always checked.&lt;/p&gt;
&lt;p&gt;For more information, make sure you check &lt;a href="http://paulmoore.livejournal.com"&gt;Paul Moore's
blog&lt;/a&gt;, such as the
&lt;a href="http://paulmoore.livejournal.com/2128.html?nojs=1"&gt;egress/ingress&lt;/a&gt;
information. And if you know of resources that show this in a more
practical setting (above is mainly to work with the SELinux policy) I'm
all ears.&lt;/p&gt;</content><category term="SELinux"></category><category term="cipso"></category><category term="ipsec"></category><category term="peer"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>SELinux policy and network controls</title><link href="https://blog.siphos.be/2013/05/selinux-policy-and-network-controls/" rel="alternate"></link><published>2013-05-11T03:50:00+02:00</published><updated>2013-05-11T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-11:/2013/05/selinux-policy-and-network-controls/</id><summary type="html">&lt;p&gt;Let's talk about how SELinux governs network streams (and how it
reflects this into the policy).&lt;/p&gt;
&lt;p&gt;When you don't do fancy stuff like SECMARK or netlabeling, then the
classes that you should keep an eye on are &lt;em&gt;tcp_socket&lt;/em&gt; and
&lt;em&gt;udp_socket&lt;/em&gt; (depending on the protocol). There used to be &lt;em&gt;node&lt;/em&gt; and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's talk about how SELinux governs network streams (and how it
reflects this into the policy).&lt;/p&gt;
&lt;p&gt;When you don't do fancy stuff like SECMARK or netlabeling, then the
classes that you should keep an eye on are &lt;em&gt;tcp_socket&lt;/em&gt; and
&lt;em&gt;udp_socket&lt;/em&gt; (depending on the protocol). There used to be &lt;em&gt;node&lt;/em&gt; and
&lt;em&gt;netif&lt;/em&gt; as well, but the support (enforcement) for these have been
&lt;a href="http://lists.openwall.net/netdev/2009/03/27/144"&gt;removed a while ago&lt;/a&gt;
for the "old style" network control enforcement. The concepts are still
available though, and I believe they take effect when netlabeling is
used. But let's first look at the regular networking aspects.&lt;/p&gt;
&lt;p&gt;The idea behind the regular network related permissions are that you
define either daemon-like behavior (which "binds" to a port) or
client-like behavior (which "connects" to a port). Consider an FTP
daemon (domain &lt;code&gt;ftpd_t&lt;/code&gt;) versus FTP client (example domain &lt;code&gt;ncftp_t&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In case of a daemon, the policy would contain the following (necessary)
rules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_bind_generic_node(ftpd_t) # Somewhat legacy but still needed
corenet_tcp_bind_ftp_port(ftpd_t)
corenet_tcp_bind_ftp_data_port(ftpd_t)
corenet_tcp_bind_all_unreserved_ports(ftpd_t) # In case of passive mode
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This gets translated to the following "real" SELinux statements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow ftpd_t node_t:tcp_socket node_bind;
allow ftpd_t ftp_port_t:tcp_socket name_bind;
allow ftpd_t ftp_data_port_t:tcp_socket name_bind;
allow ftpd_t unreserved_port_type:tcp_socket name_bind;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I mention that &lt;em&gt;corenet_tcp_bind_generic_node&lt;/em&gt; as being somewhat
legacy. When you use netlabeling, you can define different nodes (a
"node" in that case is a label assigned to an IP address or IP subnet)
and as such define policy-wise where daemons can bind on (or clients can
connect to). However, without netlabel, the only node that you get to
work with is &lt;code&gt;node_t&lt;/code&gt; which represents any possible node. Also, the use
of passive mode within the ftp policy is governed through the
&lt;em&gt;ftpd_use_passive_mode&lt;/em&gt; boolean.&lt;/p&gt;
&lt;p&gt;For a client, the following policy line would suffice:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_connect_ftp_port(ncftp_t)
# allow ncftp_t ftp_port_t:tcp_socket name_connect;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Well, I lied. Because of how FTP works, if you use active connections,
you need to allow the client to bind on an unreserved port, and allow
the server to connect to unreserved ports (cfr code snippet below), but
you get the idea.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;corenet_tcp_connect_all_unreserved_ports(ftpd_t)

corenet_tcp_bind_generic_node(ncftp_t)
corenet_tcp_bind_all_unreserved_ports(ncftp_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the past, policy developers also had to include other lines, but
these have by time become obsolete (&lt;em&gt;corenet_tcp_sendrecv_ftp_port&lt;/em&gt;
for instance). These methods defined the ability to send and receive
messages on the port, but this is no longer controlled this way. If you
need such controls, you will need to look at SELinux and SECMARK (which
uses packets with the &lt;em&gt;packet&lt;/em&gt; class) or netlabel (which uses the &lt;em&gt;peer&lt;/em&gt;
class and peer types to send or receive messages from).&lt;/p&gt;
&lt;p&gt;And that'll be for a different post.&lt;/p&gt;</content><category term="SELinux"></category><category term="networking"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Gentoo metadata support for CPE</title><link href="https://blog.siphos.be/2013/05/gentoo-metadata-support-for-cpe/" rel="alternate"></link><published>2013-05-10T03:50:00+02:00</published><updated>2013-05-10T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-10:/2013/05/gentoo-metadata-support-for-cpe/</id><summary type="html">&lt;p&gt;Recently, the &lt;code&gt;metadata.xml&lt;/code&gt; file syntax definition (the DTD for those
that know a bit of XML) has been updated to support CPE definitions. A
&lt;a href="https://nvd.nist.gov/cpe.cfm"&gt;CPE&lt;/a&gt; (Common Platform Enumeration) is an
identifier that
&lt;a href="http://cpe.mitre.org/specification/index.html"&gt;describes&lt;/a&gt; an
application, operating system or hardware device using its vendor,
product name, version, update, edition and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently, the &lt;code&gt;metadata.xml&lt;/code&gt; file syntax definition (the DTD for those
that know a bit of XML) has been updated to support CPE definitions. A
&lt;a href="https://nvd.nist.gov/cpe.cfm"&gt;CPE&lt;/a&gt; (Common Platform Enumeration) is an
identifier that
&lt;a href="http://cpe.mitre.org/specification/index.html"&gt;describes&lt;/a&gt; an
application, operating system or hardware device using its vendor,
product name, version, update, edition and language. This CPE
information is used in the CVE releases (Common Vulnerabilities and
Exposures) - announcements about vulnerabilities in applications,
operating systems or hardware. Not all security vulnerabilities are
assigned a CVE number, but this is as close as you get towards a
(public) elaborate dictionary of vulnerabilities.&lt;/p&gt;
&lt;p&gt;By allowing Gentoo package maintainers to enter (part of) the CPE
information in the &lt;code&gt;metadata.xml&lt;/code&gt; file, applications that parse the CVE
information can now more easily match if software installed on Gentoo is
related to a CVE. I had a &lt;a href="http://blog.siphos.be/2013/04/matching-packages-with-cves/"&gt;related
post&lt;/a&gt; to
this not that long ago on my blog and I'm glad this change has been
made. With this information at hand, we can start feeding CPE
information to the packages and then easily match this with CVEs.&lt;/p&gt;
&lt;p&gt;I had a request to "provide" the scripts I used for the previous post.
Mind you, these are taking too many assumptions (and probably wrong
ones) for now (and I'm not really planning on updating them as I have
different methods for getting information related to CVEs), but I'm
planning on integrating CPE data in Gentoo's packages more and then
create a small script that generates a "watchlist" that I can feed to
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt;. But anyway, here are
the scripts.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://dev.gentoo.org/~swift/blog/01/0_createcve.txt"&gt;First&lt;/a&gt;, I took
all CVE information and put it in a simple CSV file. The CSV is the same
one used by cvechecker, so check out the application to see where it
fetches the data from (there is a CVE RSS feed and a simple XSL
transformation).
&lt;a href="http://dev.gentoo.org/~swift/blog/01/1_createhitlist.txt"&gt;Second&lt;/a&gt;, I
create a "hitlist" which generates the CPEs. With the recent change to
&lt;code&gt;metadata.xml&lt;/code&gt; this step can be simplified a lot.
&lt;a href="http://dev.gentoo.org/~swift/blog/01/2_matchcve.txt"&gt;Third&lt;/a&gt;, I try to
match the CPE data with the CVE data, depending on a given time delay of
commits. In other words, you can ask possible CVE fixes for commits made
in the last few XXX days.&lt;/p&gt;</content><category term="Gentoo"></category><category term="cpe"></category><category term="cve"></category><category term="Gentoo"></category><category term="metadata"></category><category term="security"></category></entry><entry><title>Enabling Kernel Samepage Merging (KSM)</title><link href="https://blog.siphos.be/2013/05/enabling-kernel-samepage-merging-ksm/" rel="alternate"></link><published>2013-05-09T03:50:00+02:00</published><updated>2013-05-09T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-09:/2013/05/enabling-kernel-samepage-merging-ksm/</id><summary type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The first thing to enable is KSM or &lt;em&gt;Kernel
Samepage Merging&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This Linux feature looks for memory pages that the applications have
marked as being a possible candidate for optimization (sharing) which
are then reused across multiple processes. The idea is that, especially
for virtualized environments (but KSM is not limited to that), some
processes will have the same contents in memory. Without any sharing
abilities, these memory pages will be unique (meaning at different
locations in your system's memory). With KSM, such memory pages are
consolidated to a single page which is then referred to by the various
processes. When one process wants to modify the page, it is "unshared"
so that there is no corruption or unwanted modification of data for the
other processes.&lt;/p&gt;
&lt;p&gt;Such features are not new - VMWare has it named TPS (&lt;em&gt;Transparent Page
Sharing&lt;/em&gt;) and Xen calls it "Memory CoW" (Copy-on-Write). One advantage
of KSM is that it is simple to setup and advantageous for other
processes as well. For instance, if you host multiple instances of the
same service (web service, database, tomcat, whatever) there is a high
chance that several of its memory pages are prime candidates for
sharing.&lt;/p&gt;
&lt;p&gt;Now before I do mention that this sharing is only enabled when the
application has marked it as such. This is done through the &lt;em&gt;madvise()&lt;/em&gt;
method, where applications mark the memory with &lt;em&gt;MADV_MERGEABLE&lt;/em&gt;,
meaning that the applications explicitly need to support KSM in order
for it to be successful. There is work on the way to support transparent
KSM (such as
&lt;a href="http://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTEzMTI"&gt;UKSM&lt;/a&gt; and
&lt;a href="https://code.google.com/p/pksm/"&gt;PKSM&lt;/a&gt;) where no &lt;em&gt;madvise&lt;/em&gt; calls would
be needed anymore. But beyond quickly reading the home pages (or
translated home pages in case of UKSM ;-) I have no experience with
those projects.&lt;/p&gt;
&lt;p&gt;So let's get back to KSM. I am currently running three virtual machines
(all configured to take at most 1.5 Gb of memory). Together, they take
just a little over 1 Gb of memory (sum of their resident set sizes).
When I consult KSM, I get the following information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; # grep -H &amp;#39;&amp;#39; /sys/kernel/mm/ksm/pages_*
/sys/kernel/mm/ksm/pages_shared:48911
/sys/kernel/mm/ksm/pages_sharing:90090
/sys/kernel/mm/ksm/pages_to_scan:100
/sys/kernel/mm/ksm/pages_unshared:123002
/sys/kernel/mm/ksm/pages_volatile:1035
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;pages_shared&lt;/code&gt; tells me that 48911 pages are shared (which means
about 191 Mb) through 90090 references (&lt;code&gt;pages_sharing&lt;/code&gt; - meaning the
various processes have in total 90090 references to pages that are being
shared). That means a gain of 41179 pages (160 Mb). Note that the
resident set sizes do not take into account shared pages, so the sum of
the RSS has to be subtracted with this to find the "real" memory
consumption. The &lt;code&gt;pages_unshared&lt;/code&gt; value tells me that 123002 pages are
marked with the &lt;code&gt;MADV_MERGEABLE&lt;/code&gt; advise flag but are not used by other
processes.&lt;/p&gt;
&lt;p&gt;If you want to use KSM yourself, configure your kernel with &lt;code&gt;CONFIG_KSM&lt;/code&gt;
and start KSM by echo'ing the value "1" into &lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt;.
That's all there is to it.&lt;/p&gt;</content><category term="Free-Software"></category><category term="cow"></category><category term="ksm"></category><category term="kvm"></category><category term="linux"></category><category term="virtualization"></category></entry><entry><title>The Linux ".d" approach</title><link href="https://blog.siphos.be/2013/05/the-linux-d-approach/" rel="alternate"></link><published>2013-05-08T03:50:00+02:00</published><updated>2013-05-08T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-08:/2013/05/the-linux-d-approach/</id><summary type="html">&lt;p&gt;Many services on a Linux system use a &lt;code&gt;*.d&lt;/code&gt; directory approach to make
their configuration easily configurable by other services. This is a
remarkably simple yet efficient method for exposing services towards
other applications. Let's look into how this &lt;code&gt;.d&lt;/code&gt; approach works.&lt;/p&gt;
&lt;p&gt;Take a look at the &lt;code&gt;/etc/pam.d …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many services on a Linux system use a &lt;code&gt;*.d&lt;/code&gt; directory approach to make
their configuration easily configurable by other services. This is a
remarkably simple yet efficient method for exposing services towards
other applications. Let's look into how this &lt;code&gt;.d&lt;/code&gt; approach works.&lt;/p&gt;
&lt;p&gt;Take a look at the &lt;code&gt;/etc/pam.d&lt;/code&gt; structure: services that are PAM-aware
can place their PAM configuration files in this location, without
needing any additional configuration steps or registration. Same with
&lt;code&gt;/etc/cron.d&lt;/code&gt;: applications that need specific cronjobs do not need to
edit &lt;code&gt;/etc/crontab&lt;/code&gt; directly (with the problem of concurrent access,
overwriting changes, etc.) but instead can place their definitions in
the &lt;code&gt;cron.d&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;This approach is getting more traction, as can be seen from the
available "dot-d" directories on a system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ ls -d /etc/*.d
/etc/bash_completion.d  /etc/ld.so.conf.d  /etc/pam.d          /etc/sysctl.d
/etc/conf.d             /etc/local.d       /etc/profile.d      /etc/wgetpaste.d
/etc/dracut.conf.d      /etc/logrotate.d   /etc/request-key.d  /etc/xinetd.d
/etc/env.d              /etc/makedev.d     /etc/sandbox.d      /etc/cron.d
/etc/init.d             /etc/modprobe.d    /etc/sudoers.d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;An application can place its configuration files in these directories,
automatically "plugging" it in into the operating system and the
services that it provides. And the more services adopt this approach,
the easier it is for applications to be pluggable within the operating
system. Even complex systems such as database systems can easily
configure themselves this way. And for larger organizations, this is a
very interesting approach.&lt;/p&gt;
&lt;p&gt;Consider the need to deploy a database server on a Linux system in a
larger organization. Each organization has its standards for file system
locations, policies for log file management, etc. With the &lt;code&gt;*.d&lt;/code&gt;
approach, these organizations only need to put files on the file system
(a rather primitive feature that every organization supports) and manage
these files instead of using specific, proprietary interfaces to
configure the environment. But to properly control this flexibility, a
few attention points need to be taken into account.&lt;/p&gt;
&lt;p&gt;The first is to use a proper &lt;em&gt;naming convention&lt;/em&gt;. If the organization
has a data management structure, it might have specific names for
services. These names are then used throughout the organization to
properly identify owners or responsibilities. When using the &lt;code&gt;*.d&lt;/code&gt;
directories, these naming conventions also allow administrators to
easily know who to contact if a malfunctioning definition is placed. For
instance, if a log rotation definition has a wrong entry, a file called
&lt;code&gt;mylogrotation&lt;/code&gt; does not reveal much information. However,
&lt;code&gt;CDBM-postgres-querylogs&lt;/code&gt; might reveal that the file is placed there by
the customer database management team for a postgresql database. And it
isn't only about knowing who to contact (because that could easily be
done by comments as well), but also to ensure no conflicts occur. On a
shared database system, it is much more likely that two different teams
place a &lt;code&gt;postgresql&lt;/code&gt; file (which would overwrite the file already there)
unless they use a proper naming convention.&lt;/p&gt;
&lt;p&gt;The second is to use something identifying where the file comes from. A
best practice when using Puppet for instance is to add in a comment to
the file such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# This file is managed by Puppet through the org-pgsl-def module
# Please do not modify manually
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This informs the administrator how the file is put there; you might even
want to include version information.&lt;/p&gt;
&lt;p&gt;A third one is when the order of configuration entries is important.
Most &lt;code&gt;*.d&lt;/code&gt; supporting tools do not really care about ordering, but some,
like udev, do. When that is the case, the common consensus is to use
numbers in the beginning of the file name. The numbers then provide a
good ordering of the files.&lt;/p&gt;
&lt;p&gt;Not all services already offer &lt;code&gt;*.d&lt;/code&gt; functionality, although it isn't
that difficult to provide it as well. Consider the Linux audit daemon,
whose rules are managed in the &lt;code&gt;/etc/audit/audit.rules&lt;/code&gt; file. Not that
flexible, isn't it? But one can create a &lt;code&gt;/etc/audit/audit.rules.d&lt;/code&gt;
location and have the audit init script read these files (in
alphanumeric order), creating the same functionality.&lt;/p&gt;
&lt;p&gt;Given enough service adoption, software distribution can be sufficient
to configure an application completely and integrate it with all
services used by the operating system. And even services that do not
support &lt;code&gt;*.d&lt;/code&gt; directories can still be easily wrapped around so that
their configuration file itself is generated based on the information in
such directories. Consider a hypothetical
&lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;AIDE&lt;/a&gt; configuration, where the
&lt;code&gt;aide.conf&lt;/code&gt; is generated based on the &lt;code&gt;aide.conf.head&lt;/code&gt;, &lt;code&gt;aide.d/*&lt;/code&gt; and
&lt;code&gt;aide.conf.tail&lt;/code&gt; files (similar to how &lt;code&gt;resolv.conf&lt;/code&gt; is sometimes
managed). The generation is triggered right before &lt;strong&gt;aide&lt;/strong&gt; itself is
called (perhaps all in a single script).&lt;/p&gt;
&lt;p&gt;Such an approach allows full integration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A PAM configuration file is placed, allowing the service
    authentication to be easily managed by administrators. Changes on
    the authentication (for instance, switch to an LDAP authentication
    or introduce some trust relation) is done by placing an
    updated file.&lt;/li&gt;
&lt;li&gt;A log rotation configuration file is placed, making sure that the
    log files for the service do not eventually fill the partitions&lt;/li&gt;
&lt;li&gt;A syslog configuration is provided, allowing for some events to be
    sent to a different server instead of keeping it local - or perhaps
    both&lt;/li&gt;
&lt;li&gt;A cron configuration is stored so that statistics and other
    house-cleaning jobs for the service can run at night&lt;/li&gt;
&lt;li&gt;An audit configuration snippet is added to ensure critical commands
    and configuration files are properly checked&lt;/li&gt;
&lt;li&gt;Intrusion detection rules are added when needed&lt;/li&gt;
&lt;li&gt;Monitoring information is placed on the file system, causing
    additional monitoring metrics to be automatically picked up&lt;/li&gt;
&lt;li&gt;Firewall definitions are extended based on the snippets placed on
    the system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;etc. And all this by only placing files on the file system. Keep It
Simple, and efficient ;-)&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Added "predictable network interface" info into the handbook</title><link href="https://blog.siphos.be/2013/05/added-predictable-network-interface-info-into-the-handbook/" rel="alternate"></link><published>2013-05-07T03:50:00+02:00</published><updated>2013-05-07T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-07:/2013/05/added-predictable-network-interface-info-into-the-handbook/</id><summary type="html">&lt;p&gt;Being long overdue - like many of our documentation-reported bugs :-( I
worked on &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=466262"&gt;bug 466262&lt;/a&gt;
to update the &lt;a href="http://www.gentoo.org/doc/en/handbook/"&gt;Gentoo Handbook&lt;/a&gt;
with information about &lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml?part=4&amp;amp;chap=2#doc_chap4"&gt;Network Interface
Naming&lt;/a&gt;.
Of course, the installation instructions have also seen the necessary
updates to refer to this change.&lt;/p&gt;
&lt;p&gt;With some luck (read: time) I might be able …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Being long overdue - like many of our documentation-reported bugs :-( I
worked on &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=466262"&gt;bug 466262&lt;/a&gt;
to update the &lt;a href="http://www.gentoo.org/doc/en/handbook/"&gt;Gentoo Handbook&lt;/a&gt;
with information about &lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml?part=4&amp;amp;chap=2#doc_chap4"&gt;Network Interface
Naming&lt;/a&gt;.
Of course, the installation instructions have also seen the necessary
updates to refer to this change.&lt;/p&gt;
&lt;p&gt;With some luck (read: time) I might be able to fix various other
documentation-related ones soon. I had some problems with the new
SELinux userspace that I wanted to get fixed before, and then I worked
on the new SELinux policies as well as trying to figure out how SELinux
deals with network related aspects. Hence I saw time fly by at the speed
of a neutrino...&lt;/p&gt;
&lt;p&gt;BTW, the 20130424 policies are in the tree.&lt;/p&gt;</content><category term="Documentation"></category><category term="documentation"></category><category term="gdp"></category><category term="Gentoo"></category><category term="udev"></category></entry><entry><title>Overview of Linux capabilities, part 3</title><link href="https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-3/" rel="alternate"></link><published>2013-05-06T03:50:00+02:00</published><updated>2013-05-06T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-06:/2013/05/overview-of-linux-capabilities-part-3/</id><summary type="html">&lt;p&gt;In &lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;previous&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;posts&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/"&gt;I&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-2/"&gt;talked&lt;/a&gt;
about capabilities and gave an introduction to how this powerful
security feature within Linux can be used (and also exploited). I also
covered a few capabilities, so let's wrap this up with the remainder of
them.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;CAP_AUDIT_CONTROL&lt;/dt&gt;
&lt;dd&gt;Enable and disable kernel auditing; change auditing filter …&lt;/dd&gt;&lt;/dl&gt;</summary><content type="html">&lt;p&gt;In &lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;previous&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;posts&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/"&gt;I&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-2/"&gt;talked&lt;/a&gt;
about capabilities and gave an introduction to how this powerful
security feature within Linux can be used (and also exploited). I also
covered a few capabilities, so let's wrap this up with the remainder of
them.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;CAP_AUDIT_CONTROL&lt;/dt&gt;
&lt;dd&gt;Enable and disable kernel auditing; change auditing filter rules;
retrieve auditing status and filtering rules&lt;/dd&gt;
&lt;dt&gt;CAP_AUDIT_WRITE&lt;/dt&gt;
&lt;dd&gt;Write records to kernel auditing log&lt;/dd&gt;
&lt;dt&gt;CAP_BLOCK_SUSPEND&lt;/dt&gt;
&lt;dd&gt;Employ features that can block system suspend&lt;/dd&gt;
&lt;dt&gt;CAP_MAC_ADMIN&lt;/dt&gt;
&lt;dd&gt;Override Mandatory Access Control (implemented for the SMACK LSM)&lt;/dd&gt;
&lt;dt&gt;CAP_MAC_OVERRIDE&lt;/dt&gt;
&lt;dd&gt;Allow MAC configuration or state changes (implemented for the
SMACK LSM)&lt;/dd&gt;
&lt;dt&gt;CAP_NET_ADMIN&lt;/dt&gt;
&lt;dd&gt;Perform various network-related operations:
&lt;/p&gt;
-   interface configuration
-   administration of IP firewall, masquerading and accounting
-   modify routing tables
-   bind to any address for transparent proxying
-   set type-of-service (TOS)
-   clear driver statistics
-   set promiscuous mode
-   enabling multicasting
-   use &lt;em&gt;setsockopt()&lt;/em&gt; for privileged socket operations&lt;/dd&gt;
&lt;dt&gt;CAP_NET_BIND_SERVICE&lt;/dt&gt;
&lt;dd&gt;Bind a socket to Internet domain privileged ports (less than 1024)&lt;/dd&gt;
&lt;dt&gt;CAP_NET_RAW&lt;/dt&gt;
&lt;dd&gt;Use RAW and PACKET sockets, and bind to any address for transparent
proxying&lt;/dd&gt;
&lt;dt&gt;CAP_SETPCAP&lt;/dt&gt;
&lt;dd&gt;Allow the process to add any capability from the calling thread's
bounding set to its inheritable set, and drop capabilities from the
bounding set (using &lt;em&gt;prctl()&lt;/em&gt;) and make changes to the
&lt;em&gt;securebits&lt;/em&gt; flags.&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_ADMIN&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Very powerful capability, includes:
&lt;/p&gt;
-   Running quota control, mount, swap management, set hostname, ...
-   Perform &lt;em&gt;VM86_REQUEST_IRQ vm86&lt;/em&gt; command
-   Perform &lt;em&gt;IPC_SET&lt;/em&gt; and &lt;em&gt;IPC_RMID&lt;/em&gt; operations on arbitrary
    System V IPC objects
-   Perform operations on &lt;code&gt;trusted.*&lt;/code&gt; and &lt;code&gt;security.*&lt;/code&gt; extended
    attributes
-   Use &lt;em&gt;lookup_dcookie&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
and many, many more. &lt;strong&gt;man capabilities&lt;/strong&gt; gives a good overview
of them.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_BOOT&lt;/dt&gt;
&lt;dd&gt;Use &lt;em&gt;reboot()&lt;/em&gt; and &lt;em&gt;kexec_load()&lt;/em&gt;&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_CHROOT&lt;/dt&gt;
&lt;dd&gt;Use &lt;em&gt;chroot()&lt;/em&gt;&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_MODULE&lt;/dt&gt;
&lt;dd&gt;Load and unload kernel modules&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_RESOURCE&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Another capability with many consequences, including:
&lt;/p&gt;
-   Use reserved space on ext2 file systems
-   Make &lt;em&gt;ioctl()&lt;/em&gt; calls controlling ext3 journaling
-   Override disk quota limits
-   Increase resource limits
-   Override &lt;code&gt;RLIMIT_NPROC&lt;/code&gt; resource limits&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
and many more.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_TIME&lt;/dt&gt;
&lt;dd&gt;Set system clock and real-time hardware clock&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_TTY_CONFIG&lt;/dt&gt;
&lt;dd&gt;Use &lt;em&gt;vhangup()&lt;/em&gt; and employ various privileged &lt;em&gt;ioctl()&lt;/em&gt; operations
on virtual terminals&lt;/dd&gt;
&lt;dt&gt;CAP_SYSLOG&lt;/dt&gt;
&lt;dd&gt;Perform privileged &lt;em&gt;syslog()&lt;/em&gt; operations and view kernel addresses
exposed with &lt;code&gt;/proc&lt;/code&gt; and other interfaces (if &lt;code&gt;kptr_restrict&lt;/code&gt;
is set)&lt;/dd&gt;
&lt;dt&gt;CAP_WAKE_ALARM&lt;/dt&gt;
&lt;dd&gt;Trigger something that will wake up the system&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now when you look through the manual page of the capabilities, you'll
notice it talks about &lt;em&gt;securebits&lt;/em&gt; as well. This is an additional set of
flags that govern how capabilities are used, inherited etc. System
administrators don't set these flags - they are governed by the
applications themselves (when creating threads, forking, etc.) These
flags are set on a per-thread level, and govern the following behavior:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;SECBIT_KEEP_CAPS&lt;/dt&gt;
&lt;dd&gt;Allow a thread with UID 0 to retain its capabilities when it
switches its UIDs to a nonzero (non-root) value. By default, this
flag is &lt;em&gt;not&lt;/em&gt; set, and even if it is set, it is cleared on an
&lt;em&gt;execve&lt;/em&gt; call, reducing the likelihood that capabilities
are "leaked".&lt;/dd&gt;
&lt;dt&gt;SECBIT_NO_SETUID_FIXUP&lt;/dt&gt;
&lt;dd&gt;When set, the kernel will not adjust the capability sets when the
thread's effective and file system UIDs are switched between
zero (root) and non-zero values.&lt;/dd&gt;
&lt;dt&gt;SECBIT_NOROOT&lt;/dt&gt;
&lt;dd&gt;If set, the kernel does not grant capabilities when a setuid-root
program is executed, or when a process with an effective or real UID
of 0 (root) calls &lt;em&gt;execve&lt;/em&gt;.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Manipulating these bits requires the &lt;code&gt;CAP_SETPCAP&lt;/code&gt; capability. Except
for the &lt;code&gt;SECBIT_KEEP_CAPS&lt;/code&gt; security bit, the others are preserved on an
&lt;em&gt;execve()&lt;/em&gt; call, and all bits are inherited by child processes (such as
when &lt;em&gt;fork()&lt;/em&gt; is used).&lt;/p&gt;
&lt;p&gt;As a user or admin, you can also see capability-related information
through the &lt;code&gt;/proc&lt;/code&gt; file system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; # grep ^Cap /proc/$$/status
CapInh: 0000000000000000
CapPrm: 0000001fffffffff
CapEff: 0000001fffffffff
CapBnd: 0000001fffffffff

$ grep ^Cap /proc/$$/status
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000001fffffffff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The capabilities listed therein are bitmasks for the various
capabilities. The mask &lt;code&gt;1FFFFFFFFF&lt;/code&gt; holds 37 positions, which match the
37 capabilities known (again, see &lt;code&gt;uapi/linux/capabilities.h&lt;/code&gt; in the
kernel sources to see the values of each of the capabilities). Again,
the &lt;strong&gt;pscap&lt;/strong&gt; can be used to get information about the enabled
capabilities of running processes in a more human readable format. But
another tool provided by the &lt;code&gt;sys-libs/libcap&lt;/code&gt; is interested as well to
look at: &lt;strong&gt;capsh&lt;/strong&gt;. The tool offers many capability-related features,
including decoding the &lt;code&gt;status&lt;/code&gt; fields:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ capsh --decode=0000001fffffffff
0x0000001fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,
cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,
cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,
cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,
cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,
cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,
cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,
cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,
cap_syslog,35,36
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next to fancy decoding, &lt;strong&gt;capsh&lt;/strong&gt; can also launch a shell with reduced
capabilities. This makes it a good utility for jailing chroots even
more.&lt;/p&gt;</content><category term="Security"></category><category term="capabilities"></category><category term="capsh"></category><category term="libcap"></category><category term="linux"></category></entry><entry><title>Overview of Linux capabilities, part 2</title><link href="https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-2/" rel="alternate"></link><published>2013-05-05T03:50:00+02:00</published><updated>2013-05-05T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-05:/2013/05/overview-of-linux-capabilities-part-2/</id><summary type="html">&lt;p&gt;As I've (in a very high level) &lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;described
capabilities&lt;/a&gt;
and talked a bit on how to &lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;work with
them&lt;/a&gt;,
I started with a small overview of
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/"&gt;file-related&lt;/a&gt;
capabilities. So next up are process-related capabilities (note, this
isn't a conform terminology, more some categorization that I do myself).&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;CAP_IPC_LOCK&lt;/dt&gt;
&lt;dd&gt;Allow the …&lt;/dd&gt;&lt;/dl&gt;</summary><content type="html">&lt;p&gt;As I've (in a very high level) &lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;described
capabilities&lt;/a&gt;
and talked a bit on how to &lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;work with
them&lt;/a&gt;,
I started with a small overview of
&lt;a href="http://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/"&gt;file-related&lt;/a&gt;
capabilities. So next up are process-related capabilities (note, this
isn't a conform terminology, more some categorization that I do myself).&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;CAP_IPC_LOCK&lt;/dt&gt;
&lt;dd&gt;Allow the process to lock memory&lt;/dd&gt;
&lt;dt&gt;CAP_IPC_OWNER&lt;/dt&gt;
&lt;dd&gt;Bypass the permission checks for operations on System V IPC objects
(similar to the &lt;code&gt;CAP_DAC_OVERRIDE&lt;/code&gt; for files)&lt;/dd&gt;
&lt;dt&gt;CAP_KILL&lt;/dt&gt;
&lt;dd&gt;Bypass permission checks for sending signals&lt;/dd&gt;
&lt;dt&gt;CAP_SETUID&lt;/dt&gt;
&lt;dd&gt;Allow the process to make arbitrary manipulations of process UIDs
and create forged UID when passing socket credentials via UNIX
domain sockets&lt;/dd&gt;
&lt;dt&gt;CAP_SETGID&lt;/dt&gt;
&lt;dd&gt;Same, but then for GIDs&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_NICE&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;This capability governs several permissions/abilities, namely to
allow the process to
&lt;/p&gt;
-   change the &lt;em&gt;nice&lt;/em&gt; value of itself and other processes
-   set real-time scheduling priorities for itself, and set
    scheduling policies and priorities for arbitrary processes
-   set the CPU affinity for arbitrary processes
-   apply &lt;em&gt;migrate_pages&lt;/em&gt; to arbitrary processes and allow
    processes to be migrated to arbitrary nodes
-   apply &lt;em&gt;move_pages&lt;/em&gt; to arbitrary processes
-   use the &lt;code&gt;MPOL_MF_MOVE_ALL&lt;/code&gt; flag with &lt;em&gt;mbind()&lt;/em&gt; and
    &lt;em&gt;move_pages()&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
The abilities related to page moving, migration and nodes is of
importance for NUMA systems, not something most workstations have
or need.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_PACCT&lt;/dt&gt;
&lt;dd&gt;Use &lt;em&gt;acct()&lt;/em&gt;, to enable or disable system resource accounting for
the process&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_PTRACE&lt;/dt&gt;
&lt;dd&gt;Allow the process to trace arbitrary processes using &lt;em&gt;ptrace()&lt;/em&gt;,
apply &lt;em&gt;get_robust_list()&lt;/em&gt; against arbitrary processes and inspect
processes using &lt;em&gt;kcmp()&lt;/em&gt;.&lt;/dd&gt;
&lt;dt&gt;CAP_SYS_RAWIO&lt;/dt&gt;
&lt;dd&gt;Allow the process to perform I/O port operations, access
&lt;code&gt;/proc/kcore&lt;/code&gt; and employ the &lt;code&gt;FIBMAP&lt;/code&gt; &lt;em&gt;ioctl()&lt;/em&gt; operation.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Capabilities such as &lt;code&gt;CAP_KILL&lt;/code&gt; and &lt;code&gt;CAP_SETUID&lt;/code&gt; are very important to
govern correctly, but this post would be rather dull (given that the
definitions of the above capabilities can be found from the manual page)
if I wouldn't talk a bit more about its feasibility. Take a look at the
following C application code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#include &amp;lt;errno.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/capability.h&amp;gt;
#include &amp;lt;sys/prctl.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char ** argv) {
  printf(&amp;quot;cap_setuid and cap_setgid: %d\n&amp;quot;, prctl(PR_CAPBSET_READ, CAP_SETUID|CAP_SETGID, 0, 0, 0));
  printf(&amp;quot; %s\n&amp;quot;, cap_to_text(cap_get_file(argv[0]), NULL));
  printf(&amp;quot; %s\n&amp;quot;, cap_to_text(cap_get_proc(), NULL));
  if (setresuid(0, 0, 0));
    printf(&amp;quot;setresuid(): %s\n&amp;quot;, strerror(errno));
  execve(&amp;quot;/bin/sh&amp;quot;, NULL, NULL);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At first sight, it looks like an application to get root privileges
(&lt;em&gt;setresuid()&lt;/em&gt;) and then spawn a shell. If that application would be
given &lt;code&gt;CAP_SETUID&lt;/code&gt; and &lt;code&gt;CAP_SETGID&lt;/code&gt; effectively, it would allow anyone
who executed it to automatically get a root shell, wouldn't it?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ gcc -o test -lcap test.c
# setcap cap_setuid,cap_setgid+ep test
$ ./test
cap_setuid and cap_setgid: 1
 = cap_setgid,cap_setuid+ep
 =
setresuid() failed: Operation not permitted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So what happened? After all, the two capabilities are set with the &lt;em&gt;+ep&lt;/em&gt;
flags given. Then why aren't these capabilities enabled? Well, this
binary was stored on a file system that is mounted with the &lt;em&gt;nosuid&lt;/em&gt;
option. As a result, this capability is &lt;em&gt;not&lt;/em&gt; enabled and the
application didn't work. If I move the file to another file system that
doesn't have the &lt;em&gt;nosuid&lt;/em&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ /usr/local/bin/test
cap_setuid and cap_setgid: 1
 = cap_setgid,cap_setuid+ep
 = cap_setgid,cap_setuid+ep
setresuid() failed: Operation not permitted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So the capabilities now do get enabled, so why does this still fail?
This now is due to SELinux:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1367393377.342:4778): avc:  denied  { setuid } for  pid=21418 comm=&amp;quot;test&amp;quot; capability=7  scontext=staff_u:staff_r:staff_t tcontext=staff_u:staff_r:staff_t tclass=capability
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And if you enable grSecurity's TPE, we can't even start the binary to
begin with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ ./test
-bash: ./test: Permission denied
$ /lib/ld-linux-x86-64.so.2 /home/test/test
/home/test/test: error while loading shared libraries: /home/test/test: failed to map segment from shared object: Permission denied

# dmesg
...
[ 5579.567842] grsec: From 192.168.100.1: denied untrusted exec (due to not being in trusted group and file in non-root-owned directory) of /home/test/test by /home/test/test[bash:4221] uid/euid:1002/1002 gid/egid:100/100, parent /bin/bash[bash:4195] uid/euid:1002/1002 gid/egid:100/100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When all these "security obstacles" are not enabled, then the call
succeeds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ /usr/local/bin/test
cap_setuid and cap_setgid: 1
 = cap_setgid,cap_setuid+ep
 = cap_setgid,cap_setuid+ep
setresuid() failed: Success
root@hpl tmp #
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This again shows how important it is to regularly review
capability-enabled files on the file system, as this is a major security
problem that cannot be detected by only looking for setuid binaries, but
also that securing a system is not limited to one or a few settings: one
always has to take the entire setup into consideration, hardening the
system so it becomes more difficult for malicious users to abuse it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# filecap -a
file                 capabilities
/usr/local/bin/test     setgid, setuid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Security"></category><category term="capabilities"></category><category term="grsecurity"></category><category term="linux"></category><category term="nosuid"></category><category term="selinux"></category><category term="tpe"></category></entry><entry><title>Overview of Linux capabilities, part 1</title><link href="https://blog.siphos.be/2013/05/overview-of-linux-capabilities-part-1/" rel="alternate"></link><published>2013-05-04T03:50:00+02:00</published><updated>2013-05-04T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-04:/2013/05/overview-of-linux-capabilities-part-1/</id><summary type="html">&lt;p&gt;In the
&lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;previous&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;posts&lt;/a&gt;,
I talked about capabilities and how they can be used to allow processes
to run in a privileged fashion without granting them full root access to
the system. An example given was how capabilities can be leveraged to
run &lt;strong&gt;ping&lt;/strong&gt; without granting it setuid root rights …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the
&lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro/"&gt;previous&lt;/a&gt;
&lt;a href="http://blog.siphos.be/2013/05/restricting-and-granting-capabilities/"&gt;posts&lt;/a&gt;,
I talked about capabilities and how they can be used to allow processes
to run in a privileged fashion without granting them full root access to
the system. An example given was how capabilities can be leveraged to
run &lt;strong&gt;ping&lt;/strong&gt; without granting it setuid root rights. But what are the
various capabilities that Linux is, well, capable of?&lt;/p&gt;
&lt;p&gt;There are many, and as time goes by, more capabilities are added to the
set. The last capability added to the main Linux kernel tree was the
&lt;code&gt;CAP_BLOCK_SUSPEND&lt;/code&gt; in the 3.5 series. An overview of all capabilities
can be seen with &lt;strong&gt;man capabilities&lt;/strong&gt; or by looking at the Linux kernel
source code, &lt;code&gt;include/uapi/linux/capability.h&lt;/code&gt;. But because you are all
lazy, and because it is a good exercise for myself, I'll go through many
of them in this and the next few posts.&lt;/p&gt;
&lt;p&gt;For now, let's look at file related capabilities. As a reminder, if you
want to know which SELinux domains are "granted" a particular
capability, you can look this up using &lt;strong&gt;sesearch&lt;/strong&gt;. The capability is
either in the &lt;em&gt;capability&lt;/em&gt; or &lt;em&gt;capability2&lt;/em&gt; class, and is named after
the capability itself, without the &lt;code&gt;CAP_&lt;/code&gt; prefix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sesearch -c capability -p chown -A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;dl&gt;
&lt;dt&gt;CAP_CHOWN&lt;/dt&gt;
&lt;dd&gt;Allow making changes to the file UIDs and GIDs.&lt;/dd&gt;
&lt;dt&gt;CAP_DAC_OVERRIDE&lt;/dt&gt;
&lt;dd&gt;Bypass file read, write and execute permission checks. I came across
a &lt;a href="http://www.reddit.com/r/linux/comments/1cnn15/for_chmod_why_is_root_allowed_to_execute_programs/"&gt;reddit
post&lt;/a&gt;
that was about this capability not that long ago.&lt;/dd&gt;
&lt;dt&gt;CAP_DAC_READ_SEARCH&lt;/dt&gt;
&lt;dd&gt;Bypass file read permission and directory read/search
permission checks.&lt;/dd&gt;
&lt;dt&gt;CAP_FOWNER&lt;/dt&gt;
&lt;dd&gt;This capability governs 5 capabilities in one:
&lt;/p&gt;
-   Bypass permission checks on operations that normally require the
    file system UID of the process to match the UID of the file
    (unless already granted through &lt;code&gt;CAP_DAC_READ_SEARCH&lt;/code&gt; and/or
    &lt;code&gt;CAP_DAC_OVERRIDE&lt;/code&gt;)
-   Allow to set extended file attributes
-   Allow to set access control lists
-   Ignore directory sticky bit on file deletion
-   Allow specifying &lt;code&gt;O_NOATIME&lt;/code&gt; for files in &lt;em&gt;open()&lt;/em&gt; and &lt;em&gt;fnctl()&lt;/em&gt;
    calls&lt;/dd&gt;
&lt;dt&gt;CAP_FSETID&lt;/dt&gt;
&lt;dd&gt;Do not clear the setuid/setgid permission bits when a file is
modified&lt;/dd&gt;
&lt;dt&gt;CAP_LEASE&lt;/dt&gt;
&lt;dd&gt;Allow establishing leases on files&lt;/dd&gt;
&lt;dt&gt;CAP_LINUX_IMMUTABLE&lt;/dt&gt;
&lt;dd&gt;Allow setting &lt;em&gt;FS_APPEND_FL&lt;/em&gt; and &lt;em&gt;FP_IMMUTABLE_FL&lt;/em&gt; inode flags&lt;/dd&gt;
&lt;dt&gt;CAP_MKNOD&lt;/dt&gt;
&lt;dd&gt;Allow creating special files with &lt;strong&gt;mknod&lt;/strong&gt;&lt;/dd&gt;
&lt;dt&gt;CAP_SETFCAP&lt;/dt&gt;
&lt;dd&gt;Allow setting file capabilities (what I did with the &lt;strong&gt;anotherping&lt;/strong&gt;
binary in the previous post)&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;When working with SELinux (especially when writing applications), you'll
find that the &lt;code&gt;CAP_DAC_READ_SEARCH&lt;/code&gt; and &lt;code&gt;CAP_DAC_OVERRIDE&lt;/code&gt; capability
come up often. This is the case when applications are written to run as
root yet want to scan through, read or even execute non-root owned
files. Without SELinux, because these run as root, this is all granted.
However, when you start confining those applications, it becomes
apparent that they require this capability. Another example is when you
run user applications, as root, like when trying to play a movie or
music file with &lt;strong&gt;mplayer&lt;/strong&gt; when this file is owned by a regular user:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1367145131.860:18785): avc:  denied  { dac_read_search } for
pid=8153 comm=&amp;quot;mplayer&amp;quot; capability=2  scontext=staff_u:sysadm_r:mplayer_t
tcontext=staff_u:sysadm_r:mplayer_t tclass=capability

type=AVC msg=audit(1367145131.860:18785): avc:  denied  { dac_override } for
pid=8153 comm=&amp;quot;mplayer&amp;quot; capability=1  scontext=staff_u:sysadm_r:mplayer_t
tcontext=staff_u:sysadm_r:mplayer_t tclass=capability
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice the time stamp: both checks are triggered at the same time. What
happens is that the Linux security hooks first check for
&lt;code&gt;DAC_READ_SEARCH&lt;/code&gt; (the "lesser" grants of the two) and then for
&lt;code&gt;DAC_OVERRIDE&lt;/code&gt; (which contains &lt;code&gt;DAC_READ_SEARCH&lt;/code&gt; and more). In both
cases, the check failed in the above example.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;CAP_LEASE&lt;/code&gt; capability is one that I had not heard about before
(actually, I had not heard of getting "file leases" on Linux either). A
file lease allows for the lease holder (which requires this capability)
to be notified when another process tries to open or truncate the file.
When that happens, the call itself is blocked and the lease holder is
notified (usually using SIGIO) about the access. It is not really to
lock a file (since, if the lease holder doesn't properly release it, it
is forcefully "broken" and the other process can continue its work) but
rather to properly close the file descriptor or flushing caches, etc.&lt;/p&gt;
&lt;p&gt;BTW, on my system, only 5 SELinux domains hold the &lt;em&gt;lease&lt;/em&gt; capability.&lt;/p&gt;
&lt;p&gt;There are 37 capabilities known by the Linux kernel at this time. The
above list has 9 file related ones. So perhaps next I can talk about
process capabilities.&lt;/p&gt;</content><category term="Security"></category><category term="capabilities"></category><category term="linux"></category></entry><entry><title>Restricting and granting capabilities</title><link href="https://blog.siphos.be/2013/05/restricting-and-granting-capabilities/" rel="alternate"></link><published>2013-05-03T03:50:00+02:00</published><updated>2013-05-03T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-03:/2013/05/restricting-and-granting-capabilities/</id><summary type="html">&lt;p&gt;As
&lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro"&gt;capabilities&lt;/a&gt;
are a way for running processes with some privileges, without having the
need to grant them root privileges, it is important to understand that
they exist if you are a system administrator, but also as an auditor or
other security-related function. Having processes run as a non-root user …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As
&lt;a href="http://blog.siphos.be/2013/05/capabilities-a-short-intro"&gt;capabilities&lt;/a&gt;
are a way for running processes with some privileges, without having the
need to grant them root privileges, it is important to understand that
they exist if you are a system administrator, but also as an auditor or
other security-related function. Having processes run as a non-root user
is no longer sufficient to assume that they do not hold any rights to
mess up the system or read files they shouldn't be able to read.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://grsecurity.net/"&gt;grsecurity&lt;/a&gt; kernel patch set, which is
applied to the Gentoo hardened kernel sources, contains for instance
&lt;code&gt;CONFIG_GRKERNSEC_CHROOT_CAPS&lt;/code&gt; which, as per its documentation,
"restrcts the capabilities on all root processes within a chroot jail to
stop module insertion, raw i/o, system and net admin tasks, rebooting
the system, modifying immutable files, modifying IPC owned by another,
and changing the system time." But other implementations might even use
capabilities to restrict the users. Consider
&lt;a href="http://lxc.sourceforge.net/"&gt;LXC&lt;/a&gt; (Linux Containers). When a container
is started, &lt;code&gt;CAP_SYS_BOOT&lt;/code&gt; (the ability to shutdown/reboot the
system/container) is removed so that users cannot abuse this privilege.&lt;/p&gt;
&lt;p&gt;You can also grant capabilities to users selectively, using &lt;code&gt;pam_cap.so&lt;/code&gt;
(the Capabilities Pluggable Authentication Module). For instance, to
allow some users to ping, instead of granting the &lt;code&gt;cap_net_raw&lt;/code&gt;
immediately (&lt;em&gt;+ep&lt;/em&gt;), we can assign the capability to some users through
PAM, and have the &lt;strong&gt;ping&lt;/strong&gt; binary inherit and use this capability
instead (&lt;em&gt;+p&lt;/em&gt;). That doesn't mean that the capability is in effect, but
rather that it is in a sort-of permitted set. Applications that are
granted a certain permission this way can either use this capability if
the user is allowed to have it, or won't otherwise.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# setcap cap_net_raw+p anotherping

# vim /etc/pam.d/system-login
... add in something like
auth     required     pam_cap.so

# vim /etc/security/capability.conf
... add in something like
cap_net_raw           user1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The logic used with capabilities can be described as follows (it is not
as difficult as it looks):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;        pI&amp;#39; = pI
  (***) pP&amp;#39; = fP | (fI &amp;amp; pI)
        pE&amp;#39; = pP&amp;#39; &amp;amp; fE          [NB. fE is 0 or ~0]

  I=Inheritable, P=Permitted, E=Effective // p=process, f=file
  &amp;#39; indicates post-exec().
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So, for instance, the second line reads "The permitted set of
capabilities of the newly forked process is set to the permitted set of
capabilities of its executable file, together with the result of the AND
operation between the inherited capabilities of the file and the
inherited capabilities of the parent process."&lt;/p&gt;
&lt;p&gt;As an admin, you might want to keep an eye out for binaries that have
particular capabilities set. With &lt;strong&gt;filecap&lt;/strong&gt; you can list which
capabilities are in the effective set of files found on the file system
(for instance, &lt;em&gt;+ep&lt;/em&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# filecap 
file                 capabilities
/bin/anotherping     net_raw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly, with &lt;strong&gt;pscap&lt;/strong&gt; you can see the capabilities set on running
processes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# pscap -a
ppid  pid   name        command           capabilities
6148  6152  root        bash              full
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It might be wise to take this up in the daily audit reports.&lt;/p&gt;</content><category term="Security"></category><category term="capabilities"></category><category term="linux"></category></entry><entry><title>Capabilities, a short intro</title><link href="https://blog.siphos.be/2013/05/capabilities-a-short-intro/" rel="alternate"></link><published>2013-05-02T03:50:00+02:00</published><updated>2013-05-02T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-02:/2013/05/capabilities-a-short-intro/</id><summary type="html">&lt;p&gt;Capabilities. You probably have heard of them already, but when you
start developing SELinux policies, you'll notice that you come in closer
contact with them than before. This is because SELinux, when
applications want to do something "root-like", checks the capability of
that application. Without SELinux, this either requires the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Capabilities. You probably have heard of them already, but when you
start developing SELinux policies, you'll notice that you come in closer
contact with them than before. This is because SELinux, when
applications want to do something "root-like", checks the capability of
that application. Without SELinux, this either requires the binary to
have the proper capability set, or the application to run in root modus.
With SELinux, the capability also needs to be granted to the SELinux
context (the domain in which the application runs).&lt;/p&gt;
&lt;p&gt;But forget about SELinux for now, and let's focus on capabilities.
Capabilities in Linux are flags that tell the kernel what the
application is allowed to do, but unlike file access, capabilities for
an application are system-wide: there is no "target" to which it
applies. Think about an "ability" of an application. See for yourself
through &lt;strong&gt;man capabilities&lt;/strong&gt;. If you have no additional security
mechanism in place, the Linux root user has all capabilities assigned to
it. And you can remove capabilities from the root user if you want to,
but generally, capabilities are used to grant applications that tiny bit
more privileges, without needing to grant them root rights.&lt;/p&gt;
&lt;p&gt;Consider the &lt;strong&gt;ping&lt;/strong&gt; utility. It is marked setuid root on some
distributions, because the utility requires the (cap)ability to send raw
packets. This capability is known as &lt;code&gt;CAP_NET_RAW&lt;/code&gt;. However, thanks to
capabilities, you can now mark the &lt;strong&gt;ping&lt;/strong&gt; application with this
capability and drop the setuid from the file. As a result, the
application does not run with full root privileges anymore, but with the
restricted privileges of the user plus one capability, namely the
&lt;code&gt;CAP_NET_RAW&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's take this &lt;strong&gt;ping&lt;/strong&gt; example to the next level: copy the binary
(possibly relabel it as &lt;code&gt;ping_exec_t&lt;/code&gt; if you run with SELinux), make
sure it does not hold the setuid and try it out:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cp ping anotherping
# chcon -t ping_exec_t anotherping
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now as a regular user:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ ping -c 1 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.057 ms

$ anotherping -c 1 127.0.0.1
ping: icmp open socket: Operation not permitted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's assign the binary with the &lt;code&gt;CAP_NET_RAW&lt;/code&gt; capability flag:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# setcap cap_net_raw+ep anotherping
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And tadaa:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ anotherping -c 1 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.054 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What &lt;strong&gt;setcap&lt;/strong&gt; did was place an extended attribute to the file, which
is a binary representation of the capabilities assigned to the
application. The additional information (&lt;code&gt;+ep&lt;/code&gt;) means that the
capability is &lt;em&gt;p&lt;/em&gt;ermitted and &lt;em&gt;e&lt;/em&gt;ffective.&lt;/p&gt;
&lt;p&gt;So long for the primer, I'll talk about the various capabilities in a
later post.&lt;/p&gt;</content><category term="Security"></category><category term="capabilities"></category><category term="linux"></category><category term="ping"></category><category term="selinux"></category></entry><entry><title>SELinux mount options</title><link href="https://blog.siphos.be/2013/05/selinux-mount-options/" rel="alternate"></link><published>2013-05-01T03:50:00+02:00</published><updated>2013-05-01T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-05-01:/2013/05/selinux-mount-options/</id><summary type="html">&lt;p&gt;When you read through the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;Gentoo Hardened SELinux
handbook&lt;/a&gt;,
you'll notice that we sometimes update &lt;code&gt;/etc/fstab&lt;/code&gt; with some
SELinux-specific settings. So, what are these settings about and are
there more of them?&lt;/p&gt;
&lt;p&gt;First of all, let's look at a particular example from the installation
instructions so you see what …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When you read through the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;Gentoo Hardened SELinux
handbook&lt;/a&gt;,
you'll notice that we sometimes update &lt;code&gt;/etc/fstab&lt;/code&gt; with some
SELinux-specific settings. So, what are these settings about and are
there more of them?&lt;/p&gt;
&lt;p&gt;First of all, let's look at a particular example from the installation
instructions so you see what I am talking about:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tmpfs  /tmp  tmpfs  defaults,noexec,nosuid,rootcontext=system_u:object_r:tmp_t  0 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What the &lt;em&gt;rootcontext=&lt;/em&gt; option does here is to set the context of the
"root" of that file system (meaning, the context of &lt;code&gt;/tmp&lt;/code&gt; in the
example) to the specified context &lt;em&gt;before&lt;/em&gt; the file system is made
visible to the userspace. Because we do it soon, the file system is
known as &lt;code&gt;tmp_t&lt;/code&gt; throughout its life cycle (not just after the mount or
so).&lt;/p&gt;
&lt;p&gt;Another option that you'll frequently see on the Internet is the
&lt;em&gt;context=&lt;/em&gt; option. This option is most frequently used for file systems
that do not support extended attributes, and as such cannot store the
context of files on the file system. With the &lt;em&gt;context=&lt;/em&gt; mount option
set, all files on that file system get the specified context. For
instance, &lt;em&gt;context=system_u:object_r:removable_t&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If the file system does support extended attributes, you might find some
benefit in using the &lt;em&gt;defcontext=&lt;/em&gt; option. When set, the context of
files and directories (and other resources on that file system) that do
not have a SELinux context set yet will use this default context.
However, once a context is set, it will use that context instead.&lt;/p&gt;
&lt;p&gt;The last context-related mount option is &lt;em&gt;fscontext=&lt;/em&gt;. With this option,
you set the context of the "filesystem" class object of the file system
rather than the mount itself (or the files). Within SELinux,
"filesystem" is one of the resource classes that can get a context.
Remember the &lt;code&gt;/tmp&lt;/code&gt; mount example from before? Well, even though the
files are labeled &lt;code&gt;tmp_t&lt;/code&gt;, the file system context itself is still
&lt;code&gt;tmpfs_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is important to know that, if you use one of these mount options,
&lt;em&gt;context=&lt;/em&gt; is mutually exclusive to the other options as it "forces" the
context on all resources (including the filesystem class).&lt;/p&gt;</content><category term="SELinux"></category><category term="mount"></category><category term="selinux"></category></entry><entry><title>Qemu-KVM monitor tips and tricks</title><link href="https://blog.siphos.be/2013/04/qemu-kvm-monitor-tips-and-tricks/" rel="alternate"></link><published>2013-04-30T03:50:00+02:00</published><updated>2013-04-30T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-30:/2013/04/qemu-kvm-monitor-tips-and-tricks/</id><summary type="html">&lt;p&gt;When running KVM guests, the &lt;a href="https://en.wikibooks.org/wiki/QEMU/Monitor"&gt;Qemu/KVM
monitor&lt;/a&gt; is a nice interface
to interact with the VM and do specific maintenance tasks on. If you run
the KVM guests with VNC, then you can get to this monitor through
&lt;code&gt;Ctrl-Alt-2&lt;/code&gt; (and &lt;code&gt;Ctrl-Alt-1&lt;/code&gt; to get back to the VM display). I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When running KVM guests, the &lt;a href="https://en.wikibooks.org/wiki/QEMU/Monitor"&gt;Qemu/KVM
monitor&lt;/a&gt; is a nice interface
to interact with the VM and do specific maintenance tasks on. If you run
the KVM guests with VNC, then you can get to this monitor through
&lt;code&gt;Ctrl-Alt-2&lt;/code&gt; (and &lt;code&gt;Ctrl-Alt-1&lt;/code&gt; to get back to the VM display). I
personally run with the monitor on the standard input/output where the
VM is launched as its output is often large and scrolling in the VNC
doesn't seem to work well.&lt;/p&gt;
&lt;p&gt;I decided to give you a few tricks that I use often on the monitor to
handle the VMs.&lt;/p&gt;
&lt;p&gt;When I do not start the VNC server associated with the VM by default, I
can enable it on the monitor using &lt;strong&gt;change vnc&lt;/strong&gt; while getting details
is done using &lt;strong&gt;info vnc&lt;/strong&gt;. To disable VNC again, use &lt;strong&gt;change vnc
none&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) info vnc
Server: disabled
(qemu) change vnc 127.0.0.1:20
(qemu) change vnc password
Password: ******
(qemu) info vnc
Server:
     address: 127.0.0.1:5920
        auth: vnc
Client: none
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly, if you need to enable remote debugging, you can use the
&lt;strong&gt;gdbserver&lt;/strong&gt; option.&lt;/p&gt;
&lt;p&gt;Getting information using &lt;strong&gt;info&lt;/strong&gt; is dead-easy, and supports a wide
area of categories: balloon info, block devices, character devices,
cpus, memory mappings, network information, etcetera etcetera... Just
enter &lt;strong&gt;info&lt;/strong&gt; to get an overview of all supported commands.&lt;/p&gt;
&lt;p&gt;To easily manage block devices, you can see the current state of devices
using &lt;strong&gt;info block&lt;/strong&gt; and then use &lt;strong&gt;change &amp;lt;blockdevice&amp;gt;
&amp;lt;path&amp;gt;&lt;/strong&gt; to update it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) info block
virtio0: removable=0 io-status=ok file=/srv/virt/gentoo/hardened2selinux/selinux-base.img ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0
ide1-cd0: removable=1 locked=0 tray-open=0 io-status=ok [not inserted]
floppy0: removable=1 locked=0 tray-open=0 [not inserted]
sd0: removable=1 locked=0 tray-open=0 [not inserted]
(qemu) change ide1-cd0 /srv/virt/media/systemrescuecd-x86-2.2.0.iso
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To powerdown the system, use &lt;strong&gt;system_powerdown&lt;/strong&gt;. If that fails, you
can use &lt;strong&gt;quit&lt;/strong&gt; to immediately shut down (terminate) the VM. To reset
it, use &lt;strong&gt;system_reset&lt;/strong&gt;. You can also hot-add PCI devices and
manipulate CPU states, or even perform &lt;a href="http://www.linux-kvm.org/page/Migration"&gt;live
migrations&lt;/a&gt; between systems.&lt;/p&gt;
&lt;p&gt;When you use qcow2 image formats, you can take a full VM snapshot using
&lt;strong&gt;savevm&lt;/strong&gt; and, when you later want to return to this point again, use
&lt;strong&gt;loadvm&lt;/strong&gt;. This is interesting when you want to do potentially harmful
changes on the system and want to easily revert back if things got
broken.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) savevm 20130419
(qemu) info snapshots
     ID        TAG                 VM SIZE                DATE       VM CLOCK
     1         20130419               224M 2013-04-19 12:05:16   00:00:17.294
(qemu) loadvm 20130419
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free-Software"></category><category term="kvm"></category><category term="monitor"></category><category term="qemu"></category></entry><entry><title>photorec to the rescue</title><link href="https://blog.siphos.be/2013/04/photorec-to-the-rescue/" rel="alternate"></link><published>2013-04-29T03:50:00+02:00</published><updated>2013-04-29T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-29:/2013/04/photorec-to-the-rescue/</id><summary type="html">&lt;p&gt;Once again
&lt;a href="http://www.cgsecurity.org/wiki/PhotoRec_Step_By_Step"&gt;PhotoRec&lt;/a&gt; has
been able to save files from a corrupt FAT USB drive. The application
scans the partition, looking for known files (based on the file magic)
and then restores those files. The files are not named as they were
though, so there is still some manual work …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Once again
&lt;a href="http://www.cgsecurity.org/wiki/PhotoRec_Step_By_Step"&gt;PhotoRec&lt;/a&gt; has
been able to save files from a corrupt FAT USB drive. The application
scans the partition, looking for known files (based on the file magic)
and then restores those files. The files are not named as they were
though, so there is still some manual work left, but the recovery works
pretty well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;PhotoRec 6.12, Data Recovery Utility, May 2011
Christophe GRENIER 
http://www.cgsecurity.org

Disk /dev/sdc1 - 1000 GB / 931 GiB (RO) - WD My Book
     Partition                  Start        End    Size in sectors
     No partition             0   0  1 121600 253 63 1953520002 [Whole disk]


Pass 1 - Reading sector  464342462/1953520002, 10738 files found
Elapsed time 2h46m01s - Estimated time to completion 8h52m25
jpg: 7429 recovered
txt: 961 recovered
mp3: 558 recovered
tx?: 373 recovered
riff: 297 recovered
gif: 218 recovered
exe: 151 recovered
ifo: 126 recovered
mpg: 91 recovered
pdf: 83 recovered
others: 451 recovered
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In Gentoo, you can find the package as part of &lt;code&gt;app-admin/testdisk&lt;/code&gt;. To
recover the files, I ran the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ photorec /log /d /path/to/recovery/dest /dev/sdc1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While skimming through the recovered files, I found a few ones that I
deleted a long time ago but apparently never got overwritten (the data,
that is). Scary to see how easy such recovery is... Makes me remember
that, if you really want to delete files in a less recoverable manner,
you can use &lt;strong&gt;shred&lt;/strong&gt; for that.&lt;/p&gt;
&lt;p&gt;And for those out there yelling to backup this data - you're absolutely
correct, but no. I backup my systems and important files daily, but this
disk contained (mainly) raw picture images and videorecordings. The
manipulated, finished images and recordings are backed up (or at least
on a disk &lt;em&gt;and&lt;/em&gt; somewhere online), but the raw images and recordings are
often too much to introduce a backup for, and if I would really lost
them, I wouldn't shed a tear (nor panic).&lt;/p&gt;</content><category term="Free-Software"></category><category term="corruption"></category><category term="photorec"></category><category term="recovery"></category><category term="shred"></category></entry><entry><title>Securely handling libffi</title><link href="https://blog.siphos.be/2013/04/securely-handling-libffi/" rel="alternate"></link><published>2013-04-28T03:50:00+02:00</published><updated>2013-04-28T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-28:/2013/04/securely-handling-libffi/</id><summary type="html">&lt;p&gt;I've recently came across &lt;a href="http://sourceware.org/libffi/"&gt;libffi&lt;/a&gt; again.
No, not because it was mentioned during the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; online meeting, but
because my &lt;code&gt;/var/tmp&lt;/code&gt; wasn't mounted correctly, and &lt;strong&gt;emerge&lt;/strong&gt; (actually
python) uses libffi. Most users won't notice this, because libffi works
behind the scenes. But when it fails, it fails bad …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've recently came across &lt;a href="http://sourceware.org/libffi/"&gt;libffi&lt;/a&gt; again.
No, not because it was mentioned during the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; online meeting, but
because my &lt;code&gt;/var/tmp&lt;/code&gt; wasn't mounted correctly, and &lt;strong&gt;emerge&lt;/strong&gt; (actually
python) uses libffi. Most users won't notice this, because libffi works
behind the scenes. But when it fails, it fails bad. And SELinux actually
helped me quickly identify what the problem is.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ emerge --info
segmentation fault
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The abbreviation "libffi" comes from &lt;em&gt;Foreign Function Interface&lt;/em&gt;, and
is a library that allows developers to dynamically call code from
another application or library. But the method how it approaches this
concerns me a bit. Let's look at some &lt;strong&gt;strace&lt;/strong&gt; output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;8560  open(&amp;quot;/var/tmp/ffiZ8gKPd&amp;quot;, O_RDWR|O_CREAT|O_EXCL, 0600) = 11
8560  unlink(&amp;quot;/var/tmp/ffiZ8gKPd&amp;quot;)      = 0
8560  ftruncate(11, 4096)               = 0
8560  mmap(NULL, 4096, PROT_READ|PROT_EXEC, MAP_SHARED, 11, 0) = -1 EACCES (Permission denied)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Generally, what libffi does, is to create a file somewhere where it can
write files (it checks the various mounts on a system to get a list of
possible target file systems), adds the necessary data (that it wants to
execute) to it, unlinks the file from the file system (but keep the file
descriptor open, so that the file cannot (easily) be modified on the
system anymore) and then maps it to memory for executable access. &lt;em&gt;If&lt;/em&gt;
executing is allowed by the system (for instance because the mount point
does not have &lt;code&gt;noexec&lt;/code&gt;), then SELinux will trap it because the domain
(in our case now, &lt;code&gt;portage_t&lt;/code&gt;) is trying to execute an (unlinked) file
for which it holds no execute rights on:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type=AVC msg=audit(1366656205.201:2221): avc:  denied  { execute } for  
pid=8560 comm=&amp;quot;emerge&amp;quot; path=2F7661722F66666962713154465A202864656C6574656429 
dev=&amp;quot;dm-3&amp;quot; ino=6912 scontext=staff_u:sysadm_r:portage_t tcontext=staff_u:object_r:var_t
tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you notice something like this (an execute on an unnamed file),
then this is because the file descriptor points to a file already
unlinked from the system. Finding out what it was about might be hard
(but with &lt;strong&gt;strace&lt;/strong&gt; it is easy as ... well, whatever is easy for you).&lt;/p&gt;
&lt;p&gt;Now what happened was that, because &lt;code&gt;/var/tmp&lt;/code&gt; wasn't mounted, files
created inside it got the standard type (&lt;code&gt;var_t&lt;/code&gt;) which the Portage
domain isn't allowed to execute. It is allowed to execute a lot of
types, but not that one ;-) When &lt;code&gt;/var/tmp&lt;/code&gt; is properly mounted, the
file gets the &lt;code&gt;portage_tmp_t&lt;/code&gt; type where it does hold execute rights
for.&lt;/p&gt;
&lt;p&gt;Now generally, I don't like having world-writeable locations without
&lt;code&gt;noexec&lt;/code&gt;. For &lt;code&gt;/tmp&lt;/code&gt;, &lt;code&gt;noexec&lt;/code&gt; is enabled, but for &lt;code&gt;/var/tmp&lt;/code&gt; I have
(well, had ;-) to allow execution from the file system, mainly because
some (many?) Gentoo package builds require it. So how about this dual
requirement, of allowing Portage to write (and execute) its own files,
and allow libffi to do its magic? Certainly, from a security point of
view, I might want to restrict this further...&lt;/p&gt;
&lt;p&gt;Well, we need to make sure that the location where Portage works with
(the location pointed to by &lt;code&gt;$PORTAGE_TMPDIR&lt;/code&gt;) is specifically made
available for Portage: have the directory only writable by the Portage
user. I keep it labeled as &lt;code&gt;tmp_t&lt;/code&gt; so that the existing policies apply,
but it might work with &lt;code&gt;portage_tmp_t&lt;/code&gt; immediately set as well. Perhaps
I'll try that one later. With that set, we can have this mount-point set
with exec rights (so that libffi can place its file there) in a somewhat
more secure manner than allowing exec on world-writeable locations.&lt;/p&gt;
&lt;p&gt;So now my &lt;code&gt;/tmp&lt;/code&gt; and &lt;code&gt;/var/tmp&lt;/code&gt; (and &lt;code&gt;/run&lt;/code&gt; and &lt;code&gt;/dev/shm&lt;/code&gt; and
&lt;code&gt;/lib64/rc/init.d&lt;/code&gt;) are tmpfs-mounts with the &lt;code&gt;noexec&lt;/code&gt; (as well as
&lt;code&gt;nodev&lt;/code&gt; and &lt;code&gt;nosuid&lt;/code&gt;) bits set, with the location pointed towards by
&lt;code&gt;$PORTAGE_TMPDIR&lt;/code&gt; being only really usable by the Portage user:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ ls -ldZ /var/portage
drwxr-x---. 4 portage root system_u:object_r:tmp_t 4096 Apr 22 21:45 /var/portage/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And libffi? Well, allowing applications to create their own executables
and executing it is something that should be carefully governed. I'm not
aware of any existing or past vulnerabilities, but I can imagine that
opening the &lt;code&gt;ffi*&lt;/code&gt; file(s) the moment they come up (to make sure you
have a file descriptor) allows you to overwrite the content after libffi
has created it but before the application actually executes it. By
limiting the locations where applications can write files to (important
step one) and the types they can execute (important step two) we can
already manage this a bit more. Using regular DAC, this is quite
difficult to achieve, but with SELinux, we can actually control this a
bit more.&lt;/p&gt;
&lt;p&gt;Let's first see how many domains are allowed to create, write and
execute files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sesearch -c file -p write,create,execute -A | grep write | grep create   
 | grep execute | awk &amp;#39;{print $1}&amp;#39; | sort | uniq | wc -l
32
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Okay, 32 target domains. Not that bad, and certainly doable to verify
manually (hell, even in a scripted manner). You can now check which of
those domains have rights to execute generic binaries (&lt;code&gt;bin_t&lt;/code&gt;),
possibly needed for command execution vulnerabilities or privilege
escalation. Or that have specific capabilities. And if you want to know
which of those domains use libffi, you can use &lt;strong&gt;revdep-rebuild&lt;/strong&gt; to
find out which files are linked to the libffi libraries.&lt;/p&gt;
&lt;p&gt;It goes to show that trying to keep your box secure is a never-ending
story (please, companies, allow your system administrators to do their
job by giving them the ability to continuously increase security rather
than have them ask for budget to investigate potential security
mitigation directives based on the paradigm of business case and return
on investment using pareto-analytics blaaaahhhh....), and that SELinux
can certainly be an important method to help achieve it.&lt;/p&gt;</content><category term="Security"></category><category term="libffi"></category><category term="selinux"></category><category term="strace"></category></entry><entry><title>How logins get their SELinux user context</title><link href="https://blog.siphos.be/2013/04/how-logins-get-their-selinux-user-context/" rel="alternate"></link><published>2013-04-27T03:50:00+02:00</published><updated>2013-04-27T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-27:/2013/04/how-logins-get-their-selinux-user-context/</id><summary type="html">&lt;p&gt;Sometimes, especially when users are converting their systems to be
SELinux-enabled, their user context is wrong. An example would be when,
after logon (in permissive mode), the user is in the
&lt;code&gt;system_u:system_r:local_login_t&lt;/code&gt; domain instead of a user domain like
&lt;code&gt;staff_u:staff_r:staff_t&lt;/code&gt;.&lt;br&gt;
So, how does a login get …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes, especially when users are converting their systems to be
SELinux-enabled, their user context is wrong. An example would be when,
after logon (in permissive mode), the user is in the
&lt;code&gt;system_u:system_r:local_login_t&lt;/code&gt; domain instead of a user domain like
&lt;code&gt;staff_u:staff_r:staff_t&lt;/code&gt;.&lt;br&gt;
So, how does a login get its SELinux user context?&lt;/p&gt;
&lt;p&gt;Let's look at the entire chain of SELinux context changes across a boot.
At first, when the system boots, the kernel (and all processes invoked
from it) run in the &lt;code&gt;kernel_t&lt;/code&gt; domain (I'm going to ignore the other
context fields for now until they become relevant). When the kernel
initialization has been completed, the kernel executes the &lt;strong&gt;init&lt;/strong&gt;
binary. When you use an initramfs, then a script might be called. This
actually doesn't matter that much yet, since SELinux stays within the
&lt;code&gt;kernel_t&lt;/code&gt; domain &lt;em&gt;until&lt;/em&gt; a SELinux-aware &lt;strong&gt;init&lt;/strong&gt; is launched.&lt;/p&gt;
&lt;p&gt;When the &lt;strong&gt;init&lt;/strong&gt; binary is executed, init of course starts. But as
mentioned, init is SELinux-aware, meaning it will invoke SELinux-related
commands. One of these is that it will load the SELinux policy (as
stored in &lt;code&gt;/etc/selinux&lt;/code&gt;) and then reexecute itself. Because of that,
its process context changes from &lt;code&gt;kernel_t&lt;/code&gt; towards &lt;code&gt;init_t&lt;/code&gt;. This is
because the &lt;strong&gt;init&lt;/strong&gt; binary itself is labeled as &lt;code&gt;init_exec_t&lt;/code&gt; and a
type transition is defined from &lt;code&gt;kernel_t&lt;/code&gt; towards &lt;code&gt;init_t&lt;/code&gt; when
&lt;code&gt;init_exec_t&lt;/code&gt; is executed.&lt;/p&gt;
&lt;p&gt;Ok, so &lt;strong&gt;init&lt;/strong&gt; now runs in &lt;code&gt;init_t&lt;/code&gt; and it goes on with whatever it
needs to do. This includes invoking init scripts (which, btw, run in
&lt;code&gt;initrc_t&lt;/code&gt; because the scripts are labeled &lt;code&gt;initrc_exec_t&lt;/code&gt; or with a
type that has the &lt;code&gt;init_script_file_type&lt;/code&gt; attribute set, and a
transition from &lt;code&gt;init_t&lt;/code&gt; to &lt;code&gt;initrc_t&lt;/code&gt; is defined when such files are
executed). When the bootup is finally completed, &lt;strong&gt;init&lt;/strong&gt; launches the
&lt;em&gt;getty&lt;/em&gt; processes. The commands are mentioned in &lt;code&gt;/etc/inittab&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ grep getty /etc/inittab
c1:12345:respawn:/sbin/agetty --noclear 38400 tty1 linux
c2:2345:respawn:/sbin/agetty 38400 tty2 linux
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;These binaries are also explicitly labeled &lt;code&gt;getty_exec_t&lt;/code&gt;. As a result,
the &lt;strong&gt;getty&lt;/strong&gt; (or &lt;strong&gt;agetty&lt;/strong&gt;) processes run in the &lt;code&gt;getty_t&lt;/code&gt; domain
(because a transition is defined from &lt;code&gt;init_t&lt;/code&gt; to &lt;code&gt;getty_t&lt;/code&gt; when
&lt;code&gt;getty_exec_t&lt;/code&gt; is executed). Ok, so gettys run in &lt;code&gt;getty_t&lt;/code&gt;. But what
happens when a user now logs on to the system?&lt;/p&gt;
&lt;p&gt;Well, the getty's invoke the &lt;strong&gt;login&lt;/strong&gt; binary which, you guessed it
right, is labeled as something: &lt;code&gt;login_exec_t&lt;/code&gt;. As a result (because,
again, a transition is defined in the policy), the login process runs as
&lt;code&gt;local_login_t&lt;/code&gt;. Now the login process invokes the various PAM
subroutines which follow the definitions in &lt;code&gt;/etc/pam.d/login&lt;/code&gt;. On
Gentoo systems, this by default points to the &lt;code&gt;system-local-login&lt;/code&gt;
definitions which points to the &lt;code&gt;system-login&lt;/code&gt; definitions. And in this
definition, especially under the sessions section, we find a reference
to &lt;code&gt;pam_selinux.so&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;session         required        pam_selinux.so close
...
session         required        pam_selinux.so multiple open
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now here is where some of the magic starts (see my post on &lt;a href="http://blog.siphos.be/2012/12/using-pam_selinux-to-switch-contexts/"&gt;Using
pam_selinux to switch
contexts&lt;/a&gt;
for the gritty details). The methods inside the &lt;code&gt;pam_selinux.so&lt;/code&gt; binary
will look up what the context should be for a user login. For instance,
when the &lt;em&gt;root&lt;/em&gt; user logs on, it has SELinux checking what SELinux user
&lt;em&gt;root&lt;/em&gt; is mapped to, equivalent to running &lt;strong&gt;semanage login -l&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ semanage login -l | grep ^root
root                      root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the SELinux user for root is &lt;em&gt;root&lt;/em&gt;, but this is not
always the case (that login and user are the same). For instance, my
regular administrative account maps to the &lt;em&gt;staff_u&lt;/em&gt; SELinux user.&lt;/p&gt;
&lt;p&gt;Next, it checks what the default context should be for this user. This
is done by checking the &lt;code&gt;default_contexts&lt;/code&gt; file (such as the one in
&lt;code&gt;/etc/selinux/strict/contexts&lt;/code&gt; although user-specific overrides can be
(and are) placed in the &lt;code&gt;users&lt;/code&gt; subdirectory) based on the context of
the process that is asking SELinux what the default context should be.
In our case, it is the &lt;strong&gt;login&lt;/strong&gt; process running as &lt;code&gt;local_login_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ grep -HR local_login_t /etc/selinux/strict/contexts/*
default_contexts:system_r:local_login_t user_r:user_t staff_r:staff_t sysadm_r:sysadm_t unconfined_r:unconfined_t
users/unconfined_u:system_r:local_login_t               unconfined_r:unconfined_t
users/guest_u:system_r:local_login_t            guest_r:guest_t
users/user_u:system_r:local_login_t             user_r:user_t
users/staff_u:system_r:local_login_t            staff_r:staff_t sysadm_r:sysadm_t
users/root:system_r:local_login_t  unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t
users/xguest_u:system_r:local_login_t   xguest_r:xguest_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since we are verifying this for the &lt;em&gt;root&lt;/em&gt; SELinux user, the following
line of the &lt;code&gt;users/root&lt;/code&gt; file is what matters:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;system_r:local_login_t  unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, SELinux looks for the first match in that line that the user has
access to. This is defined by the roles that the user is allowed to
access:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ semanage user -l | grep root
root            staff_r sysadm_r
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As &lt;em&gt;root&lt;/em&gt; is allowed both the &lt;em&gt;staff_r&lt;/em&gt; and &lt;em&gt;sysadm_r&lt;/em&gt; roles, the
first one found &lt;em&gt;in the default context file&lt;/em&gt; that matches will be used.
So it is &lt;em&gt;not&lt;/em&gt; the order in which the roles are displayed in the
&lt;strong&gt;semanage user -l&lt;/strong&gt; output that matters, but the order of the contexts
in the &lt;em&gt;default context&lt;/em&gt; file. In the example, this is
&lt;code&gt;sysadm_r:sysadm_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;system_r:local_login_t  unconfined_r:unconfined_t sysadm_r:sysadm_t staff_r:staff_t user_r:user_t
                        &amp;lt;-----------+-----------&amp;gt; &amp;lt;-------+-------&amp;gt; &amp;lt;------+------&amp;gt; &amp;lt;-----+-----&amp;gt;
                                    `- no matching role   `- first (!)     `- second      `- no match
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now that we know what the context &lt;em&gt;should&lt;/em&gt; be, this is used for the
first execution that the process (still &lt;strong&gt;login&lt;/strong&gt;) will do. So &lt;strong&gt;login&lt;/strong&gt;
changes the Linux user (if applicable) and invokes the shell of that
user. Because this is the first execution that is done by &lt;strong&gt;login&lt;/strong&gt;, the
new context is set (being &lt;code&gt;root:sysadm_r:sysadm_t&lt;/code&gt;) for the shell.&lt;/p&gt;
&lt;p&gt;And that is why, if you run &lt;strong&gt;id -Z&lt;/strong&gt;, it returns the user context
(&lt;code&gt;root:sysadm_r:sysadm_t&lt;/code&gt;) if everything works out fine ;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="context"></category><category term="selinux"></category><category term="user"></category></entry><entry><title>New SELinux userspace release</title><link href="https://blog.siphos.be/2013/04/new-selinux-userspace-release/" rel="alternate"></link><published>2013-04-26T03:50:00+02:00</published><updated>2013-04-26T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-26:/2013/04/new-selinux-userspace-release/</id><summary type="html">&lt;p&gt;A new &lt;a href="http://userspace.selinuxproject.org/trac/wiki/Releases"&gt;release&lt;/a&gt;
of the SELinux userspace utilities was recently announced. I have made
the packages for Gentoo available and they should now be in the main
tree (\~arch of course). During the testing of the packages however, I
made a stupid mistake of running the tests on the wrong …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A new &lt;a href="http://userspace.selinuxproject.org/trac/wiki/Releases"&gt;release&lt;/a&gt;
of the SELinux userspace utilities was recently announced. I have made
the packages for Gentoo available and they should now be in the main
tree (\~arch of course). During the testing of the packages however, I
made a stupid mistake of running the tests on the wrong VM, one that
didn't contain the new packages. Result: no regressions (of course). My
fault for not using in-ebuild tests properly, as I
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=465846"&gt;should&lt;/a&gt;. So you'll
probably see me blogging about the in-ebuild testing soon ;-)&lt;/p&gt;
&lt;p&gt;In any case, the regressions I did find out (quite fast after I updated
my main laptop with them as well) where a &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=467258"&gt;missing function in
libselinux&lt;/a&gt;, a &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=467264"&gt;referral
to a non-existing makefile when using "semanage
permissive"&lt;/a&gt; and the new
&lt;strong&gt;sepolicy&lt;/strong&gt; application &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=467268"&gt;requiring yum python
bindings&lt;/a&gt;. At least,
with the missing function (hopefully correctly) resolved, all tests I
usually do (except for the permissive domains) are now running well
again.&lt;/p&gt;
&lt;p&gt;This only goes to show how important testing is. Of course, I
&lt;a href="http://marc.info/?l=selinux&amp;amp;m=136692033821285&amp;amp;w=2"&gt;reported&lt;/a&gt; the bugs
on the mailinglist of the userspace utilities as well. Hopefully they
can look at them while I'm asleep so I can integrate fixes tomorrow more
easily ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="automation"></category><category term="regression"></category><category term="release"></category><category term="selinux"></category><category term="test"></category><category term="testing"></category><category term="userspace"></category></entry><entry><title>Gentoo protip: using buildpkgonly</title><link href="https://blog.siphos.be/2013/04/gentoo-protip-using-buildpkgonly/" rel="alternate"></link><published>2013-04-25T03:50:00+02:00</published><updated>2013-04-25T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-25:/2013/04/gentoo-protip-using-buildpkgonly/</id><summary type="html">&lt;p&gt;If you don't want to have the majority of builds run in the background
while you are busy on the system, but you don't want to automatically
install software in the background when you are not behind your desk,
then perhaps you can settle for using &lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary
packages&lt;/a&gt;. I'm not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you don't want to have the majority of builds run in the background
while you are busy on the system, but you don't want to automatically
install software in the background when you are not behind your desk,
then perhaps you can settle for using &lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary
packages&lt;/a&gt;. I'm not
saying you need to setup a build server and such or do your updates
first in a chroot.&lt;/p&gt;
&lt;p&gt;No, what this tip is for is to use the &lt;em&gt;--buildpkgonly&lt;/em&gt; parameter of
&lt;strong&gt;emerge&lt;/strong&gt; at night, building some of your software (often the larger
ones) as binary packages only (storing those in the &lt;code&gt;${PKGDIR}&lt;/code&gt; which
defaults to &lt;code&gt;/usr/portage/packages&lt;/code&gt;). When you are then on your system,
you can run the update with binary package included:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# emerge -puDk world
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To use &lt;em&gt;--buildpkgonly&lt;/em&gt;, all package(s) that Portage wants to build must
have all their dependencies met. If not, then the build will not go
through and you're left with no binary packages at all. So what we do is
to create a script that looks at the set of packages that would be
build, and then one for one building the binary package.&lt;/p&gt;
&lt;p&gt;When ran, the script will attempt to build binary packages for those
that have no dependency requirements anymore. Those builds will then
create a binary package but will not be merged on the system. When you
later update your system, including binary packages, those packages that
have been build during the night will be merged quickly, reducing the
build load on your system while you are working on it.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nv"&gt;LIST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;mktemp&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

emerge -puDN --color&lt;span class="o"&gt;=&lt;/span&gt;n --columns --quiet&lt;span class="o"&gt;=&lt;/span&gt;y world &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt; &amp;gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;LIST&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; PACKAGE &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;cat &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;LIST&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="nb"&gt;printf&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Building binary package for &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PACKAGE&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;... &amp;quot;&lt;/span&gt;
  emerge -uN --quiet-build --quiet&lt;span class="o"&gt;=&lt;/span&gt;y --buildpkgonly &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PACKAGE&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$?&lt;/span&gt; -eq &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;failed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;I ran this a couple of days ago when &lt;em&gt;-uDN world&lt;/em&gt; showed 46 package
updates (including a few hefty ones like chromium). After running this
script, 35 of them had a binary package ready so the &lt;em&gt;-uDN world&lt;/em&gt; now
only needed to build 11 packages, merging the remainder from binary
packages.&lt;/p&gt;</content><category term="Gentoo"></category><category term="binpkg"></category><category term="emerge"></category><category term="Gentoo"></category><category term="protip"></category></entry><entry><title>Using strace to troubleshoot SELinux problems</title><link href="https://blog.siphos.be/2013/04/using-strace-to-troubleshoot-selinux-problems/" rel="alternate"></link><published>2013-04-24T03:50:00+02:00</published><updated>2013-04-24T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-24:/2013/04/using-strace-to-troubleshoot-selinux-problems/</id><summary type="html">&lt;p&gt;When SELinux is playing tricks on you, you can just "allow" whatever it
wants to do, but that is not always an option: sometimes, there is no
denial in sight because the problem lays within SELinux-aware
applications (applications that might change their behavior based on
what the policy sais or …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When SELinux is playing tricks on you, you can just "allow" whatever it
wants to do, but that is not always an option: sometimes, there is no
denial in sight because the problem lays within SELinux-aware
applications (applications that might change their behavior based on
what the policy sais or even based on if SELinux is enabled or not). At
other times, you get a strange behavior that isn't directly visible what
the cause is. But mainly, if you want to make sure that allowing
something is correct (and not just a corrective action), you need to be
absolutely certain that what you want to allow is security-wise
acceptable.&lt;/p&gt;
&lt;p&gt;To debug such issues, I often take the &lt;strong&gt;strace&lt;/strong&gt; command to debug the
application at hand. To use &lt;strong&gt;strace&lt;/strong&gt;, I toggle the &lt;em&gt;allow_ptrace&lt;/em&gt;
boolean (&lt;strong&gt;strace&lt;/strong&gt; uses &lt;code&gt;ptrace()&lt;/code&gt; which, by default, isn't allowed
policy-wise) and then run the offending application through &lt;strong&gt;strace&lt;/strong&gt;
(or attach to the running process if it is a daemon). For instance, to
debug a &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=463222"&gt;tmux issue&lt;/a&gt; we
had with the policy not that long ago:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# setsebool allow_ptrace on
# strace -o strace.log -f -s 256 tmux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting log file (strace.log) might seem daunting at first to look
at. What you see are the system calls that the process is performing,
together with their options but also the return code of each call. This
is especially important as SELinux, if it denies something, often
returns something like EACCESS (Permission Denied).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;7313  futex(0x349e016f080, FUTEX_WAKE_PRIVATE, 2147483647) = 0
7313  futex(0x5aad58fd84, FUTEX_WAKE_PRIVATE, 2147483647) = 0
7313  stat(&amp;quot;/&amp;quot;, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
7313  stat(&amp;quot;/home&amp;quot;, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
7313  stat(&amp;quot;/home/swift&amp;quot;, {st_mode=S_IFDIR|0755, st_size=12288, ...}) = 0
7313  stat(&amp;quot;/home/swift/.pki&amp;quot;, {st_mode=S_IFDIR|0700, st_size=4096, ...}) = 0
7313  stat(&amp;quot;/home/swift/.pki/nssdb&amp;quot;, {st_mode=S_IFDIR|0700, st_size=4096, ...}) = 0
7313  statfs(&amp;quot;/home/swift/.pki/nssdb&amp;quot;, 0x3c3cab6fa50) = -1 EACCES (Permission denied)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Most (if not all) of the methods shown in a strace log are documented
through manpages, so you can quickly find out that &lt;code&gt;futex()&lt;/code&gt; is about
fast user-space locking, &lt;code&gt;stat()&lt;/code&gt; (&lt;strong&gt;man 2 stat&lt;/strong&gt; to see the information
about the method instead of the application) is about getting file
status and &lt;code&gt;statfs()&lt;/code&gt; is for getting file system statistics.&lt;/p&gt;
&lt;p&gt;The most common permission issues you'll find are file related:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;7313  open(&amp;quot;/proc/filesystems&amp;quot;, O_RDONLY) = -1 EACCES (Permission denied)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above case, you notice that the application is trying to open the
&lt;code&gt;/proc/filesystems&lt;/code&gt; file read-only. In the SELinux logs, this might be
displayed as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit.log:type=AVC msg=audit(1365794728.180:3192): avc:  denied  { read } for  
pid=860 comm=&amp;quot;nacl_helper_boo&amp;quot; name=&amp;quot;filesystems&amp;quot; dev=&amp;quot;proc&amp;quot; ino=4026532034 
scontext=staff_u:staff_r:chromium_naclhelper_t tcontext=system_u:object_r:proc_t tclass=file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the case of &lt;strong&gt;tmux&lt;/strong&gt; before was not an obvious one. In the end, I
compared the strace output's of two runs (one in enforcing and one in
permissive) to find what the difference would be. This is the result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Enforcing:

10905 fcntl(9, F_GETFL) = 0x8000 (flags O_RDONLY|O_LARGEFILE) 
10905 fcntl(9, F_SETFL, O_RDONLY|O_NONBLOCK|O_LARGEFILE) = 0

Permissive:

10905 fcntl(9, F_GETFL) = 0x8002 (flags O_RDWR|O_LARGEFILE) 
10905 fcntl(9, F_SETFL, O_RDWR|O_NONBLOCK|O_LARGEFILE) = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You notice the difference? In enforcing-mode, one of the flags on the
file descriptor has &lt;code&gt;O_RDONLY&lt;/code&gt; whereas the one in permissive mode as
&lt;code&gt;O_RDWR&lt;/code&gt;. This means that the file descriptor in enforcing mode is
read-only whereas in permissive-mode is read-write. What we then do in
the strace logs is to see where this file descriptor (with id=9) comes
from:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;10905 dup(0)     = 9
10905 dup(1)     = 10
10905 dup(2)     = 11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As the man-pages sais, &lt;code&gt;dup()&lt;/code&gt; duplicates a file descriptor. And
because, by convention, the first three file descriptors of an
application correspond with standard input (0), standard output (1) and
error output (2), we now know that the file descriptor with id=9 comes
from the standard input file descriptor. Although this one should be
read-only (it is the input that the application gets = reads), it seems
that tmux might want to use this for writes as well. And that is what
happens - tmux sends the file descriptor to the tmux server to check if
it is a tty and then uses it to write to the screen.&lt;/p&gt;
&lt;p&gt;Now what does that have to do with SELinux? It has to mean something,
otherwise running in permissive mode would give the same result. After
some investigation, we found out that using &lt;strong&gt;newrole&lt;/strong&gt; to switch roles
changes the flags of the standard input (as then provided by
&lt;strong&gt;newrole&lt;/strong&gt;) from &lt;code&gt;O_RDWR&lt;/code&gt; to &lt;code&gt;O_RDONLY&lt;/code&gt; (code snippet from &lt;code&gt;newrole.c&lt;/code&gt;
- look at the first call to &lt;code&gt;open()&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/* Close the tty and reopen descriptors 0 through 2 */
if (ttyn) {
        if (close(fd) || close(0) || close(1) || close(2)) {
                fprintf(stderr, _(&amp;quot;Could not close descriptors.\n&amp;quot;));
                goto err_close_pam;
        }
        fd = open(ttyn, O_RDONLY | O_NONBLOCK);
        if (fd != 0)
                goto err_close_pam;
        fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) &amp;amp; ~O_NONBLOCK);
        fd = open(ttyn, O_RDWR | O_NONBLOCK);
        if (fd != 1)
                goto err_close_pam;
        fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) &amp;amp; ~O_NONBLOCK);
        fd = open(ttyn, O_RDWR | O_NONBLOCK);
        if (fd != 2)
                goto err_close_pam;
        fcntl(fd, F_SETFL, fcntl(fd, F_GETFL, 0) &amp;amp; ~O_NONBLOCK);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Such obscure problems are much easier to detect and troubleshoot thanks
to tools like &lt;strong&gt;strace&lt;/strong&gt;.&lt;/p&gt;</content><category term="SELinux"></category><category term="debug"></category><category term="selinux"></category><category term="strace"></category></entry><entry><title>SLOT'ing the old swig-1</title><link href="https://blog.siphos.be/2013/04/sloting-the-old-swig-1/" rel="alternate"></link><published>2013-04-23T03:50:00+02:00</published><updated>2013-04-23T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-23:/2013/04/sloting-the-old-swig-1/</id><summary type="html">&lt;p&gt;The &lt;a href="http://www.swig.org"&gt;SWIG&lt;/a&gt; tool helps developers in building
interfaces/libraries that can be accessed from many other languages than
the ones the library is initially written in or for. The SELinux
userland utility &lt;a href="http://oss.tresys.com/projects/setools"&gt;setools&lt;/a&gt; uses
it to provide Python and Ruby interfaces even though the application
itself is written in C …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="http://www.swig.org"&gt;SWIG&lt;/a&gt; tool helps developers in building
interfaces/libraries that can be accessed from many other languages than
the ones the library is initially written in or for. The SELinux
userland utility &lt;a href="http://oss.tresys.com/projects/setools"&gt;setools&lt;/a&gt; uses
it to provide Python and Ruby interfaces even though the application
itself is written in C. Sadly, the tool currently requires swig-1 for
its building of the interfaces and uses constructs that do not seem to
be compatible with swig-2 (same with the apse package, btw).&lt;/p&gt;
&lt;p&gt;I first tried to &lt;a href="http://comments.gmane.org/gmane.comp.security.selinux/17822"&gt;patch
setools&lt;/a&gt; to
support swig-2, but eventually found regressions in the libapol library
it provides so the patch didn't work out (that is why some users
mentioned that a previous setools version did build with swig - yes it
did, but the result wasn't correct). Recently, a &lt;a href="https://plus.google.com/117641514179258643044/posts/gNrhVDuwGzp"&gt;post on Google Plus'
SELinux
community&lt;/a&gt;
showed me that I wasn't wrong in this matter (it really does require
swig-1 and doesn't seem to be trivial to fix).&lt;/p&gt;
&lt;p&gt;Hence, I have to fix the &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=453512"&gt;gentoo build
problem&lt;/a&gt; where one set
of tools requires swig-1 and another swig-2. Otherwise world-updates and
even building stages for SELinux systems would fail as Portage finds
incompatible dependencies. One way to approach this is to use Gentoo's
support for
&lt;a href="http://devmanual.gentoo.org/general-concepts/slotting/"&gt;SLOTs&lt;/a&gt;. When a
package (ebuild) in Gentoo defines a SLOT, it tells the package manager
that the same package but a different version might be installed
alongside the package if that has a different SLOT version. In case of
swig, the idea is to give swig-1 a different slot than swig-2 (which
uses &lt;code&gt;SLOT="0"&lt;/code&gt;) and make sure that both do not install the same files
(otherwise you get file collisions).&lt;/p&gt;
&lt;p&gt;Luckily, swig places all of its files except for the &lt;strong&gt;swig&lt;/strong&gt; binary
itself in &lt;code&gt;/usr/share/swig/&amp;lt;version&amp;gt;&lt;/code&gt;, so all I had left to do was to
make sure the binary itself is renamed. I chose to use &lt;strong&gt;swig1.3&lt;/strong&gt;
(similar as to how tools like &lt;strong&gt;ruby&lt;/strong&gt; and &lt;strong&gt;python&lt;/strong&gt; and for some
packages even &lt;strong&gt;java&lt;/strong&gt; is implemented on Gentoo). The result (through
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=466650"&gt;bug 466650&lt;/a&gt;) is now in
the tree, as well as an adapted setools package that uses the new swig
SLOT.&lt;/p&gt;
&lt;p&gt;Thanks to Samuli Suominen for getting me on the (hopefully ;-) right
track. I don't know why I was afraid of doing this, it was much less
complex than I thought (now let's hope I didn't break other things ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="selinux"></category><category term="setools"></category><category term="slot"></category><category term="swig"></category></entry><entry><title>Mitigating DDoS attacks</title><link href="https://blog.siphos.be/2013/04/mitigating-ddos-attacks/" rel="alternate"></link><published>2013-04-22T03:50:00+02:00</published><updated>2013-04-22T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-22:/2013/04/mitigating-ddos-attacks/</id><summary type="html">&lt;p&gt;Lately, DDoS attacks have been in the news more than I was hoping for.
It seems that the botnets or other methods that are used to generate
high-volume traffic to a legitimate service are becoming more and more
easy to get and direct. At the time that I'm writing this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately, DDoS attacks have been in the news more than I was hoping for.
It seems that the botnets or other methods that are used to generate
high-volume traffic to a legitimate service are becoming more and more
easy to get and direct. At the time that I'm writing this post (a few
days before its published though), the popular
&lt;a href="http://www.reddit.com"&gt;Reddit&lt;/a&gt; site is undergoing a DDoS attack which I
hope will be finished (or mitigated) soon.&lt;/p&gt;
&lt;p&gt;But what can a service do against DDoS attacks? After all, DDoS is like
gasping for air if you can't swim and are (almost) drowning: the air is
the legitimate traffic, but the water is overwhelming and your mouth,
pharynx and trachea just aren't made to deal with this properly. And
unlike specific Denial-of-Service attacks that use a vulnerability or
malcrafted URL, you cannot just install some filter or upgrade a
component to be safe again.&lt;/p&gt;
&lt;p&gt;Methods for mitigating DDoS attacks (beyond increasing your bandwidth as
that is very expensive and the botnets involved can go &lt;a href="http://arstechnica.com/security/2013/04/fueled-by-super-botnets-ddos-attacks-grow-meaner-and-ever-more-powerful/"&gt;up to 130
Gbps&lt;/a&gt;,
not a bandwidth you are probably willing to pay for if legitimate
services on your site have enough with 10 Mbps) that come to mind are of
all sorts of "classes"...&lt;/p&gt;
&lt;p&gt;Configure your servers and services that they &lt;strong&gt;stay alive under
pressure&lt;/strong&gt;. Look for the sweet spot where performance of the services is
still stable where a higher load means performance degradation. If you
have some experience with load testing, you know that throughput on a
service initially goes up linearly with the load (first phase). Then, it
slows down (but still rises - phase 2) up to a point that, when you
increase the load even further just a bit, the service degrades (and
sometimes doesn't even get back to its feed when you remove the
additional load again - phase3). You need to look for the spot where
load and performance is stable (somewhere at the middle of the second
phase) and configure your systems so that additional load is dropped.
Yes, this means that the DDoS will be more effective, but also means
that your systems can easily get back up to their feet when the attack
has finished (and you get a more predictable load and consequences).&lt;/p&gt;
&lt;p&gt;Investigate if you can have a &lt;strong&gt;backup service&lt;/strong&gt; that has a higher
throughput ability (with reduced functionality). If the DDoS attack
focuses on the system resources rather than network resources involved,
such a backup "lighter" service can be used to still provide basic
functionality (for instance a more static website), but even in case of
network resource consumption it can have the advantage that the network
consumption that your servers are placing (while replying to the
requests) are lower.&lt;/p&gt;
&lt;p&gt;Depending on the service you offer (and financial means you have at your
disposal) you can look at &lt;strong&gt;redirecting traffic&lt;/strong&gt; to more specialized
services. Companies like &lt;a href="http://www.prolexic.com"&gt;Prolexic&lt;/a&gt; have
systems that "scrub" the DDoS traffic from all traffic and only send
legitimate requests to your systems. There are several methods for
redirecting load, but a common one is to change the DNS records for your
service(s) to point to the addresses of those specialized services
instead. The lower the TTL (Time To Live) is of the records, the faster
the redirect might take place. If you want to be able to handle an
increase in load without specialized services, you might want to be able
to redirect traffic to cloud services (where you host your service as
well) which are generally capable of handling higher throughput than
your own equipment (but this too comes at an additional cost).&lt;/p&gt;
&lt;p&gt;Some people mention that you can &lt;strong&gt;switch IP address&lt;/strong&gt;. This is true
only if the DDoS attack is targeting IP addresses and not (DNS-resolved)
URIs. You could set up additional IP addresses that are not registered
in DNS (yet) and during the attack, extend the service resolving towards
the additional addresses as well. If you do not notice a load spread of
the DDoS attack towards the new addresses, you can remove the old
addresses from DNS. But again, this won't work generally - not only are
most DDoS attacks using DNS-resolved URIs, most of the time attackers
are actively involved in the attack and will quickly notice if such a
"failover" has occurred (and react against it).&lt;/p&gt;
&lt;p&gt;Depending on your relationship with your provider or location service,
you can ask if the edge routers (preferably those of the ISP) can have
&lt;strong&gt;fallback source filtering rules&lt;/strong&gt; available to quickly enable. Those
fallback rules would then only allow traffic from networks that you know
most (all?) of your customers and clients are at. This isn't always
possible, but if you have a service that targets mainly people within
your country, have the filter only allow traffic from networks of that
country. If the DDoS attack uses geographically spread resources, it
might be that the number of bots inside those allowed networks are low
enough that your service can continue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configure your firewalls&lt;/strong&gt; (and ask that your ISP does the same) to
not accept (drop) traffic not expected. If the services on your
architecture do not use external DNS, then you can drop incoming DNS
response packets (a popular DDoS attack method is by using spoofed
addresses towards open DNS resolvers; called a DNS reflection attack).&lt;/p&gt;
&lt;p&gt;And finally, if you are not bound to a single data center, you might
want to spread services across &lt;strong&gt;multiple locations&lt;/strong&gt;. Although more
difficult from a management point of view, a dispersed/distributed
architecture allows other services to continue running while one is
being attacked.&lt;/p&gt;</content><category term="Security"></category><category term="ddos"></category><category term="dns"></category><category term="mitigation"></category><category term="security"></category></entry><entry><title>Introducing selocal for small SELinux policy enhancements</title><link href="https://blog.siphos.be/2013/04/introducing-selocal-for-small-selinux-policy-enhancements/" rel="alternate"></link><published>2013-04-21T03:50:00+02:00</published><updated>2013-04-21T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-21:/2013/04/introducing-selocal-for-small-selinux-policy-enhancements/</id><summary type="html">&lt;p&gt;When working with a SELinux-enabled system, administrators will
eventually need to make small updates to the existing policy. Instead of
building their own full policy (always an option, but most likely not
maintainable in the long term) one or more SELinux policy modules are
created (most distributions use a modular …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When working with a SELinux-enabled system, administrators will
eventually need to make small updates to the existing policy. Instead of
building their own full policy (always an option, but most likely not
maintainable in the long term) one or more SELinux policy modules are
created (most distributions use a modular approach to the SELinux policy
development) which are then loaded on their target systems.&lt;/p&gt;
&lt;p&gt;In the past, users had to create their own policy module by creating
(and maintaining) the necessary &lt;code&gt;.te&lt;/code&gt; file(s), building the proper &lt;code&gt;.pp&lt;/code&gt;
files and loading it in the active policy store. In Gentoo, from
&lt;code&gt;policycoreutils-2.1.13-r11&lt;/code&gt; onwards, a script is provided to the users
that hopefully makes this a bit more intuitive for regular users:
&lt;strong&gt;selocal&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As the name implies, &lt;strong&gt;selocal&lt;/strong&gt; aims to provide an interface for
handling &lt;em&gt;local&lt;/em&gt; policy updates that do not need to be packaged or
distributed otherwise. It is a command-line application that you feed
single policy rules at one at a time. Each rule can be accompanied with
a single-line comment, making it obvious for the user to know why he
added the rule in the first place.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal --help
Usage: selocal [] []

Command can be one of:
  -l, --list            List the content of a SELinux module
  -a, --add             Add an entry to a SELinux module
  -d, --delete          Remove an entry from a SELinux module
  -M, --list-modules    List the modules currently known by selocal
  -u, --update-dep      Update the dependencies for the rules
  -b, --build           Build the SELinux module (.pp) file (requires privs)
  -L, --load            Load the SELinux module (.pp) file (requires privs)

Options can be one of:
  -m, --module          Module name to use (default: selocal)
  -c, --comment        Comment (with --add)

The option -a requires that a rule is given, like so:
  selocal -a &amp;quot;dbadm_role_change(staff_r)&amp;quot;
The option -d requires that a line number, as shown by the --list, is given, like so:
  selocal -d 12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's say that you need to launch a small script you written as a
daemon, but you want this to run while you are still in the &lt;em&gt;staff_t&lt;/em&gt;
domain (it is a user-sided daemon you use personally). As regular
&lt;em&gt;staff_t&lt;/em&gt; isn't allowed to have processes bind on generic ports/nodes,
you need to enhance the SELinux policy a bit. With &lt;strong&gt;selocal&lt;/strong&gt;, you can
do so as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal --add &amp;quot;corenet_tcp_bind_generic_node(staff_t)&amp;quot; --comment &amp;quot;Launch local webserv.py daemon&amp;quot;
# selocal --add &amp;quot;corenet_tcp_bind_generic_port(staff_t)&amp;quot; --comment &amp;quot;Launch local webserv.my daemon&amp;quot;
# selocal --build --load
(some output on building the policy module)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When finished, the local policy is enhanced with the two mentioned
rules. You can query which rules are currently stored in the policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal --list
12: corenet_tcp_bind_generic_node(staff_t) # Launch local webserv.py daemon
13: corenet_tcp_bind_generic_port(staff_t) # Launch local webserv.py daemon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you need to delete a rule, just pass the line number:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# selocal --delete 13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Having this tool around also makes it easier to test out changes
suggested through bugreports. When I test such changes, I add in the bug
report ID as the comment so I can track which settings are still local
and which ones have been pushed to our policy repository. Underlyingly,
&lt;strong&gt;selocal&lt;/strong&gt; creates and maintains the necessary policy file in
&lt;path&gt;\~/.selocal&lt;/path&gt; and by default uses the &lt;em&gt;selocal&lt;/em&gt; policy module
name.&lt;/p&gt;
&lt;p&gt;I hope this tool helps users with their quest on using SELinux. Feedback
and comments are always appreciated! It is a small bash script and might
still have a few bugs in it, but I have been using it for a few months
so most quirks should be handled.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="policy"></category><category term="selinux"></category><category term="selocal"></category></entry><entry><title>Transforming GuideXML to DocBook</title><link href="https://blog.siphos.be/2013/04/transforming-guidexml-to-docbook/" rel="alternate"></link><published>2013-04-20T03:50:00+02:00</published><updated>2013-04-20T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-20:/2013/04/transforming-guidexml-to-docbook/</id><summary type="html">&lt;p&gt;I recently
&lt;a href="http://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/docbook.xsl?sortby=date&amp;amp;view=log"&gt;committed&lt;/a&gt;
an XSL stylesheet that allows us to transform the GuideXML documents
(both guides and handbooks) to DocBook. This isn't part of a more
elaborate move to try and push DocBook instead of GuideXML for the
Gentoo Documentation though (I'd rather direct documentation development
more to the Gentoo …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently
&lt;a href="http://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/docbook.xsl?sortby=date&amp;amp;view=log"&gt;committed&lt;/a&gt;
an XSL stylesheet that allows us to transform the GuideXML documents
(both guides and handbooks) to DocBook. This isn't part of a more
elaborate move to try and push DocBook instead of GuideXML for the
Gentoo Documentation though (I'd rather direct documentation development
more to the Gentoo wiki instead once translations are allowed): instead,
I use it to be able to generate our documentation in other formats (such
as PDF but also ePub) when asked.&lt;/p&gt;
&lt;p&gt;If you're not experienced with XSL: XSL stands for &lt;em&gt;Extensible
Stylesheet Language&lt;/em&gt; and can be seen as a way of "programming" in XML. A
stylesheet allows developers to transform one XML document towards
another format (either another XML, or as &lt;a href="http://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"&gt;text-like output like
wiki&lt;/a&gt;)
while manipulating its contents. In case of documentation, we try to
keep as much structure in the document as possible, but other uses could
be to transform a large XML with only a few interesting fields towards a
very small XML (only containing those fields you need) for further
processing.&lt;/p&gt;
&lt;p&gt;For now (and probably for the foreseeable future), the stylesheet is to
be used in an offline mode (we are not going to provide auto-generated
PDFs of all documents) as the process to convert a document from
GuideXML to DocBook to XML:FO to PDF is quite resource-intensive. But
users that are interested can use the stylesheet as linked above to
create their own PDFs of the documentation.&lt;/p&gt;
&lt;p&gt;Assuming you have a checkout of the Gentoo documentation, this process
can be done as follows (example for the AMD64 handbook):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ xsltproc docbook.xsl /path/to/handbook-amd64.xml &amp;gt; /somewhere/handbook-amd64.docbook
$ cd /somewhere
$ xsltproc --output handbook-amd64.fo --stringparam paper.type A4   
 /usr/share/sgml/docbook/xsl-stylesheets/fo/docbook.xsl handbook-amd64.docbook
$ fop handbook-amd64.fo handbook-amd64.pdf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The docbook stylesheets are offered by the
&lt;em&gt;app-text/docbook-xsl-stylesheets&lt;/em&gt; package whereas the &lt;strong&gt;fop&lt;/strong&gt; command
is provided by &lt;em&gt;dev-java/fop&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I have an example output available (temporarily) at my &lt;a href="http://dev.gentoo.org/~swift/tmp/handbook-amd64.pdf"&gt;dev space (amd64
handbook)&lt;/a&gt; but I'm
not going to maintain this for long (so the link might not work in the
near future).&lt;/p&gt;</content><category term="Gentoo"></category><category term="docbook"></category><category term="Gentoo"></category><category term="guidexml"></category><category term="pdf"></category><category term="xsl"></category></entry><entry><title>Comparing performance with sysbench: performance analysis</title><link href="https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-3/" rel="alternate"></link><published>2013-04-19T16:22:00+02:00</published><updated>2013-04-19T16:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-3/</id><summary type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with
the &lt;em&gt;memory&lt;/em&gt; workload test. Let's view this again, using parameters more
optimized to view the regressions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=32M --memory-block-size=64 run
Host:
  Operations performed: 524288 (2988653.44 ops/sec)
  32.00 MB transferred (182.41 MB/sec)

Guest:
  Operations performed: 524288 (24920.74 ops/sec)
  32.00 MB transferred (1.52 MB/sec)

$ sysbench --test=memory --memory-total-size=32M --memory-block-size=32M run
Host:
  Operations performed: 1 (  116.36 ops/sec)
  32.00 MB transferred (3723.36 MB/sec)

Guest:
  Operations performed: 1 (   89.27 ops/sec)
  32.00 MB transferred (2856.77 MB/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From looking at the code (gotta love Gentoo for making this obvious ;-)
we know that the memory workload, with a single thread, does something
like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;total_bytes = 0;
repeat until total_bytes &amp;gt;= memory-total-size:
  thread_mutex_lock()
  total_bytes += memory-block-size
  thread_mutex_unlock()

  (start event timer)
  pointer -&amp;gt; buffer;
  while pointer &amp;lt;-&amp;gt; end-of(buffer)
    write somevalue at pointer
    pointer++
  (stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Given that the regression is most noticeable when the memory-block-size
is very small, the part of the code whose execution count is much
different between the two runs is the mutex locking, global memory
increment and the start/stop of event timer.&lt;/p&gt;
&lt;p&gt;In a second phase, we also saw that mutex locking itself is not
impacted. In the above case, we have 524288 executions. However, if we
run the mutex workload this number of times, we see that this hardly has
any effect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=524288 --mutex-loops=0 run
Host:      total time:        0.0275s
Guest:     total time:        0.0286s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code for the mutex workload, knowing that we run with one thread,
looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mutex_locks = 524288
(start event timer)
do
  lock = get_mutex()
  thread_mutex_lock()
  global_var++
  thread_mutex_unlock()
  mutex_locks--
until mutex_locks = 0;
(stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check if the timer might be the culprit, let's look for a benchmark
that mainly does timer checks. The &lt;em&gt;cpu&lt;/em&gt; workload can be used, when we
tell sysbench that the prime to check is 3 (as its internal loop runs
from 3 till the given number, and when the given number is 3 it skips
the loop completely) and we ask for 524288 executions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=3 --max-requests=524288 run
Host:  total time:  0.1640s
Guest: total time: 21.0306s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Gotcha! Now, the event timer (again from looking at the code) contains
two parts: getting the current time (using &lt;code&gt;clock_gettime()&lt;/code&gt;) and
logging the start/stop (which is done in memory structures). Let's make
a small test application that gets the current time (using the real-time
clock as the sysbench application does) and see if we get similar
results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cat test.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;time.h&amp;gt;

int main(int argc, char **argv, char **arge) {
  struct timespec tps;
  long int i = 524288;
  while (i-- &amp;gt; 0)
    clock_gettime(CLOCK_REALTIME, &amp;amp;tps);
}

$ gcc -lrt -o test test.c
$ time ./test
Host:  0m0.019s
Guest: 0m5.030s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So given that the &lt;code&gt;clock_gettime()&lt;/code&gt; is ran twice in the sysbench, we
already have 10 seconds of overhead on the guest (and only 0,04s on the
host). When such time-related functions are slow, it is wise to take a
look at the clock source configured on the system. On Linux, you can
check this by looking at &lt;code&gt;/sys/devices/system/clocksource/*&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cd /sys/devices/system/clocksource/clocksource0
# cat current_clocksource
kvm-clock
# cat available_clocksource
kvm-clock tsc hpet acpi_pm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although kvm-clock is supposed to be the best clock source, let's switch
to the tsc clock:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# echo tsc &amp;gt; current_clocksource
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If we rerun our test application, we get a much more appreciative
result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ time ./test
Host:  0m0.019s
Guest: 0m0.024s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So, what does that mean for our previous benchmark results?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
Host:            35,3049 sec
Guest (before):  36,5582 sec
Guest (now):     35,6416 sec

$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
Host:            1,8424 MB/sec
Guest (before):  1,5591 MB/sec
Guest (now):     1,5912 MB/sec

$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host:            3959,78 MB/sec
Guest (before)   3079,29 MB/sec
Guest (now):     3821,89 MB/sec

$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:            9765 executions
Guest (before):   512 executions
Guest (now):      529 executions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So we notice that this small change has nice effects on some of the
tests. The CPU benchmark improves from 3,55% overhead to 0,95%; fileio
is the same (from 15,38% to 13,63%), memory improves from 22,24%
overhead to 3,48% and threads remains about status quo (from 94,76%
slower to 94,58%).&lt;/p&gt;
&lt;p&gt;That doesn't mean that the VM is now suddenly faster or better than
before - what we changed was how fast a certain time measurement takes,
which the benchmark software itself uses rigorously. This goes to show
how important it is to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;understand fully how the benchmark software works and measures&lt;/li&gt;
&lt;li&gt;realize the importance of access to source code is not to be
    misunderstood&lt;/li&gt;
&lt;li&gt;know that performance benchmarks give figures, but do not tell you
    how your users will experience the system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it for the sysbench benchmark for now (the MySQL part will need
to wait until a later stage).&lt;/p&gt;</content><category term="Free-Software"></category><category term="performance"></category><category term="sysbench"></category></entry><entry><title>Comparing performance with sysbench: memory, threads and mutexes</title><link href="https://blog.siphos.be/2013/04/comparing-performance-with-sysbench-part-2/" rel="alternate"></link><published>2013-04-19T04:11:00+02:00</published><updated>2013-04-19T04:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-2/</id><summary type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each execution will read or write to this memory
(&lt;em&gt;--memory-oper&lt;/em&gt;, defaults to write) in a random or sequential manner
(&lt;em&gt;--memory-access-mode&lt;/em&gt;, defaults to &lt;strong&gt;seq&lt;/strong&gt;uential).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host throughput, 1M:  3959,78 MB/sec
Guest throughput, 1M: 3079,29 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The guest has a lower throughput (about 77% of the host), which is lower
than what most online posts provide on KVM performance. We'll get back
to that later. Let's look at the default block size of 1k (meaning that
the benchmark will do a lot more executions before it reaches the total
memory (in load):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=1G run
Host throughput, 1k:  1702,59 MB/sec
Guest throughput, 1k:   23,67 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is a lot worse: the guest' throughput is only 1,4% of the host
throughput! The &lt;code&gt;qemu-kvm&lt;/code&gt; process on the host is also taking up a lot
of CPU.&lt;/p&gt;
&lt;p&gt;Now let's take a look at the other workload, &lt;em&gt;threads&lt;/em&gt;. In this
particular workload, you identify the number of threads
(&lt;em&gt;--num-threads&lt;/em&gt;), the number of locks (&lt;em&gt;--thread-locks&lt;/em&gt;) and the number
of times a thread should run its 'lock-yield..unlock' workload
(&lt;em&gt;--thread-yields&lt;/em&gt;). The more locks you identify, the less number of
threads will have the same lock (each thread is allocated a single lock
during an execution, but every new execution will give it a new lock so
the threads do not always take the same lock).&lt;/p&gt;
&lt;p&gt;Note that parts of this is also handled by the other tests: mutex'es are
used when a new operation (execution) for the thread is prepared. In
case of the memory-related workload above, the smaller the buffer size,
the more frequent thread operations are needed. In the last run we did
(with the bad performance), millions of operations were executed
(although no yields were performed). Something similar can be simulated
using a single lock, single thread and a very high number of operations
and no yields:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=1 --thread-yields=0 --max-requests=1000000 --thread-locks=1 run
Host runtime:    0,3267 s  (event:    0,2278)
Guest runtime:  40,7672 s  (event:   30,6084)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This means that the guest "throughput" problems from the memory
identified above seem to be related to this rather than memory-specific
regressions. To verify if the scheduler itself also shows regressions,
we can run more threads concurrently. For instance, running 128 threads
simultaneously, using the otherwise default settings, during 10 seconds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:   9765 executions (events)
Guest:   512 executions (events)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we get only 5% throughput.&lt;/p&gt;
&lt;p&gt;Let's focus on the mutex again, as sysbench has an additional mutex
workload test. The workload has each thread running a local fast loop
(simple increments, &lt;em&gt;--mutex-loops&lt;/em&gt;) after which it takes a random mutex
(one of &lt;em&gt;--mutex-num&lt;/em&gt;), locks it, increments a global variable and then
releases the mutex again. This is repeated for the number of locks
identified (&lt;em&gt;--mutex-locks&lt;/em&gt;). If mutex operations would be the cause of
the performance issues above, then we would notice that the mutex
operations are a major performance regression on my system.&lt;/p&gt;
&lt;p&gt;Let's run that workload with a single thread (default), no loops and a
single mutex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=50000000 --mutex-loops=1 run
Host (duration):   2600,57 ms
Guest (duration):  2571,44 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, we see that the mutex operations are almost at the same
speed (99%) of the host, so pure mutex operations are not likely to be
the cause of the performance regressions earlier on. So what does give
the performance problems? Well, that investigation will be for the third
and last post in this series ;-)&lt;/p&gt;</content><category term="Free-Software"></category><category term="memory"></category><category term="mutex"></category><category term="performance"></category><category term="sysbench"></category><category term="threading"></category><category term="threads"></category></entry><entry><title>Another Gentoo Hardened month has passed</title><link href="https://blog.siphos.be/2013/04/another-gentoo-hardened-month-has-passed/" rel="alternate"></link><published>2013-04-18T23:36:00+02:00</published><updated>2013-04-18T23:36:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-18:/2013/04/another-gentoo-hardened-month-has-passed/</id><summary type="html">&lt;p&gt;Another month has passed, so time to mention again what we have all been
doing lately ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Version 4.8 of GCC is available in the tree, but currently masked. The
package contains a fix needed to build hardened-sources, and a fix for
the asan (address sanitizer).
&lt;a href="http://www.internetnews.com/blog/skerner/open-source-gcc-4.8-compiler-including-address-sanitizer-security.html"&gt;asan&lt;/a&gt;
support in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another month has passed, so time to mention again what we have all been
doing lately ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Version 4.8 of GCC is available in the tree, but currently masked. The
package contains a fix needed to build hardened-sources, and a fix for
the asan (address sanitizer).
&lt;a href="http://www.internetnews.com/blog/skerner/open-source-gcc-4.8-compiler-including-address-sanitizer-security.html"&gt;asan&lt;/a&gt;
support in GCC 4.8 might be seen as an improvement security-wise, but it
is yet unclear if it is an integral part of GCC or could be disabled
with a configure flag. Apparently, asan "makes building gcc 4.8 crazy".
Seeing that it comes from Google, and building Google Chromium is also
crazy, I start seeing a pattern here.&lt;/p&gt;
&lt;p&gt;Anyway, it turns out that PaX/grSec and asan do not get along yet (ASAN
assumes/uses hardcoded userland address space size values, which breaks
when UDEREF is set as it pitches a bit from the size):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ERROR: AddressSanitizer failed to allocate 0x20000001000 (2199023259648) bytes at address 0x0ffffffff000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Given that this is hardcoded in the resulting binaries, it isn't
sufficient to change the size value from 47 bits to 46 bits as hardened
systems can very well boot a kernel with and another kernel without
UDEREF, causing the binaries to fail on the other kernel. Instead, a
proper method would be to dynamically check the size of a userland
address.&lt;/p&gt;
&lt;p&gt;However, GCC 4.8 also brings along some nice enhancements and features.
uclibc profiles work just fine with GCC 4.8, including armv7a and
mips/mipsel. The latter is especially nice to hear, since mips used to
require significant effort with previous GCCs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grSecurity/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;More recent kernels have now been stabilized to stay close to the
grSecurity/PaX upstream developments. The most recent stable kernel now
is hardened-sources-3.8.3. Others still available are hardened-sources
versions 3.2.40-r1 and 2.6.32-r156.&lt;/p&gt;
&lt;p&gt;The support for XATTR_PAX is still progressing, but a few issues have
come up. One is that non-hardened systems are seeing warnings about
&lt;strong&gt;pax-mark&lt;/strong&gt; not being able to set the XATTR_PAX on tmpfs since vanilla
kernels do not have the patch to support &lt;code&gt;user.*&lt;/code&gt; extended attribute
namespaces for tmpfs. A second issue is that the &lt;strong&gt;install&lt;/strong&gt;
application, as provided by &lt;code&gt;coreutils&lt;/code&gt;, does not copy extended
attributes. This has impact on ebuilds where pax markings are done
before the install phase of a package. But only doing pax markings after
the install phase isn't sufficient either, since sometimes we need the
binaries to be marked already for test phases or even in the compile
phase. So this is still something on the near horizon.&lt;/p&gt;
&lt;p&gt;Most likely the necessary tools will be patched to include extended
attributes on copy operations. However, we need to take care only to
copy over those attributes that make sense: &lt;code&gt;user.pax&lt;/code&gt; does, but
security ones like &lt;code&gt;security.evm&lt;/code&gt; and &lt;code&gt;security.selinux&lt;/code&gt; shouldn't as
those are either recomputed when needed, or governed through policy. The
idea is that USE="pax_kernel" will enable the above on &lt;code&gt;coreutils&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The SELinux support in Gentoo has seen a fair share of updates on the
userland utilities (like policycoreutils, setools, libselinux and such).
Most of these have already made the stable tree or are close to be
bumped to stable. The SELinux policy also has been updated a lot: most
changes can be tracked through bugzilla, looking for the
&lt;code&gt;sec-policy r13&lt;/code&gt; whiteboard. The changes can be applied to the system
immediately if you use the live ebuilds (like &lt;code&gt;selinux-base-9999&lt;/code&gt;), but
I'm planning on releasing revision 13 of our policy set soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Some of the "early adopter" problems we've noticed on Gentoo Hardened
have been integrated in the repositories upstream and are slowly
progressing towards the main Linux kernel tree.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All hardened profiles have been moved to the 13.0 base. Some people
frowned when they noticed that the uclibc profiles do not inherit from
any architecture-related profile. This is however with reason: the
architecture profiles are (amongst other reasons) focusing on the glibc
specifics of the architecture. Since the profile intended here is for
uclibc, those changes are not needed (nor wanted). Hence, these are
collapsed in a single profile.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For SELinux, the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;SELinux
handbook&lt;/a&gt;
now includes information about USE="unconfined" as well as the
&lt;code&gt;selinux_gentoo&lt;/code&gt; init script as provided by &lt;code&gt;policycoreutils&lt;/code&gt;. Users who
are already running with SELinux enabled can just look at the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml?part=2&amp;amp;chap=7"&gt;Change
History&lt;/a&gt;
to see which changes affect them.&lt;/p&gt;
&lt;p&gt;A set of &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials"&gt;tutorials&lt;/a&gt;
(which I've blogged about earlier as well) have been put online at the
&lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt;. Next to the SELinux tutorials,
an article pertaining to &lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;AIDE&lt;/a&gt; has
been added as well as it fits nicely within the principles/concepts of
the &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/index.xml"&gt;System
Integrity&lt;/a&gt;
subproject.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Media&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you don't do it already, start following
&lt;a href="https://twitter.com/GentooHardened"&gt;@GentooHardened&lt;/a&gt; ;-)&lt;/p&gt;</content><category term="Gentoo"></category><category term="asan"></category><category term="gcc"></category><category term="Gentoo"></category><category term="grsecurity"></category><category term="hardened"></category><category term="integrity"></category><category term="irc"></category><category term="meeting"></category><category term="pax"></category><category term="selinux"></category><category term="uderef"></category></entry><entry><title>Comparing performance with sysbench: cpu and fileio</title><link href="https://blog.siphos.be/2013/04/comparing-performance-with-sysbench/" rel="alternate"></link><published>2013-04-18T21:31:00+02:00</published><updated>2013-04-18T21:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-18:/2013/04/comparing-performance-with-sysbench/</id><summary type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out what one can do to compare system (and later application)
performance, so I decided to take a look at the various benchmark
utilities available. In this first post, I'll take a look at
&lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; (using 0.4.12, released on March 2009
- unlike what you would think from the looks of the site alone) to
compare the performance of my KVM guest versus host.&lt;/p&gt;
&lt;p&gt;The obligatory system information: the host is a HP Pavilion dv7 3160eb
with an Intel Core i5-430M processor (dual-core with 2 threads per
core). Frequency scaling is disabled - the CPU is fixed at 2.13 Ghz. The
system has 4Gb of memory (DDR3), the internal hard disks are configured
as a software RAID1 and with LVM on top (except for the file system that
hosts the virtual guest images, which is a plain software RAID1). The
guests run with the boot options given below, meaning 1.5Gb of memory, 2
virtual CPUs of the KVM64 type. The CFLAGS for both are given below as
well, together with the expanded set given by &lt;strong&gt;gcc \${CFLAGS} -E -v -
&lt;/dev&gt;&amp;amp;1 | grep cc1&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/bin/qemu-kvm -monitor stdio -nographic -gdb tcp::1301   
 -vnc 127.0.0.1:14   
 -net nic,model=virtio,macaddr=00:11:22:33:44:b3,vlan=0   
 -net vde,vlan=0   
 -drive file=/srv/virt/gentoo/test/pg1.img,if=virtio,cache=none   
 -k nl-be -m 1536 -cpu kvm64 -smp 2

# For host
CFLAGS=&amp;quot;-march=core2 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-D_FORTIFY_SOURCE=2 -fno-strict-overflow -march=core2   
        -fPIE -O2 -fstack-protector-all&amp;quot;
# For guest
CFLAGS=&amp;quot;-march=x86-64 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-fno-strict-overflow -march=x86-64 -fPIE -O2   
        -fstack-protector-all&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I am aware that the CFLAGS between the two are not the same (duh), and I
know as well that the expansion given above isn't entirely accurate. But
still, it gives some idea on the differences.&lt;/p&gt;
&lt;p&gt;Now before I go on to the results, please keep in mind that I am &lt;em&gt;not a
performance expert&lt;/em&gt;, not even a &lt;em&gt;performance experienced&lt;/em&gt; or even
&lt;em&gt;performance wanna-be experienced&lt;/em&gt; person: the more I learn about the
inner workings of an operating system such as Linux, the more complex it
becomes. And when you throw in additional layers such as virtualization,
I'm almost completely lost. In my day-job, some people think they can
"prove" the inefficiency of a hypervisor by counting from 1 to 100'000
and adding the numbers, and then take a look at how long this takes. I
think this is short-sighted, as this puts load on a system that does not
simulate reality. If you really want to do performance measures for
particular workloads, you need to run those workloads and not some small
script you hacked up. That is why I tend to focus on applications that
use workload simulations for infrastructural performance measurements
(like &lt;a href="http://hammerora.sf.net"&gt;HammerDB&lt;/a&gt; for performance testing
databases). But for this blog post series, I'm first going to start with
basic operations and later posts will go into more detail for particular
workloads (such as database performance measurements).&lt;/p&gt;
&lt;p&gt;Oh, and BTW, when I display figures with a comma (","), that comma means
decimal (so "1,00" = "1").&lt;/p&gt;
&lt;p&gt;The figures below are numbers that can be interpreted in many ways, and
can prove everything. I'll sometimes give my interpretation to it, but
don't expect to learn much from it - there are probably much better
guides out there for this. The posts are more of a way to describe how
&lt;strong&gt;sysbench&lt;/strong&gt; works and what you should take into account when doing
performance benchmarks.&lt;/p&gt;
&lt;p&gt;So the testing is done using &lt;strong&gt;sysbench&lt;/strong&gt;, which is capable of running
CPU, I/O, memory, threading, mutex and MySQL tests. The first run of it
that I did was a single-thread run for CPU performance testing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This test verifies prime numbers by dividing the number with
sequentially increasing numbers and verifying that the remainder (modulo
calculation) is zero. If it is, then the number is not prime and the
calculation goes on to the next number; otherwise, if none have a
remainder of 0, then the number is prime. The maximum number that it
divides by is calculated by taking the integer part of the square root
of the number (so for 17, this is 4). This algorithm is very simple, so
you should also take into account that during the compilation of the
benchmark, the compiler might already have optimized some of it.&lt;/p&gt;
&lt;p&gt;Let's look at the numbers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   35,4331   37,0528
     e.total   35,4312   36,8917
1.2    total   35,1482   36,1951
     e.total   35,1462   36,0405
1.3    total   35,3334   36,4266
     e.total   35,3314   36,2640
================================
avg    total   35,3049   36,5582
     e.total   35,3029   36,3987
med    total   35,3334   36,4266
     e.total   35,3314   36,2640
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On average (I did three runs on each system), the guest took 3,55% more
time to finish the test than the host (&lt;code&gt;total&lt;/code&gt;). If we look at the pure
calculation (so not the remaining overhead of the inner workings -
&lt;code&gt;e.total&lt;/code&gt;) then the guest took 3,10% more time. The median however (the
run that wasn't the fastest nor the slowest of the three) has the guest
taking 3,09% more time (total) and 2,64% more time (e.total).&lt;/p&gt;
&lt;p&gt;Let's look at the two-thread results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   17,5185   18,0905
     e.total   35,0296   36,0217
1.2    total   17,8084   18,1070
     e.total   35,6131   36,0518
1.3    total   18,0683   18,0921
     e.total   36,1322   36,0194
================================
avg    total   17,5185   18,0965
     e.total   35,0296   36,0310
med    total   17,8084   18,0921
     e.total   35,6131   36,0194
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With these figures, we notice that the guest average total run time
takes 1,67% more time to complete, and the event time only 1,23%. I was
personally expecting that the guest would have a higher percentage than
previously (gut feeling - never trust it when dealing with complex
matter) but was happy to see that the difference wasn't higher. I'm not
going to start analyze this in more detail and just go to the next test:
fileio.&lt;/p&gt;
&lt;p&gt;In case of fileio testing, I assume that the hypervisor will take up
&lt;a href="http://www.linux-kvm.org/page/Virtio/Block/Latency"&gt;more overhead&lt;/a&gt;, but
keep in mind that you also need to consider the environmental factors:
LVM or not, RAID1 or not, mount options, etc. Since I am comparing
guests versus hosts here, I should look for a somewhat comparable setup.
Hence, I will look for the performance of the host (software raid, LVM,
ext4 file system with data=ordered) and the guest (images on software
raid, ext4 file system with data=ordered and barrier=0, and LVM in
guest).&lt;/p&gt;
&lt;p&gt;Furthermore, running a sysbench test suggests a file that is much larger
than the available RAM. I'm going to run the tests on a 6Gb file size,
but enable O_DIRECT for writes so that some caches (page cache) are not
used. This can be done using &lt;em&gt;--file-extra-flags=direct&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As with all I/O-related benchmarks, you need to define which kind of
load you want to test with. Are the I/Os sequential (like reading or
writing a large file completely) or random? For databases, you are most
likely interested in random reads (data, for selects) and sequential
writes (into transaction logs). A file server usually has random
read/write. In the below test, I'll use a combined &lt;strong&gt;r&lt;/strong&gt;a&lt;strong&gt;nd&lt;/strong&gt;om
&lt;strong&gt;r&lt;/strong&gt;ead/&lt;strong&gt;w&lt;/strong&gt;rite.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=fileio --file-total-size=6G prepare
$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
$ sysbench --test=fileio --file-total-size=6G cleanup
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the output, the throughput seems to be most important:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Operations performed:  4348 Read, 2898 Write, 9216 Other = 16462 Total
Read 67.938Mb  Written 45.281Mb  Total transferred 113.22Mb  (1.8869Mb/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above case, the throughput is 1,8869 Mbps. So let's look at the
(averaged) results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Host:  1,8424 Mbps
Guest: 1,5591 Mbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above figures (which are an average of 3 runs) tell us that the
guest has a throughput of about 84,75% (so we take about 15% performance
hit on random read/write I/O). Now I used sysbench here for some I/O
validation of guest between host, but other usages apply as well. For
instance, let's look at the impact of &lt;code&gt;data=ordered&lt;/code&gt; versus
&lt;code&gt;data=journal&lt;/code&gt; (taken on the host):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;6G, data=ordered, barrier=1: 1,8435 Mbps
6G, data=ordered, barrier=0: 2,1328 Mbps
6G, data=journal, barrier=1: 599,85 Kbps
6G, data=journal, barrier=0: 767,93 Kbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From the figures, we can see that the &lt;code&gt;data=journal&lt;/code&gt; option slows down
the throughput to a final figure about 30% of the original throughput
(70% decrease!). Also, disabling barriers has a positive impact on
performance, giving about 15% throughput gain. This is also why some
people report performance improvements when switching to LVM, as - as
far as I can tell (but finding a good source on this is difficult) - LVM
&lt;em&gt;by default&lt;/em&gt; disables barriers (but does honor the &lt;code&gt;barrier=1&lt;/code&gt; mount
option if you provide it).&lt;/p&gt;
&lt;p&gt;That's about it for now - the next post will be about the memory and
threads tests within sysbench.&lt;/p&gt;</content><category term="Free-Software"></category><category term="cpu"></category><category term="hypervisor"></category><category term="io"></category><category term="kvm"></category><category term="performance"></category><category term="sysbench"></category></entry><entry><title>Simple drawing for I/O positioning</title><link href="https://blog.siphos.be/2013/04/simple-drawing-for-io-positionin/" rel="alternate"></link><published>2013-04-18T01:00:00+02:00</published><updated>2013-04-18T01:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-18:/2013/04/simple-drawing-for-io-positionin/</id><summary type="html">&lt;p&gt;Instead of repeatedly trying to create an overview of the various layers
involved with I/O operations within Linux on whatever white-board is in
the vicinity, I decided to draw one up in &lt;a href="http://www.draw.io"&gt;Draw.io&lt;/a&gt;
that I can then update as I learn more from this fascinating world. The
drawing's …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Instead of repeatedly trying to create an overview of the various layers
involved with I/O operations within Linux on whatever white-board is in
the vicinity, I decided to draw one up in &lt;a href="http://www.draw.io"&gt;Draw.io&lt;/a&gt;
that I can then update as I learn more from this fascinating world. The
drawing's smaller blocks within the layers are meant to give some
guidance to what is handled where, so they are definitely not complete.&lt;/p&gt;
&lt;p&gt;So for those interested (or those that know more of it than I ever will
and prepared to help me out):&lt;/p&gt;
&lt;p&gt;&lt;a href="http://blog.siphos.be/wp-content/uploads/2013/04/io-layers.png"&gt;&lt;img alt="io-layers" src="http://blog.siphos.be/wp-content/uploads/2013/04/io-layers-231x300.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I hope it isn't too far from the truth.&lt;/p&gt;</content><category term="Documentation"></category><category term="io"></category><category term="linux"></category></entry><entry><title>What could SELinux have done to mitigate the postgresql vulnerability?</title><link href="https://blog.siphos.be/2013/04/what-could-selinux-have-done-to-mitigate-the-postgresql-vulnerability/" rel="alternate"></link><published>2013-04-16T14:00:00+02:00</published><updated>2013-04-16T14:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-16:/2013/04/what-could-selinux-have-done-to-mitigate-the-postgresql-vulnerability/</id><summary type="html">&lt;p&gt;&lt;a href="http://www.gentoo.org"&gt;Gentoo&lt;/a&gt; is one of the various distributions
which supports &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux"&gt;SELinux&lt;/a&gt;
as a &lt;em&gt;Mandatory Access Control&lt;/em&gt; system to, amongst other things,
mitigate the results of a succesfull exploit against software. So what
about the recent &lt;a href="http://www.postgresql.org/support/security/faq/2013-04-04/"&gt;PostgreSQL
vulnerability&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;When correctly configured, the PostgreSQL daemon will run in the
&lt;code&gt;postgresql_t&lt;/code&gt; domain. In SELinux-speak …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.gentoo.org"&gt;Gentoo&lt;/a&gt; is one of the various distributions
which supports &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux"&gt;SELinux&lt;/a&gt;
as a &lt;em&gt;Mandatory Access Control&lt;/em&gt; system to, amongst other things,
mitigate the results of a succesfull exploit against software. So what
about the recent &lt;a href="http://www.postgresql.org/support/security/faq/2013-04-04/"&gt;PostgreSQL
vulnerability&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;When correctly configured, the PostgreSQL daemon will run in the
&lt;code&gt;postgresql_t&lt;/code&gt; domain. In SELinux-speak, a domain can be seen as a name
granted to a set of permissions (what is allowed) and assigned to one or
more processes. A process that "runs in domain postgresql_t" will be
governed by the policy rules (what is and isn't allowed) for that
domain.&lt;/p&gt;
&lt;p&gt;The vulnerability we speak of is about creating new files or overwriting
existing files, potentially corrupting the database itself (when the
database files are overwritten). Creating new files is handled through
the &lt;em&gt;create&lt;/em&gt; privilege on files (and &lt;em&gt;add_name&lt;/em&gt; on directories),
writing into files is handled through the &lt;em&gt;write&lt;/em&gt; privilege. Given
certain circumstances, one could even &lt;a href="http://blog.blackwinghq.com/2013/04/08/2/"&gt;write commands inside
files&lt;/a&gt; that are executed by
particular users on the system (btw, the link gives a great explanation
on the vulnerability).&lt;/p&gt;
&lt;p&gt;So let's look at what SELinux does and could have done.&lt;/p&gt;
&lt;p&gt;In the current situation, as we explained, &lt;code&gt;postgresql_t&lt;/code&gt; is the only
domain we need to take into account (the PostgreSQL policy does not use
separate domains for the runtime processes). Let's look at what
directory labels it is allowed to write into:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sesearch -s postgresql_t -c dir -p add_name -SCATd
Found 11 semantic av rules:
   allow postgresql_t postgresql_log_t : dir { add_name } ; 
   allow postgresql_t var_log_t : dir { add_name } ; 
   allow postgresql_t var_lock_t : dir { add_name } ; 
   allow postgresql_t tmp_t : dir { add_name } ; 
   allow postgresql_t postgresql_tmp_t : dir { add_name } ; 
   allow postgresql_t postgresql_var_run_t : dir { add_name } ; 
   allow postgresql_t postgresql_db_t : dir { add_name } ; 
   allow postgresql_t etc_t : dir { add_name } ; 
   allow postgresql_t tmpfs_t : dir { add_name } ; 
   allow postgresql_t var_lib_t : dir { add_name } ; 
   allow postgresql_t var_run_t : dir { add_name } ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So the PostgreSQL service is allowed to create files inside directories
labeled with one of the following labels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;postgresql_log_t&lt;/code&gt;, used for PostgreSQL log files
    (&lt;code&gt;/var/log/postgresql&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var_log_t&lt;/code&gt;, used for the generic log files (&lt;code&gt;/var/log&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var_lock_t&lt;/code&gt;, used for lock files (&lt;code&gt;/run/lock&lt;/code&gt; or &lt;code&gt;/var/lock&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tmp_t&lt;/code&gt;, used for the temporary file directory (&lt;code&gt;/tmp&lt;/code&gt; or
    &lt;code&gt;/var/tmp&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;postgresql_tmp_t&lt;/code&gt;, used for the PostgreSQL temporary
    files/directories&lt;/li&gt;
&lt;li&gt;&lt;code&gt;postgresql_var_run_t&lt;/code&gt;, used for the runtime information (like
    PID files) of PostgreSQL (&lt;code&gt;/var/run/postgresql&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;postgresql_db_t&lt;/code&gt;, used for the PostgreSQL database files
    (&lt;code&gt;/var/lib/postgresql&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;etc_t&lt;/code&gt;, used for the generic system configuration files (&lt;code&gt;/etc/&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var_lib_t&lt;/code&gt;, used for the &lt;code&gt;/var/lib&lt;/code&gt; data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var_run_t&lt;/code&gt;, used for the &lt;code&gt;/var/run&lt;/code&gt; or &lt;code&gt;/run&lt;/code&gt; data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next to this, depending on the label of the directory, the PostgreSQL
service is allowed to write into files with the following label assigned
(of importance to both creating new files as well as overwriting
existing ones):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sesearch -s postgresql_t -c file -p write -SCATd
Found 11 semantic av rules:
   allow postgresql_t postgresql_log_t : file { write } ; 
   allow postgresql_t postgresql_lock_t : file { write } ; 
   allow postgresql_t faillog_t : file { write } ; 
   allow postgresql_t lastlog_t : file { write } ; 
   allow postgresql_t postgresql_tmp_t : file { write } ; 
   allow postgresql_t hugetlbfs_t : file { write } ; 
   allow postgresql_t postgresql_var_run_t : file { write } ; 
   allow postgresql_t postgresql_db_t : file { write } ; 
   allow postgresql_t postgresql_t : file { write } ; 
   allow postgresql_t security_t : file { write } ; 
   allow postgresql_t etc_t : file { write } ;

Found 6 semantic te rules:
   type_transition postgresql_t var_log_t : file postgresql_log_t; 
   type_transition postgresql_t var_lock_t : file postgresql_lock_t; 
   type_transition postgresql_t tmp_t : file postgresql_tmp_t; 
   type_transition postgresql_t tmpfs_t : file postgresql_tmp_t; 
   type_transition postgresql_t var_lib_t : file postgresql_db_t; 
   type_transition postgresql_t var_run_t : file postgresql_var_run_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If an exploit creates a new file, the &lt;em&gt;add_name&lt;/em&gt; permission on the
directory is needed. If otoh the exploit is overwriting existing files,
I think the only permission needed here is the &lt;em&gt;write&lt;/em&gt; on the files
(also &lt;em&gt;open&lt;/em&gt; but all the writes have &lt;em&gt;open&lt;/em&gt; as well in the above case).&lt;/p&gt;
&lt;p&gt;Now accessing and being able to write files into the database file
directory is expected - it is the functionality of the server, so unless
we could separate domains more, this is a "hit" we need to take. Sadly
though, this is also the label used for the PostgreSQL service account
home directory here (not sure if this is for all distributions), making
it more realistic that an attacker writes something in the home
directory &lt;code&gt;.profile&lt;/code&gt; file and hopes for the administrator to do
something like &lt;strong&gt;su postgres -&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Next, the &lt;code&gt;etc_t&lt;/code&gt; write privileges also worry me, not mainly because it
can write there, but also because I can hardly understand why -
PostgreSQL is supposed to run under its own, non-root user (luckily) so
unless there are &lt;code&gt;etc_t&lt;/code&gt; labeled directories owned by the PostgreSQL
service account (or world writeable - please no, kthx). And this isn't
an "inherited" permission from something - the policy currently has
&lt;code&gt;files_manage_etc_files(postgresql_t)&lt;/code&gt; set, and has been since 2005 or
earlier. I'm really wondering if this is still needed.&lt;/p&gt;
&lt;p&gt;But I digress. Given that there are no PostgreSQL-owned directories nor
world-writeable ones in &lt;code&gt;/etc&lt;/code&gt;, let's look at a few other ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;security_t&lt;/code&gt; is used for the SELinux pseudo file system, and is used
    for the SEPostgreSQL support. From the looks of it, only the root
    Linux user has the rights to do really harmful things on this file
    system (and only if he too has write permissions on &lt;code&gt;security_t&lt;/code&gt;),
    non-root should be limited to verifying if contexts exist or have
    particular rights. Still, I might investigate this further as I'm
    intrigued about many of the pseudo files in &lt;code&gt;/sys/fs/selinux&lt;/code&gt; that
    I'm not fully sure yet what they deal with.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tmp_t&lt;/code&gt; should not be a major concern. Most (if not all) daemons and
    services that use temporary files have file transitions to their own
    type so that access to these files, even if it would be allowed by
    regular permissions, is still prohibited by SELinux&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lastlog_t&lt;/code&gt; is also a weird one, again because it shouldn't be
    writeable for anyone else but root accounts; if succesfull, an
    attacker can overwrite the lastlog information which might be used
    by some as a means for debugging who was logged on when (part
    of forensics).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the information above, it is a bit sad to see that SELinux can't
protect PostgreSQL users from this particular vulnerability - most of
the "mitigation" (if any) is because the process runs as non-root to
begin with (which is another hint at users not to think SELinux is
sufficient to restrict the permissions of processes). But could it have
been different?&lt;/p&gt;
&lt;p&gt;In my opinion, yes, and I'll see if we can learn from it for the future.&lt;/p&gt;
&lt;p&gt;First of all, we should do more policy code auditing. It might not be
easy to remove policy rules generally, but we should at least try. I use
a small script that enables auditing (SELinux auditing, so &lt;em&gt;auditallow&lt;/em&gt;
statements) for the entire domain, and then selectively disables
auditing until I get no hits anymore. The remainder of &lt;em&gt;auditallow&lt;/em&gt;
statements warrant a closer look to see if they are still needed or not.
I'll get onto that in the next few days.&lt;/p&gt;
&lt;p&gt;Second, we might want to have service accounts use a different home
directory, where they do have the necessary search privileges for, but
no write privileges. Exploits that write stuff into a home directory
(hoping for a &lt;strong&gt;su postgresql -&lt;/strong&gt;) are then mitigated a bit.&lt;/p&gt;
&lt;p&gt;Third, we might want to look into separating the domains according to
the architecture of the service. This requires intimate knowledge of the
ins and outs of PostgreSQL and might even require PostgreSQL patching,
so is not something light. But if no patching is needed (such as when
all process launches are done using known file executions) we could have
a separate domain for the master process, server processes and perhaps
even the various subfunction processes (like the WAL writer, BG writer,
etc.). The Postfix service has such a more diverse (but also complex)
policy. Such a subdomain structure in the policy might reduce the risk
if the vulnerable process (I think this is the master process) does not
need to write to database files (as this is handled by other processes),
so no &lt;code&gt;postgresql_db_t&lt;/code&gt; write privileges.&lt;/p&gt;
&lt;p&gt;If others have ideas on how we can improve service security (for
instance through SELinux policy development) or knows of other exploits
related to this vulnerability that I didn't come across yet, please give
a comment on it below.&lt;/p&gt;</content><category term="Security"></category><category term="postgresql"></category><category term="selinux"></category><category term="vulnerability"></category></entry><entry><title>Integrity checking with AIDE</title><link href="https://blog.siphos.be/2013/04/integrity-checking-with-aide/" rel="alternate"></link><published>2013-04-11T17:02:00+02:00</published><updated>2013-04-11T17:02:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-11:/2013/04/integrity-checking-with-aide/</id><summary type="html">&lt;p&gt;As to at least do some progress in the integrity part of Gentoo Hardened
(a subproject I'd like to extend towards greater heights), I dediced to
write up a &lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;small guide&lt;/a&gt; on how to
work with &lt;a href="http://aide.sourceforge.net"&gt;AIDE&lt;/a&gt;. The tool is simple enough
(and it allowed me to test its SELinux …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As to at least do some progress in the integrity part of Gentoo Hardened
(a subproject I'd like to extend towards greater heights), I dediced to
write up a &lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;small guide&lt;/a&gt; on how to
work with &lt;a href="http://aide.sourceforge.net"&gt;AIDE&lt;/a&gt;. The tool is simple enough
(and it allowed me to test its SELinux policy module a bit) so you'll
get by fairly quickly.&lt;/p&gt;
&lt;p&gt;However, what I'd like to know a bit more about is on how to use AIDE on
a hypervisor level, scanning through the file systems of the guests,
without needing in-guest daemons. I wrote a small part in the guide, but
I need to test it more thoroughly. In the end, I'd like to have a
configuration that AIDE is running on the host, mounting the guest file
systems, scanning the necessary files and sending out reports, all one
at a time (snapshot, mount, scan+report, unmount, destroy snapshot,
next).&lt;/p&gt;
&lt;p&gt;If anyone has pointers towards such a setup, it'd be greatly
appreciated. It provides, in my opinion, a secure way of scanning
systems even if they are completely compromised (in other words you
couldn't trust anything running inside the guest or running with the
libraries or software within the guest).&lt;/p&gt;</content><category term="Documentation"></category><category term="aide"></category><category term="integrity"></category></entry><entry><title>Not needing run_init for password-less service management</title><link href="https://blog.siphos.be/2013/04/not-needing-run_init-for-password-less-service-management/" rel="alternate"></link><published>2013-04-09T22:14:00+02:00</published><updated>2013-04-09T22:14:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-09:/2013/04/not-needing-run_init-for-password-less-service-management/</id><summary type="html">&lt;p&gt;One of the things that has been bugging me was why, even with having
&lt;code&gt;pam_rootok.so&lt;/code&gt; set in &lt;code&gt;/etc/pam.d/run_init&lt;/code&gt;, I cannot enjoy
passwordless service management without using &lt;strong&gt;run_init&lt;/strong&gt; directly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# rc-service postgresql-9.2 status
Authenticating root.
Password:

# run_init rc-service postgresql-9.2 status
Authenticating root.
 * status: started
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the things that has been bugging me was why, even with having
&lt;code&gt;pam_rootok.so&lt;/code&gt; set in &lt;code&gt;/etc/pam.d/run_init&lt;/code&gt;, I cannot enjoy
passwordless service management without using &lt;strong&gt;run_init&lt;/strong&gt; directly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# rc-service postgresql-9.2 status
Authenticating root.
Password:

# run_init rc-service postgresql-9.2 status
Authenticating root.
 * status: started
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I decided to &lt;strong&gt;strace&lt;/strong&gt; the two commands and look for the
differences. I found out that there is even a SELinux permission for
being able to use the &lt;em&gt;rootok&lt;/em&gt; setting for passwords! Apparently,
&lt;code&gt;pam_rootok.so&lt;/code&gt; is SELinux-aware and does some additional checks.&lt;/p&gt;
&lt;p&gt;Although I don't know the exact details of it, it looks for the context
before the call (exec) of &lt;strong&gt;run_init&lt;/strong&gt; occurred. Then it checks if this
domain has the right for &lt;em&gt;passwd { rootok }&lt;/em&gt; (unless SELinux is in
permissive, in which case it just continues) and only then it allows the
"rootok" to succeed.&lt;/p&gt;
&lt;p&gt;Now why doesn't this work without using &lt;strong&gt;run_init&lt;/strong&gt;? I think it has to
do with how we integrate &lt;strong&gt;run_init&lt;/strong&gt; in the scripts, because out of
the trace I found that the previous context was also &lt;em&gt;run_init_t&lt;/em&gt;
(instead of &lt;em&gt;sysadm_t&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;20451 open(&amp;quot;/proc/self/task/20451/attr/current&amp;quot;, O_RDONLY) = 3
20451 read(3, &amp;quot;root:sysadm_r:run_init_t\0&amp;quot;, 4095) = 25
20451 close(3)                          = 0
20451 gettid()                          = 20451
20451 open(&amp;quot;/proc/self/task/20451/attr/prev&amp;quot;, O_RDONLY) = 3
20451 read(3, &amp;quot;root:sysadm_r:run_init_t\0&amp;quot;, 4095) = 25
20451 close(3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Because there already is a transition to &lt;em&gt;run_init_t&lt;/em&gt; upon calling the
scripts, the underlying call to &lt;strong&gt;runscripts&lt;/strong&gt; causes the "previous"
attribute to be set to &lt;em&gt;run_init_t&lt;/em&gt; as well, and only then is
&lt;strong&gt;run_init&lt;/strong&gt; called (which then causes the PAM functions to be called).
But by prepending the commands with &lt;strong&gt;run_init&lt;/strong&gt; (which quickly causes
the PAM functions to be called) the previous context is &lt;em&gt;sysadm_t&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I tested on a system with the following policy update, and this succeeds
nicely.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(localruninit, 1.0)

gen_require(`
  class passwd { passwd chfn chsh rootok };
  type run_init_t;
&amp;#39;)

allow run_init_t self:passwd rootok;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I'll probably add this in Gentoo's policy.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="hardened"></category><category term="pam"></category><category term="rootok"></category><category term="run_init"></category><category term="selinux"></category></entry><entry><title>How far reaching vulnerabilities can go</title><link href="https://blog.siphos.be/2013/04/how-far-reaching-vulnerabilities-can-go/" rel="alternate"></link><published>2013-04-09T19:39:00+02:00</published><updated>2013-04-09T19:39:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-09:/2013/04/how-far-reaching-vulnerabilities-can-go/</id><summary type="html">&lt;p&gt;If you follow the news a bit, you know that PostgreSQL has had a
significant security vulnerability. The PostgreSQL team announced it up
front and communicated how they would deal with the vulnerability (which
basically comes down to saying that it is severe, that the public
repositories will be temporarily …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you follow the news a bit, you know that PostgreSQL has had a
significant security vulnerability. The PostgreSQL team announced it up
front and communicated how they would deal with the vulnerability (which
basically comes down to saying that it is severe, that the public
repositories will be temporarily frozen as developers add in the
necessary fixes and start building the necessary software for a new
release, and at the release moment give more details about the
vulnerability.&lt;/p&gt;
&lt;p&gt;The exploitability of the vulnerability was quickly identified, and we
know that compromises wouldn't take long. A &lt;a href="http://schemaverse.tumblr.com/post/47312545952/the-schemaverse-was-hacked"&gt;blog
post&lt;/a&gt;
from the schemaverse tells us that exploits won't take long (less than
24 hours) and due to the significance of the vulnerability, it cannot be
stressed enough that patching should really be part of the minimal
security requirements of any security-conscious organization. But
patching alone isn't the only thing to consider.&lt;/p&gt;
&lt;p&gt;The notice that PostgreSQL mentions also that restricting access to the
database through &lt;code&gt;pg_hba.conf&lt;/code&gt; isn't sufficient, as the vulnerable code
is executed before the &lt;code&gt;pg_hba.conf&lt;/code&gt; file is read. So one of the
mitigations for the vulnerability would be a firewall (hostbased or
network) that restricts access to the database so only trusted addresses
are allowed. I'm personally an advocate in favor of hostbased firewalls.&lt;/p&gt;
&lt;p&gt;But the thing that hits me the most, is the amount of applications that
use "embedded" postgresql database services in their product. If you
take part of a larger organization with a large portfolio of software
titles running in the data center, you'll undoubtedly have seen lists
(through network scans or otherwise) of systems that are running
PostgreSQL as part of the product installation (and not as a "managed"
database service). The HP GUIDManager or the NNMI components or the
Systems Insight Manager use embedded PostgreSQL services. The cloudera
manager can be easily set up with an "embedded" PostgreSQL (which
doesn't mean it isn't a full-fledged PostgreSQL, but rather that the
setup and management of the service is handled by the product instead of
by "your own" DBA team). Same with Servoy.&lt;/p&gt;
&lt;p&gt;I don't disagree with all products providing embedded database
platforms, and especially not with choosing for PostgreSQL which I
consider a very mature, stable and feature-rich (and not to be
forgotten, very active community) database platform. But I do hope that
these products take up their responsibility and release updated versions
or patches for their installations to their customers &lt;em&gt;very&lt;/em&gt; soon.&lt;/p&gt;
&lt;p&gt;Perhaps I should ask our security operational team to take a scan to
actively follow-up on these...&lt;/p&gt;</content><category term="Security"></category><category term="firewall"></category><category term="patching"></category><category term="postgresql"></category><category term="security"></category></entry><entry><title>Separate puppet provider for Gentoo/SELinux?</title><link href="https://blog.siphos.be/2013/04/separate-puppet-provider-for-gentooselinux/" rel="alternate"></link><published>2013-04-07T19:22:00+02:00</published><updated>2013-04-07T19:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-07:/2013/04/separate-puppet-provider-for-gentooselinux/</id><summary type="html">&lt;p&gt;While slowly transitioning my playground infrastructure towards Puppet,
I already am in process of creating a custom provider for things such as
services. Puppet uses providers as "implementations" for the functions
Puppet needs. For instance, for the &lt;em&gt;service&lt;/em&gt; type (which handles init
script services), there are providers for RedHat, Debian …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While slowly transitioning my playground infrastructure towards Puppet,
I already am in process of creating a custom provider for things such as
services. Puppet uses providers as "implementations" for the functions
Puppet needs. For instance, for the &lt;em&gt;service&lt;/em&gt; type (which handles init
script services), there are providers for RedHat, Debian, FreeBSD, ...
and it also has providers called &lt;em&gt;gentoo&lt;/em&gt; and &lt;em&gt;openrc&lt;/em&gt;. The &lt;em&gt;openrc&lt;/em&gt; one
uses the service scripts that Gentoo's OpenRC provides, such as
&lt;strong&gt;rc-service&lt;/strong&gt; and &lt;strong&gt;rc-status&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On a SELinux-enabled system, and especially when using a decentralized
Puppet environment (I dropped the puppet master set in favor of a
decentralized usage of Puppet), if you call &lt;strong&gt;rc-service&lt;/strong&gt; to, say,
start a service, it will ask for the users' password. Of course, Puppet
doesn't want this, so I have to prefix the commands with &lt;strong&gt;run_init&lt;/strong&gt;
and have a &lt;code&gt;pam_rootok.so&lt;/code&gt; rule in run_init's PAM definition.&lt;/p&gt;
&lt;p&gt;So far that's a simple change - I just patched the &lt;code&gt;openrc.rb&lt;/code&gt; file to
do so. But then the second problem I'm facing is that Puppet wants to
use return code based commands for checking the run-time state of
services. Even though some of my services weren't running, Puppet either
thought they were or called the start routine and consider the service
started. Sadly that wasn't the case, as the rc-* scripts always return
0 (you'll need to parse the output).&lt;/p&gt;
&lt;p&gt;So what I did now is to create a simple script called &lt;code&gt;runstatus&lt;/code&gt; which
returns the state of services. It's crude, but seems to work:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nv"&gt;SERVICENAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;# We need to exit:&lt;/span&gt;
&lt;span class="c1"&gt;# 0 - if running&lt;/span&gt;
&lt;span class="c1"&gt;# 1 - if dead but PID exists&lt;/span&gt;
&lt;span class="c1"&gt;# 2 - if dead but lock file exists&lt;/span&gt;
&lt;span class="c1"&gt;# 3 - if not running&lt;/span&gt;
&lt;span class="c1"&gt;# 4 - if unknown&lt;/span&gt;

rc-status -a -C &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICENAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -q started &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
rc-status -a -C &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICENAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -q stopped &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;I then have the service provider (I now provide my own instead of
patching the openrc one) call &lt;strong&gt;runstatus&lt;/strong&gt; to get the state of a
service, as well as call it after trying to start a service. But as this
is quite basic functioning, I'm wondering if I'm doing things the right
way or not. Who else has experience with Puppet and Gentoo, and did you
have to tweak things to get services and such working?&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="openrc"></category><category term="provider"></category><category term="puppet"></category><category term="selinux"></category></entry><entry><title>Matching packages with CVEs</title><link href="https://blog.siphos.be/2013/04/matching-packages-with-cves/" rel="alternate"></link><published>2013-04-04T21:44:00+02:00</published><updated>2013-04-04T21:44:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-04:/2013/04/matching-packages-with-cves/</id><summary type="html">&lt;p&gt;I've come across a few posts on forums (Gentoo and elsewhere) asking why
Gentoo doesn't make security-related patches on the tree. Some people
think this is the case because they do not notice (m)any GLSAs, which
are Gentoo's security advisories. However, it isn't that Gentoo doesn't
push out security …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've come across a few posts on forums (Gentoo and elsewhere) asking why
Gentoo doesn't make security-related patches on the tree. Some people
think this is the case because they do not notice (m)any GLSAs, which
are Gentoo's security advisories. However, it isn't that Gentoo doesn't
push out security fixes - it is a matter of putting the necessary human
resources against it to write down the GLSAs.&lt;/p&gt;
&lt;p&gt;Gentoo is often quick with creating the necessary ebuilds for newer
versions of software. And newer versions often contain security fixes
that mitigate problems detected in earlier versions. So by keeping your
system up to date, you get those security fixes as well. But without
GLSA, it is difficult to really know which packages are necessary and
which aren't, let alone be aware that there are potential problems with
your system.&lt;/p&gt;
&lt;p&gt;I already captured one of those needs through the
&lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt; application, so I took a step
further and wrote an extremely ugly script (it's so ugly, it would
spontaneously become a joke of itself when published) which compiles a
list of &lt;em&gt;potential&lt;/em&gt; CPEs (identifiers for products used in CVEs) from
the Gentoo package list (ugliness 1: it assumes that the package name is
the product name). It then tries to &lt;em&gt;assume&lt;/em&gt; what the version of that
software is based on the ebuild version (ugliness 2: it just takes the
a.b.c number). Then, it lists the CVEs affiliated with a particular
package, and checks this list with the list of CVEs from an earlier
version (ugliness 3: it requires the previous, vulnerable version to
still be in the tree). If one of the CVEs has "disappeared", it will
report that the given package might fix that CVE. Oh, and if the CVE has
a CPE that contains more than just a version, the script ignores it
(ugliness 4). And it probably ignores a lot of other things as well
while not checking the input (ugliness 5 and higher).&lt;/p&gt;
&lt;p&gt;But if we ignore all that, what does that give for the Gentoo portage
tree for the last 7 days? In other words, what releases have been made
on the tree that &lt;em&gt;might&lt;/em&gt; contain security fixes (and that do comply with
the above ugliness)?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;app-editors/emacs-23.4-r5 might fix CVE-2010-0825
app-editors/emacs-24.2-r1 might fix CVE-2012-0035
app-editors/emacs-24.2-r1 might fix CVE-2012-3479
dev-lang/python-2.6.8-r1 might fix CVE-2010-3492
dev-lang/python-2.6.8-r1 might fix CVE-2011-1521
dev-lang/python-2.6.8-r1 might fix CVE-2012-0845
dev-lang/python-2.6.8-r1 might fix CVE-2012-1150
dev-lang/python-2.6.8-r1 might fix CVE-2008-5983
dev-php/smarty-2.6.27 might fix CVE-2009-5052
dev-php/smarty-2.6.27 might fix CVE-2009-5053
dev-php/smarty-2.6.27 might fix CVE-2009-5054
dev-php/smarty-2.6.27 might fix CVE-2010-4722
dev-php/smarty-2.6.27 might fix CVE-2010-4723
dev-php/smarty-2.6.27 might fix CVE-2010-4724
dev-php/smarty-2.6.27 might fix CVE-2010-4725
dev-php/smarty-2.6.27 might fix CVE-2010-4726
dev-php/smarty-2.6.27 might fix CVE-2010-4727
dev-php/smarty-2.6.27 might fix CVE-2012-4277
dev-php/smarty-2.6.27 might fix CVE-2012-4437
media-sound/rhythmbox-2.97 might fix CVE-2012-3355
net-im/empathy-3.6.3 might fix CVE-2011-3635
net-im/empathy-3.6.3 might fix CVE-2011-4170
sys-cluster/glusterfs-3.3.1-r2 might fix CVE-2012-4417
www-client/seamonkey-2.17 might fix CVE-2013-0788
www-client/seamonkey-2.17 might fix CVE-2013-0789
www-client/seamonkey-2.17 might fix CVE-2013-0791
www-client/seamonkey-2.17 might fix CVE-2013-0792
www-client/seamonkey-2.17 might fix CVE-2013-0793
www-client/seamonkey-2.17 might fix CVE-2013-0794
www-client/seamonkey-2.17 might fix CVE-2013-0795
www-client/seamonkey-2.17 might fix CVE-2013-0796
www-client/seamonkey-2.17 might fix CVE-2013-0797
www-client/seamonkey-2.17 might fix CVE-2013-0800
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, there is still a lot of work to remove bad ones (and add
matches for non-default ones), but at least it gives an impression
(especially those that have CVEs of 2012 or even 2013 are noteworthy),
which is the purpose of this post.&lt;/p&gt;
&lt;p&gt;It would be very neat if ebuilds, or the package metadata, could give
pointers on the CPEs. That way, it would be much easier to check a
system for known vulnerabilities through the (publicly) available CVE
databases as we then only have to do simple matching. A glsa-check-ng
(what's in a name) script would then construct the necessary CPEs based
on the installed package list (and the metadata on it), check if there
are CVEs against it, and if there are, see if a newer version of the
same package is available that has no (or fewer) CVEs assigned to it.&lt;/p&gt;
&lt;p&gt;Perhaps someone can create a GSoC proposal out of that?&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Linux Sea and ePub update</title><link href="https://blog.siphos.be/2013/04/linux-sea-and-epub-update/" rel="alternate"></link><published>2013-04-02T20:16:00+02:00</published><updated>2013-04-02T20:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-04-02:/2013/04/linux-sea-and-epub-update/</id><summary type="html">&lt;p&gt;I just "published" a small update on the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; online book. Nothing major, some
path updates (like the move to /etc/portage for the make.conf file). But
I wouldn't put a blog post online if there wasn't anything else to say
;-)&lt;/p&gt;
&lt;p&gt;Recently I was made aware that the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just "published" a small update on the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; online book. Nothing major, some
path updates (like the move to /etc/portage for the make.conf file). But
I wouldn't put a blog post online if there wasn't anything else to say
;-)&lt;/p&gt;
&lt;p&gt;Recently I was made aware that the ePub versions I publish were broken.
I don't use ePub readers myself, so all I do is read the ePubs through a
Firefox plug-in and it's been a while that I did that on my own ePubs.
Apparently, the stylesheets I used to convert the Docbook to ePub
changes behavior (or my scripts abused an error in the previous
stylesheets that are fixed now). So right now the
&lt;a href="http://swift.siphos.be/linux_sea/linux_sea.epub"&gt;ePub&lt;/a&gt; version should
work again, and the code snippet below is what I use now to build it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;xsltproc --stringparam base.dir linuxsea-epub/OEBPS/ /usr/share/sgml/docbook/xsl-stylesheets/epub3/chunk.xsl LINUXSEA.xml;
cp -r /path/to/src/linux_sea/images linuxsea-epub/OEBPS;
cd linuxsea-epub;
zip -X0 linux_sea.epub mimetype;
zip -r -X9 linux_sea.epub META-INF OEBPS;
mv linux_sea.epub ../;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Documentation"></category><category term="epub"></category><category term="linux sea"></category><category term="linux_sea"></category></entry><entry><title>Fiddling with puppet apply</title><link href="https://blog.siphos.be/2013/03/fiddling-with-puppet-apply/" rel="alternate"></link><published>2013-03-20T12:31:00+01:00</published><updated>2013-03-20T12:31:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-03-20:/2013/03/fiddling-with-puppet-apply/</id><summary type="html">&lt;p&gt;As part of a larger exercise, I am switching my local VM set from a
more-or-less scripted manual configuration towards a fully
Puppet-powered one. Of course, it still uses a lot of custom modules and
is most likely too ugly to expose to the wider internet, but it does
seem …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As part of a larger exercise, I am switching my local VM set from a
more-or-less scripted manual configuration towards a fully
Puppet-powered one. Of course, it still uses a lot of custom modules and
is most likely too ugly to expose to the wider internet, but it does
seem to improve my ability to quickly rebuild images if I corrupt them
somehow.&lt;/p&gt;
&lt;p&gt;One of the tricks I am using is to use a local apply instead of using a
Puppet master server - mainly because that master server is again a VM
that might need to be build up and consumes some resources that I'd
rather have free for other VMs. So what I do now is akin to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# puppet apply --modulepath /mnt/puppet/modules /mnt/puppet/manifests/site.pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All I have to do is make sure that the /mnt/puppet location is a shared
resource (in my case, an NFSv4 read-only mount) which I can just mount
on a fresh image.&lt;/p&gt;
&lt;p&gt;Part of this exercise I noticed that Puppet by default uses the regular
&lt;em&gt;gentoo&lt;/em&gt; provider for the services. I'd like to use the &lt;em&gt;openrc&lt;/em&gt;
provider instead, as I can easily tweak that one to work with SELinux (I
need to prepend &lt;strong&gt;run_init&lt;/strong&gt; to the &lt;strong&gt;rc-service&lt;/strong&gt; calls, otherwise
SELinux wants to authenticate the user and Puppet doesn't like that; I
have a pam_rootok.so statement in the run_init PAM file to allow
unattended calls towards rc-service).&lt;/p&gt;
&lt;p&gt;A quick Google revealed that all I had to do was to add a &lt;em&gt;provider
=&amp;gt; openrc&lt;/em&gt; in the service definitions, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;service { &amp;quot;net.eth0&amp;quot;:
  provider =&amp;gt; openrc,
  ensure =&amp;gt; running,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As mentioned, I still manually patch the openrc provider (located in
/usr/lib64/ruby/site_ruby/1.9.1/puppet/provider/service) so that the
run_init command is known as well, and that all invocations of the
rc-service is prepended with run_init:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;...
  commands :runinit =&amp;gt; &amp;#39;/usr/sbin/run_init&amp;#39;
  commands :rcservice =&amp;gt; &amp;#39;/sbin/rc-service&amp;#39;
...
 [command(:runinit), command(:rcservice), @resource[:name], :start ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And the same for the stop and status definitions. I might use Portage'
postinst hook to automatically apply the patch so I don't need to do
this manually each time.&lt;/p&gt;</content><category term="Gentoo"></category><category term="provider"></category><category term="puppet"></category><category term="selinux"></category><category term="service"></category></entry><entry><title>SELinux tutorial series, update</title><link href="https://blog.siphos.be/2013/03/selinux-tutorial-series-update/" rel="alternate"></link><published>2013-03-18T23:22:00+01:00</published><updated>2013-03-18T23:22:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-03-18:/2013/03/selinux-tutorial-series-update/</id><summary type="html">&lt;p&gt;Just a small update - the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials"&gt;set of SELinux
tutorials&lt;/a&gt; has been
enhanced since my last blog post about it with information on SELinux
booleans, customizable types, run-time modi (enforcing versus
permissive), some bits about unconfined domains, information on policy
loading, purpose of SELinux roles, SELinux users and an example on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just a small update - the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials"&gt;set of SELinux
tutorials&lt;/a&gt; has been
enhanced since my last blog post about it with information on SELinux
booleans, customizable types, run-time modi (enforcing versus
permissive), some bits about unconfined domains, information on policy
loading, purpose of SELinux roles, SELinux users and an example on how a
policy works regarding init scripts.&lt;/p&gt;
&lt;p&gt;The near future will give more information about the multi-level
security aspect, about multi-category support, a review on the SELinux
context (as we then have handled each field in the context string) and
i'll also start with the second series that focuses more on policy
enhancements and policy building.&lt;/p&gt;
&lt;p&gt;And probably a few dozen more. Happy reading!&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>SELinux tutorial series</title><link href="https://blog.siphos.be/2013/03/selinux-tutorial-series/" rel="alternate"></link><published>2013-03-15T00:34:00+01:00</published><updated>2013-03-15T00:34:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-03-15:/2013/03/selinux-tutorial-series/</id><summary type="html">&lt;p&gt;As we get a growing number of SELinux users within Gentoo Hardened and
because the SELinux usage at the firm I work at is most likely going to
grow as well, I decided to join the bunch of documents on SELinux that
are "out there" and start a series of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we get a growing number of SELinux users within Gentoo Hardened and
because the SELinux usage at the firm I work at is most likely going to
grow as well, I decided to join the bunch of documents on SELinux that
are "out there" and start a series of my own. After all, too much
documentation probably doesn't hurt, and SELinux definitely deserves a
lot of documentation.&lt;/p&gt;
&lt;p&gt;I decided to use the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; for this
endeavour instead of a GuideXML approach (which is the format used for
Gentoo documentation on the main site). The set of tutorials that I
already wrote can be found under the
&lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;SELinux&lt;/a&gt; : &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials"&gt;Gentoo Hardened
SELinux Tutorials&lt;/a&gt;
location. Although of course meant to support the Gentoo Hardened
SELinux users, I'm hoping to keep the initial set of tutorial articles
deliberately distribution-independent so I can refer to them at work as
well.&lt;/p&gt;
&lt;p&gt;For now (this is a week's work, so don't expect this amount of tutorials
to double in the next few days) I wrote about the security context of a
process, how SELinux controls file and directory accesses, where to find
SELinux permission denial details, controlling file contexts yourself
and how a process gets into a certain context.&lt;/p&gt;
&lt;p&gt;I hope I can keep the articles in good shape and with a gradual step-up
in complexity. That does mean that most articles are not complete (for
instance, when talking about domain transitions, I don't talk about
constraints that might prohibit them, or about the role and type
mismatches (invalid context) that you might get, etc.) and that those
details will follow in later articles. Hopefully that allows users to
learn step by step.&lt;/p&gt;
&lt;p&gt;At the end of each tutorial, you will find a "What you need to remember"
section. This is a very short overview of what was said in the tutorial
and that you will need to know in future articles. If you ever read a
tutorial article, then this section might be sufficient for you to
remember again what it was about - no need to reread the entire article.&lt;/p&gt;
&lt;p&gt;Consider it an attempt at a &lt;code&gt;tl;dr&lt;/code&gt; for articles ;-) Enjoy your reading,
and if you have any remarks, don't hesitate to contribute on the wiki or
talk through the "Talk" pages.&lt;/p&gt;</content><category term="SELinux"></category><category term="articles"></category><category term="documentation"></category><category term="Gentoo"></category><category term="hardened"></category><category term="selinux"></category><category term="tutorials"></category><category term="wiki"></category></entry><entry><title>Gentoo Hardened progress meeting of march 2013</title><link href="https://blog.siphos.be/2013/03/gentoo-hardened-progress-meeting-of-march-2013/" rel="alternate"></link><published>2013-03-07T22:46:00+01:00</published><updated>2013-03-07T22:46:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-03-07:/2013/03/gentoo-hardened-progress-meeting-of-march-2013/</id><summary type="html">&lt;p&gt;Another month has passed, so time for a new progress meeting...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Toolchain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GCC v4.7 has been unmasked, allowing a large set of users to test out
the new GCC. It is also expected that GCC 4.8-rc1 will hit the tree next
week. In the hardened-dev overlay, hardened support …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another month has passed, so time for a new progress meeting...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Toolchain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GCC v4.7 has been unmasked, allowing a large set of users to test out
the new GCC. It is also expected that GCC 4.8-rc1 will hit the tree next
week. In the hardened-dev overlay, hardened support for x86, amd64 and
arm has been added (SPEC updates) and the remainder of architectures
will be added by the end of the week.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel and grSecurity/PaX&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kernel 3.7.5 had a security issue (local root privilege escalation) so
3.7.5-r1 which held a fix for this was stabilized quickly. However,
other (non-security) problems have been reported, such as one with
dovecot, regarding the VSIZE memory size. This should be fixed in the
3.8 series, so these are candidate for a faster stabilization. This
faster stabilization is never fun, as it increases the likelihood that
we miss other things, but they are needed as the vulnerability in the
previous stable kernel was too severe.&lt;/p&gt;
&lt;p&gt;Regarding XATTR_PAX, we are getting pretty close to the migration. The
eclass is ready and will be announced for review on the appropriate
mailinglists later this week. A small problem still remains on
Paludis-using systems (Paludis does not record NEEDED.ELF.2 information
- linkage information - so it is hard to get all the linkage information
on a system). A different revdep-pax and migrate-pax toolset will be
built that detects the necessary linkage information, but much slower
than on a Portage-running system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The 11th revision of the policies are now stable, and work is on the way
for the 12th revision which will hit the tree soon. Some work is on the
way for setools and policycoreutils (one due to a new release - setools
- and the other one due to a build failure if PAM is not set). Both
packages will hit the hardened-dev overlay soon.&lt;/p&gt;
&lt;p&gt;A new "edition" of the selinuxnode virtual image has been pushed to the
mirror system, providing a SELinux-enabled (enforcing) Gentoo Hardened
system with grSecurity and PaX, as well as IMA and EVM enabled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The 13.0 profiles have been running fine for a while at a few of our
developer systems. No changes have been needed (yet) so things are
looking good.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System Integrity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The necessary userland utilities have been moved to the main tree. The
documentation for IMA/EVM has been updated as well to reflec the current
state of IMA/EVM within Gentoo Hardened. IMA, even with the custom
policies, seems to be working well. EVM on the other hand has some
issues, so you might need to run with EVM=fix for now. Debugging on this
issue is on the way.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the user oriented documentation (integrity and SELinux) have
been moved to the Gentoo Wiki for easier user contributions and
simplified management. Other documents will follow soon.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="grsecurity"></category><category term="hardened"></category><category term="kernel"></category><category term="pax"></category><category term="profiles"></category><category term="selinux"></category><category term="toolchain"></category></entry><entry><title>Uploading selinuxnode test VM</title><link href="https://blog.siphos.be/2013/02/uploading-selinuxnode-test-vm/" rel="alternate"></link><published>2013-02-25T03:05:00+01:00</published><updated>2013-02-25T03:05:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-02-25:/2013/02/uploading-selinuxnode-test-vm/</id><summary type="html">&lt;p&gt;At the time of writing (but I'll delay the publication of this post a
few hours), I'm uploading a new SELinux-enabled KVM guest image. This is
not an update on the previous image though (it's a reinstalled system -
after all, I use VMs for testing, so it makes sense to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;At the time of writing (but I'll delay the publication of this post a
few hours), I'm uploading a new SELinux-enabled KVM guest image. This is
not an update on the previous image though (it's a reinstalled system -
after all, I use VMs for testing, so it makes sense to reinstall from
time to time to check if the installation instructions are still
accurate). However, the focus remains the same:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A minimal Gentoo Linux installation for amd64 (x86_64) as guest
    within a KVM hypervisor. The image is about 190 Mb in size
    compressed, and 1.6 Gb in size uncompressed. The file format is
    Qemu's QCOW2 so expect the image to grow as you work with it. The
    file systems are, in total, sized to about 50 Gb.&lt;/li&gt;
&lt;li&gt;The installation has SELinux enabled (strict policy, enforcing
    mode), various grSecurity settings enabled (including PaX and TPE),
    but now also includes IMA (Integrity Measurement Architecture) and
    EVM (Extended Verification Module) although EVM is by default
    started in fix mode.&lt;/li&gt;
&lt;li&gt;The image will not start any network-facing daemons by default
    (unlike the previous image) for security reasons (if I let this
    image stay around this long as I did with the previous, it's prone
    to have some vulnerabilities in the future, although I'm hoping I
    can update the image more frequently). This includes SSH, so you'll
    need access to the image console first after which you can configure
    the network and start SSH (&lt;strong&gt;run_init rc-service sshd start&lt;/strong&gt; does
    the trick).&lt;/li&gt;
&lt;li&gt;A couple of default accounts are created, and the image will display
    those accounts and their passwords on the screen (it is a test/play
    VM, not a production VM).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are still a few minor issues with it, that I hope to fix by the
next upload:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=457812"&gt;Bug 457812&lt;/a&gt; is
    still applicable to the image, so you'll notice lots of SELinux
    denials on the mknod capability. They seem to be cosmetic though.&lt;/li&gt;
&lt;li&gt;At shutdown, udev somewhere fails with a SELinux initial
    context problem. I thought I had it covered, but I noticed after
    compressing the image that it is still there. I'll fix it - I
    promise ;)&lt;/li&gt;
&lt;li&gt;EVM is enabled in fix mode, because otherwise EVM is &lt;a href="http://sourceforge.net/mailarchive/forum.php?thread_name=1361476641.29360.114.camel%40falcor1&amp;amp;forum_name=linux-ima-user"&gt;prohibiting
    mode
    changes&lt;/a&gt;
    on files in /run. I still have to investigate this further though -
    I had to use the EVM=fix workaround due to time pressure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When uploaded, I'll ask the Gentoo infrastructure team to synchronise
the image with our mirrors so you can enjoy it. It'll be on the
distfiles, under experimental/amd64/qemu-selinux (it has the 20130224
date in the name, so you can see for yourself if the sync has already
occurred or not).&lt;/p&gt;</content><category term="Gentoo"></category><category term="evm"></category><category term="Gentoo"></category><category term="grsecurity"></category><category term="hardened"></category><category term="ima"></category><category term="kvm"></category><category term="selinux"></category><category term="virtual"></category></entry><entry><title>Working on a new selinuxnode VM</title><link href="https://blog.siphos.be/2013/02/working-on-a-new-selinuxnode-vm/" rel="alternate"></link><published>2013-02-23T14:04:00+01:00</published><updated>2013-02-23T14:04:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-02-23:/2013/02/working-on-a-new-selinuxnode-vm/</id><summary type="html">&lt;p&gt;A long time ago, I made a &lt;a href="http://distfiles.gentoo.org/experimental/amd64/qemu-selinux/"&gt;SELinux enabled
VM&lt;/a&gt; for
people to play with, displaying a minimal Gentoo installation, including
the hardening features it supports (PIE/PIC toolchain, grSecurity, PaX
and SELinux). I'm currently trying to create a new one, which also
includes IMA/EVM, but it looks like …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A long time ago, I made a &lt;a href="http://distfiles.gentoo.org/experimental/amd64/qemu-selinux/"&gt;SELinux enabled
VM&lt;/a&gt; for
people to play with, displaying a minimal Gentoo installation, including
the hardening features it supports (PIE/PIC toolchain, grSecurity, PaX
and SELinux). I'm currently trying to create a new one, which also
includes IMA/EVM, but it looks like I still have many things to
investigate further...&lt;/p&gt;
&lt;p&gt;First of all, I notice that many SELinux domains want to use the mknod
capability, even for domains of which I have no idea whatsoever why they
need it. I don't notice any downsides though, and running in permissive
mode doesn't change the domain behavior. But still, I'm reluctant to
mark them dontaudit as long as I'm not 100% sure.&lt;/p&gt;
&lt;p&gt;Second, the gettys (I think it is the getty) result in a "Cannot change
SELinux context: permission denied" error, even though everything is
running in the right SELinux context. I still have to confirm if it
really is the getty process or something else (the last run I had the
impression it was a udev-related process). But there are no denials and
no SELinux errors in the logs.&lt;/p&gt;
&lt;p&gt;Third, during shutdown, many domains have problems accessing their PID
files in /var/run (which is a link to /run). I most likely need to allow
read privileges on all domains that have access to var_run_t towards
the var_t symlinks. It isn't a problem per se (the processes still run
correctly) but ugly as hell, and if you introduce monitoring it'll go
haywire (as no PID files were either found, or were stale).&lt;/p&gt;
&lt;p&gt;Also, EVM is giving me a hard time, not allowing me to change mode and
ownership in files on /var/run. I have received some feedback from the
IMA user list on this so it is still very much a work-in-progress.&lt;/p&gt;
&lt;p&gt;Finally, the first attempt to generate a new VM resulted in a download
of 817 MB (instead of the 158 MB of the previous release), so I still
have to correct my USE flags and doublecheck the installed applications.
Anyway, definitely to be continued. Too bad time is a scarce resource
:-(&lt;/p&gt;</content><category term="Gentoo"></category><category term="evm"></category><category term="Gentoo"></category><category term="hardened"></category><category term="ima"></category><category term="selinux"></category><category term="selinuxnode"></category><category term="vm"></category></entry><entry><title>Transforming GuideXML to wiki</title><link href="https://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/" rel="alternate"></link><published>2013-02-12T20:12:00+01:00</published><updated>2013-02-12T20:12:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-02-12:/2013/02/transforming-guidexml-to-wiki/</id><summary type="html">&lt;p&gt;The &lt;a href="http://www.gentoo.org"&gt;Gentoo project&lt;/a&gt; has its own &lt;a href="https://wiki.gentoo.org"&gt;official
wiki&lt;/a&gt; for some time now, and we are going to
use it more and more in the next few months. For instance, in the last
Gentoo Hardened meeting, we already discussed that most user-oriented
documentation should be put on the wiki, and I've …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="http://www.gentoo.org"&gt;Gentoo project&lt;/a&gt; has its own &lt;a href="https://wiki.gentoo.org"&gt;official
wiki&lt;/a&gt; for some time now, and we are going to
use it more and more in the next few months. For instance, in the last
Gentoo Hardened meeting, we already discussed that most user-oriented
documentation should be put on the wiki, and I've heard that there are
ideas on moving Gentoo project pages at large towards the wiki. And also
for the regular &lt;a href="http://www.gentoo.org/doc/en/list.xml"&gt;Gentoo
documentation&lt;/a&gt; I will be moving
those guides that we cannot maintain ourselves anymore easily towards
the wiki.&lt;/p&gt;
&lt;p&gt;To support migrations of documents, I created a
&lt;a href="https://github.com/sjvermeu/small.coding/blob/master/gxml2docbook/gxml2wiki.xsl"&gt;gxml2wiki.xsl&lt;/a&gt;
stylesheet. Such a stylesheet can be used, together with tools like
&lt;strong&gt;xsltproc&lt;/strong&gt;, to transform GuideXML documents into text output
&lt;em&gt;somewhat&lt;/em&gt; suitable for the wiki. It isn't perfect (far from it
actually) but at least it allows for a more simple migration of
documents with minor editing afterwards.&lt;/p&gt;
&lt;p&gt;Currently, using it is as simple as invoking it against the GuideXML
document you want to transform:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ xsltproc gxml2wiki.xsl /path/to/document.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output shown on the screen can then be used as a page. The following
things still need to be corrected manually:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whitespace is broken, sometimes there are too many newlines. I had
    to make the decision to put in newlines when needed (which makes too
    many newlines) rather than a few newlines too few (which makes it
    more difficult to find where to add in).&lt;/li&gt;
&lt;li&gt;Links need to be double/triple checked, but i'll try to fix that in
    later editions of the stylesheet&lt;/li&gt;
&lt;li&gt;Commands will have "INTERNAL" in them - you'll need to move the
    commands themselves into the proper location and only put the
    necessary output in the pre-tags. This is because the wiki format
    has more structure than GuideXML in this matter, thus
    transformations are more difficult to write in this regard.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The stylesheet currently automatically adds in a link towards a Server
and security category, but of course you'll need to change that to the
proper category for the document you are converting.&lt;/p&gt;
&lt;p&gt;Happy documentation hacking!&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="guidexml"></category><category term="stylesheet"></category><category term="wiki"></category><category term="xml"></category><category term="xsl"></category></entry><entry><title>Gentoo Hardened goes onward (aka project meeting)</title><link href="https://blog.siphos.be/2013/02/gentoo-hardened-goes-onward-aka-project-meeting/" rel="alternate"></link><published>2013-02-07T23:40:00+01:00</published><updated>2013-02-07T23:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2013-02-07:/2013/02/gentoo-hardened-goes-onward-aka-project-meeting/</id><summary type="html">&lt;p&gt;It's been a while again, so time for another Gentoo Hardened online
progress meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8 is on development stage 4, so the hardened patches will be
worked on next week. Some help on it is needed to test the patches on
ARM, PPC and MIPS though. For …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been a while again, so time for another Gentoo Hardened online
progress meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8 is on development stage 4, so the hardened patches will be
worked on next week. Some help on it is needed to test the patches on
ARM, PPC and MIPS though. For those interested, keep a close eye on the
hardened-dev overlay as those will contain the latest fixes. When GCC
4.9 starts development phase 1, Zorry will again try to upstream the
patches.&lt;/p&gt;
&lt;p&gt;With the coming fixes, we might probably (need to) remove the various
hardenedno* GCC profiles from the hardened Gentoo profiles. This
shouldn't impact too many users as ebuilds add in the correct flags
anyhow (for instance when needing to turn off PIE/PIC).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel, grSecurity and PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The kernel release 3.7.0 that we have stable in our tree has seen a few
setbacks, but no higher version is stable yet (mainly due to the
stabilization period needed). 3.7.4-r1 and 3.7.5 are prime candidates
with good track record,&lt;br&gt;
so we might be stabilizing 3.7.5 in the very near future (next week
probably).&lt;/p&gt;
&lt;p&gt;On the PaX flag migration (you know, from ELF-header based marking to
extended attributes marking), the documentation has seen its necessary
upgrades and the userland utilities have been updated to reflect the use
of xattr markings. The eclass we use for the markings will use the
correct utility based on the environment.&lt;/p&gt;
&lt;p&gt;One issue faced when trying to support both markings is that some
actions (like the "paxctl -Cc" which creates the PT_PAX header if it is
missing) make no sense with the other (as there is no header when using
XATTR_PAX). The eclass will be updated to ignore these flags when
XATTR_PAX is selected.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Revision 10 is stable in the tree, and revision 11 is waiting
stabilization period. A few more changes have been put in the policy
repository already (which are installed when using the live ebuilds) and
will of course be part of&lt;br&gt;
revision 12.&lt;/p&gt;
&lt;p&gt;A change in the userland utilities was also pushed out to allow
permissive domains (so run a single domain in permissive mode instead of
the entire system).&lt;/p&gt;
&lt;p&gt;Finally, the SELinux eclass has been updated to remove SELinux modules
from all defined SELinux module stores if the SELinux policy package is
removed from the system. Before that, the user had to remove the modules
from the store himself manually, but this is error-prone and easily
forgotten, especially for the non-default SELinux policy stores.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All hardened subprofiles are marked as deprecated now (you've seen the
discussions on this on the mailinglist probably on this) so we now have
a sane set of hardened profiles to manage. The subprofiles were used for
things like&lt;br&gt;
"desktop" or "server", whereas users can easily stack their profiles as
they see fit anyhow - so there was little reason for the project to
continue managing those subprofiles.&lt;/p&gt;
&lt;p&gt;Also, now that Gentoo has released its 13.0 profile, we will need to
migrate our profiles to the 13.0 ones as well. So, the idea is to
temporarily support 13.0 in a subprofile, test it thoroughly, and then
remove the subprofile and switch the main one to 13.0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The documentation for IMA and EVM is available on the Gentoo Hardened
project site. They currently still refer to the IMA and EVM subsystems
as development-only, but they are available in the stable kernels now.
Especially the default policy that is available in the kernel is pretty
useful. When you want to consider custom policies (for instance with
SELinux integration) you'll need a kernel patch that is already
upstreamed but not applied to the stable kernels yet.&lt;/p&gt;
&lt;p&gt;To support IMA/EVM, a package called ima-evm-utils is available in the
hardened-dev overlay, which will be moved to the main tree soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As mentioned before, the PaX documentation has seen quite a lot of
updates. Other documents that have seen updates are the Hardened FAQ,
Integrity subproject and SELinux documentation although most of them
were small changes.&lt;/p&gt;
&lt;p&gt;Another suggestion given is to clean up the Hardened project page;
however, there has been some talk within Gentoo to move project pages to
the Gentoo wiki. Such a move might make the suggestion easier to handle.
And while on the subject of the wiki, we might want to move user guides
to the wiki already.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bugs&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Bug &lt;a href="https://bugs.gentoo.org/443630"&gt;443630&lt;/a&gt; refers to segmentation
faults with libvirt when starting Qemu domains on a SELinux-enabled
host. Sadly, I'm not able to test libvirt myself so either someone with
SELinux and libvirt&lt;br&gt;
expertise can chime in, or we will need to troubleshoot it by bug
(using gdb, strace'ing more, ...) which might take quite some time and
is not user friendly...&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Media&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Various talks where held at FOSDEM regarding Gentoo Hardened, and a lot
of people attended those talks. Also the round table was quite
effective, with many users interacting with developers all around. For
next year, chances are very high that we'll give a "What has changed
since last year" session and a round table again.&lt;/p&gt;
&lt;p&gt;With many thanks to the usual suspects: Zorry, blueness, prometheanfire,
lejonet, klondike and the several dozen contributors that are going to
kill me for not mentioning their (nick)names.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="grsecurity"></category><category term="hardened"></category><category term="kernel"></category><category term="meeting"></category><category term="minutes"></category><category term="online"></category><category term="pax"></category><category term="profiles"></category><category term="selinux"></category></entry><entry><title>Why would paid-for support be better?</title><link href="https://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/" rel="alternate"></link><published>2012-12-31T22:46:00+01:00</published><updated>2012-12-31T22:46:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-31:/2012/12/why-would-paid-for-support-be-better/</id><summary type="html">&lt;p&gt;Last Saturday evening, I sent an e-mail to a low-volume mailinglist
regarding IMA problems that I'm facing. I wasn't expecting an answer
very fast of course, being holidays, weekend and a low-volume
mailinglist. But hey - it is the free software world, so I should expect
some slack on this, right …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last Saturday evening, I sent an e-mail to a low-volume mailinglist
regarding IMA problems that I'm facing. I wasn't expecting an answer
very fast of course, being holidays, weekend and a low-volume
mailinglist. But hey - it is the free software world, so I should expect
some slack on this, right?&lt;/p&gt;
&lt;p&gt;Well, not really. I got a reply on sunday - and not just an
acknowledgement e-mail, but a to-the-point answer. It was immediately
correct and described why, and helped me figure out things further. And
this is not a unique case in the free software world: because you are
dealing with the developers and users that have written the code that
you are running/testing, you get a bunch of very motivated souls, all
looking at your request when they can, and giving input when they can.&lt;/p&gt;
&lt;p&gt;Compare that to commercial support from bigger vendors: in these cases,
your request probably gets read by a single person whose state of mind
is difficult to know (but from the communication you often get the
impression that they either couldn't care less or they are swamped with
request tasks so they cannot devote enough time on your request). In
most cases, they check the request for containing the right amount of
information in the right format on the right fields, or even ignore that
you did all that right and just ask you for (the same) information
again. And who knows how many times I had to "state your business
impact".&lt;/p&gt;
&lt;p&gt;Now, I know that commercial support from bigger vendor has the burden of
a huge overload in requests, but is that truely that different in the
free software world? Mailinglists such as the Linux kernel mailinglist
(for kernel development) gets hundreds (thousands?) mails a day, and
those with request for feedback or with questions get a reply quite
swiftly. Mailinglists for distribution users get a lot of traffic as
well, and each and every request is handled with due care and responded
to within a very good timeframe (24h or less most of the time, sometimes
a few days if the user is using a strange or exotic environment that not
everyone knows how to handle).&lt;/p&gt;
&lt;p&gt;I think one of the biggest advantages of the free software world is that
the requests are public. That both teaches the many users on those
mailinglists and fora on how to handle problems they haven't seen
before, as well as allows users to first look for a problem before
reporting it. Everybody wins with this. And because it is public, many
users are happily answering more and more questions because they get the
visibility (with acknowledgements) they deserve: they gain a specific
position in that particular area that others respect, because we can see
how much effort (and good results) they gave earlier on.&lt;/p&gt;
&lt;p&gt;So kudos to the free software world, a happy new year - and keep going
forward.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>IMA and EVM on Gentoo, part 2</title><link href="https://blog.siphos.be/2012/12/ima-and-evm-on-gentoo-part-2/" rel="alternate"></link><published>2012-12-29T23:42:00+01:00</published><updated>2012-12-29T23:42:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-29:/2012/12/ima-and-evm-on-gentoo-part-2/</id><summary type="html">&lt;p&gt;I have been playing with &lt;a href="https://sourceforge.net/apps/mediawiki/linux-ima/index.php?title=Main_Page"&gt;Linux
IMA/EVM&lt;/a&gt;
on a Gentoo Hardened (with SELinux) system for a while and have been
documenting what I think is interesting/necessary for Gentoo Linux users
when they want to use IMA/EVM as well. Note that the documentation of
the Linux IMA/EVM …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been playing with &lt;a href="https://sourceforge.net/apps/mediawiki/linux-ima/index.php?title=Main_Page"&gt;Linux
IMA/EVM&lt;/a&gt;
on a Gentoo Hardened (with SELinux) system for a while and have been
documenting what I think is interesting/necessary for Gentoo Linux users
when they want to use IMA/EVM as well. Note that the documentation of
the Linux IMA/EVM project itself is very decent. It's all on a single
wiki page, but it's decent and I learned a lot from it.&lt;/p&gt;
&lt;p&gt;That being said, I do have the impression that the method they suggest
for generating IMA hashes for the entire system is not always working
properly. It might be because of SELinux on my system, but for now I'm
searching for another method that does seem to work well (I'm currently
trying my luck with a &lt;strong&gt;find ... -exec evmctl&lt;/strong&gt; based command). But once
the hashes are registered, it works pretty well (well, there's a
probably small SELinux problem where loading a new policy or updating
the existing policies seems to generate stale rules and I have to reboot
my system, but I'll find the culprit of that soon ;-)&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/docs/ima-guide.xml"&gt;IMA
Guide&lt;/a&gt;
has been updated to reflect recent findings - including how to load a
custom policy, and I have also started on the &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/docs/evm-guide.xml"&gt;EVM
Guide&lt;/a&gt;.
I think it'll take me a day or three to finish off the rough edges and
then I'll start creating a new SELinux node (KVM) image that users can
use with various Gentoo Hardened-supported technologies enabled (PaX,
grSecurity, SELinux, IMA and EVM).&lt;/p&gt;
&lt;p&gt;So if you're curious about IMA/EVM and willing to try it out on Gentoo
Linux, please have a look at those documents and see if they assist you
(or confuse you even more).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Hardened IMA support</title><link href="https://blog.siphos.be/2012/12/gentoo-hardened-ima-support/" rel="alternate"></link><published>2012-12-27T22:40:00+01:00</published><updated>2012-12-27T22:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-27:/2012/12/gentoo-hardened-ima-support/</id><summary type="html">&lt;p&gt;Adventurous users, contributors and developers can enable the &lt;em&gt;Integrity
Measurement Architecture&lt;/em&gt; subsystem in the Linux kernel with appraisal
(since Linux kernel 3.7). In an attempt to support IMA (and EVM and
other technologies) properly, the &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/index.xml"&gt;System
Integrity&lt;/a&gt;
subproject within &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; was launched a few
months ago. And now …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Adventurous users, contributors and developers can enable the &lt;em&gt;Integrity
Measurement Architecture&lt;/em&gt; subsystem in the Linux kernel with appraisal
(since Linux kernel 3.7). In an attempt to support IMA (and EVM and
other technologies) properly, the &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/index.xml"&gt;System
Integrity&lt;/a&gt;
subproject within &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; was launched a few
months ago. And now that Linux kernel 3.7 is out (and stable) you can
start enjoying this additional security feature.&lt;/p&gt;
&lt;p&gt;With IMA (and IMA appraisal), you are able to protect your system from
offline tampering: modifications made to your files while the system is
offline will be detected as their hash values do not match the hash
values stored in extended attributes (whereas the extended attributes
are then protected through digitally signed values using the EVM
technology).&lt;/p&gt;
&lt;p&gt;I'm working on integrating IMA (and later EVM) properly, which of course
includes the necessary documentation:
&lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/docs/concepts.xml"&gt;concepts&lt;/a&gt;
and a &lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/docs/ima-guide.xml"&gt;ima
guide&lt;/a&gt;
for starters, with more to follow. Be aware though that the integration
is still in its infancy, but any questions and feedback is greatly
appreciated, and bugreports (like &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=448872"&gt;bug
448872&lt;/a&gt;) are definitely
welcome.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Switching policy types in Gentoo/SELinux</title><link href="https://blog.siphos.be/2012/12/switching-policy-types-in-gentooselinux/" rel="alternate"></link><published>2012-12-20T11:31:00+01:00</published><updated>2012-12-20T11:31:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-20:/2012/12/switching-policy-types-in-gentooselinux/</id><summary type="html">&lt;p&gt;When you are running Gentoo with SELinux enabled, you will be running
with a particular policy type, which you can devise from either
&lt;code&gt;/etc/selinux/config&lt;/code&gt; or from the output of the &lt;strong&gt;sestatus&lt;/strong&gt; command. As
a user on our IRC channel had some issues converting his strict-policy
system to mcs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When you are running Gentoo with SELinux enabled, you will be running
with a particular policy type, which you can devise from either
&lt;code&gt;/etc/selinux/config&lt;/code&gt; or from the output of the &lt;strong&gt;sestatus&lt;/strong&gt; command. As
a user on our IRC channel had some issues converting his strict-policy
system to mcs, I thought about testing it out myself. Below are the
steps I did and the reasoning why (and I will update the docs to reflect
this accordingly).&lt;/p&gt;
&lt;p&gt;Let's first see if the type I am running at this moment is indeed
strict, and that the mcs type is defined in the POLICY_TYPES variable.
This is necessary because the &lt;em&gt;sec-policy/selinux-*&lt;/em&gt; packages will then
build the policy modules for the other types referenced in this variable
as well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             strict
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              disabled
Policy deny_unknown status:     denied
Max kernel policy version:      28

test ~ # grep POLICY_TYPES /etc/portage/make.conf
POLICY_TYPES=&amp;quot;targeted strict mcs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you notice that this is not the case, update the &lt;em&gt;POLICY_TYPES&lt;/em&gt;
variable and rebuild all SELinux policy packages using &lt;strong&gt;emerge \$(qlist
-IC sec-policy)&lt;/strong&gt; first.&lt;/p&gt;
&lt;p&gt;Let's see if I indeed have policies for the other types available and
that they are recent (modification date):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # ls -l /etc/selinux/*/policy
/etc/selinux/mcs/policy:
total 408
-rw-r--r--. 1 root root 417228 Dec 19 21:01 policy.27

/etc/selinux/strict/policy:
total 384
-rw-r--r--. 1 root root 392168 Dec 19 21:15 policy.27

/etc/selinux/targeted/policy:
total 396
-rw-r--r--. 1 root root 402931 Dec 19 21:01 policy.27
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great, we're now going to switch to permissive mode and edit the SELinux
configuration file to reflect that we are going to boot (later) into the
mcs policy. Only change the type - I will not boot in permissive mode so
the SELINUX=enforcing can stay.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # setenforce 0

test ~ # vim /etc/selinux/config
[... set SELINUXTYPE=mcs ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can run &lt;strong&gt;sestatus&lt;/strong&gt; to verify the changes, but be aware that -
while the command does say that the mcs policy is loaded, this is not
the case. The mcs policy is just defined as the policy to load:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             mcs
Current mode:                   permissive
Mode from config file:          enforcing
Policy MLS status:              disabled
Policy deny_unknown status:     denied
Max kernel policy version:      28
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So let's load the mcs policy shall we?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # cd /usr/share/selinux/mcs/
test mcs # semodule -b base.pp -i $(ls *.pp | grep -v base | grep -v unconfined)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we are going to relabel all files on the file system, because the
mcs policy adds in another component in the context (a sensitivity label
- always set to 0 for mcs). We will also re-do the &lt;strong&gt;setfiles&lt;/strong&gt; steps
done initially while setting up SELinux on our system. This is because
we need to relabel files that are "hidden" from the current file system
because other file systems are mounted on top of it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test mcs # rlpkg -a -r
Relabeling filesystem types: btrfs ext2 ext3 ext4 jfs xfs
Scanning for shared libraries with text relocations...
0 libraries with text relocations, 0 not relabeled.
Scanning for PIE binaries with text relocations...
0 binaries with text relocations detected.

test mcs # mount -o bind / /mnt/gentoo
test mcs # setfiles -r /mnt/gentoo /etc/selinux/mcs/contexts/files/file_contexts /mnt/gentoo/dev
test mcs # setfiles -r /mnt/gentoo /etc/selinux/mcs/contexts/files/file_contexts /mnt/gentoo/lib64
test mcs # umount /mnt/gentoo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, edit &lt;code&gt;/etc/fstab&lt;/code&gt; and change all &lt;em&gt;rootcontext=&lt;/em&gt; parameters to
include a trailing &lt;code&gt;:s0&lt;/code&gt;, otherwise the root contexts of these file
systems will be illegal (in the mcs-sense) as they do not contain the
sensitivity level information.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test mcs # vim /etc/fstab
[... edit rootcontext&amp;#39;s to now include &amp;quot;:s0&amp;quot; ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There ya go. Now reboot and notice that all is okay, and we're running
with the mcs policy loaded.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;test ~ # id -Z
root:sysadm_r:sysadm_t:s0-s0:c0.c1023
test ~ # sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             mcs
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     denied
Max kernel policy version:      28
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Gentoo"></category></entry><entry><title>Another hardened month has passed...</title><link href="https://blog.siphos.be/2012/12/another-hardened-month-has-passed/" rel="alternate"></link><published>2012-12-13T10:02:00+01:00</published><updated>2012-12-13T10:02:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-13:/2012/12/another-hardened-month-has-passed/</id><summary type="html">&lt;p&gt;... so it's time for a new update ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8 is still in its stage 3 development phase, so Zorry will send
out the patches to the GCC development community when this phase is
done. For Gentoo hardened itself, we now support all architectures
except for IA64 (which never …&lt;/p&gt;</summary><content type="html">&lt;p&gt;... so it's time for a new update ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Toolchain&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;GCC 4.8 is still in its stage 3 development phase, so Zorry will send
out the patches to the GCC development community when this phase is
done. For Gentoo hardened itself, we now support all architectures
except for IA64 (which never had SSP).&lt;/p&gt;
&lt;p&gt;Full uclibc support is now in place for amd64, i686, mips32r2: not only
is their technological support ok, but stages are now also automatically
built to support installations through the regular installation
instructions. The next target to get stages automatically built for is
armv7a.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kernel and grSecurity/PaX&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Stabilization on 3.6.x is still showing some difficulties. Until those
are resolved, we're still stable in 3.5.4. We have a couple of panics in
some odd cases, but these will need to be resolved before we can
stabilize further.&lt;/p&gt;
&lt;p&gt;glibc-2.16 will also drop the declarations for PT_PAX (in elf.h) and
the binutils will also not cover PT_PAX phdr anymore. So, we will
standardize fully on xattr-based PaX flags. This will get some proper
focus in the next period to ensure this is done correctly. Most work on
this support is focusing on communication towards users and the
pax-utils eclass support.&lt;/p&gt;
&lt;p&gt;There was some confusion if the tmpfs-xattr patch would or would not
properly restrict access, but it looks like the PaX patch on mm/shmem.c
was based upon the Gentoo patch and enhanced with the needed
restrictions, so we can just keep the PaX code.&lt;/p&gt;
&lt;p&gt;On USE="pax_kernel", which should enable some updates on userland
utilities when applications are run under a PaX enabled kernel,
prometheanfire tried to get this as a global USE flag (as many
applications might eventually want to get a trigger on it). However, due
to some confusion on the meaning of the USE flag, and potential need to
depend on additional tools, we're going to stick with a local flag for
now.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;SELinux&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;schmitt953 will help in the testing and possible development of SELinux
policies for Samba 4.&lt;/p&gt;
&lt;p&gt;Furthermore, the userspace utilities have been stabilized (except for
the setools-3.3.7-r5+ due to some swig problems, but those have been
worked around in setools-3.3.7-r6). Also, the rev8 policies are in the
tree and no big problems were reported on them. They are currently still
\~arch, but will be stabilized in the next few days. A new rev9 release
will be pushed to the hardened-dev overlay soon as well.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profiles&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;nvidia is unmasked for the hardened profiles, but still has X and tools
USE flags masked, and is only supported on kernels 3.0.x and higher.&lt;/p&gt;
&lt;p&gt;Also, the hardened/linux/uclibc/arm/armv7a profile is now available as a
development profile. Profiles will be updated as the architectures for
ARM are getting supported, so expect more in the next month.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;System Integrity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We were waiting for kernel 3.7, which just got released, so we can now
start integrating this further. Expect more updates by next meeting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Docs&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For SELinux, some information on USE="unconfined" is added to the
SELinux handbook. Blueness will also start documenting the xattr pax
stuff.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Using pam_selinux to switch contexts</title><link href="https://blog.siphos.be/2012/12/using-pam_selinux-to-switch-contexts/" rel="alternate"></link><published>2012-12-10T22:11:00+01:00</published><updated>2012-12-10T22:11:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-10:/2012/12/using-pam_selinux-to-switch-contexts/</id><summary type="html">&lt;p&gt;With SELinux managing the access controls of applications towards the
resources on the system, a not-to-be forgotten important component on
any Unix/Linux system is the authentication part. Most systems use or
support PAM, the &lt;em&gt;Pluggable Authentication Modules&lt;/em&gt;, and for SELinux
this plays an important role.&lt;/p&gt;
&lt;p&gt;Applications that are PAM-enabled …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With SELinux managing the access controls of applications towards the
resources on the system, a not-to-be forgotten important component on
any Unix/Linux system is the authentication part. Most systems use or
support PAM, the &lt;em&gt;Pluggable Authentication Modules&lt;/em&gt;, and for SELinux
this plays an important role.&lt;/p&gt;
&lt;p&gt;Applications that are PAM-enabled use PAM for the authentication of user
activities. If this includes setting up an authenticated session, then
the "session" part of the PAM configuration is also handled. And for
SELinux, this is a nice-to-have, since this means applications that are
not SELinux-aware can still enjoy transitions towards specified domains
depending on the user that is authenticated.&lt;/p&gt;
&lt;p&gt;The "not SELinux-aware" here is important. By default, applications keep
running in one security context for their lifetime. If they invoke a
&lt;code&gt;execve&lt;/code&gt; or similar call (which is used to start another application or
command when used in combination with a &lt;code&gt;fork&lt;/code&gt;), then the SELinux policy
&lt;em&gt;might&lt;/em&gt; trigger an automatic transition if the holy grail of fourfold
rules is set:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a transition from the current context to the new one is allowed&lt;/li&gt;
&lt;li&gt;the label of the executed command/label is marked as an entrypoint
    for the new context&lt;/li&gt;
&lt;li&gt;the current context is allowed to execute that application&lt;/li&gt;
&lt;li&gt;an automatic transition rule is made from the current context to the
    new one over the command label&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Or, in SELinux policy terms, assuming the domains are &lt;code&gt;source_t&lt;/code&gt; and
&lt;code&gt;destination_t&lt;/code&gt; with the label of the executed file being &lt;code&gt;file_exec_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow source_t destination_t:process transition;
allow destination_t file_exec_t:file entrypoint;
allow source_t file_exec_t:file execute;
type_transition source_t file_exec_t : process destination_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If those four settings are valid, then (and only then) can the automatic
transition be active.&lt;/p&gt;
&lt;p&gt;Sadly, for applications that run user actions (like cron systems, remote
logon services and more) this is not sufficient, since there are two
major downsides to this "flexibility":&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rules to transition are static and do not depend on the identity
    of the user for which activities are launched. The policy can not
    deduce this identity from a file context either.&lt;/li&gt;
&lt;li&gt;The policy is statically defined: different transitions based on
    different user identities are not possibel.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To overcome this problem, applications can be made SELinux-aware,
linking with the libselinux library and invoking the necessary switches
themselves (or running the commands with &lt;code&gt;runcon&lt;/code&gt;). Luckily, this is
where the PAM system comes to play to aide us in setting up this policy
behavior.&lt;/p&gt;
&lt;p&gt;When an application is PAM-enabled, it will invoke PAM calls to
authenticate and possibly set up the user session. The actions that PAM
invokes are defined by the PAM configuration files. For instance, for
the at daemon:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## /etc/pam.d/atd
#
# The PAM configuration file for the at daemon
#

auth    required        pam_env.so
auth    include         system-services
account include         system-services
session include         system-services
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I am not going to dive into the details of PAM in this blog post, so
let's just jump to the session management part. In the above example
file, if PAM sets up (or shuts down) a user session for the service (at
in our case), it will go through the PAM services that are listed in the
&lt;em&gt;system-services&lt;/em&gt; definition, which looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## /etc/pam.d/system-services
auth            sufficient      pam_permit.so
account         include         system-auth
session         optional        pam_loginuid.so
session         required        pam_limits.so 
session         required        pam_env.so 
session         required        pam_unix.so 
session         optional        pam_permit.so
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Until now, nothing SELinux-specific is enabled. But if we change the
session section of the at service to the following, then the SELinux pam
module will be called as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;session optional        pam_selinux.so close
session include         system-services
session optional        pam_selinux.so multiple open
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now that the SELinux module is called, pam_selinux will try to switch
the context of the process based on the definitions in the
/etc/selinux/strict/contexts location (substitute strict with the policy
type you use). The outcome of this switching can be checked with the
&lt;strong&gt;getseuser&lt;/strong&gt; application:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# getseuser root system_u:system_r:crond_t
seuser:  root, level (null)
Context 0       root:sysadm_r:cronjob_t
Context 1       root:staff_r:cronjob_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By providing the contexts in configurable files in
/etc/selinux/strict/contexts, a non-SELinux aware application suddenly
becomes SELinux-aware (through the PAM support it already has) without
needing to patch or even rebuild the application. All that is need is to
allow the security context of the application to switch ids and roles
(as that is by default not allowed), which I believe is offered through
the following statements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;domain_subj_id_change_exemption(atd_t)
domain_role_change_exemption(atd_t)

selinux_validate_context(atd_t)
selinux_compute_access_vector(atd_t)
selinux_compute_create_context(atd_t)
selinux_compute_relabel_context(atd_t)
selinux_compute_user_contexts(atd_t)

seutil_read_config(atd_t)
seutil_read_default_contexts(atd_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category></entry><entry><title>Using stunnel for mutual authentication</title><link href="https://blog.siphos.be/2012/12/using-stunnel-for-mutual-authentication/" rel="alternate"></link><published>2012-12-08T14:24:00+01:00</published><updated>2012-12-08T14:24:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-08:/2012/12/using-stunnel-for-mutual-authentication/</id><summary type="html">&lt;p&gt;Sometimes services do not support SSL/TLS, or if they do, they do not
support using mutual authentication (i.e. requesting that the client
also provides a certificate which is trusted by the service). If that is
a requirement in your architecture, you can use &lt;strong&gt;stunnel&lt;/strong&gt; to provide
this additional …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes services do not support SSL/TLS, or if they do, they do not
support using mutual authentication (i.e. requesting that the client
also provides a certificate which is trusted by the service). If that is
a requirement in your architecture, you can use &lt;strong&gt;stunnel&lt;/strong&gt; to provide
this additional SSL/TLS layer.&lt;/p&gt;
&lt;p&gt;As an example, I have a mail server running on localhost, and I want to
provide SSMTP services with mutual authentication on top of this
service, using stunnel. First of all, I provide two certificates and
private keys that are both signed by the same CA, and keep the CA
certificate close as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;client.key is the private key for the client&lt;/li&gt;
&lt;li&gt;client.pem is the certificate for the client (which contains the
    public key and CA signature)&lt;/li&gt;
&lt;li&gt;server.key and server.pem are the same but for the server&lt;/li&gt;
&lt;li&gt;root-genfic.crt is the certificate of the signing CA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, we setup the stunnel, listening on 1465 (as 465 requires
the stunnel service to run as root, which I'd rather not) and fowarding
towards 127.0.0.1:25:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cert = /etc/ssl/services/stunnel/server.pem
key = /etc/ssl/services/stunnel/server.key
setuid = stunnel
setgid = stunnel
pid = /var/run/stunnel/stunnel.pid
socket = l:TCP_NODELAY=1
socket = r:TCP_NODELAY=1
verify = 2 # This enables the mutual authentication
CAfile = /etc/ssl/certs/root-genfic.crt

[smtp]
accept = 1465
connect = 127.0.0.1:25
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To test out mutual authentication this way, I used the following
command-line snippet. The delays between the lines are because the mail
client is supposed to wait for the mail server to give its reply and if
not, the data gets lost. I'm sure this can be made easier (with netcat I
could just use "-i 1" to print a line with a one-second delay), but it
works ;-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$  (sleep 1; echo &amp;quot;EHLO localdomain&amp;quot;; sleep 1; echo &amp;quot;MAIL FROM:remote@test.localdomain&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;sleep 1; echo "RCPT TO:user@localhost"; sleep 1; echo "DATA"; sleep 1; cat TEMPFILE) | &lt;br&gt;
   openssl s_client -connect 192.168.100.102:1465 -crlf -ign_eof -ssl3 -key client.key -cert client.pem&lt;/p&gt;
&lt;p&gt;The TEMPFILE file contains the email content (you know, Subject, From,
To, other headers, data, ...).&lt;/p&gt;
&lt;p&gt;If the provided certificate isn't trusted, then you'll find the
following in the log file (on Gentoo, thats /var/log/daemon.log by
default but you can setup logging in stunnel as well):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Dec  8 13:17:32 testsys stunnel: LOG7[20237:2766895953664]: Starting certificate verification: depth=0, /C=US/ST=California/L=Santa Barbara/O=SSL Server/OU=For Testing Purposes Only/CN=localhost/emailAddress=root@localhost
Dec  8 13:17:32 testsys stunnel: LOG4[20237:2766895953664]: CERT: Verification error: unable to get local issuer certificate
Dec  8 13:17:32 testsys stunnel: LOG4[20237:2766895953664]: Certificate check failed: depth=0, /C=US/ST=California/L=Santa Barbara/O=SSL Server/OU=For Testing Purposes Only/CN=localhost/emailAddress=root@localhost
Dec  8 13:17:32 testsys stunnel: LOG7[20237:2766895953664]: SSL alert (write): fatal: bad certificate
Dec  8 13:17:32 testsys stunnel: LOG3[20237:2766895953664]: SSL_accept: 140890B2: error:140890B2:SSL routines:SSL3_GET_CLIENT_CERTIFICATE:no certificate returned
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When a trusted certificate is shown, the connection goes through.&lt;/p&gt;
&lt;p&gt;Finally, if you not only want to validate if the certificate is trusted,
but also only want to accept a given number of certificates, you can set
the stunnel variable &lt;em&gt;verify&lt;/em&gt; to 3. If you set it to 4, it will not
check the CA and only allow a connection to go through if the presented
certificate is one in the stunnel trusted certificates.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>nginx as reverse SMTP proxy</title><link href="https://blog.siphos.be/2012/12/nginx-as-reverse-smtp-proxy/" rel="alternate"></link><published>2012-12-06T00:03:00+01:00</published><updated>2012-12-06T00:03:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-12-06:/2012/12/nginx-as-reverse-smtp-proxy/</id><summary type="html">&lt;p&gt;I've noticed that not that many resources are online telling you how you
can use nginx as a reverse SMTP proxy. Using a reverse SMTP proxy makes
sense even if you have just one mail server back-end, either because you
can easily switch towards another one, or because you want …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've noticed that not that many resources are online telling you how you
can use nginx as a reverse SMTP proxy. Using a reverse SMTP proxy makes
sense even if you have just one mail server back-end, either because you
can easily switch towards another one, or because you want to put
additional checks before handing off the mail to the back-end.&lt;/p&gt;
&lt;p&gt;In the below example, a back-end mail server is running on localhost (in
my case it's a Postfix back-end, but that doesn't matter). Mails
received by Nginx will be forwarded to this server.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;user nginx nginx;
worker_processes 1;

error_log /var/log/nginx/error_log debug;

events {
        worker_connections 1024;
        use epoll;
}
http {

        log_format main
                &amp;#39;$remote_addr - $remote_user [$time_local] &amp;#39;
                &amp;#39;&amp;quot;$request&amp;quot; $status $bytes_sent &amp;#39;
                &amp;#39;&amp;quot;$http_referer&amp;quot; &amp;quot;$http_user_agent&amp;quot; &amp;#39;
                &amp;#39;&amp;quot;$gzip_ratio&amp;quot;&amp;#39;;


        server {
                listen 127.0.0.1:8008;
                server_name localhost;
                access_log /var/log/nginx/localhost.access_log main;
                error_log /var/log/nginx/localhost.error_log info;

                root /var/www/localhost/htdocs;

                location ~ .php$ {
                        add_header Auth-Server 127.0.0.1;
                        add_header Auth-Port 25;
                        return 200;
                }
        }
}

mail {
        server_name localhost;

        auth_http localhost:8008/auth-smtppass.php;

        server {
                listen 192.168.100.102:25;
                protocol smtp;
                timeout 5s;
                proxy on;
                xclient off;
                smtp_auth none;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you first look at the &lt;em&gt;mail&lt;/em&gt; setting, you notice that I include an
&lt;em&gt;auth_http&lt;/em&gt; directive. This is needed by Nginx as it will consult this
back-end service on what to do with the mail (the moment that it
receives the recipient information). The URL I use is arbitrarily chosen
here, as I don't really run a PHP service in the background (yet).&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;http&lt;/em&gt; section, I create the same resource that the mails'
auth_http wants to connect to. I then declare the two return headers
that Nginx needs (Auth-Server and Auth-Port) with the back-end
information (127.0.0.1:25). If I ever need to do load balancing or other
tricks, I'll write up a simple PHP script and serve it from PHP-FPM or
so.&lt;/p&gt;
&lt;p&gt;Next on the list is to enable SSL (not difficult) with client
authentication (which isn't supported by Nginx for the mail module (yet)
sadly, so I'll need to look at a different approach for that).&lt;/p&gt;
&lt;p&gt;BTW, this is all on a simple Gentoo Hardened with SELinux enabled. The
following booleans were set to true: &lt;em&gt;nginx_enable_http_server&lt;/em&gt;,
&lt;em&gt;nginx_enable_smtp_server&lt;/em&gt; and &lt;em&gt;nginx_can_network_connect_http&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This page has been translated into
&lt;a href="http://www.webhostinghub.com/support/es/misc/nginx-como-poder"&gt;Spanish&lt;/a&gt;
language by Maria Ramos from
&lt;a href="http://www.webhostinghub.com/support/edu"&gt;Webhostinghub.com/support/edu&lt;/a&gt;.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Why you need the real_* thing with genkernel</title><link href="https://blog.siphos.be/2012/11/why-you-need-the-real_-thing-with-genkernel/" rel="alternate"></link><published>2012-11-25T21:05:00+01:00</published><updated>2012-11-25T21:05:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-11-25:/2012/11/why-you-need-the-real_-thing-with-genkernel/</id><summary type="html">&lt;p&gt;Today it bit me. I rebooted my workstation, and all hell broke loose.
Well, actually, it froze. Literally, if you consider my root file
system. When the system tried to remount the root file system
read-write, it gave me this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mount: / not mounted or bad option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I did the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today it bit me. I rebooted my workstation, and all hell broke loose.
Well, actually, it froze. Literally, if you consider my root file
system. When the system tried to remount the root file system
read-write, it gave me this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mount: / not mounted or bad option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I did the first thing that always helps me, and that is to disable
the initramfs booting and boot straight from the kernel. Now for those
wondering why I boot with an initramfs while it still works directly
with a kernel: it's a safety measure. Ever since there are talks,
rumours, fear, uncertainty and doubt about supporting a separate /usr
file system I started supporting an initramfs on my system in case an
update really breaks the regular boot cycle. Same because I use lvm on
most file systems, and software RAID on all of them. If I wouldn't have
an initramfs laying around, I would be screwed the moment userspace
decides not to support this straight from a kernel boot. Luckily, this
isn't the case (yet) so I could continue working without an initramfs.
But I digress. Back to the situation.&lt;/p&gt;
&lt;p&gt;Booting without initramfs worked without errors of any kind. Next thing
is to investigate why it fails. I reboot back with the initramfs, get my
read-only root file system and start looking around. In my &lt;strong&gt;dmesg&lt;/strong&gt;
output, I notice the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;EXT4-fs (md3): Cannot change data mode on remount
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So that's weird, not? What is this data mode? Well, the &lt;a href="https://www.kernel.org/doc/Documentation/filesystems/ext4.txt"&gt;data
mode&lt;/a&gt;
tells the file system (ext4 for me) how to handle writing data to disk.
As you are all aware, ext4 is a journaled file system, meaning it writes
changes into a journal before applying, allowing changes to be replayed
when the system suddenly crashes. By default, ext4 uses ordered mode,
writing the metadata (information about files and such, like inode
information, timestamps, block maps, extended attributes, ... but not
the data itself) to the journal right after writing data to the disk,
after which the metadata is then written to disk as well.&lt;/p&gt;
&lt;p&gt;On my system though, I use &lt;code&gt;data=journal&lt;/code&gt; so data too is written to the
journal first. This gives a higher degree of protection in case of a
system crash (or immediate powerdown - my laptop doesn't recognize
batteries anymore and with a daughter playing around, I've had my share
of sudden powerdowns). I do boot with the &lt;code&gt;rootflags=data=journal&lt;/code&gt; and I
have &lt;code&gt;data=journal&lt;/code&gt; in my fstab.&lt;/p&gt;
&lt;p&gt;But the above error tells me otherwise. It tells me that the mode is not
what I want it to be. So after fiddling a bit with the options and (of
course) using Google to find more information, I found out that my
initramfs doesn't check the &lt;e&gt;rootflags&lt;/e&gt; parameter, so it mounts the
root file system with the standard (ordered) mode. Trying to remount it
later will fail, as my fstab contains the &lt;code&gt;data=journal&lt;/code&gt; tag, and
running &lt;strong&gt;mount -o remount,rw,data=ordered&lt;/strong&gt; for fun doesn't give many
smiles.&lt;/p&gt;
&lt;p&gt;The man page for &lt;strong&gt;genkernel&lt;/strong&gt; however showed me that it uses
&lt;code&gt;real_rootflags&lt;/code&gt;. So I reboot with that parameter set to
&lt;code&gt;real_rootflags=data=journal&lt;/code&gt; and all is okay again.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; I wrote that even changing the default mount options in the file
system itself (using &lt;strong&gt;tune2fs /dev/md3 -o journal_data&lt;/strong&gt;) didn't help.
However, that seems to be an error on my part, I didn't reboot after
toggling this, which is apparently required. Thanks to Xake for pointing
that out.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>The hardened project continues going forward...</title><link href="https://blog.siphos.be/2012/11/the-hardened-project-continues-going-forward/" rel="alternate"></link><published>2012-11-17T21:34:00+01:00</published><updated>2012-11-17T21:34:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-11-17:/2012/11/the-hardened-project-continues-going-forward/</id><summary type="html">&lt;p&gt;This wednesday, the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; team held its monthly
online meeting, discussing the things that have been done the last few
weeks and the ideas that are being worked out for the next. As I did
with the last few meetings, allow me to summarize it for all interested
parties …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This wednesday, the &lt;a href="http://www.gentoo.org/proj/en/hardened"&gt;Gentoo
Hardened&lt;/a&gt; team held its monthly
online meeting, discussing the things that have been done the last few
weeks and the ideas that are being worked out for the next. As I did
with the last few meetings, allow me to summarize it for all interested
parties...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Toolchain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The upstream GCC development on the 4.8 version progressed into its 3rd
stage of its development cycle. Sadly, many of our hardened patches
didn't make the release. Zorry will continue working on these things,
hopefully still being able to merge a few - and otherwise it'll be for
the next release.&lt;/p&gt;
&lt;p&gt;For the MIPS platform, we might not be able to support the hardenedno*
GCC profiles [1] in time. However, this is not seen as a blocker
(we're mostly interested in the hardened ones, not the ones without
hardening ;-) so this could be done later on.&lt;/p&gt;
&lt;p&gt;Blueness is migrating the stage building for the uclibc stages towards
catalyst, providing more clean stages. For the amd64 and i686 platforms,
the uclibc-hardened and uclibc-vanilla stages are already done, and
mips32r2/uclibc is on the way. Later, ARM stages will be looked at.
Other platforms, like little endian MIPS, are also on the roadmap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The latest hardened-sources (\~arch) package contains a patch supporting
the &lt;em&gt;user.*&lt;/em&gt; namespace for extended attributes in tmpfs, as needed for
the XATTR_PAX support [2]. However, this patch has not been properly
investigated nor tested, so input is definitely welcome. During the
meeting, it was suggested to cap the length of the attribute value and
only allow the &lt;em&gt;user.pax&lt;/em&gt; attribute, as we are otherwise allowing
unprivileged applications to "grow data" in the kernel memory space (the
tmpfs).&lt;/p&gt;
&lt;p&gt;Prometheanfire confirmed that recent-enough kernels (3.5.4-r1 and later)
with nested paging do not exhibit the performance issues reported
earlier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The 20120725 upstream policies are stabilized on revision 5. Although a
next revision is already available in the hardened-dev overlay, it will
not be pushed to the main tree due to a broken admin interface. Revision
7 is slated to be made available later the same day to fix this, and is
the next candidate for being pushed to the main tree.&lt;/p&gt;
&lt;p&gt;The september-released newer userspace utilities for SELinux are also
going to be stabilized in the next few days (at the time of writing this
post, they are ;-). These also support &lt;em&gt;epatch_user&lt;/em&gt; so that users and
developers can easily add in patches to try out stuff without having to
repackage the application themselves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;grSecurity and PaX&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The toolchain support for PT_PAX (the ELF-header based PaX markings) is
due to be removed soon, meaning that the XATTR_PAX support will need to
be matured by then. This has a few consequences on available packages
(which will need a bump and fix) such as elfix, but also on the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=commit;h=eb3a2e198c926aca7063aa036793bb94bfbec1ef"&gt;pax-utils.eclass&lt;/a&gt;
file (interested parties are kindly requested to test out the new eclass
before it reaches "production"). Of course, it will also mean that the
new PaX approach needs to be properly documented for end users and
developers.&lt;/p&gt;
&lt;p&gt;pipacs also mentioned that he is working on a paxctld daemon. Just like
SELinux' restorecond daemon, this deamon will look for files and check
them against a known database of binaries with their appropriate PaX
markings. If the markings are set differently (or not set), the paxctld
daemon will rectify the situation. For Gentoo, this is less of a concern
as we already set the proper information through the ebuilds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The old SELinux profiles, which were already deprecated for a while,
have been removed from the portage tree. That means that all
SELinux-using profiles use the &lt;em&gt;features/selinux&lt;/em&gt; inclusion rather than
a fully build (yet difficult to maintain) profile definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System Integrity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few packages, needed to support or work with ima/evm, have been pushed
to the hardened-dev overlay.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux handbook has been updated with the latest policy changes
(such as supporting the named init scripts). We also &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-constraints.xml"&gt;documented SELinux
policy
constraints&lt;/a&gt;
which was long overdue.&lt;/p&gt;
&lt;p&gt;So again a nice month of (volunteer) work on the security state of
Gentoo Hardened. Thanks again to all (developers, contributors and
users) for making Gentoo Hardened where it is today. Zorry will send out
the meeting log later to the mailinglist, so you can look at the more
gory details of the meeting if you want.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] GCC profiles are a set of parameters passed on to GCC as a
    "default" setting. Gentoo hardened uses GCC profiles to support
    using non-hardening features if the users wants to (through the
    &lt;strong&gt;gcc-config&lt;/strong&gt; application).&lt;/li&gt;
&lt;li&gt;[2] XATTR_PAX is a new way of handling PaX markings on binaries.
    Previously, we kept the PaX markings (i.e. flags telling the kernel
    PaX code to allow or deny specific behavior or enable certain
    memory-related hardening features for a specific application) as
    flags in the binary itself (inside the ELF header). With XATTR_PAX,
    this is moved to an extended attribute called "user.pax".&lt;/li&gt;
&lt;/ul&gt;</content><category term="Gentoo"></category></entry><entry><title>Local policy management script</title><link href="https://blog.siphos.be/2012/11/local-policy-management-script/" rel="alternate"></link><published>2012-11-11T13:37:00+01:00</published><updated>2012-11-11T13:37:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-11-11:/2012/11/local-policy-management-script/</id><summary type="html">&lt;p&gt;I've written a small script that I call &lt;strong&gt;selocal&lt;/strong&gt; which manages
locally needed SELinux rules. It allows me to add or remove SELinux
rules from the command line and have them loaded up without needing to
edit a .te file and building the .pp file manually. If you are
interested …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've written a small script that I call &lt;strong&gt;selocal&lt;/strong&gt; which manages
locally needed SELinux rules. It allows me to add or remove SELinux
rules from the command line and have them loaded up without needing to
edit a .te file and building the .pp file manually. If you are
interested, you can download it from my &lt;a href="https://raw.github.com/sjvermeu/small.coding/master/se_scripts/selocal"&gt;github
location&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Its usage is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can &lt;em&gt;add&lt;/em&gt; a rule to the policy with &lt;strong&gt;selocal -a "rule"&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can &lt;em&gt;list&lt;/em&gt; the current rules with &lt;strong&gt;selocal -l&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can &lt;em&gt;remove&lt;/em&gt; entries by referring to their number (in the
    listing output), like &lt;strong&gt;semodule -d 19&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;You can ask it to build (&lt;strong&gt;-b&lt;/strong&gt;) and load (&lt;strong&gt;-L&lt;/strong&gt;) the policy when
    you think it is appropriate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It even supports multiple modules in case you don't want to have all
local rules in a single module set.&lt;/p&gt;
&lt;p&gt;So when I wanted to give a presentation on Tor, I had to allow the
torbrowser to connect to an unreserved port. The torbrowser runs in the
mozilla domain, so all I did was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# selocal -a &amp;quot;corenet_tcp_connect_all_unreserved_ports(mozilla_t)&amp;quot; -b -L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At the end of the presentation, I removed the line from the policy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# selocal -l | grep mozilla_t
19. corenet_tcp_connect_all_unreserved_ports(mozilla_t)
~# selocal -d 19 -b -L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can also add in comments in case I would forget why I added it in the
first place:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# selocal -a &amp;quot;allow mplayer_t self:udp_socket create_socket_perms;&amp;quot;   
 -c &amp;quot;MPlayer plays HTTP resources&amp;quot; -b -L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This then also comes up when listing the current local policy rules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# selocal -l
...
40: allow mplayer_t self:udp_socket create_socket_perms; # MPlayer plays HTTP resources
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category></entry><entry><title>Gentoo Hardened progress meeting</title><link href="https://blog.siphos.be/2012/10/gentoo-hardened-progress-meeting/" rel="alternate"></link><published>2012-10-14T15:00:00+02:00</published><updated>2012-10-14T15:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-10-14:/2012/10/gentoo-hardened-progress-meeting/</id><summary type="html">&lt;p&gt;Not that long ago we had our monthly Gentoo Hardened project meeting (on
October 3rd to be exact). On these meetings, we discuss the progress of
the project since the last meeting.&lt;/p&gt;
&lt;p&gt;For our &lt;em&gt;toolchain&lt;/em&gt; domain, Zorry reported that the PIE patchset is
updated for GCC, fixing bug &lt;a href="https://bugs.gentoo.org/436924"&gt;#436924&lt;/a&gt;.
Blueness …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Not that long ago we had our monthly Gentoo Hardened project meeting (on
October 3rd to be exact). On these meetings, we discuss the progress of
the project since the last meeting.&lt;/p&gt;
&lt;p&gt;For our &lt;em&gt;toolchain&lt;/em&gt; domain, Zorry reported that the PIE patchset is
updated for GCC, fixing bug &lt;a href="https://bugs.gentoo.org/436924"&gt;#436924&lt;/a&gt;.
Blueness also mentioned that he will most likely create a separate
subproject for the alternative hardened systems (such as mips and arm).
This is mostly for management reasons (as the information is currently
scattered throughout the Gentoo project at large).&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;kernel&lt;/em&gt; domain, since version 3.5.4-r2 (and higher), the
kernexec and uderef settings (for grSecurity) should no longer impact
performance on virtualized platforms (when hardware acceleration is used
of course), something that has been bothering Intel-based systems for
quite some time already. Also, the problem with guest systems
immediately reserving (committing) all memory on the host should be
fixed with recent kernels as well. Of course, this is only true as long
as you don't sanitize your memory, otherwise all memory gets allocated
regardless.&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;SELinux&lt;/em&gt; subproject, we now have live ebuilds allowing users to
pull in the latest policy changes directly from the git repository where
we keep our policy at. Also, we will see a high commit frequency in the
next few weeks (or perhaps even months) as Fedora's changes are being
merged with upstream. Another change is that our patchbundles no longer
contain all individual patches, but a merged patch. This increases the
deployment time of a SELinux policy package considerably (up to 30%
faster since patching is now only a second or less). And finally, the
latest userspace utilities are in the hardened-dev overlay ready for
broader testing.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;grSecurity&lt;/em&gt; is still focusing on the XATTR-based PaX flags. The eclass
(pax-utils) has been updated, and we will now be looking at supporting
the PaX extended attributes for file systems such as tmpfs.&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;profiles&lt;/em&gt;, people will notice that in the next few weeks, we will
be dropping the (extremely) old SELinux profiles as the current ones
have been marked stable long time ago.&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;system integrity&lt;/em&gt; domain, IMA is being worked on (packages and
documentation) after which we'll move to the EVM support to protect
extended attributes.&lt;/p&gt;
&lt;p&gt;And finally, klondike held a good talk about Gentoo Hardened at the
Flossk conference in Kosovo.&lt;/p&gt;
&lt;p&gt;All in all a good month of work, again with many thanks to the
volunteers that are keeping Gentoo Hardened alive and kicking!&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>git patch apply</title><link href="https://blog.siphos.be/2012/09/git-patch-apply/" rel="alternate"></link><published>2012-09-27T20:45:00+02:00</published><updated>2012-09-27T20:45:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-09-27:/2012/09/git-patch-apply/</id><summary type="html">&lt;p&gt;I recently had to merge the changes made to an upstream project with a
local repository. I took out the changes as patches through
&lt;strong&gt;&lt;code&gt;git format-patch&lt;/code&gt;&lt;/strong&gt; (as the local repository isn't a clone of the
remote one so I couldn't just create a branch and merge) and hoped to
apply …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently had to merge the changes made to an upstream project with a
local repository. I took out the changes as patches through
&lt;strong&gt;&lt;code&gt;git format-patch&lt;/code&gt;&lt;/strong&gt; (as the local repository isn't a clone of the
remote one so I couldn't just create a branch and merge) and hoped to
apply them with &lt;strong&gt;&lt;code&gt;git am&lt;/code&gt;&lt;/strong&gt;. Sadly, trying this resulted in an error
equivalent with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;error: test.txt: does not match index&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Git suggested to fix the index, and then continue with
&lt;strong&gt;&lt;code&gt;git am --resolved&lt;/code&gt;&lt;/strong&gt;. But what the ... does it mean with fixing the
index? Basically, it means that the change needs to be recorded by git
in order to be applied, but why does the patch fail to recognize this?
The &lt;code&gt;test.txt&lt;/code&gt; file exists and is known by git.&lt;/p&gt;
&lt;p&gt;After some searching, I found a way to handle this - it might not be
pretty, but it did the trick, and I succesfully merged about 200 commits
in an hour or so. You can see this post as a "backup" for my memory ;-)&lt;/p&gt;
&lt;p&gt;First of all, I tried to apply the patch using
&lt;strong&gt;&lt;code&gt;git am 0001-some-stuff.patch&lt;/code&gt;&lt;/strong&gt;. If it succeeds, continue. If it
doesn't, apply the patch manually using
&lt;strong&gt;&lt;code&gt;patch &amp;lt; 0001-some-stuff.patch&lt;/code&gt;&lt;/strong&gt;. Then make sure that the changed
files (see &lt;strong&gt;&lt;code&gt;git status&lt;/code&gt;&lt;/strong&gt;) are taking part of the commit (use
&lt;strong&gt;&lt;code&gt;git add&lt;/code&gt;&lt;/strong&gt;). When the changes are made and recorded, run
&lt;strong&gt;&lt;code&gt;git am --resolved&lt;/code&gt;&lt;/strong&gt;. Or if you want to discard it, make sure no
changes are made/recorded and run &lt;strong&gt;&lt;code&gt;git am --skip&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That's it. Some scripting made this a whole lot easier. Check the return
code of &lt;strong&gt;&lt;code&gt;git am&lt;/code&gt;&lt;/strong&gt;. If it is zero, continue with the next patch. If it
isn't, run patch and again check for the return code. If it is zero,
remove all &lt;code&gt;*.orig&lt;/code&gt; files (or change the patch command so it doesn't
write orig files), add all (changed) files to the git index and run
&lt;strong&gt;&lt;code&gt;git am --resolved&lt;/code&gt;&lt;/strong&gt;. And if the patch fails, have the user fix
things manually and continue.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Perimeter security testing</title><link href="https://blog.siphos.be/2012/08/perimeter-security-testing/" rel="alternate"></link><published>2012-08-28T22:47:00+02:00</published><updated>2012-08-28T22:47:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-08-28:/2012/08/perimeter-security-testing/</id><summary type="html">&lt;p&gt;I've been asked a few times how I would do perimeter security testing.
Personally, I'm not an offensive security guy, more a defensive one,
meaning I'm more about security-related defensive methods rather than
PEN testing of any kind. But still, even in a defensive position, having
a "view" on how …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been asked a few times how I would do perimeter security testing.
Personally, I'm not an offensive security guy, more a defensive one,
meaning I'm more about security-related defensive methods rather than
PEN testing of any kind. But still, even in a defensive position, having
a "view" on how to do security testing is important. For me, I would use
the following testing categorisation to look at IT architectures and see
how they would react against certain attacks. I'm calling this one about
&lt;em&gt;perimeter&lt;/em&gt; testing as I am interested here in remote attacks (or
differentiation), not local ones (which requires, in my opinion, a
different way of looking at things).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eggs and a basket&lt;/li&gt;
&lt;li&gt;Overhead testing&lt;/li&gt;
&lt;li&gt;Protocol insecurity or misuse&lt;/li&gt;
&lt;li&gt;Application insecurity or misuse&lt;/li&gt;
&lt;li&gt;Client insecurity&lt;/li&gt;
&lt;li&gt;Correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Eggs and a basket&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, don't put all your eggs in the same basket. I would never
trust myself enough to say things are secure. Always see if you can't
benefit from other people's knowledge (or even other companies
knowledge). If you are doing testing to choose a specific
security-related technology, use analysis made by independent analysis
firms or organizations to further steer your choice. But make sure that
the organization is truely independent and doesn't give "reports" that
are heavily in favor of whomever asked for them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overhead testing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Most technologies you use to counter certain threats will incur some
overhead. This is true for application firewalls, network firewalls,
isolation technologies, confidentiality technologies, access controls
and more. You should set yourself a baseline of what you consider too
much overhead and what not.&lt;/p&gt;
&lt;p&gt;Overhead comes in many layers, so it is important to be able to perform
load testing based on real loads, not fake lab-specific situations.
Running one thousand clients with the same client certificate, same
hosts, same reaction times against one SSL resource has an entirely
different performance profile than running one thousand clients with
different certificates, using different encryption libraries (other
ciphers and such) and different speeds/reaction times (including things
like SSL handshake timings). And that's just one example.&lt;/p&gt;
&lt;p&gt;I always find it very important to be able to run load testing
regularly. I would even go as far as recommend organizations to run load
testing as a "business as usual" test, or at least allow your
technology-inspired teams to easily request such loads against their new
applications or technologies.&lt;/p&gt;
&lt;p&gt;But enough of that. Let's talk about attack methods (or categorisation).&lt;/p&gt;
&lt;p&gt;I tend to look first towards &lt;em&gt;protocol&lt;/em&gt; insecurity, then &lt;em&gt;application&lt;/em&gt;
insecurity and finally &lt;em&gt;client-level&lt;/em&gt; insecurity. Protocol insecurity is
primarily about knowing how the protocol works (or should work) and
finding ways to attack that. Some protocols are inherently insecure, and
introducing proper protection against these is extremely important as
the technology that implements the protocol might not be able to do that
itself. Then I look at application-specific insecurity, which is more
about knowing the application (vendor/product). And finally it is about
client insecurity (such as browser-based attacks, ActiveX component
attacks, and more).&lt;/p&gt;
&lt;p&gt;In each of these cases, I consider the following attack methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Denial-of-Service - what could be done to disable the protocol or
    service behind it completely or partially&lt;/li&gt;
&lt;li&gt;Out-of-order Execution - can the protocol or application be tricked
    into executing tasks when it isn't meant to, which most of the time
    leads to either information leakage or the next attack method&lt;/li&gt;
&lt;li&gt;Privilege escalation - to get more rights/privileges (or switch from
    unauthenticated to authenticated access)&lt;/li&gt;
&lt;li&gt;Remote command execution - executing whatever the attacker wants on
    the remote system&lt;/li&gt;
&lt;li&gt;Application switching/routing - updating the behavior of the
    application to become a service that can be used to further
    expose/explore the remote servers' environment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Protocol insecurity or misuse&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many protocols are inherently insecure. Good security solutions will
need to detect if a protocol is being used in a way that does not match
the behavior expected. And this goes beyond the standard TCP/IP
protocols and the application-level HTTP protocol. Consider SMTP and
VoIP-related protocols as well as a nice example.&lt;/p&gt;
&lt;p&gt;Denial of service attacks against TCP/IP are widely documented. Be it
the well-known SYN flooding, a &lt;a href="http://www-ece.rice.edu/networks/papers/lrdos.pdf"&gt;low-rate tcp-targeted
DoS&lt;/a&gt; or messing with
the TCP stack itself (like with the &lt;a href="http://support.microsoft.com/kb/2563894"&gt;Microsoft Windows TCP/IP Stack
Vulnerabilities&lt;/a&gt;), these
attacks can be easily evaluated against your architecture.&lt;/p&gt;
&lt;p&gt;With TCP/IP, I would generally also look at how the stacks present their
information. Can an attacker use &lt;a href="https://en.wikipedia.org/wiki/TCP_sequence_prediction_attack"&gt;TCP sequence prediction
attacks&lt;/a&gt;?
Can he get information on when is the most feasible period to launch an
attack (for instance from a reasonably stable TCP window size value
reading)? And how about TCP session hijacking?&lt;/p&gt;
&lt;p&gt;Or if we look at HTTP, can attacks such as
&lt;a href="http://ha.ckers.org/slowloris/"&gt;Slowloris&lt;/a&gt; or an &lt;a href="http://www.acunetix.com/blog/web-security-zone/articles/http-post-denial-service/"&gt;HTTP POST
DOS&lt;/a&gt;
attack bring down the service? And what if a user comes to a certain
page after an obscure redirection, where the attacker hopes that the
user authenticates against? Perhaps an attacker might hijack an HTTP
session, or force a user to use a non-secure connection.&lt;/p&gt;
&lt;p&gt;E-mail services too are particularly interesting to look at. Does it
expose information (settings, or account identification)? Does it accept
large time-outs (giving attackers time to just "play" with the service
using netcat/telnet)?&lt;/p&gt;
&lt;p&gt;And in case of VoIP, have you checked common &lt;a href="http://www.slideshare.net/null0x00/voip-vulnerabilities-and-attacks"&gt;voip-based
attacks&lt;/a&gt;
lately? VoIP is (imo) a complex set of protocols and whomever implements
it has to follow strict rules. I would be very surprised if this can't
be heavily influenced.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application insecurity or misuse&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Of course, protocols are implemented by applications, and applications
have their own set of problems. And if you're running software that
isn't properly configured or up to date, you'll definitely need to take
a good read at my blog posting series on &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-1/"&gt;mitigating
risks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider Citrix for instance: a commonly found remote management
toolsuite (well yeah, Citrix offers a lot more, I'm not going to delve
into that right now). It has seen its share of vulnerabilities in the
past, like &lt;a href="http://support.citrix.com/article/CTX121172"&gt;DoS&lt;/a&gt;
vulnerabilities, &lt;a href="http://support.citrix.com/article/CTX133648"&gt;directory traversal or open
proxy&lt;/a&gt;, &lt;a href="http://www.vsecurity.com/resources/advisory/20101221-1/"&gt;command
execution&lt;/a&gt; and
more. And Citrix is far from an insecure platform.&lt;/p&gt;
&lt;p&gt;Just like with all other applications, it is extremely important to have
a good view / knowledge of each product you expose. Some applications
can even mimic other protocols (like
&lt;a href="http://wiki.nginx.org/Modules"&gt;Nginx&lt;/a&gt; handling HTTP, IMAP, POP3, SMTP,
WebDAV, ... which, if exploited by an attacker, can provide a new
fall-out base to work from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Client insecurity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, the last thing to consider is most likely the one most
difficult to manage: client insecurity. Especially with internet-facing
services, it is very hard to protect yourself from client systems that
are not properly protected. How to deal with user authentication if the
user could have a keystroke logger running in the background? A browser
is a commonly used application for service access, but what about things
like a Citrix client (especially if local drive mapping is enabled)?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correlation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A good security system is integrated with the various security
technologies in place. An attacked that did discovery or even tried out
a few other attacks before should already alert most of your security
components, possibly even invoking a temporary countermeasure against
the users' location. It is not sufficient to block the IP address on the
webserver when the attacker tried an HTTP-based attack only to have him
try his luck on the next service that you expose...&lt;/p&gt;
&lt;p&gt;Now for each "category" I tend to look at the attack from a "hit and
run" aspect (exploitable with a single attack or burst), "build up"
(most of the slower attacks tend to be like this) or "evaded" (trying to
work around detection of the previous ones), and this for a single host,
a relayed host or distributed. All these factors combined give me enough
things to consider while evaluating an architecture (or security
technology implementation) for remote attacks.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Gentoo Hardened in August</title><link href="https://blog.siphos.be/2012/08/gentoo-hardened-in-august/" rel="alternate"></link><published>2012-08-25T17:18:00+02:00</published><updated>2012-08-25T17:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-08-25:/2012/08/gentoo-hardened-in-august/</id><summary type="html">&lt;p&gt;Last wednesday &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; held its
monthly online meeting to discuss the progress of the various
subprojects, reconfirm the current project leads, talk about potential
new projects and discuss some bugs that were getting on our nerves...&lt;/p&gt;
&lt;p&gt;For the project leads, all current leads were reconfirmed: Zorry will
keep tight …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last wednesday &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; held its
monthly online meeting to discuss the progress of the various
subprojects, reconfirm the current project leads, talk about potential
new projects and discuss some bugs that were getting on our nerves...&lt;/p&gt;
&lt;p&gt;For the project leads, all current leads were reconfirmed: Zorry will
keep tight ship as Gentoo Hardened project lead, and will also continue
as the lead for the toolchain-related projects. Blueness keeps tackling
the kernel, pax, grsec and rsbac subprojects, klondike the documentation
and media and I will continue with the SELinux and integrity
subprojects.&lt;/p&gt;
&lt;p&gt;On the toolchain progress, Zorry is working on the 4.8 patches and hopes
to be able to submit them upstream later this month. Blueness continues
maintaining the uclibc architectures mentioned last month and is working
on the documentation related to it.&lt;/p&gt;
&lt;p&gt;On the kernel side, there were some reports submitted that were
triggered by the integer overflow plugin. This plugin, called
&lt;a href="https://grsecurity.net/pipermail/grsecurity/2012-March/001091.html"&gt;size_overflow&lt;/a&gt;
aims to detect integer overflows where an increase of an integer value
goes beyond its maximum and wraps around (resulting in either a negative
or a small integer result). This is of course unwanted behavior, so a
gcc plugin (by Emese Revfy) is used to detect such occurrences.
Basically, this plugin will recalculate whatever is done with the
integers on a double precision integer level and see if the logic result
is the same. If it isn't, then an overflow has most likely occurred.
This is of course overly simply explained, but from what I can fond in
the interwebs, not that far from the truth.&lt;/p&gt;
&lt;p&gt;The reports are generally about network-related applications, like
&lt;a href="http://www.torproject.org"&gt;tor&lt;/a&gt;, which are terminated because something
fishy occurred within the network handling code of the kernel (see for
instance bug
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=430906"&gt;#430906&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In the SELinux camp, the documentation has been updated to inform users
on how to create a new role (see also an earlier post of mine) and a few
patches to the setools package have been added to support
Python-2.7-only systems as well as systems using the latest swig. Also,
all userspace utilities for SELinux should support both Python 2.7 and
Python 3.x - the only remaining aspect is the SELinux code within
Portage (see &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=430488"&gt;bug
#430488&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Regarding grSecurity and PaX, blueness is working on the xattr PaX
markings support in Gentoo, and a &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=427888"&gt;tracker
bug&lt;/a&gt; has been opened to
manage the changes needed. Vapier suggested to move towards xattr
markings completely and drop the PT_PAX ELF header support, but this
cannot be done until all file systems support user-level extended
attributes. That being said, it is a good idea to do this in the long
run though as extended attributes give greater flexibility and don't
manipulate the binaries of an application.&lt;/p&gt;
&lt;p&gt;On the integrity subproject, the concepts and introduction documentation
is online. I'm working on a few ebuilds that are needed to support
IMA/EVM and should hopefully hit the hardened development overlay the
next week. The primary focus now is to support creating a "secure image"
which, when uploaded to a hosting service, would detect if the hosting
service tampered with the image outside (i.e. by manipulating the image
file itself).&lt;/p&gt;
&lt;p&gt;Finally, on documentation and media, we will need to look into updating
the prelude/LIDS documentation (host intrusion prevention/detection
documentation) as it is quite old and obsoleted currently. Klondike also
recently gave a talk about Gentoo Hardened (put the stuff online
Francisco !) but I don't recall anymore where - I'lll update when I see
the meeting log ;-)&lt;/p&gt;
&lt;p&gt;All by all a nice month! Good going guys.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Lots of work on supporting swig-2</title><link href="https://blog.siphos.be/2012/08/lots-of-work-on-supporting-swig-2/" rel="alternate"></link><published>2012-08-20T20:50:00+02:00</published><updated>2012-08-20T20:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-08-20:/2012/08/lots-of-work-on-supporting-swig-2/</id><summary type="html">&lt;p&gt;The SELinux &lt;a href="http://oss.tresys.com/projects/setools/"&gt;setools&lt;/a&gt;
&lt;a href="http://packages.gentoo.org/package/app-admin/setools"&gt;package&lt;/a&gt; provides
a few of the commands I used the most when working with SELinux:
&lt;strong&gt;sesearch&lt;/strong&gt; for looking through the policy and &lt;strong&gt;seinfo&lt;/strong&gt; to get
information on type/attribute/role/... from the currently loaded policy.&lt;/p&gt;
&lt;p&gt;This package uses &lt;a href="http://www.swig.org/"&gt;swig&lt;/a&gt;, the Simplified (sic)
Wrapper and Interface Generator to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The SELinux &lt;a href="http://oss.tresys.com/projects/setools/"&gt;setools&lt;/a&gt;
&lt;a href="http://packages.gentoo.org/package/app-admin/setools"&gt;package&lt;/a&gt; provides
a few of the commands I used the most when working with SELinux:
&lt;strong&gt;sesearch&lt;/strong&gt; for looking through the policy and &lt;strong&gt;seinfo&lt;/strong&gt; to get
information on type/attribute/role/... from the currently loaded policy.&lt;/p&gt;
&lt;p&gt;This package uses &lt;a href="http://www.swig.org/"&gt;swig&lt;/a&gt;, the Simplified (sic)
Wrapper and Interface Generator to provide libraries that can be loaded
by Python and used as regular Python modules, based on the C code that
setools uses. Or, in other words: you write C code, and swig transforms
it into libraries that can be loaded by a dozen higher generation
languages such as Python.&lt;/p&gt;
&lt;p&gt;The change from swig-1 to swig-2 however broke the setools build. It
seems that the swig interface code that setools uses doesn't work
properly anymore with more recent swig versions. The last few days (yes,
days) I have been trying to get setools to build again. The fixes that I
put in were not extremely difficult, but very labour-intensive (beyond
the point that I think I'm doing things wrong, but hey - this is my
first time I'm working on swig stuff, and I'm glad I already got it to
build again).&lt;/p&gt;
&lt;p&gt;The first thing I had to do was fix constructor/destructor logic. It
looks like swig-1 supported the C shorthand notation for structures
whereas swig-2 sees the name of the structure as its class (note that I
just got it to build, I still need to see if things still work):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  typedef struct apol_ip {...} apol_ip_t;
  %extend apol_ip_t {
-   apol_ip_t(const char * str) {
+   apol_ip(const char * str) {
  ...
-   ~apol_ip_t() {
+   ~apol_ip() {
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Without this, I got "Illegal destructor" errors (not related to the
illegal destructor fix made to swig itself) and "Method apol_ip_t
requires a return type" (because it doesn't see them as constructors).&lt;/p&gt;
&lt;p&gt;The second fix I had to introduce was to rename all functions that swig
would generate which would then have the same name as the C-function.
For instance, suppose a C function in the code has
&lt;code&gt;apol_vector_get_size&lt;/code&gt; then swig would, for the &lt;code&gt;apol_vector&lt;/code&gt; class with
method &lt;code&gt;get_size&lt;/code&gt; in the swig interface, generate a function called
&lt;code&gt;apol_vector_get_size&lt;/code&gt; which of course collides with the already defined
function, giving an error like "Conflicting types for
apol_vector_get_size" followed by a "previous declaration was here:".&lt;/p&gt;
&lt;p&gt;Swig supports the &lt;code&gt;%rename&lt;/code&gt; method for this, so I had to do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  typedef struct apol_vector {} apol_vector_t;
+ %rename(apol_vector_get_size) apol_vector_wrap_get_size;
+ %rename(apol_vector_get_capacity) apol_vector_wrap_get_capacity;
...
- size_t get_size() {
+ size_t wrap_get_size() {
...
- size_t get_capacity() {
+ size_t wrap_get_capacity() {
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The patch that I had to add to finally get it to build again is 7019
lines (229 Kbyte). Too much manual labour. Now let's hope this really
fixes things, and doesn't just masquerade the build failures but
introduces runtime failures. Of course, if I think it works, I'll send
it upstream so that, if it is indeed the right fix, other developers
don't have to go through this...&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Adding roles to the Gentoo Hardened SELinux policy</title><link href="https://blog.siphos.be/2012/08/adding-roles-to-the-gentoo-hardened-selinux-policy/" rel="alternate"></link><published>2012-08-14T20:39:00+02:00</published><updated>2012-08-14T20:39:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-08-14:/2012/08/adding-roles-to-the-gentoo-hardened-selinux-policy/</id><summary type="html">&lt;p&gt;I &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml?part=2&amp;amp;chap=5#doc_chap4"&gt;wrote a small
section&lt;/a&gt;
on how to create additional roles to the SELinux policy offered by
Gentoo Hardened. Whereas the default policy that we provide only offers
a few basic roles, any policy administrator can provide additional roles
for the system.&lt;/p&gt;
&lt;p&gt;By using additional roles, you can grant users …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml?part=2&amp;amp;chap=5#doc_chap4"&gt;wrote a small
section&lt;/a&gt;
on how to create additional roles to the SELinux policy offered by
Gentoo Hardened. Whereas the default policy that we provide only offers
a few basic roles, any policy administrator can provide additional roles
for the system.&lt;/p&gt;
&lt;p&gt;By using additional roles, you can grant users administrative rights to
particular services without risking having them elevate their privileges
to root (+ sysadmin). You should even allow them to get a root shell
while remaining confined within their domain (and role).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Kickstarting the Integrity subproject</title><link href="https://blog.siphos.be/2012/07/kickstarting-the-integrity-subproject/" rel="alternate"></link><published>2012-07-30T21:34:00+02:00</published><updated>2012-07-30T21:34:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-30:/2012/07/kickstarting-the-integrity-subproject/</id><summary type="html">&lt;p&gt;Now that Gentoo Hardened has its
&lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/index.xml"&gt;integrity&lt;/a&gt;
subproject, I started with writing down the
&lt;a href="http://goo.gl/57K8g"&gt;concepts&lt;/a&gt; (draft - will move to the project site
when finished!) used within the subproject: what is integrity, how does
trust fit into this, what kind of technologies will we look at, etc. I'm
hoping that this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Now that Gentoo Hardened has its
&lt;a href="http://www.gentoo.org/proj/en/hardened/integrity/index.xml"&gt;integrity&lt;/a&gt;
subproject, I started with writing down the
&lt;a href="http://goo.gl/57K8g"&gt;concepts&lt;/a&gt; (draft - will move to the project site
when finished!) used within the subproject: what is integrity, how does
trust fit into this, what kind of technologies will we look at, etc. I'm
hoping that this document will help users in positioning this project as
well as already identify a few areas where I think we need to work on.&lt;/p&gt;
&lt;p&gt;The guide starts with talking about hashes (since hashes are often used
in integrity validation schemes), continuing towards HMAC (for
authenticated hashes) and signed HMAC digests (for better protection of
the cryptographic keys while verifying the integrity). It already talks
a bit about trust (and trust chains) and how it works in both ways
(top-down and bottom up - the latter especially when considering you are
running services on platforms you do not manage yourself).&lt;/p&gt;
&lt;p&gt;I will be working further on this, describing how the trusted computing
group's vision and the trusted platform module standard they developed
fits into this as a &lt;em&gt;possible implementation&lt;/em&gt; of trust validation
(hopefully without getting to the religious part of it) as well as
giving first highlights on other technologies we will look at as well.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Hardened on the move</title><link href="https://blog.siphos.be/2012/07/gentoo-hardened-on-the-move/" rel="alternate"></link><published>2012-07-26T00:41:00+02:00</published><updated>2012-07-26T00:41:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-26:/2012/07/gentoo-hardened-on-the-move/</id><summary type="html">&lt;p&gt;Gentoo Hardened is thriving and going forward. For those that don't
exactly know what &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; is - it
is a Gentoo project dedicated to bring Gentoo in a shape ready for
highly secure, high stability production server environments. This is
what we live by, and why we do what we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Gentoo Hardened is thriving and going forward. For those that don't
exactly know what &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; is - it
is a Gentoo project dedicated to bring Gentoo in a shape ready for
highly secure, high stability production server environments. This is
what we live by, and why we do what we do. To accomplish this goal, we
use a great community of developers &amp;amp; users that work on several
subprojects: the implementation of kernel hardening features such as
grSecurity, memory-based protection schemes such as PaX, toolchain
updates to harden against buffer overflows and memory attacks, mandatory
access control schemes such as SELinux and RSBAC.&lt;/p&gt;
&lt;p&gt;In Gentoo Hardened we then integrate these technologies in Gentoo Linux
so that it is usable by a larger community, well documented and
supported. I'm myself heavily working on the SELinux integration &amp;amp;
documentation aspects, and am hoping to contribute even further - but
more about that in a minute.&lt;/p&gt;
&lt;p&gt;Today, we had an online meeting where developers present their current
"state of affairs" and the upcoming things they are going to work on.
This is done about once every month in the IRC chat channel
#gentoo-hardened on the freenode network. Of course, most of the
developers are available on the chat channel on an (almost) daily basis.&lt;/p&gt;
&lt;p&gt;Todays meeting gave us feedback on the following (and remind you, this
is one month of volunteer-driven work)...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Toolchain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When we talk about the toolchain, we mean the set of tools and libraries
needed to build a (hardened) system. We put most focus on the GCC
compiler because it contains most of the changes we support (like stack
smashing protection, position independent code/executable changes, etc.)
but work on libraries like glibc and uclibc are on their way as well.&lt;/p&gt;
&lt;p&gt;Zorry (yeah, I'm going to use nicknames here so you know who you're
talking to on IRC ;-) is working on getting our patches upstream
(meaning that the main GCC development can incorporate our patches).
Sending and working together with the main projects is very important as
it provides not only continuity on the patches (once they are upstream,
more people are maintaining the code than just you/us), but also gives a
multi-eye view on the code: is it of high quality? Does it comply with
the proper security guidelines? What about impact of the code on things
we don't or haven't considered yet?&lt;/p&gt;
&lt;p&gt;On the library part, blueness (one of our Gentoo Hardened developers and
- imho - an expert in many fields) has been working on Hardened support
on ARM (armv7a) with uclibc. He has put up stage4 files for armv7a
softfloat uclibc hardened and is working on those for hardfloat. This
means that ARM with uclibc+hardened or ARM with glibc+hardened are
working - he has even tested an xfce4 desktop on ARM with uclibc and
hardened toolchain.&lt;/p&gt;
&lt;p&gt;ARM support is becoming more and more important in the technology field.
Other major processor players like SPARC, Itanium, PowerPC, ... are
slowly seeing less and less market share, whereas ARM - albeit currently
still a very small player - is rapidly gaining momentum. You all know
ARM from the smartphones and other embedded-like platforms, but ARM on
servers is coming faster than you expect. Being a simple platform with
low energy consumption and good commercial backing (both on CPU level as
well as platform support), we can see ARM becoming a major player on
this - and Gentoo Hardened is actively working towards it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within Gentoo Hardened, we support the
&lt;a href="https://grsecurity.net/"&gt;grSecurity&lt;/a&gt; and
&lt;a href="https://pax.grsecurity.net/"&gt;PaX&lt;/a&gt; kernel patches for a more hardened
Linux kernel. But this additional hardening can also sometimes interfere
with the normal functioning of systems. To help users in their
configuration quest, grSecurity allows users to select a few "prebuilt
configuration types" in the kernel build.&lt;/p&gt;
&lt;p&gt;Previously, these types where one of the following label:
"virtualization", "workstation" or "server". Based on these labels, the
security settings that did not negatively effect the functioning of the
system were selected. Recently, the labels have changed into a
question-based configuration: is it a server or not? will you use it for
virtualization and if so, on host or guest? Is performance for you more
important than security? These questions are now also
&lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_00005.xml"&gt;integrated&lt;/a&gt;
in our hardened-sources.&lt;/p&gt;
&lt;p&gt;While working and testing one of the kernel settings (KERNEXEC - kernel
non-executable pages, to protect non-code containing memory pages from
being used to run (potentially hostile/injected) code from) in a
virtualized environment, prometheanfire (another Gentoo Hardened
developer) noticed a possible regression on the performance of guests if
the host had KERNEXEC set. A severe performance hit is to be expected if
the host processor doesn't support hardware-assisted nested page tables
(a method for supporting memory page virtualization), but this also
seemed to occur on systems with nested page tables (&lt;code&gt;/proc/cpuinfo&lt;/code&gt; flag
&lt;code&gt;ept&lt;/code&gt; for Intel, or &lt;code&gt;rvi&lt;/code&gt; for AMD). So more testing (from others as
well) is therefore needed to confirm and work on this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of Gentoo Hardened's subprojects (and one I'm most actively working
on) is its support for SELinux or Security Enhanced Linux. It offers a
Mandatory Access Control implementation for Linux, ensuring that users
cannot change the security settings that an administrator has set (which
is Discretionary Access Control if they can), but also enforce that
services/processes can not be forced to do things they are not meant to
do. This provides reasonable protection against things like remote code
execution exploits, or just limit what an administrator wants particular
processes to do. With SELinux, you can even define roles to properly
identify and segregate tasks, providing a method for "segregation of
duties" on OS level.&lt;/p&gt;
&lt;p&gt;Anyway, as I said, Gentoo Hardened is actively working on SELinux
integration. First of all is stages support (providing a small,
deployable system unit that users can use to install a SELinux-enabled
system) as currently, users are forced to switch to SELinux after having
installed Gentoo, which is a &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml?part=2&amp;amp;chap=1"&gt;multi-step
approach&lt;/a&gt;.
By offering stages, we can simplify the deployment of Gentoo Hardened
SELinux. Currently, building stages works but requires some manual steps
(labeling mostly) which need to be removed before we can automatically
build stages. The next steps here are to see if we can build SELinux
stages on non-SELinux systems (as all we need is to link the proper
files with the SELinux-supporting libraries, which should work
regardless of SELinux being enabled or not). The fact that users need to
relabel their system during deployment is just a minor inconvenience
(and a one-command fix, so easy to document too).&lt;/p&gt;
&lt;p&gt;Another item of progression made is a SELinux-enabled (well, Gentoo
Hardened grSecurity with PaX and SELinux enforcing enabled) &lt;a href="http://distfiles.gentoo.org/experimental/amd64/qemu-selinux/"&gt;virtual
image&lt;/a&gt;
called "selinuxnode". This Qemu/KVM image is a simple Gentoo base
installation but with those security features already enabled, allowing
users to take a first look at SELinux before trying it out on their own
system. But this image has the potential (and now roadmap ;-) to become
much more:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide a play-ground for users to test things in. Try out hammering
    the SELinux policy, or reproducing potential issues before reporting
    them (to make sure they are easily reproduceable).&lt;/li&gt;
&lt;li&gt;Become a Proof-of-Concept location for new enhancements: not only
    updates on SELinux, but also on other hardening measures and
    technologies that Gentoo Hardened can support. Implementing the
    technologies in the VM allow other developers to test and work on it
    without needing to sacrifice one of their own systems.&lt;/li&gt;
&lt;li&gt;Become the main system for educational (course-like) documentation.
    If we develop HOWTO documents, using this VM as a base allows users
    to follow the instructions to the letter and try things out while
    keeping the documentation consistent. The documentation can, in the
    future, also contain instructions that users need to follow as a
    sort-of test. At the end of the test, a simple script can easily
    verify on the VM if the test was finished succesfully or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even further down the road, it might evolve into a system for building
appliance-like, hardened services based on Gentoo Hardened. But that's a
milestone too far for now. But you can always dream ;-)&lt;/p&gt;
&lt;p&gt;On the SELinux policy development side, I'm recently focusing on two
aspects: the change towards &lt;code&gt;/run&lt;/code&gt; (which already required a few
"urgent" updates and will probably need a lot more) as well as confining
popular attack surfaces like browsers. Not many SELinux users run their
browser in a confined space, but I personally don't run anything in
unconfined domains and feel that browsers are too popular in the
security area to not put attention to. So I'm struggling to have the
browsers (first focus is Chromium as that one has an open bug for it,
and Firefox because that is my main browser platform) fully confined yet
still flexible enough (using SELinux booleans) to support users that
have other wishes on their browsers.&lt;/p&gt;
&lt;p&gt;Speaking of policy development, in the meeting it was also brought
forward to support a change of stabilization of SELinux policies from
the standard 30-days towards a 14-day stabilization period. In most
cases, this doesn't harm users as policies are usually enhanced (allow
something that was denied before) and less about reducing privileges (as
it is quite hard to find out why a rule was enabled in the first place,
hence our reluctant approach to "quickly" update policies). For such
updates, We're suggesting a 14-day stabilization period, while retaining
30 days for larger updates such as domain policy rewrites (which are
sometimes needed if an application changes too much - think init and
systemd - or when its segregated into multiple parts that each need (or
can have) their own SELinux domain.&lt;/p&gt;
&lt;p&gt;Finally, we gave a quick update on our status for upstream support (as I
mentioned before, having patches supported and accepted upstream is very
important for us): we have 116 changesets to the policy in comparison
with the 20120215 refpolicy release (which is our "upstream"). Of those
changesets, 45 have been accepted and implemented upstream, 12 are
pending. 55 have not been sent yet (because they still need work or more
documentation before they can be accepted) and 4 will not be sent
(mostly because they are gentoo-only or deviate from upstream's
acceptance guidelines but fit in Gentoo's approach).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;grSecurity's PaX&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Blueness worked on the &lt;em&gt;xattr pax&lt;/em&gt; support implementation (using
extended attributes to store and manage the PaX flags, rather than using
the ELF header changes used in the past) within Gentoo Hardened. This is
now production-ready, so the proper tools will be made generally
available shortly whereas the older method (mainly chpax) will be
decommissioned in the very near future.&lt;/p&gt;
&lt;p&gt;PaX markings allow the Linux kernel to toggle specific PaX settings on
or off for processes so that the general state of the system can use the
PaX protections while a very few set of programs that cannot work with
these settings (often binary software or third party software, but some
self-built software can have difficulties with PaX as well) can run
without them (or with a lower set). This is much more flexible than an
all-or-nothing approach. By using extended attributes, managing these
markings can be done without modifying the binaries themselves. In
Gentoo, proper support is also given through the &lt;code&gt;paxctl-ng.eclass&lt;/code&gt; so
developers can automatically set markings at deploy-time when needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Gentoo, users select "profiles" as a way to define the defaults for
their system. Profiles define stuff like the default kernel, C library,
specific USE flags, toolchain, etc. For instance, users that want to use
a Gentoo Hardened system with SELinux on an x86_64 system with
no-multilib (all 64-bit only) select the
hardened/linux/amd64/no-multilib/selinux profile.&lt;/p&gt;
&lt;p&gt;In the last few weeks, blueness has been working on the uclibc-related
profiles (hardened/linux/uclibc/\${ARCH}) using a clean slate. Gentoo
supports profile inheritance, so you can "stack" one profile on top of
the other. This is great for manageability, but when the profile is to
support systems that are quite different from what Gentoo developers are
used to, it makes sense to use a clean setup and start from there. And
this is the case for hardened uclibc systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System integrity subproject&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this meeting the initial kick-off (after approval) was given of a new
hardened subproject called &lt;em&gt;system integrity&lt;/em&gt;. This project will focus
on the implementation and support of integrity-related technologies such
as (well, mainly) &lt;a href="http://linux-ima.sf.net"&gt;Linux IMA/EVM&lt;/a&gt; and its
supporting userspace utilities and documentation. Integrity validation &amp;amp;
enforcement is an important aspect of system security and, since I
already work with SELinux, feel this is a natural improvement (since you
need a MAC to enforce runtime security and use integrity to enforce
detection and prevention of offline tampering).&lt;/p&gt;
&lt;p&gt;We have great plans with IMA/EVM here, and can hopefully introduce the
first few steps towards it in the selinuxnode virtual image soon ;-)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Of course, technologies are great, but documentation is always needed
(even if nobody reads it (sic)). I have been documenting hardening of
some settings/services using the XCCDF/OVAL languages (part of the
&lt;a href="https://wiki.gentoo.org/wiki/SCAP"&gt;SCAP&lt;/a&gt; set of standards) since not
only do they provide the means to generate guides (we can generate
guides in every language, XCCDF is probably the least flexible of them
all) but they also support the validation of the settings in an
automated manner.&lt;/p&gt;
&lt;p&gt;By using XCCDF/OVAL-supporting software such as
&lt;a href="http://open-scap.org/page/Main_Page"&gt;Open-SCAP&lt;/a&gt;
(&lt;code&gt;app-forensics/openscap&lt;/code&gt; in Gentoo) you can interpret these guides in
an unattended manner, generating reports on the state of your services
compared to these guides, and even have specific profiles (one system
uses a different set of hardening guidelines than another). Since Gentoo
Hardened is about supporting secure &amp;amp; stable production environments, it
is logical that we can offer best practices on how to handle
Gentoo-provided/supported services. And by using these within the SCAP
standard, the guides might even be leveraged further than a regular
online HOWTO could.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And all that from one project?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not really. Gentoo Hardened here plays several roles: integrator for
technologies that are managed in other (free software) projects, and
development for technologies or settings that are either specific to
Gentoo or not available to the public to the extend Gentoo Hardened
believes is needed. You must understand this is possible thanks to the
tremendous effort that all these projects perform. Gentoo Hardened here
plays the role that every Linux distribution has: making all these
technologies and advancements fit in a way that the users can easily
work with it - integrated and well supported.&lt;/p&gt;
&lt;p&gt;Thanks to the free software nature though, Gentoo Hardened does more
than what "commercial integrators" do when they deal with closed,
propriatary software: it updates the code, improves it and brings it
back for broader re-use. As such, it also acts a bit as development
within those projects to assist them in their quest. And in my book,
users are more likely to believe in an integrator that can react
code-wise rather than using workarounds or "helping create a service
request".&lt;/p&gt;
&lt;p&gt;The full excerpts of this meeting (the meeting minutes - well, actually
an IRC chat log excerpt) will be sent out soon by the Gentoo Hardened
project lead, Zorry. Big thanks to him (and the rest of the crew) to
make all this happen! I love to be part of it, and hope I can remain so
for a long, long time.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; &lt;a href="http://www.rsbac.org/"&gt;RSBAC&lt;/a&gt;, not grSecurity's RBAC.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Dynamic transitions in SELinux</title><link href="https://blog.siphos.be/2012/07/dynamic-transitions-in-selinux/" rel="alternate"></link><published>2012-07-22T21:11:00+02:00</published><updated>2012-07-22T21:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-22:/2012/07/dynamic-transitions-in-selinux/</id><summary type="html">&lt;p&gt;In between talks on heap spraying techniques and visualization of data
for fast analysis, I'm working on integrating the chromium SELinux
policy that was offered in bug &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=412637"&gt;bug
#412637&lt;/a&gt; within Gentoo
Hardened. If you take a look at the bug, you notice I'm not really fond
of the policy because …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In between talks on heap spraying techniques and visualization of data
for fast analysis, I'm working on integrating the chromium SELinux
policy that was offered in bug &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=412637"&gt;bug
#412637&lt;/a&gt; within Gentoo
Hardened. If you take a look at the bug, you notice I'm not really fond
of the policy because it uses &lt;em&gt;dynamic transitions&lt;/em&gt;. That's not
something the policy writer can do anything about if he can't access the
source code of the application though, since it means that the
application is SELinux aware and will trigger transitions when needed.&lt;/p&gt;
&lt;p&gt;So what's this dynamic transitioning? Well, in short, it means that a
process can decide to switch domains whenever it pleases (hence the
dynamic part) instead of doing this on fork/exec's. Generally, that
sounds like a flexible feature - and it is. But it's also dangerous.&lt;/p&gt;
&lt;p&gt;Dynamic transitions might seem like a way to enhance security - the
application knows it will start a "dangerous" or more risky piece of
code, and thus transitions towards another domain with less privileges.
Once the dangerous code is passed, it transitions back to the main
domain. The problem with this is that the entire process is still live -
anything that happened within the transitioned domain remains, and
SELinux cannot prevent what happens within the domain itself (like
memory accesses within the same process space). If the more risky code
resulted in corruption or modification of memory, this remains
regardless of the SELinux context transitioning back or not. Assume that
some code is "injected" in the transitioned domain (which isn't allowed
to execute other applications) the moment it transitions back to the
main domain which is allowed to execute applications, this injected code
can become active and do its thing.&lt;/p&gt;
&lt;p&gt;This is why I didn't allow the original code (which ran chromium in the
main user domain and used dynamic transitions towards
&lt;code&gt;chromium_renderer_t&lt;/code&gt;) to be used, asking to confine the browser itself
within its own domain too (&lt;code&gt;chromium_t&lt;/code&gt;) so that we have a more clear
view on the allowed privileges (which is the set of the chromium domain
and the renderer domain together). It is that policy that I'm now
enhancing to work on a fully confined system (no unconfined domains).&lt;/p&gt;
&lt;p&gt;If you want to know more about dynamic transitions, it seems that the
blog post &lt;a href="http://beyondabstraction.net/2005/11/07/subject-object-tranquility-part-2/"&gt;Subject &amp;amp; Object Tranquility, part
2&lt;/a&gt;
(and don't forget to read the comments too) is a fine read.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Hardening the Linux kernel updates</title><link href="https://blog.siphos.be/2012/07/hardening-the-linux-kernel-updates/" rel="alternate"></link><published>2012-07-21T21:06:00+02:00</published><updated>2012-07-21T21:06:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-21:/2012/07/hardening-the-linux-kernel-updates/</id><summary type="html">&lt;p&gt;Thanks to a comment by Andy, the
&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/kernel.html"&gt;guide&lt;/a&gt;
now has information about additional settings: stackprotector, read-only
data, restrict access to /dev/mem, disable /proc/kcore and restrict
kernel syslog (dmesg). One suggestion he made didn't make it to the
guide (about CONFIG_DEBUG_STACKOVERFLOW) since I can't find any
resources about the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Thanks to a comment by Andy, the
&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/kernel.html"&gt;guide&lt;/a&gt;
now has information about additional settings: stackprotector, read-only
data, restrict access to /dev/mem, disable /proc/kcore and restrict
kernel syslog (dmesg). One suggestion he made didn't make it to the
guide (about CONFIG_DEBUG_STACKOVERFLOW) since I can't find any
resources about the setting on how it would made the system more secure
or more resilient against attacks.&lt;/p&gt;
&lt;p&gt;Underlyingly, the
&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/scap-kernel-oval.txt"&gt;OVAL&lt;/a&gt;
now correctly identifies unset variables (it previously searched for "is
not set" strings in the kernel configuration, and now it searches for
the key entry definition and validates if it doesn't find it - e.g.
"CONFIG_PROC_KCORE=" - since that matches both the definition not
being there, or "# CONFIG_PROC_KCORE has not been set").&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Hardening the Linux kernel</title><link href="https://blog.siphos.be/2012/07/hardening-the-linux-kernel/" rel="alternate"></link><published>2012-07-20T22:05:00+02:00</published><updated>2012-07-20T22:05:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-20:/2012/07/hardening-the-linux-kernel/</id><summary type="html">&lt;p&gt;I have moved out the kernel configuration settings (and &lt;strong&gt;sysctl&lt;/strong&gt;
stuff) from the &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Hardening Gentoo Linux
benchmark&lt;/a&gt;
into its own &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/kernel.html"&gt;Hardening the Linux
kernel&lt;/a&gt;
guide. It covers some common hardening-related kernel configuration
entries (although I'm sure I'm missing a lot of them still) as well as
grSecurity and PaX settings …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have moved out the kernel configuration settings (and &lt;strong&gt;sysctl&lt;/strong&gt;
stuff) from the &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Hardening Gentoo Linux
benchmark&lt;/a&gt;
into its own &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/kernel.html"&gt;Hardening the Linux
kernel&lt;/a&gt;
guide. It covers some common hardening-related kernel configuration
entries (although I'm sure I'm missing a lot of them still) as well as
grSecurity and PaX settings (which is something the &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt; project works on), and finally the
system controls (sysctl) that are commonly suggested for a more secure
system.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/"&gt;overview of hardening
guides&lt;/a&gt; now
thus contains three guides: one for Gentoo, one for OpenSSH and one for
the kernel. These ones were definitions I already had in the past so
were "quickly" possible to write down. I'm going to look at BIND and
DHCP next.&lt;/p&gt;
&lt;p&gt;But simultaneously, I'm looking at &lt;a href="http://linux-ima.sourceforge.net/"&gt;Linux
IMA/EVM&lt;/a&gt; support in the hope I can
have this supported in Gentoo as well. Looks like a promising
technology, and if I can get it working, it'll definitely deserve its
place within Gentoo Hardened!&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Hardening OpenSSH</title><link href="https://blog.siphos.be/2012/07/hardening-openssh/" rel="alternate"></link><published>2012-07-18T22:20:00+02:00</published><updated>2012-07-18T22:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-18:/2012/07/hardening-openssh/</id><summary type="html">&lt;p&gt;A while ago I wrote about a &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
which would talk about hardening a Gentoo Linux installation. Within
that document, I was documenting how to harden specific services as
well. However, I recently changed my mind and wanted to move the
hardening stuff for the services in separate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while ago I wrote about a &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/gentoo.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
which would talk about hardening a Gentoo Linux installation. Within
that document, I was documenting how to harden specific services as
well. However, I recently changed my mind and wanted to move the
hardening stuff for the services in separate documents.&lt;/p&gt;
&lt;p&gt;The first one is now finished - &lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/openssh.html"&gt;Hardening
OpenSSH&lt;/a&gt;
is a benchmark informing you how to potentially harden your SSH
installation further. It uses
&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/openssh-xccdf.txt"&gt;XCCDF&lt;/a&gt;/&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/openssh-oval.txt"&gt;OVAL&lt;/a&gt;
so that users of &lt;strong&gt;openscap&lt;/strong&gt; (and other compliant tools) can test their
system automatically, generating nice
&lt;a href="https://dev.gentoo.org/~swift/docs/security_benchmarks/openssh-report.html"&gt;reports&lt;/a&gt;
on the state of their SSH configuration.&lt;/p&gt;
&lt;p&gt;For now, the SSH stuff is also still part of the Gentoo document, but
I'll move that out soon and refer to this new document.&lt;/p&gt;
&lt;p&gt;Hardened Gentoo's purpose is to make Gentoo viable for highly secure,
high stability production server environments. Hence, hardening
documents&lt;br&gt;
should be one of its deliverables as well. So, dear users, do you think
it is wise for the Gentoo Hardened project to also focus on delivering
hardening guides for services? If so, I'm sure we can draft up others...&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Updated Gentoo Hardened/SELinux VM image</title><link href="https://blog.siphos.be/2012/07/updated-gentoo-hardenedselinux-vm-image/" rel="alternate"></link><published>2012-07-16T18:31:00+02:00</published><updated>2012-07-16T18:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-16:/2012/07/updated-gentoo-hardenedselinux-vm-image/</id><summary type="html">&lt;p&gt;I have updated the Gentoo Hardened/SELinux VM image, available on the
mirrors under &lt;code&gt;experimental/amd64/qemu-selinux&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The new image now asks for the keyboard layout, has a short DHCP timeout
value (5 seconds) and provides the nano editor. If you plan on running
the image using qemu, please use …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have updated the Gentoo Hardened/SELinux VM image, available on the
mirrors under &lt;code&gt;experimental/amd64/qemu-selinux&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The new image now asks for the keyboard layout, has a short DHCP timeout
value (5 seconds) and provides the nano editor. If you plan on running
the image using qemu, please use &lt;code&gt;-cpu kvm64&lt;/code&gt; to use a 64-bit virtual
processor.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Hardened/SELinux VM image</title><link href="https://blog.siphos.be/2012/07/gentoo-hardenedselinux-vm-image/" rel="alternate"></link><published>2012-07-10T21:27:00+02:00</published><updated>2012-07-10T21:27:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-07-10:/2012/07/gentoo-hardenedselinux-vm-image/</id><summary type="html">&lt;p&gt;A few weeks ago, I pushed out a VM image (Qemu QCOW2 format) to the
&lt;code&gt;/experimental/amd64/qemu-selinux/&lt;/code&gt; location in our mirrors. This VM
image (which is about 1.6 Gib large decompressed) provides a
SELinux-enabled, Gentoo Hardened (with PaX and other grSecurity security
settings) base installation. Thanks to the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, I pushed out a VM image (Qemu QCOW2 format) to the
&lt;code&gt;/experimental/amd64/qemu-selinux/&lt;/code&gt; location in our mirrors. This VM
image (which is about 1.6 Gib large decompressed) provides a
SELinux-enabled, Gentoo Hardened (with PaX and other grSecurity security
settings) base installation. Thanks to the Qcow2 image format, only the
used 1.6 Gib of data is taken on your disk, even though the image is
made for a 50 Gib deployment).&lt;/p&gt;
&lt;p&gt;The purpose of this image is, eventually, to allow users to test our
Gentoo Hardened with SELinux in a virtual environment, offering decent
isolation (so you can mess things up if you want, it doesn't hurt your
own system). I'm also contemplating of providing more serious
SELinux-focused course material (self-teaching stuff) based on this
image, so that users can learn about Gentoo Hardened (and SELinux) in a
structured manner.&lt;/p&gt;
&lt;p&gt;But before all that, I first need to see if the image is usable by most
people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does it boot? It is an amd64 image for the Qemu KVM64 CPU, but the
    kernel uses paravirtualization for disk and network access, and I
    don't know if that's a safe bet to do or not. People that know KVM
    know that the paravirtualization support is needed for decent
    performance, but I'm not sure if it still makes the images
    sufficiently portable or not.&lt;/li&gt;
&lt;li&gt;Does it work? The build is done based on my own systems, but these
    are all built in a similar fashion (and use binhosts to
    simplify deployment) so in effect, I can only test the images on a
    single system type (multiple, but they're all the same, so
    doesn't matter).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If I can get some comments on this (it boots, it doesn't boot, it sucks,
...) and can work out things, I hope I can have the images better for
all of us.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; yes, keyboard layout is azerty, not qwerty. So your rootpass
will be rootpqss. Updates-are-a-comin'&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Summer of Documentation - Let's do it!</title><link href="https://blog.siphos.be/2012/06/gentoo-summer-of-documentation-lets-do-it/" rel="alternate"></link><published>2012-06-29T19:16:00+02:00</published><updated>2012-06-29T19:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-06-29:/2012/06/gentoo-summer-of-documentation-lets-do-it/</id><summary type="html">&lt;p&gt;The &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki folks&lt;/a&gt; have started a great
idea (and immediately set a nice milestone), namely the &lt;a href="https://wiki.gentoo.org/wiki/Gentoo_Wiki:Summer_of_Documentation/2012"&gt;Gentoo Wiki
Summer of
Documentation&lt;/a&gt;.
By september, they want to double the amount of articles on the wiki.&lt;/p&gt;
&lt;p&gt;I'll surely help out and participate where I can, and perhaps we can
even go …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki folks&lt;/a&gt; have started a great
idea (and immediately set a nice milestone), namely the &lt;a href="https://wiki.gentoo.org/wiki/Gentoo_Wiki:Summer_of_Documentation/2012"&gt;Gentoo Wiki
Summer of
Documentation&lt;/a&gt;.
By september, they want to double the amount of articles on the wiki.&lt;/p&gt;
&lt;p&gt;I'll surely help out and participate where I can, and perhaps we can
even go far above the targeted 500 articles!&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Had to edit /etc/init.d/root</title><link href="https://blog.siphos.be/2012/06/had-to-edit-etcinit-droot/" rel="alternate"></link><published>2012-06-24T15:38:00+02:00</published><updated>2012-06-24T15:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-06-24:/2012/06/had-to-edit-etcinit-droot/</id><summary type="html">&lt;p&gt;For some reason, I had to edit my /etc/init.d/root file to use "mount
/dev/root -n -o remount,rw /" instead of the standard "mount -n -o
remount,rw /". Without this, it failed to remount the root file system
in a read-write mode, which is of course not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For some reason, I had to edit my /etc/init.d/root file to use "mount
/dev/root -n -o remount,rw /" instead of the standard "mount -n -o
remount,rw /". Without this, it failed to remount the root file system
in a read-write mode, which is of course not that funny...&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Overview of SELinux changes</title><link href="https://blog.siphos.be/2012/06/overview-of-selinux-changes/" rel="alternate"></link><published>2012-06-24T14:32:00+02:00</published><updated>2012-06-24T14:32:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-06-24:/2012/06/overview-of-selinux-changes/</id><summary type="html">&lt;p&gt;Most users of Gentoo hardly take a look at the (installation)
documentation when their installation has finished. After all, being a
rolling distribution, there is little need to take a look at the
instructions again. And for most Gentoo users, changes that are needed
to be reviewed by existing users …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most users of Gentoo hardly take a look at the (installation)
documentation when their installation has finished. After all, being a
rolling distribution, there is little need to take a look at the
instructions again. And for most Gentoo users, changes that are needed
to be reviewed by existing users are pushed to them through news items
(see &lt;strong&gt;eselect news&lt;/strong&gt;) or ebuild warnings.&lt;/p&gt;
&lt;p&gt;For SELinux users, since we're still improving actively (but also often
lagging behind in updating our policies to reflect recent changes) I've
decided to keep track of changes in the &lt;a href="http://hardened.gentoo.org/selinux/selinux-handbook.xml"&gt;SELinux
Handbook&lt;/a&gt; as a
separate chapter. With this information at hand, existing users should
be able to see if there are any changes needed on their part to stay up
to date.&lt;/p&gt;
&lt;p&gt;I haven't used news items here yet, since we're still too volatile, but
I do plan on using them in the (near) future.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Python 3 support for SELinux userland, tests and policy rev 10</title><link href="https://blog.siphos.be/2012/05/python-3-support-for-selinux-userland-tests-and-policy-rev-10/" rel="alternate"></link><published>2012-05-26T18:59:00+02:00</published><updated>2012-05-26T18:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-05-26:/2012/05/python-3-support-for-selinux-userland-tests-and-policy-rev-10/</id><summary type="html">&lt;p&gt;In the last few hours I pushed my local changes on the SELinux userland
utilities towards the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=tree"&gt;hardened-development&lt;/a&gt;
overlay. The utilities not only include some bugfixes, but have now also
seen a first set of tests towards Python 3.2. In the past, I've made a
few attempts at making …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the last few hours I pushed my local changes on the SELinux userland
utilities towards the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=tree"&gt;hardened-development&lt;/a&gt;
overlay. The utilities not only include some bugfixes, but have now also
seen a first set of tests towards Python 3.2. In the past, I've made a
few attempts at making the tools support Python 3, but I failed
miserably. Although chances are still high that I failed, at least I got
quite a bit further.&lt;/p&gt;
&lt;p&gt;To make testing a bit easier, I previously made quite a few scripts that
did all sorts of things, in order to catch regressions. However, along
the way, I've started noticing I had to put lots of effort in
streamlining these tests (cleanups), introducing dependency information
(test A before B, or cleanup before test, ...) and parallellism (after
all, if you have many, many tests, lots of cores but run in
single-thread/process mode, it'll take a while). So I started looking at
a good way to handle this for me. I switched my tests into a
Makefile-driven approach.&lt;/p&gt;
&lt;p&gt;Why Makefiles? Well, first of all, Makefiles support &lt;em&gt;dependencies&lt;/em&gt;. You
can define a target and then say which other targets need to be ran
first before this target can run. If you want to support these
dependencies in a run-independent manner, you can use trigger files but
I'm not going to do that yet. Another simple feature is that you can
tell make to not show output (&lt;em&gt;silent mode&lt;/em&gt;) when not necessary. And of
course, with make, you can execute targets &lt;em&gt;concurrently&lt;/em&gt;. By using a
simple, yet manageable directory structure and traverse the Makefiles in
them, I am able to easily add more tests and add them to the runqueue.&lt;/p&gt;
&lt;p&gt;But I'd like to hear from you what infrastructural testing tools you use
because I can imagine Makefiles aren't the best solution here.&lt;/p&gt;
&lt;p&gt;In the mean time, I also pushed the 10th revision of our SELinux
policies to the hardened-dev overlay. The most notable fix in it is to
improve support for those running \~arch systems (some fix on the
kdevtmpfs support).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Catching up, but stuff is piling...</title><link href="https://blog.siphos.be/2012/05/catching-up-but-stuff-is-piling/" rel="alternate"></link><published>2012-05-24T18:46:00+02:00</published><updated>2012-05-24T18:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-05-24:/2012/05/catching-up-but-stuff-is-piling/</id><summary type="html">&lt;p&gt;Those that are frequent the #gentoo-hardened chat channel know that I'm
currently trying to get the SELinux related utilities working under
Python 3. This has progressed quite far, but I'm still not there yet.
I'm now hitting a weird
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=416301"&gt;bug&lt;/a&gt; which seems to
come down to an incorrect free() on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Those that are frequent the #gentoo-hardened chat channel know that I'm
currently trying to get the SELinux related utilities working under
Python 3. This has progressed quite far, but I'm still not there yet.
I'm now hitting a weird
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=416301"&gt;bug&lt;/a&gt; which seems to
come down to an incorrect free() on some memory (well, I don't know
this, this is my current assumption) but which seems hard to catch. So
I'm learning a lot (thanks to an active community) about debugging
Python and memory issues.&lt;/p&gt;
&lt;p&gt;These past few weeks have been enlightening for me on the matter of
Python 2 to 3 conversions. Enough that I can fully understand Diego's
pain when dealing with Ruby upgrades ;-) I hope that, if Perl 6 ever
comes out (right now, Perl 6 is the future - now and in the future ;-),
that they think about the children... err, package maintainers.&lt;/p&gt;
&lt;p&gt;Because it takes some time to work on these matters, other reported
SELinux issues have been piling up; I hope I can close down this Python
migration in the near future and work on the remainder of bugs...&lt;/p&gt;
&lt;p&gt;Next to all this, I'm slowly going through some documentation related
bugs, but also mentoring &lt;a href="http://twitch153-awesomecode.blogspot.com/"&gt;Devan
Franchini&lt;/a&gt; in his GSoC
project on a SELinux policy originator. And now that I linked his blog,
he's going to feel obliged to blog on his progress! ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Keeping /selinux</title><link href="https://blog.siphos.be/2012/05/keeping-selinux/" rel="alternate"></link><published>2012-05-04T22:26:00+02:00</published><updated>2012-05-04T22:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-05-04:/2012/05/keeping-selinux/</id><summary type="html">&lt;p&gt;Just a very quick paragraph on a just-reported issue: if you upgrade
your SELinux utilities to the latest version &lt;em&gt;and&lt;/em&gt; you switch from
&lt;code&gt;/selinux&lt;/code&gt; to &lt;code&gt;/sys/fs/selinux&lt;/code&gt; as the mountpoint for the SELinux file
system, you might get into issues. Apparently, &lt;strong&gt;init&lt;/strong&gt; (which is
responsible for mounting the SELinux …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just a very quick paragraph on a just-reported issue: if you upgrade
your SELinux utilities to the latest version &lt;em&gt;and&lt;/em&gt; you switch from
&lt;code&gt;/selinux&lt;/code&gt; to &lt;code&gt;/sys/fs/selinux&lt;/code&gt; as the mountpoint for the SELinux file
system, you might get into issues. Apparently, &lt;strong&gt;init&lt;/strong&gt; (which is
responsible for mounting the SELinux file system through a call to
libselinux) is trying to mount it on - well yes - &lt;code&gt;/sys/fs/selinux&lt;/code&gt; but
at that time, &lt;code&gt;/sys&lt;/code&gt; is not mounted yet.&lt;/p&gt;
&lt;p&gt;I haven't been able to reproduce just yet, because I just recently had
to move all my systems to use an initramfs (thank you
you-need-an-initramfs-when-you-have-a-separate-usr-partition) which
premounts /sys. But the current workaround should be to keep &lt;code&gt;/selinux&lt;/code&gt;
for now. The utilities support it still, and that gives me some time to
look and investigate the issue.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>20120215 policies now stable</title><link href="https://blog.siphos.be/2012/04/20120215-policies-now-stable/" rel="alternate"></link><published>2012-04-29T16:43:00+02:00</published><updated>2012-04-29T16:43:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-29:/2012/04/20120215-policies-now-stable/</id><summary type="html">&lt;p&gt;Today I've stabilized the &lt;code&gt;sec-policy/selinux-*&lt;/code&gt; packages that provide
the 20120215 "series" of SELinux policies. Together with the
stabilization, the more recent userspace tools (like the policycoreutils
as well as libraries like libsemanage and libselinux) have been pushed
out as well. I will be dropping the older policies and userspace …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I've stabilized the &lt;code&gt;sec-policy/selinux-*&lt;/code&gt; packages that provide
the 20120215 "series" of SELinux policies. Together with the
stabilization, the more recent userspace tools (like the policycoreutils
as well as libraries like libsemanage and libselinux) have been pushed
out as well. I will be dropping the older policies and userspace tools
soon (as they are now deprecated). The documentation has been updated to
reflect this too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;support for permissive domains (allowing users to mark one specific
    SELinux domain, such as mplayer_t, as permissive (even though the
    rest of the system is running in enforcing mode)&lt;/li&gt;
&lt;li&gt;support for file context translations, so we can now say "/usr/lib64
    (and below) should have the same contexts as /usr/lib"&lt;/li&gt;
&lt;li&gt;support for role attributes, which means for policy developers, we
    now have similar freedom as with type attributes&lt;/li&gt;
&lt;li&gt;support for named file transitions, so a policy rule can say that
    domain A, if creating a file in a directory labeled B, then that
    specific file should have label C. Same for directories, btw.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although some of these enhancements were available as features
individually, the policies we had were not aligned with it - and now,
that has changed ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Linux Sea now in ePub</title><link href="https://blog.siphos.be/2012/04/linux-sea-now-in-epub/" rel="alternate"></link><published>2012-04-20T17:31:00+02:00</published><updated>2012-04-20T17:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-20:/2012/04/linux-sea-now-in-epub/</id><summary type="html">&lt;p&gt;On request of Matthew Marchese, I now automatically build an &lt;a href="http://swift.siphos.be/linux_sea/linux_sea.epub"&gt;ePub
version&lt;/a&gt; of &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; for those that like to read such
resources on a digital reader. Thanks to the use of DocBook, this was
simply a matter of using its xsl-stylesheets/epub/docbook.xsl stylesheet
against the DocBook sources …&lt;/p&gt;</summary><content type="html">&lt;p&gt;On request of Matthew Marchese, I now automatically build an &lt;a href="http://swift.siphos.be/linux_sea/linux_sea.epub"&gt;ePub
version&lt;/a&gt; of &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; for those that like to read such
resources on a digital reader. Thanks to the use of DocBook, this was
simply a matter of using its xsl-stylesheets/epub/docbook.xsl stylesheet
against the DocBook sources and zip the created directory structures
(OEBPS and META-INF) to get to the ePub file.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Why both chroot and SELinux?</title><link href="https://blog.siphos.be/2012/04/why-both-chroot-and-selinux/" rel="alternate"></link><published>2012-04-15T09:41:00+02:00</published><updated>2012-04-15T09:41:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-15:/2012/04/why-both-chroot-and-selinux/</id><summary type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2012/04/chrooted-bind-for-ipv6-with-selinux/"&gt;previous
post&lt;/a&gt;,
a very valid question was raised by Alexander E. Patrakov: why still use
chroot if you have SELinux?&lt;/p&gt;
&lt;p&gt;Both chroot (especially with the additional restrictions that grSecurity
enables on chroots that make it more difficult to break out of a chroot)
and SELinux try to isolate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="http://blog.siphos.be/2012/04/chrooted-bind-for-ipv6-with-selinux/"&gt;previous
post&lt;/a&gt;,
a very valid question was raised by Alexander E. Patrakov: why still use
chroot if you have SELinux?&lt;/p&gt;
&lt;p&gt;Both chroot (especially with the additional restrictions that grSecurity
enables on chroots that make it more difficult to break out of a chroot)
and SELinux try to isolate an application so it only has access to those
resources it needs. Chroot does this on file-level basis (and a bit more
with grSecurity), SELinux on more general resources. However, things
that make SELinux strong (flexible and detailed policy language,
fine-grained authorizations) are also its weakness (consolidating files
into groups having the same file label), and chroot does have an
advantage on this.&lt;/p&gt;
&lt;p&gt;Suppose that a flaw exists in BIND through which an attacker can read
files on the host (through BIND). With SELinux, the domain in which BIND
runs is prohibited from accessing and reading files whose label is not
one of the labels that the policy thinks BIND should be able to read.
More specifically, the BIND policy in the reference policy (which is
what both Gentoo and RedHat base their policies on, and generally
policies are only enlarged, never really shrinked):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etc_runtime_t (read) means access to the files in /etc that are
    modified at runtime (like mtab, profile.env, gentoo's /etc/env.d)&lt;/li&gt;
&lt;li&gt;named_var_run_t (read) is access to /var/run/bind and
    /var/run/named (and a few other related locations)&lt;/li&gt;
&lt;li&gt;named_checkconf_exec_t (read/execute) is access to read and
    execute /usr/sbin/named-checkconf&lt;/li&gt;
&lt;li&gt;named_conf_t (read) to read the BIND-related configuration files&lt;/li&gt;
&lt;li&gt;dnssec_t (read) to read the DNSSEC keyfiles&lt;/li&gt;
&lt;li&gt;locale_t (read) to access /etc/localtime, /usr/share/locale/*,
    /usr/share/zoneinfo/*&lt;/li&gt;
&lt;li&gt;etc_t (read) to read the general configuration files in /etc
    (including passwd, fstab, ...)&lt;/li&gt;
&lt;li&gt;proc_t (read), proc_net_t (read) and sysfs_t (read) to access
    those pseudo filesystems&lt;/li&gt;
&lt;li&gt;udev_tbl_t (read) to access /dev/.udev and /var/run/udev (but I
    have no idea yet why this is in)&lt;/li&gt;
&lt;li&gt;named_log_t (read/write) for the log files of BIND&lt;/li&gt;
&lt;li&gt;net_conf_t (read) to access /etc/hosts (including deny/allow),
    resolv.conf, ...&lt;/li&gt;
&lt;li&gt;named_exec_t (read/execute) the BIND executables&lt;/li&gt;
&lt;li&gt;named_zone_t (read) to access the zone files, also write access in
    case of slave system&lt;/li&gt;
&lt;li&gt;cert_t (read) to read certificate information&lt;/li&gt;
&lt;li&gt;named_cache_t (read/write) to access its cache&lt;/li&gt;
&lt;li&gt;named_tmp_t (read/write) to work with temporary files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Isolation provided by SELinux is as powerful as the width of its
labeling. For instance, by giving the named daemon read access to /etc
files like passwd, fstab, group, hosts, resolv.conf and more, a
malicious user who can exploit this hypothetical vulnerability can
obtain information that might help him in his further attempts. By
chrooting BIND, the files placed in the chroot itself should not offer
the information he might be looking for (for instance, the passwd file,
if needed at all, is limited to just the named and root accounts, etc.)&lt;/p&gt;
&lt;p&gt;Chrooting, but not enabling SELinux, could lead to escalation. A chroot
cannot restrict what a process is allowed to do beyond the regular
access privileges that are given on the user. If a user can upload an
exploit through BIND and have BIND execute it, he can use this as an
attack vector for further activities. SELinux here prohibits BIND to
write stuff it can also execute (there is no write and execute privilege
defined here). It also ensures that the BIND daemon never exists his
security domain (transitioning towards another domain with perhaps other
privileges) as there are no transition rules from named_t to any other
domain.&lt;/p&gt;
&lt;p&gt;Another MAC system that would be better suited to fit both is
grSecurity's RBAC model. Iirc, it uses path definitions to say which
files are allowed to access and which not. The weakness SELinux here has
(aggregation into sets of files with the same label) doesn't exist for
grSecurity. This debate on path-based versus label-based access controls
have been going on for very long time now - just google it ;-)&lt;/p&gt;
&lt;p&gt;So, Alexander, in short: chroot further limits the SELinux-allowed
privileges to a more fine-grained set of file system resources
(files/directories).&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Chrooted BIND for IPv6 with SELinux</title><link href="https://blog.siphos.be/2012/04/chrooted-bind-for-ipv6-with-selinux/" rel="alternate"></link><published>2012-04-14T23:08:00+02:00</published><updated>2012-04-14T23:08:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-14:/2012/04/chrooted-bind-for-ipv6-with-selinux/</id><summary type="html">&lt;p&gt;BIND, or Berkeley Internet Name Domain, is one of the Internet's most
popular domain name service software (DNS). It has seen its set of
security flaws in the past, which is not that strange as it is such a
frequently used service on the Internet. In this post, I'll give …&lt;/p&gt;</summary><content type="html">&lt;p&gt;BIND, or Berkeley Internet Name Domain, is one of the Internet's most
popular domain name service software (DNS). It has seen its set of
security flaws in the past, which is not that strange as it is such a
frequently used service on the Internet. In this post, I'll give a quick
intro on how to use it in Gentoo Hardened (with PaX)... chrooted... for
IPv6... with SELinux ;-)&lt;/p&gt;
&lt;p&gt;Installing is of course, as usual, dead easy on Gentoo
(Hardened/SELinux). Make sure you have USE="ipv6" set, and then &lt;strong&gt;emerge
bind&lt;/strong&gt;. Also install &lt;strong&gt;bind-tools&lt;/strong&gt; as they contain some great tools to
help with DNS troubleshooting. Then we're editing /etc/conf.d/named to
set the CHROOT variable. I also set CHROOT_NOMOUNT so that Gentoo
doesn't bind-mount the information in the chroot but instead uses the
files in the chroot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;CHROOT=&amp;quot;/var/named/chroot&amp;quot;
CHROOT_NOMOUNT=&amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we need to either temporarily add some privileges in SELinux, or run
the portage_t domain in permissive mode. If you go for privileges, then
add the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow portage_t var_t:chr_file { create getattr setattr };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you however want to temporarily run the portage_t domain in
permissive mode, do that as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage permissive -a portage_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We are doing this because we are now going to ask the BIND ebuild to
prepare the chroot for us. Doing so however requires portage to work on
our live file system (and not in the regular "sandbox" mode). SELinux
however forces portage in the portage_t domain and only gives it the
privileges it needs for building and installing software.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge --config bind
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When done, remove the previous SELinux allow rules again (or set the
portage_t domain back in enforcing mode, through &lt;strong&gt;semanage permissive
-d portage_t&lt;/strong&gt;). Next we need to relabel the files in the chroot. By
default, all files are labeled by SELinux as var_t in that location
because it isn't aware that it needs to see /var/named/chroot as a
"root" location.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# setfiles -r /var/named/chroot /etc/selinux/strict/contexts/files/file_contexts /var/named/chroot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So far so good. Now let's create a simple named.conf file (in
/var/named/chroot/etc/bind):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;options {
  directory &amp;quot;/var/bind&amp;quot;;
  pid-file &amp;quot;/var/run/named/named.pid&amp;quot;;
  statistics-file &amp;quot;/var/run/named/named.stats&amp;quot;;
  listen-on { 127.0.0.1; };
  listen-on-v6 { 2001:db8:81:21::ac:98ad:5fe1; };
  allow-query { any; };
  zone-statistics yes;
  allow-transfer { 2001:db8:81:22::ae:6b01:e3d8; };
  notify yes;
  recursion no;
  version &amp;quot;[nope]&amp;quot;;
};

# Access to DNS for local addresses (i.e. genfic-owned)
view &amp;quot;local&amp;quot; {
  match-clients { 2001:db8:81::/48; };
  recursion yes;
  zone &amp;quot;genfic.com&amp;quot; { type master; file &amp;quot;pri/com.genfic&amp;quot;; };
  zone &amp;quot;1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa&amp;quot; { type master; file &amp;quot;pri/inv.com.genfic&amp;quot;; };
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The zone files referenced in the configuration file are located in
/var/named/chroot/var/bind (in a subdirectory called pri - which I use
for "primary"). The regular one would look similar to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$TTL 1h ;
$ORIGIN genfic.com.
@       IN      SOA     ns.genfic.com. ns.genfic.com. (
                        2012041101
                        1d      
                        2h
                        4w
                        1h )

        IN      NS      ns.genfic.com.
        IN      NS      ns2.genfic.com.
        IN      MX      10      mail.genfic.com.
        IN      MX      20      mail2.genfic.com.

genfic.com.     IN      AAAA    2001:db8:81:80::dd:13ed:c49e;
ns              IN      AAAA    2001:db8:81:21::ac:98ad:5fe1;
ns2             IN      AAAA    2001:db8:81:22::ae:6b01:e3d8;
www             IN      CNAME   genfic.com.;
mail            IN      AAAA    2001:db8:81:21::b0:0738:8ad5;
mail2           IN      AAAA    2001:db8:81:22::50:5e9f:e569;
; (...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;while the one for reverse lookups looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$TTL 1h ;
@       IN      SOA     1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa ns.genfic.com. (
                        2012041101
                        1d
                        2h
                        4w
                        1h )

        IN      NS      ns.genfic.com.
        IN      NS      ns2.genfic.com.

$ORIGIN 1.8.0.0.8.b.d.0.1.0.0.2.ip6.arpa.

1.e.f.5.d.a.8.9.c.a.0.0.0.0.0.0.1.2.0.0         IN      PTR     ns.genfic.com.
8.d.3.e.1.0.b.6.e.a.0.0.0.0.0.0.2.2.0.0         IN      PTR     ns2.genfic.com.
; (...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now start the init script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rc-service named start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On the slave, don't set the allow-transfer directive and set its type to
"slave". In each zone, you will need to tell where the master is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;zone &amp;quot;genfic.com&amp;quot; {
  type slave;
  masters { 2001:db8:81:21::ac:98ad:5fe1; }
  file &amp;quot;sec/com.genfic&amp;quot;;
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By default, the SELinux policy for BIND does not allow BIND to write
stuff in its directories. On the slave system, you will need to change
this. A SELinux boolean here does the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# setsebool -P named_write_master_zones on;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There ya go ;-) Okay, all very condensely written, but it should give
some feedback on how to proceed. I'm adding this information to the new
online resource I'm writing - &lt;a href="http://swift.siphos.be/aglara"&gt;A Gentoo Linux Advanced Reference
Architecture&lt;/a&gt;. Nothing really ready yet,
just writing as I go forward with exploring these technologies...&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Documentation updates for initramfs needed?</title><link href="https://blog.siphos.be/2012/04/documentation-updates-for-initramfs-needed/" rel="alternate"></link><published>2012-04-12T17:40:00+02:00</published><updated>2012-04-12T17:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-12:/2012/04/documentation-updates-for-initramfs-needed/</id><summary type="html">&lt;p&gt;A quick help request from the community: if you know of any Gentoo
documents that need updates in order for end users to know when and how
to use initramfs, please file &lt;a href="https://bugs.gentoo.org"&gt;bugreports&lt;/a&gt; and
have them &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=407959"&gt;block bug
#407959&lt;/a&gt;. Currently, we
have updated the Gentoo Handbook, Gentoo Quickinstall guides and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A quick help request from the community: if you know of any Gentoo
documents that need updates in order for end users to know when and how
to use initramfs, please file &lt;a href="https://bugs.gentoo.org"&gt;bugreports&lt;/a&gt; and
have them &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=407959"&gt;block bug
#407959&lt;/a&gt;. Currently, we
have updated the Gentoo Handbook, Gentoo Quickinstall guides and added
an Initial ramfs Guide.&lt;/p&gt;
&lt;p&gt;The tracker bug is also used to check if and when the eventual roll-out
of software can happen, and we want to make sure that we do not forget
documentation (something we learned from the openrc migration). Not that
the change is as large as was the case with openrc, but it is still nice
to have updated documentation in time ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Get your devtmpfs ready</title><link href="https://blog.siphos.be/2012/04/get-your-devtmpfs-ready/" rel="alternate"></link><published>2012-04-07T22:10:00+02:00</published><updated>2012-04-07T22:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-04-07:/2012/04/get-your-devtmpfs-ready/</id><summary type="html">&lt;p&gt;If you are using stable profiles, you might want to verify if you are
already running a kernel with devtmpfs support enabled. Why? Well,
currently you might not need it, but the upcoming openrc/udev packages
require it and they currently do not fail at install time if you have …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are using stable profiles, you might want to verify if you are
already running a kernel with devtmpfs support enabled. Why? Well,
currently you might not need it, but the upcoming openrc/udev packages
require it and they currently do not fail at install time if you have it
enabled or not. As a result, upgrading these packages might give you a
system that might fail to boot (if you have no initramfs but separate
/usr partition) or gives many errors (if you have an initramfs).&lt;/p&gt;
&lt;p&gt;To verify if it is enabled, check your kernel configuration:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;# zgrep DEVTMPFS /proc/config.gz # CONFIG_DEVTMPFS is not set&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you get the output as described above, best update your kernel
configuration to include it. The second devtmpfs-related option (to
automatically mount it on /dev) is not needed afaik.&lt;/p&gt;
&lt;p&gt;And for those that have been with Gentoo for a while - devtmpfs is not
devfs. Well, it is. But it isn't. Somewhat. Oh well, there's discussion
on that which I'm not going to elaborate on. Safe to say that we're
getting older if we start feeling "Been there, done that, got the
t-shirt" ;-)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; as Robin mentioned in the comments, the udev ebuild does check
at it. However, it doesn't fail an installation so you could miss the
message. Apologies for the lies, Robin ;-) Post updated.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>More on initramfs and SELinux</title><link href="https://blog.siphos.be/2012/03/more-on-initramfs-and-selinux/" rel="alternate"></link><published>2012-03-25T19:44:00+02:00</published><updated>2012-03-25T19:44:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-03-25:/2012/03/more-on-initramfs-and-selinux/</id><summary type="html">&lt;p&gt;With the upcoming udev version &lt;em&gt;not&lt;/em&gt; supporting separate &lt;code&gt;/usr&lt;/code&gt;
locations unless you boot with an initramfs, we are
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=407959"&gt;now&lt;/a&gt;
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=408691"&gt;starting&lt;/a&gt;
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=408971"&gt;to&lt;/a&gt; document how to
create an initramfs to boot with. After all, systems with a separate
&lt;code&gt;/usr&lt;/code&gt; are not that uncommon.&lt;/p&gt;
&lt;p&gt;As I've blogged about
&lt;a href="http://blog.siphos.be/2012/01/trying-out-initramfs-with-selinux-and-grsec/"&gt;before&lt;/a&gt;,
getting an initramfs to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the upcoming udev version &lt;em&gt;not&lt;/em&gt; supporting separate &lt;code&gt;/usr&lt;/code&gt;
locations unless you boot with an initramfs, we are
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=407959"&gt;now&lt;/a&gt;
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=408691"&gt;starting&lt;/a&gt;
&lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=408971"&gt;to&lt;/a&gt; document how to
create an initramfs to boot with. After all, systems with a separate
&lt;code&gt;/usr&lt;/code&gt; are not that uncommon.&lt;/p&gt;
&lt;p&gt;As I've blogged about
&lt;a href="http://blog.siphos.be/2012/01/trying-out-initramfs-with-selinux-and-grsec/"&gt;before&lt;/a&gt;,
getting an initramfs to work well with SELinux has not been an easy
drift. In effect, I'm going to push out the FAQ (the &lt;a href="http://wiki.gentoo.org/wiki/Knowledge_Base:Booting_SELinux_with_an_initramfs"&gt;Gentoo
wiki&lt;/a&gt;
already has it) that the user will need to boot in permissive mode, and
have an init script in the boot runlevel that will reset the contexts of
&lt;code&gt;/dev&lt;/code&gt; and then switch to enforcing mode. And those that want to make
sure SELinux stays on can then also enable the
&lt;em&gt;secure_mode_policyload&lt;/em&gt; SELinux boolean so that you cannot go back to
permissive mode (without rebooting).&lt;/p&gt;
&lt;p&gt;For those interested, this is the init script I use on my guest systems
(which are for development purposes, so they do not toggle the SELinux
boolean):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#!/sbin/runscript # Copyright (c) 2007-2009 Roy Marples  # Released under the 2-clause BSD license.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;description="Switch into SELinux enforcing mode"&lt;/p&gt;
&lt;p&gt;depend()&lt;br&gt;
{&lt;br&gt;
need localmount&lt;br&gt;
}&lt;/p&gt;
&lt;p&gt;start()&lt;br&gt;
{&lt;br&gt;
ebegin "Restoring file contexts in /dev"&lt;br&gt;
restorecon -R /dev&lt;br&gt;
eend 0&lt;/p&gt;
&lt;p&gt;ebegin "Switching to enforcing mode"&lt;br&gt;
setenforce 1&lt;br&gt;
eend \$?&lt;br&gt;
}&lt;br&gt;
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I call it &lt;code&gt;selinux_enforce&lt;/code&gt; for a lack of imagination (and to make it
more clear, because if I'd name it "wookie" I'll be scratching my head
in a few weeks trying to figure out why I did that in the first place).&lt;/p&gt;
&lt;p&gt;With that enabled, I cannot provide a "denial-free" boot-up anymore
(you'll see many denials from the &lt;code&gt;init_t&lt;/code&gt; domain, amongst others, which
are best not hidden). That is to say, until I take some time to patch
the initramfs to handle SELinux.&lt;/p&gt;
&lt;p&gt;Oh, btw, this is for both dracut-generated as well as
genkernel-generated initramfs's. At least the technologies are
consistent there.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Hunting fuser</title><link href="https://blog.siphos.be/2012/03/hunting-fuser/" rel="alternate"></link><published>2012-03-12T21:54:00+01:00</published><updated>2012-03-12T21:54:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-03-12:/2012/03/hunting-fuser/</id><summary type="html">&lt;p&gt;I am able to work on Gentoo and SELinux about one hour per day. It's
more in total time, but being a bit exhausted makes me act a bit more
slowly which boils down to about one hour per day. And one hour per day
isn't bad, you're able to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am able to work on Gentoo and SELinux about one hour per day. It's
more in total time, but being a bit exhausted makes me act a bit more
slowly which boils down to about one hour per day. And one hour per day
isn't bad, you're able to do many things in that hour.&lt;/p&gt;
&lt;p&gt;The last few days, I've been hunting SELinux denials. I've set my mind
onto releasing the 2.20120215-r5 policy only when I've been able to boot
a minimalistic Gentoo installation without any visible denials, so
either dontaudit them or fix them. Of course, I only want to allow if
I'm absolutely confident that they are needed on some systems, but I
also only want to dontaudit when I understand what it is doing (and find
that it isn't something needed).&lt;/p&gt;
&lt;p&gt;Some of the denials are driving me up the walls, often having an entire
evening hunting for a single one... and this is why you haven't seen
many updates since a week or so. The one I'm hunting now, is shown in
the logs as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Mar 12 20:21:32 testsys kernel: [    6.550618] type=1400 audit(1331580090.874:4): avc:  denied  { getattr } 
  for  pid=1512 comm=&amp;quot;fuser&amp;quot; path=&amp;quot;socket:[3159]&amp;quot; dev=sockfs ino=3159 
  scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=unix_stream_socket
Mar 12 20:21:32 testsys kernel: [    6.551232] type=1400 audit(1331580090.875:5): avc:  denied  { getattr }
  for  pid=1513 comm=&amp;quot;fuser&amp;quot; path=&amp;quot;socket:[3160]&amp;quot; dev=sockfs ino=3160
  scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=netlink_kobject_uevent_socket
Mar 12 20:21:32 testsys kernel: [    6.562005] type=1400 audit(1331580090.885:6): avc:  denied  { getattr }
  for  pid=1530 comm=&amp;quot;fuser&amp;quot; path=&amp;quot;socket:[3705]&amp;quot; dev=sockfs ino=3705
  scontext=system_u:system_r:initrc_t tcontext=system_u:system_r:udev_t tclass=netlink_kobject_uevent_socket
... (these netlink_kobject_uevent_socket ones are repeated a few times)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I have &lt;em&gt;no idea&lt;/em&gt; who (or what) is executing fuser to find some
information. The shown PIDs are those of fuser, and of course that isn't
running anymore when the system is booted. The timeframe shown also
doesn't seem to provide much information, because it is the time that it
is logged by the system logger apparently (I once hoped I was wrong
here, but repeated tests and introducing delays and such seems to
confirm it). And because the target is on dev=sockfs, it's hardly
something I'm able to actively search for.&lt;/p&gt;
&lt;p&gt;Or at least that I know of.&lt;/p&gt;
&lt;p&gt;The source context is initrc_t, so it is started from an init script.
And the target is always udev_t, so it might be triggered by the udev
(or a udev-related) init script (as it seems to only look for these of
udev, but that might be a coincidence). But alas, I still don't know
what is calling it as I can't find a script or udev rule that calls
fuser :-( It doesn't affect the runtime behavior of my system
(everything seems to work just fine) so I might go on and dontaudit it.
But I so want to know what this is about.&lt;/p&gt;
&lt;p&gt;To be continued...&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Introducing 2.20120215 policies</title><link href="https://blog.siphos.be/2012/02/introducing-2-20120215-policies/" rel="alternate"></link><published>2012-02-26T18:40:00+01:00</published><updated>2012-02-26T18:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-02-26:/2012/02/introducing-2-20120215-policies/</id><summary type="html">&lt;p&gt;A few weeks after being
&lt;a href="http://oss.tresys.com/pipermail/refpolicy/2012-February/004953.html"&gt;released&lt;/a&gt;,
we now have the 20120215-based policies available for our users (and
also the newer userspace utilities). The packages currently reside in
the hardened-dev overlay as they will need to see sufficient testing
before we merge those to the main tree. For most users, nothing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks after being
&lt;a href="http://oss.tresys.com/pipermail/refpolicy/2012-February/004953.html"&gt;released&lt;/a&gt;,
we now have the 20120215-based policies available for our users (and
also the newer userspace utilities). The packages currently reside in
the hardened-dev overlay as they will need to see sufficient testing
before we merge those to the main tree. For most users, nothing changes,
albeit there are a few changes under the hood that you might get in
contact with later...&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;selinux-base-policy&lt;/em&gt; package now depends on a new package called
&lt;em&gt;selinux-base&lt;/em&gt;. This is the "real" base policy package, and now only
includes those modules that upstream (reference policy) marks as being
base modules. The rest of the modules that we (Gentoo) originally
included in base are now built by the &lt;em&gt;selinux-base-policy&lt;/em&gt; package and
inserted in the policy store together with the base policy. This change
is done to make future development a bit more flexible, but also because
the policy build fails when we include too many packages.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;selinux-unconfined&lt;/em&gt; package loads in the unconfined module. Users
that know the difference between the &lt;em&gt;strict&lt;/em&gt; and &lt;em&gt;targeted&lt;/em&gt; policy
types: loading the unconfined module in a "strict" policy will make the
system support domains like in "targeted" mode. Currently, there is
little use in this module as we (err, I) still need to get this in a
good shape. This change is needed to support unconfined domains later
when we work with MCS or MLS. The older definitions (using targeted or
strict) remain supported though.&lt;/p&gt;
&lt;p&gt;The pesky change we had to do to
&lt;code&gt;/lib64/rcscripts/addons/lvm-st{art,op}.sh&lt;/code&gt; is not necessary anymore.
This has nothing to do with the tools, but more with an update on the
policy itself. I have to give you some reason to upgrade, don't I ;-)&lt;/p&gt;
&lt;p&gt;Now that the new policy is in, we can start using named transitions as
well as use translations so that our file contexts aren't cluttered with
all those /lib64 + /lib definitions. These changes will go in later.&lt;/p&gt;
&lt;p&gt;For those interested in helping, please give these policies thorough
testing. I had some work in "forward-porting" the patches we had that
weren't included upstream yet because of changes in the underlying
structure. I hope none are forgotten. If you do find regressions, either
ping me on IRC or file a bugreport.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Transitioning to MCS policies</title><link href="https://blog.siphos.be/2012/02/transitioning-to-mcs-policies/" rel="alternate"></link><published>2012-02-24T22:12:00+01:00</published><updated>2012-02-24T22:12:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-02-24:/2012/02/transitioning-to-mcs-policies/</id><summary type="html">&lt;p&gt;Since I started maintaining the &lt;a href="http://hardened.gentoo.org/selinux"&gt;SELinux
policies&lt;/a&gt; for &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt;, the policy types we supported
were primarily &lt;code&gt;strict&lt;/code&gt; and &lt;code&gt;targeted&lt;/code&gt;. About half a year ago, we also
started supported &lt;code&gt;mcs&lt;/code&gt; and offered the possibility for using &lt;code&gt;mls&lt;/code&gt; as
well (but didn't really support that one).&lt;/p&gt;
&lt;p&gt;With the recent release of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since I started maintaining the &lt;a href="http://hardened.gentoo.org/selinux"&gt;SELinux
policies&lt;/a&gt; for &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt;, the policy types we supported
were primarily &lt;code&gt;strict&lt;/code&gt; and &lt;code&gt;targeted&lt;/code&gt;. About half a year ago, we also
started supported &lt;code&gt;mcs&lt;/code&gt; and offered the possibility for using &lt;code&gt;mls&lt;/code&gt; as
well (but didn't really support that one).&lt;/p&gt;
&lt;p&gt;With the recent release of the newer userspace utilities, we found out
that Gentoo Hardened is one of the few distributions that still really
supports policy types without levels (MCS and MLS have support for
levels, strict and targeted don't) as libsemanage had a failure when
running simple activities on a system without level support. The fix is
fairly trivial, but it does gave me the signal to start moving towards
MCS.&lt;/p&gt;
&lt;p&gt;So, now that the new userspace utilities are in the hardened-dev overlay
(please test them ;-) I will now focus on the 2.20120215 policy release,
getting that in good shape (forward-porting the patches that haven't
made it to the refpolicy repository yet) and then see how we can
transition users from strict or targeted to MCS (documentation, upgrade
guide and software or packages when needed) so that we are up to par
with the majority of other distributions.&lt;/p&gt;
&lt;p&gt;I personally like the strict policy type as it is fairly simple to
explain to users, but I'm sure I can deal with MCS and (in the future)
MLS equally well ;-)&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>This months' stabilization done, more to come</title><link href="https://blog.siphos.be/2012/01/this-months-stabilization-done-more-to-come/" rel="alternate"></link><published>2012-01-29T13:33:00+01:00</published><updated>2012-01-29T13:33:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-01-29:/2012/01/this-months-stabilization-done-more-to-come/</id><summary type="html">&lt;p&gt;A small notification to tell you that the SELinux policies that were
pushed to the main tree 30 days (or more) ago have now been stabilized
(none of them introduced problems, although some of them have other bugs
still open which are either fixed in \~arch or will be fixed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A small notification to tell you that the SELinux policies that were
pushed to the main tree 30 days (or more) ago have now been stabilized
(none of them introduced problems, although some of them have other bugs
still open which are either fixed in \~arch or will be fixed in the
hardened-dev overlay soon). I'll be working on pushing an additional set
of changes to hardened-dev overlay today as it includes fixes for openrc
that are quite important, and might even push this to the tree faster
than usual.&lt;/p&gt;
&lt;p&gt;The reference policy is also working on a new release, so the moment it
is released we will be picking that up as well (give or take a month,
since my availability will be a bit less the next month).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Trying out initramfs with selinux and grsec</title><link href="https://blog.siphos.be/2012/01/trying-out-initramfs-with-selinux-and-grsec/" rel="alternate"></link><published>2012-01-15T12:58:00+01:00</published><updated>2012-01-15T12:58:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2012-01-15:/2012/01/trying-out-initramfs-with-selinux-and-grsec/</id><summary type="html">&lt;p&gt;I'm no fan of initramfs. All my systems boot up just fine without it, so
I often see it as an additional layer of obfuscation. But there are
definitely cases where initramfs is needed, and from the &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.devel/74464"&gt;looks of
it&lt;/a&gt;, we might be
needing to push out some documentation and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm no fan of initramfs. All my systems boot up just fine without it, so
I often see it as an additional layer of obfuscation. But there are
definitely cases where initramfs is needed, and from the &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.devel/74464"&gt;looks of
it&lt;/a&gt;, we might be
needing to push out some documentation and support for initramfs. Since
my primary focus is to look at a hardened system, I started playing with
initramfs together with Gentoo Hardened, grSecurity and SELinux. And
what a challenge it was...&lt;/p&gt;
&lt;p&gt;But first, a quick introduction to initramfs. The Linux kernel supports
&lt;em&gt;initrd&lt;/em&gt; images for quite some time. These images are best seen as
loopback-mountable images containing a whole file system that the Linux
kernel boots as the root device. On this initrd image, a set of tools
and scripts then prepare the system and finally switch towards the real
root device. The initrd feature was often used when the root device is a
network-mounted location or on a file system that requires additional
activities (like an encrypted file system or even on LVM. But it also
had some difficulties with it.&lt;/p&gt;
&lt;p&gt;Using a loopback-mountable image means that this is seen as a full
device (with file system on it), so the Linux kernel also tries caching
the files on it, which leads to some unwanted memory consumption. It is
a static environment, so it is hard to grow or shrink it. Every time an
administrator creates an initrd, he needs to carefully design
(capacity-wise) the environment not to request too much or too little
memory.&lt;/p&gt;
&lt;p&gt;Enter &lt;em&gt;initramfs&lt;/em&gt;. The concept is similar: an environment that the Linux
kernel boots as a root device which is used to prepare for booting
further from the real root file systems. But it uses a different
approach. First of all, it is no longer a loopback-mountable image, but
a cpio archive that is used on a tmpfs file system. Unlike initrd, tmpfs
can grow or shrink as necessary, so the administrator doesn't need to
plan the capacity of the image. And because it is a tmpfs file system,
the Linux kernel doesn't try to cache the files in memory (as it knows
they already are in memory).&lt;/p&gt;
&lt;p&gt;There are undoubtedly more advantages to initramfs, but let's stick to
the primary objective of this post: talk about its implementation on a
hardened system.&lt;/p&gt;
&lt;p&gt;I started playing with &lt;strong&gt;dracut&lt;/strong&gt;, a tool to create initramfs archives
which is seen as a widely popular implementation (and suggested on the
gentoo development mailinglist). It uses a simple, modular approach to
building initramfs archives. It has a base, which includes a small
&lt;code&gt;init&lt;/code&gt; script and some device handling (based on &lt;code&gt;udev&lt;/code&gt;), and modules
that you can add depending on your situation (such as adding support for
RAID devices, LVM, NFS mounted file systems etc.)&lt;/p&gt;
&lt;p&gt;On a SELinux system (using a strict policy, enforcing mode) running
dracut in the &lt;code&gt;sysadm_t&lt;/code&gt; domain doesn't work, so I had to create a
&lt;code&gt;dracut_t&lt;/code&gt; domain (which has been pushed to the Portage tree yesterday).
But other than that, it is for me sufficient to call dracut to create an
initramfs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# dracut -f &amp;quot;&amp;quot; 3.1.6-hardened
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;My grub then has an additional set of lines like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;title Gentoo Linux Hardened (initramfs)
root (hd0,0)
kernel /boot/vmlinuz-3.1.6-hardened root=/dev/vda1 console=ttyS0 console=tty0
initrd /boot/initramfs-3.1.6-hardened.img
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Sadly, the bugger didn't boot. The first problem I hit was that the
Linux kernel I boot has chroot restrictions in it (grSecurity). These
restrictions further tighten chroot environments so that it is much more
difficult to "escape" a chroot. But &lt;strong&gt;dracut&lt;/strong&gt;, and probably all others,
use &lt;strong&gt;chroot&lt;/strong&gt; to further prepare the bootup and eventually switch to
the chrooted environment to boot up further. Having the chroot
restrictions enabled effectively means that I cannot use initramfs
environments. To work around, I enabled &lt;em&gt;sysctl&lt;/em&gt; support for all the
chroot restrictions and made sure that their default behavior is to be
disabled. Then, when the system boots up, it enables the restrictions
later in the boot process (through the &lt;code&gt;sysctl.conf&lt;/code&gt; settings) and then
locks these settings (thanks to grSecurity's &lt;code&gt;grsec_lock&lt;/code&gt; feature) so
that they cannot be disabled anymore later.&lt;/p&gt;
&lt;p&gt;But no, I did get further, up to the point that either the openrc init
is called (which tries to load in the SELinux policy and then breaks) or
that the initramfs tries to load the SELinux policy - and then breaks.
The problem here is that there is too much happening before the SELinux
policy is loaded. Files are created (such as device files) or
manipulated, chroots are prepared, udev is (temporarily) ran, mounts are
created, ... all before a SELinux policy is loaded. As a result, the
files on the system have incorrect contexts and the moment the SELinux
policy is loaded, the processes get denied all access and other
privileges they want against these (wrongly) labeled files. And since
after loading the SELinux policy, the process runs in &lt;code&gt;kernel_t&lt;/code&gt; domain,
it doesn't have the privileges to relabel the entire system, let alone
call commands.&lt;/p&gt;
&lt;p&gt;This is currently where I'm stuck. I can get the thing boot up, if you
temporarily work in permissive mode. When the openrc init is eventually
called, things proceed as usual and the moment udev is started (again,
now from the openrc init) it is possible to switch to enforcing mode.
All processes are running by then in the correct domain and there do not
seem to be any files left with wrong contexts (since the initramfs is
not reachable anymore and the device files in &lt;code&gt;/dev&lt;/code&gt; are now set again
by udev which is SELinux aware.&lt;/p&gt;
&lt;p&gt;But if you want to boot up in enforcing straight away, there are still
things to investigate. I think I'll need to put the policy in the
initramfs as well (which has the huge downside that every update on the
policy requires a rebuild of the initramfs as well). In that case I can
load the policy early up the chain and have the initramfs work further
running in an enforced situation. Or I completely regard the initramfs
as an "always trusted" environment and wait for openrc's init to load
the SELinux policy. In that case, I need to find a way to relabel the
(temporarily created) &lt;code&gt;/dev&lt;/code&gt; entries (like console, kmsg, ...) before
the policy is loaded.&lt;/p&gt;
&lt;p&gt;Definitely to be continued...&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Unix domain sockets are files</title><link href="https://blog.siphos.be/2011/12/unix-domain-sockets-are-files/" rel="alternate"></link><published>2011-12-31T17:48:00+01:00</published><updated>2011-12-31T17:48:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-12-31:/2011/12/unix-domain-sockets-are-files/</id><summary type="html">&lt;p&gt;Probably not a first for many seasoned Linux administrators, and
probably not correct accordingly to more advanced users than myself, but
I just found out that Unix domain sockets are files. Even when they're
not.&lt;/p&gt;
&lt;p&gt;I have been looking at a weird SELinux denial I had occuring on my
system …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Probably not a first for many seasoned Linux administrators, and
probably not correct accordingly to more advanced users than myself, but
I just found out that Unix domain sockets are files. Even when they're
not.&lt;/p&gt;
&lt;p&gt;I have been looking at a weird SELinux denial I had occuring on my
system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;avc:  denied  { read write } for  pid=10012 comm=&amp;quot;hostname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;path="socket:[318867]" dev=sockfs ino=318867 &lt;br&gt;
   scontext=system_u:system_r:hostname_t &lt;br&gt;
   tcontext=system_u:system_r:dhcpc_t &lt;br&gt;
   tclass=unix_stream_socket&lt;/p&gt;
&lt;p&gt;I had a tough time trying to figure out why in earth the &lt;strong&gt;hostname&lt;/strong&gt;
application was trying to read/write to a &lt;em&gt;socket&lt;/em&gt; that was owned by
&lt;strong&gt;dhcpcd&lt;/strong&gt;. Even more, I didn't see a &lt;em&gt;connectto&lt;/em&gt; attempt, and there is
nothing in my policy that would allow the &lt;code&gt;hostname_t&lt;/code&gt; domain to connect
to a unix_stream_socket of &lt;code&gt;dhcpc_t&lt;/code&gt;. But moreover I was intrigued why
the given path was no real path, even though it has an inode.&lt;/p&gt;
&lt;p&gt;So I dug up &lt;strong&gt;lsof&lt;/strong&gt;, which returned the following on this socket:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# lsof -p 10017
COMMAND   PID USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
...
dhcpcd  10017 root    3u     unix 0x0000000000000000      0t0 318867 socket
dhcpcd  10017 root    4w      REG              252,3        6 268749 /var/run/dhcpcd-eth1.pid
dhcpcd  10017 root    5u     unix 0x0000000000000000      0t0 318869 socket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Still no luck in figuring out what that is. And even &lt;code&gt;/proc/net/unix&lt;/code&gt;
didn't give anything back:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# grep 318867 /proc/net/unix
Num               RefCount Protocol Flags    Type St Inode Path
0000000000000000: 00000002 00000000 00000000 0001 01 318867
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I started looking at Unix domain sockets, what they are, how they are
used, etc. And I learned that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix domain sockets are just files. Well, most of the time. To use a
    socket (from server-perspective), a programmer first calls
    &lt;code&gt;socket()&lt;/code&gt; to create a socket descriptor, which is a special type of
    file descriptor. It then &lt;code&gt;bind()&lt;/code&gt;'s the socket to a (socket)file on
    the file system, &lt;code&gt;listen()&lt;/code&gt;'s for incoming connections and
    eventually &lt;code&gt;accept()&lt;/code&gt;'s them. Clients also use &lt;code&gt;socket()&lt;/code&gt; but then
    call &lt;code&gt;connectto()&lt;/code&gt; to have its socket connected to a (socket)file
    and eventually &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; (or &lt;code&gt;send()&lt;/code&gt; and &lt;code&gt;recv()&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Linux supports an abstract namespace for sockets, so not all of
    these are actually bound/connected to a file. Instead, they connect
    to a "name" instead, which cannot be traced back to a file. For
    those interested, looking at &lt;code&gt;/proc/net/unix&lt;/code&gt; or &lt;code&gt;netstat -xa&lt;/code&gt; shows
    the abstract ones starting with an &lt;code&gt;@&lt;/code&gt; sign.&lt;/li&gt;
&lt;li&gt;Not all Unix sockets (actually almost the majority of sockets on
    my system) can be traced back to either a file or abstract name.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And this latter is eating me up. I assume that these sockets were
originally created on a file system, but immediately after they were
&lt;code&gt;bind()&lt;/code&gt;'ed, the file is unlinked, making it harder (impossible?) to
find what the socket file was called to begin with. I first thought it
were sockets that were not &lt;code&gt;bind()&lt;/code&gt;'ed to, but many of them have the
state &lt;code&gt;CONNECTED&lt;/code&gt; displayed (in the &lt;strong&gt;netstat -xa&lt;/strong&gt; output) so that's
not a likely scenario. In any case, if you know how these sockets can
have an inode without a known path, please let me know.&lt;/p&gt;
&lt;p&gt;But what has this to do with my previous investigation? Well, because
the sockets are descriptors, they are passed when a process uses
&lt;code&gt;fork()&lt;/code&gt; and &lt;code&gt;execve()&lt;/code&gt;. And looking at the source code of dhcpcd, I
noticed that it does not close its file descriptors when it calls its
hook scripts (through the &lt;code&gt;exec_script()&lt;/code&gt; function of its sources). As a
result, the open file descriptors (including the sockets) are passed on
to the hook scripts - one of them calling &lt;strong&gt;hostname&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So what I saw in the AVC denials was a leaked socket (so there was no
&lt;code&gt;connectto&lt;/code&gt; originating from the &lt;code&gt;hostname_t&lt;/code&gt; domain since the
connection was made by &lt;strong&gt;dhcpc&lt;/strong&gt; in the &lt;code&gt;dhcpc_t&lt;/code&gt; domain) that is for
some reason being read/written to. A leaked unix stream socket.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Gentoo WiKi &amp; Knowledge Base</title><link href="https://blog.siphos.be/2011/12/gentoo-wiki-knowledge-base/" rel="alternate"></link><published>2011-12-26T20:01:00+01:00</published><updated>2011-12-26T20:01:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-12-26:/2011/12/gentoo-wiki-knowledge-base/</id><summary type="html">&lt;p&gt;I have been playing with the &lt;a href="http://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; the
last few days and am very impressed with the work that both the wiki
teams as well as existing contributors have already done to the place.
The look and feel is very slick and editing works just as expected. One
of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been playing with the &lt;a href="http://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; the
last few days and am very impressed with the work that both the wiki
teams as well as existing contributors have already done to the place.
The look and feel is very slick and editing works just as expected. One
of the changes I made was to move &lt;a href="http://wiki.gentoo.org/wiki/SELinux"&gt;SELinux module
information&lt;/a&gt; to the wiki. This
documentation was originally intended to be published on the &lt;a href="http://hardened.gentoo.org/selinux/"&gt;Gentoo
SELinux Project&lt;/a&gt; page, but is
easily accessible and maintainable on the wiki too.&lt;/p&gt;
&lt;p&gt;So I went a step further and dug up my original &lt;a href="http://www.gentoo.org/proj/en/glep/glep-0051.html"&gt;GLEP 0051 - Gentoo
Knowledge Base&lt;/a&gt;
proposal and checked how far I could use the Gentoo WiKi for this
purpose. From the looks of it, the WiKi can offer a great deal of
leverage for this idea and although not everything is supported through
the WiKi (like natural search language and such), that might have been
overshooting a bit. So we received a &lt;a href="http://wiki.gentoo.org/wiki/Knowledge_Base:Main_Page"&gt;Gentoo WiKi Knowledge
Base&lt;/a&gt; namespace
under which the &lt;a href="http://wiki.gentoo.org/index.php?title=Special%3AAllPages&amp;amp;from=&amp;amp;to=&amp;amp;namespace=500"&gt;Knowledgebase
entries&lt;/a&gt;
can reside.&lt;/p&gt;
&lt;p&gt;Now what is the idea behind such a knowledge base? Well, first of all,
the articles below this prefix should all follow the same structure (as
explained in the &lt;a href="http://wiki.gentoo.org/wiki/Knowledge_Base:Main_Page"&gt;main
page&lt;/a&gt;) and be
sufficiently specific so that the title of the entry should leave little
room for misinterpretation. But other than that, there is no limit as to
what the Knowledge Base could hold. To that respect, the knowledge base
section then provides a (hopefully) thorough listing of common and less
common issues with a good explanation why the problem occurred and how
to resolve it.&lt;/p&gt;
&lt;p&gt;For the time being, the location doesn't hold that many
&lt;a href="http://wiki.gentoo.org/index.php?title=Special%3AAllPages&amp;amp;from=&amp;amp;to=&amp;amp;namespace=500"&gt;entries&lt;/a&gt;
yet, but I will add them as they come along. And of course, feedback is
always appreciated ;-)&lt;/p&gt;
&lt;p&gt;On a second note, I'd like to give my PoV on the wiki and its relation
with the official Gentoo documentation. Unlike what might be
circulating, I'm definitely not against the wiki for documentation, on
the contrary. Wiki's have proven to be a good resource for
documentation, and because we can never have enough documentation
writers, every method for getting more documentation is welcome. But
because of its online nature, offline documentation development (which I
frequently do) is not possible. Also, keeping translations in sync might
be a bit more challenging compared to a file-based solution with version
control (otoh, I have little experience with WiKi translations so I
might be wrong here).&lt;/p&gt;
&lt;p&gt;I strongly believe that the wiki will play a big role in Gentoo's
documentation assets. Many of the documents currently managed by the GDP
or the subprojects might be suited to be hosted on the WiKi, especially
when those documents are too specific (and as such would require a very
specific developer profile to maintain the documents). In such cases,
the maintainers of those documents should be able to pick the most
efficient method. But for very generic documents, this might not be an
easy option.&lt;/p&gt;
&lt;p&gt;At least the Gentoo documents now support CC-BY-SA 3.0, so we can
migrate documents from the wiki to the main site, and the 2.5 version
currently used the most on the main site should be forward compatible
with 3.0 (if I read the legalese text well) so we might be able to
migrate documents from the main site to the wiki too.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; a3li created the "Knowledge Base" namespace on the wiki, so I
updated the links in my post. Thanks for the work on the wiki, a3li!&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Supporting fix scripts for XCCDF content and maintaining the documents</title><link href="https://blog.siphos.be/2011/12/supporting-fix-scripts-for-xccdf-content-and-maintaining-the-documents/" rel="alternate"></link><published>2011-12-23T16:00:00+01:00</published><updated>2011-12-23T16:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-12-23:/2011/12/supporting-fix-scripts-for-xccdf-content-and-maintaining-the-documents/</id><summary type="html">&lt;p&gt;One of the features supported through OVAL (and Open-SCAP) is to
generate fix scripts when a test has failed. The administrator can then
verify this script (of course) and then execute it to correct wrong
settings. So I decided to play around with this as well and enhanced the
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-guide.html"&gt;Gentoo …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the features supported through OVAL (and Open-SCAP) is to
generate fix scripts when a test has failed. The administrator can then
verify this script (of course) and then execute it to correct wrong
settings. So I decided to play around with this as well and enhanced the
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-guide.html"&gt;Gentoo Security
Benchmark&lt;/a&gt;
(&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-xccdf.txt"&gt;XCCDF
source&lt;/a&gt;)
with some fixables (like for the sysctl settings). And lo and behold:
the thing works ;-)&lt;/p&gt;
&lt;p&gt;After evaluating the XCCDF (together with the
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-oval.txt"&gt;OVAL&lt;/a&gt;
document) against my system, I had Open-SCAP generate a fix script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# oscap xccdf generate fix --result-id OSCAP-Test-Gentoo-Default xccdf-results.xml
#!/bin/bash
# OpenSCAP fix generator output for benchmark: Gentoo Security Benchmark

# XCCDF rule: rule-sysctl-ipv4-forward
echo 0 &amp;gt; /proc/sys/net/ipv4/ip_forward

# generated: 2011-12-23T14:53:03+01:00
# END OF SCRIPT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now isn't that nice. But generating a fix script is one thing,
&lt;em&gt;maintaining the XCCDF and OVAL documents&lt;/em&gt; is a completely other
picture.&lt;/p&gt;
&lt;p&gt;One of the downsides that I talked about earlier already is that OVAL
has quite an extensible language (it's a large XML document). Although
this extensibility is very flexible and powerful, when you want to add
generic tests (like validating sysctl values or matching regular
expressions in files) having to write over 30 lines of XML code for a
single test is time-consuming at the least. So I quickly scripted
something to help me maintain these settings.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://dev.gentoo.org/~swift/docs/genoval.xml"&gt;Generating OVAL documents with
genoval.sh&lt;/a&gt; document
explains this script (which is retrievable from my &lt;a href="https://github.com/sjvermeu/small.coding"&gt;git
repository&lt;/a&gt;) whose primary
purpose is to transform a single line into the entire OVAL structure.
With this script, I can now just say &lt;em&gt;gentoo variable USE must contain
ssl&lt;/em&gt; and it generates both the rules in the XCCDF as the OVAL statements
in the OVAL document.&lt;/p&gt;
&lt;p&gt;Okay, it's a script, not a feature-full application, but at least it
helps me (and perhaps others as well).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>SELinux Gentoo/Hardened state 2011-12-19</title><link href="https://blog.siphos.be/2011/12/selinux-gentoohardened-state-2011-12-19/" rel="alternate"></link><published>2011-12-19T18:04:00+01:00</published><updated>2011-12-19T18:04:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-12-19:/2011/12/selinux-gentoohardened-state-2011-12-19/</id><summary type="html">&lt;p&gt;On december 14th, the &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt;
project had its monthly &lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_6ee74d905f217b47446ace08da32a921.xml"&gt;online
meeting&lt;/a&gt;
to discuss the current state of affairs of its projects and subprojects.
Amongst them, the updates on the SELinux-front were presented as well.&lt;/p&gt;
&lt;p&gt;Since last meeting, the follow topics passed the revue.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://packages.gentoo.org/package/sec-policy/selinux-base-policy"&gt;sec-policy/selinux-base-policy&lt;/a&gt;,
    which is the "master …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;On december 14th, the &lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt;
project had its monthly &lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_6ee74d905f217b47446ace08da32a921.xml"&gt;online
meeting&lt;/a&gt;
to discuss the current state of affairs of its projects and subprojects.
Amongst them, the updates on the SELinux-front were presented as well.&lt;/p&gt;
&lt;p&gt;Since last meeting, the follow topics passed the revue.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://packages.gentoo.org/package/sec-policy/selinux-base-policy"&gt;sec-policy/selinux-base-policy&lt;/a&gt;,
    which is the "master" of our SELinux policies and contains those
    SELinux modules that are somewhat indivisible (hence the name,
    "base"), is now at revision 8. I tend to describe the changes on the
    gentoo-hardened mailinglist, and this is &lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_b11ef32142076034abd0616e373361da.xml"&gt;not different for rev
    8&lt;/a&gt;.
    I haven't stabilized the rev 6 one yet although I promised too, I'll
    try to find some time to do that this evening.&lt;/li&gt;
&lt;li&gt;We had a
    &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=375475"&gt;regression&lt;/a&gt; with
    &lt;strong&gt;newrole&lt;/strong&gt; for some time. Luckily, Jory "Anarchy" Pratt found
    the issue. Drop the setuid bit from the binary, and the application
    works again as it should. This will be included in the next
    &lt;a href="http://packages.gentoo.org/package/sys-apps/policycoreutils"&gt;policycoreutils&lt;/a&gt; bump.&lt;/li&gt;
&lt;li&gt;The last available
    &lt;a href="http://packages.gentoo.org/package/app-admin/sudo"&gt;sudo&lt;/a&gt; package
    now builds with native SELinux support as well, which allows users
    to add ROLE= and TYPE= information in the &lt;code&gt;sudoers&lt;/code&gt; file. As such,
    users do not need to call &lt;strong&gt;newrole&lt;/strong&gt; when they need to transition
    to a specific role for just a single command - sudo can now take
    care of that.&lt;/li&gt;
&lt;li&gt;The older &lt;code&gt;selinux/v2refpolicy/*&lt;/code&gt; profiles have been deprecated. If
    you want to use a SELinux-enabled profile, you need to use a profile
    that ends with &lt;code&gt;/selinux&lt;/code&gt;, such as
    &lt;code&gt;default/linux/amd64/10.0/selinux&lt;/code&gt; or
    &lt;code&gt;hardened/linux/amd64/selinux&lt;/code&gt;. Of course we prefer you to use a
    hardened profile ;-)&lt;/li&gt;
&lt;li&gt;Documentation-wise,
    &lt;/p&gt;&lt;ul&gt;
&lt;li&gt;the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;Gentoo Hardened SELinux
    Handbook&lt;/a&gt;
    has been updated to reflect the profile changes&lt;/li&gt;
&lt;li&gt;the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-bugreporting.xml"&gt;SELinux bugreporting
    guide&lt;/a&gt;
    has been put online to inform users what kind of information is
    needed for us to fix issues or denials that they might see&lt;/li&gt;
&lt;li&gt;the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml"&gt;SELinux
    FAQ&lt;/a&gt; has
    been updated with the questions &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml#nosuid"&gt;Applications do not transition
    on a nosuid
    partition&lt;/a&gt;
    and &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml#auth-run_init"&gt;Why do I always need to re-authenticate when operating init
    scripts?&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's about it. Not a too busy month but progress anyhow.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Supporting CC-BY-SA 3.0</title><link href="https://blog.siphos.be/2011/11/supporting-cc-by-sa-3-0/" rel="alternate"></link><published>2011-11-29T21:33:00+01:00</published><updated>2011-11-29T21:33:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-11-29:/2011/11/supporting-cc-by-sa-3-0/</id><summary type="html">&lt;p&gt;Until now, documents on the &lt;a href="http://www.gentoo.org"&gt;Gentoo website&lt;/a&gt; all
had to be licensed under the &lt;a href="https://creativecommons.org/licenses/by-sa/2.5/"&gt;Creative Commons Attribution/Share
Alike&lt;/a&gt; license, version
2.5. Why? Because at the time of the license choice, that was probably
the latest version at hand. In the XML code itself, the license tagging
was done …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Until now, documents on the &lt;a href="http://www.gentoo.org"&gt;Gentoo website&lt;/a&gt; all
had to be licensed under the &lt;a href="https://creativecommons.org/licenses/by-sa/2.5/"&gt;Creative Commons Attribution/Share
Alike&lt;/a&gt; license, version
2.5. Why? Because at the time of the license choice, that was probably
the latest version at hand. In the XML code itself, the license tagging
was done using a &lt;code&gt;&amp;lt;license /&amp;gt;&lt;/code&gt; tag. Simple and efficient. But things
change, and so do license versions.&lt;/p&gt;
&lt;p&gt;The folks over at Creative Commons have released &lt;a href="https://creativecommons.org/licenses/by-sa/3.0/"&gt;version
3.0&lt;/a&gt; somewhere in 2007.
I'm not going to cover the
&lt;a href="https://creativecommons.org/weblog/entry/7249"&gt;differences&lt;/a&gt; here, but
in general, the principle behind Gentoo's choice for the CC-BY-SA
license remains. But we didn't change our licenses and there was no real
need for it either.&lt;/p&gt;
&lt;p&gt;Recently however, the &lt;a href="http://wiki.gentoo.org"&gt;Official Gentoo Wiki&lt;/a&gt; was
announced, which uses the CC-BY-SA license as well... but the 3.0
version of it. You can't blame them for taking the latest version, but
that does made it a bit more difficult to share resources between the
two repositories (wiki versus GuideXML-ified website). The solution?
Support CC-BY-SA 3.0 for GuideXML too.&lt;/p&gt;
&lt;p&gt;A few commits in our repository made that change happen. Nothing big
though: the
&lt;a href="http://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/dtd/common.dtd?sortby=date&amp;amp;r1=1.4&amp;amp;view=log"&gt;DTD&lt;/a&gt;
is updated to allow for &lt;code&gt;&amp;lt;license version="3.0"/&amp;gt;&lt;/code&gt; tags, the
&lt;a href="http://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/guide.xsl?sortby=date&amp;amp;r1=1.252&amp;amp;view=log"&gt;XSL&lt;/a&gt;
is updated to support this attribute (and display the new license) and a
few other files (supporting files, like the &lt;a href="http://www.gentoo.org/doc/en/xml-guide.xml"&gt;GuideXML
Guide&lt;/a&gt;) have received the
necessary updates.&lt;/p&gt;
&lt;p&gt;The result of the change is that existing documents remain under the
current 2.5 license (we are not allowed to bump versions of licenses as
most documents are not copyrighted by Gentoo Foundation but by their
respective authors) but new documents can now use the 3.0 license.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; Sebastian Pipping mailed me to say that in the legal code of the
CC-BY-SA 2.5 license there is a clausule about "... a later version of
this license with the same License elements...", so perhaps I might have
a "take two" on this.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>SELinux Gentoo/Hardened state 2011-11-17</title><link href="https://blog.siphos.be/2011/11/selinux-gentoohardened-state-2011-11-17/" rel="alternate"></link><published>2011-11-17T23:29:00+01:00</published><updated>2011-11-17T23:29:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-11-17:/2011/11/selinux-gentoohardened-state-2011-11-17/</id><summary type="html">&lt;p&gt;A small write-down on the &lt;a href="http://hardened.gentoo.org/selinux"&gt;Gentoo Hardened
SELinux&lt;/a&gt; state-of-affairs, largely
triggered because there was an online meeting for the &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt; project today.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SELinux policies offered in the &lt;code&gt;sec-policy&lt;/code&gt; category are based
    on the latest refpolicy release. The older policies have been
    removed from the Portage tree. The patches …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;A small write-down on the &lt;a href="http://hardened.gentoo.org/selinux"&gt;Gentoo Hardened
SELinux&lt;/a&gt; state-of-affairs, largely
triggered because there was an online meeting for the &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt; project today.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SELinux policies offered in the &lt;code&gt;sec-policy&lt;/code&gt; category are based
    on the latest refpolicy release. The older policies have been
    removed from the Portage tree. The patches that we include in our
    policies are sent upstream and are getting eventually merged. This
    way we ensure that we keep the policies manageable (larger
    development audience), secure (more eyes looking at policy changes)
    and usable for other SELinux-enabled distributions.&lt;/li&gt;
&lt;li&gt;The userspace utilities to manage SELinux are also the latest ones
    available upstream; the older ones have been removed from the tree
    as well as to keep the number of ebuilds small enough.&lt;/li&gt;
&lt;li&gt;The Gentoo profiles that enable SELinux support are currently the
    &lt;code&gt;selinux/v2refpolicy&lt;/code&gt; ones and the &lt;code&gt;hardened/*/selinux&lt;/code&gt; ones. The
    former are the older profiles and were a bit more difficult
    to maintain. The latter ones are the newer profiles which have been
    running for quite some time now. Alas, we will be deprecating the
    &lt;code&gt;selinux/v2refpolicy&lt;/code&gt; profiles pretty soon now.&lt;/li&gt;
&lt;li&gt;The various SELinux-related documents as offered on our &lt;a href="http://hardened.gentoo.org/selinux"&gt;subproject
    page&lt;/a&gt; are regularly crosschecked
    to ensure that they are up-to-date with the latest
    SELinux state-of-affairs. An additional guide will be created on how
    to report SELinux policy bugs in bugzilla to ensure that we have the
    information that is needed to get a policy patch accepted upstream
    as well.&lt;/li&gt;
&lt;li&gt;On a HR-note: Matt Thode (known as "prometheanfire") has joined the
    ranks of SELinux developers in Gentoo Hardened. I've also taken over
    the position as Gentoo Hardened SELinux subproject lead from
    Chris Pebenito.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Security Benchmark with OVAL and Open-SCAP</title><link href="https://blog.siphos.be/2011/11/gentoo-security-benchmark-with-oval-and-open-scap/" rel="alternate"></link><published>2011-11-16T23:09:00+01:00</published><updated>2011-11-16T23:09:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-11-16:/2011/11/gentoo-security-benchmark-with-oval-and-open-scap/</id><summary type="html">&lt;p&gt;A while ago, I got referred to the &lt;a href="http://oval.mitre.org/"&gt;Open Vulnerability and Assessment
Language&lt;/a&gt;, which seems to be an open
specification (or even standard) for defining security
content/information and being able to document such things in a way that
tools can interpret it. Actually, it is a set of these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while ago, I got referred to the &lt;a href="http://oval.mitre.org/"&gt;Open Vulnerability and Assessment
Language&lt;/a&gt;, which seems to be an open
specification (or even standard) for defining security
content/information and being able to document such things in a way that
tools can interpret it. Actually, it is a set of these specifications.
But first:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;tl;dr &lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-guide.html"&gt;Gentoo Security Benchmark
Guide&lt;/a&gt;
with &lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-report.html"&gt;example report on automated
tests&lt;/a&gt;
based on
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-xccdf.txt"&gt;XCCDF&lt;/a&gt;
and
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-oval2.txt"&gt;OVAL&lt;/a&gt;,
interpreted by &lt;a href="http://www.open-scap.org/page/Main_Page"&gt;Open-SCAP&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There; now that we have that out of our way, let's continue on the
somewhat more gory details. In this first post, I'd like to talk a bit
about XCCDF and OVAL, which are both imo overly complex but interesting
XML formats.&lt;/p&gt;
&lt;p&gt;The first one, XCCDF, is better known as the &lt;strong&gt;Extensible Configuration
Checklist Description Format&lt;/strong&gt; and is an XML format in which you
document settings (what should a system look like). By itself, that
doesn't warrant another XML format. However, the power of XCCDF is that
you can define in the document &lt;em&gt;profiles&lt;/em&gt;. Each of these profiles is
then documented with the set of rules that applies to the profile. So
you can have an XCCDF document on the configuration of BIND (the
nameserver) and have two profiles: one for a single-server setup, and
one for a multi-server (master/slave) setup.&lt;/p&gt;
&lt;p&gt;These rules also define checks that you can have a tool performed
against your configuration. These checks are documented in an OVAL XML
file (&lt;strong&gt;Open Vulnerability and Assessment Language&lt;/strong&gt;) which can be
interpreted by an OVAL-compliant tool. A very simply put statement could
be: "File /etc/ssh/sshd_config must have a line that matches
'PermitRootLogin no'".&lt;/p&gt;
&lt;p&gt;Of course, XML doesn't use simple statements. In the case of OVAL, a
specific form of normalization occurs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At the beginning, you define a &lt;em&gt;definition&lt;/em&gt; that explains what you
    want to achieve (similar to the above statement) in plain text, and
    then refer to one or more criteria that needs to be passed if this
    line applies. Most checks in a configuration guide are simple
    criteria, but with OVAL you can create criteria like "If my system
    is a Gentoo x86_64 one, and I use the hardened profile, then
    criteria A must apply, but if my system does not use a hardened
    profile, it is criteria B".&lt;/li&gt;
&lt;li&gt;The criteria refers to a &lt;em&gt;test&lt;/em&gt; that needs to be executed. This test
    can be a file expression match, partition information check, service
    state, installed software, etc. but does not allow executing
    commands that the user defines (it is not considered a safe practice
    that you execute commands that are stated in the XML file since most
    OVAL interpreters will run as root). This test is based on two
    additional aspects:
    &lt;/p&gt;&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;object&lt;/em&gt; refers to the object or resource that is checked.
    This can be a partition or a file, or a list of lines that match
    an expression in a file, etc.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;state&lt;/em&gt; refers to the state that that object or resource
    should be in (or should match).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this OVAL language in place, you can now refer to several tests to
enhance your XCCDF document, and allow OVAL interpreters to test the
various rules on your system. For me, this was the major reason to look
into the language, since I had my hopes up to update or rewrite the
&lt;a href="http://www.gentoo.org/doc/en/security/security-handbook.xml"&gt;Gentoo Linux Security
Handbook&lt;/a&gt;
but with a way for users to validate if their system adheres to most/all
statements made in that document.&lt;/p&gt;
&lt;p&gt;As a matter of exercise, I started making such a security benchmark of
which you can find a &lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-guide.html"&gt;HTML version
online&lt;/a&gt;
(it's a preview URL, so might change in the future). And since it is
written with XCCDF and OVAL, I've also added an &lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/gentoo-xccdf-report.html"&gt;example report on
automated
tests&lt;/a&gt;
too. The sources of these documents are available as well
(&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-xccdf.txt"&gt;XCCDF&lt;/a&gt;
and
&lt;a href="http://dev.gentoo.org/~swift/docs/previews/oval/scap-gentoo-oval2.txt"&gt;OVAL&lt;/a&gt;
- download as txt but rename to XML then).&lt;/p&gt;
&lt;p&gt;For those adventurous enough to play with them: install
&lt;code&gt;app-forensics/openscap&lt;/code&gt; so that you can parse the files. To generate
the guide itself, use
&lt;code&gt;oscap xccdf generate scap-gentoo-xccdf.xml &amp;gt; guide.html&lt;/code&gt;. To run the
tests associated with it, use
&lt;code&gt;oscap xccdf eval --oval-results --profile Gentoo-Default --report report.html scap-gentoo-xccdf.xml&lt;/code&gt;.
Also take a look at the
&lt;a href="http://www.open-scap.org/page/Main_Page"&gt;Open-SCAP&lt;/a&gt; website which is a
good resource as well (and the mailinglists are low traffic but with
good response times!).&lt;/p&gt;
&lt;p&gt;So, what is the future on all this for me?&lt;/p&gt;
&lt;p&gt;First, I'm going to work a bit further on the OVAL statements, so that I
can automatically test the majority of settings that I currently have in
the benchmark/guide. Only when I'm far enough will I continue on the
content of the guide (since it is far from finished) and also see if
this isn't something that can be put on a somewhat more official
location. If not, I'll still continue developing it, but it'll remain on
my dev-page.&lt;/p&gt;
&lt;p&gt;When I'm somewhat satisfied with that, I might check if I can't have
OVAL enhanced with some Gentoo-specific objects (there are already
objects for RedHat-like and Debian-like distributions) so that we can
write tests for Gentoo settings (like USE flags, profiles, enabled GCC
specs, etc.) If we have that, then we can even write checks (XCCDF and
OVAL based) to validate if a system is how it is supposed to be
(wouldn't that be great, an automated test that tells you if your system
is properly set up according to our documents).&lt;/p&gt;
&lt;p&gt;But XCCDF and OVAL isn't the end. There are other formats available as
well, like CCE (for configuration entries), CVE/CPE (to check
vulnerability information and its target software/platform) and more. I
know RedHat is already actively using OVAL for its &lt;a href="https://www.redhat.com/security/data/oval/"&gt;security
advisories&lt;/a&gt;, and other sites
like &lt;a href="http://cisecurity.org/"&gt;Center for Internet Security&lt;/a&gt; are also
using XCCDF and OVAL to document and work with security guides. So why
would Gentoo not get on that train as well?&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Centers of Excellence</title><link href="https://blog.siphos.be/2011/10/centers-of-excellence/" rel="alternate"></link><published>2011-10-25T20:12:00+02:00</published><updated>2011-10-25T20:12:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-10-25:/2011/10/centers-of-excellence/</id><summary type="html">&lt;p&gt;When dealing with software (I'll talk about software here, but the
information is applicable to most technologies, such as appliances and
operating systems) many organizations want to have "centers of
excellence" with respect to the software. These teams are responsible
for positioning the software within the organization, supporting the
software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When dealing with software (I'll talk about software here, but the
information is applicable to most technologies, such as appliances and
operating systems) many organizations want to have "centers of
excellence" with respect to the software. These teams are responsible
for positioning the software within the organization, supporting the
software and if necessary, act as a link between the internal customer
and the software vendor.&lt;/p&gt;
&lt;p&gt;The approach on these "centers of excellence" is often described as a
cost efficient way of handling the software within the organization.
Sadly, many organizations go to the extreme and try to put as much
support and services within those teams as possible, hoping that full
consolidation of all service matters would yield an even better
(financial) benefit.&lt;/p&gt;
&lt;p&gt;Such further consolidation however has a negative side which is often
overlooked: &lt;em&gt;centralized teams are less aware of the internal customers'
requirements and situation&lt;/em&gt;. Most internal customers probably have their
own IT teams that are much better informed about the criticality of the
customers' systems and the services that the customer requests. Those
teams are then responsible for getting the right services from those
"centers of excellence". And that is where the difficulty lies.&lt;/p&gt;
&lt;p&gt;"Centers of excellence" are &lt;em&gt;based on products and technical services&lt;/em&gt;.
They want to provide the best-in-class service with their products and
as such keep enhancing their services in the hope that they can serve
all internal customers. But by doing so, they are becoming more and more
of a product vendor. In the end, they either focus on the product
completely, or they focus on some frameworks and tooling that they have
designed and developed to support the integration of the product within
the organization. For the IT teams of the internal customer however, the
"centers of excellence" are less seen as part of the organization and
more as a vendor (or broker).&lt;/p&gt;
&lt;p&gt;That doesn't mean that the concept of these "centers of excellence" is
wrong though, but they need to keep the organization itself in mind when
dealing with the product. The detail on services that they offer need to
be aligned with the organizations' strategy and weighted to provide a
cost efficient, yet qualitatively best-in-class service.&lt;/p&gt;
&lt;p&gt;Of course, all that is easier said than done. So let me suggest an
approach on software service(s) within an organization.&lt;/p&gt;
&lt;p&gt;The lowest service that the organization must support for any software
is &lt;strong&gt;assistance in the installation, upgrade, tracking and eventual
removal of the software&lt;/strong&gt;. Such a service is always needed and can be
provided with little knowledge of the internal customer. A "center of
excellence" should provide the means to (semi)automatically install the
software - preferably using the organizations' standard software
deployment methods, upgrade the software (both for major releases, minor
releases as well as security patches), remove and (not to forget) track
where the software is installed. Especially when dealing with
proprietary software, tracking is almost mandatory for organizations to
keep track of the licenses needed.&lt;/p&gt;
&lt;p&gt;This lowest service offering has almost only positive sides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IT teams that need the software can easily (and without further
    assistance of other teams) install the software. If the installation
    method is automated, it can even be done in a fast manner, which is
    always to the liking of the customer.&lt;/li&gt;
&lt;li&gt;The organization keeps its risks low by providing the security
    updates and product upgrades in a seem less manner.&lt;/li&gt;
&lt;li&gt;By tracking deployments, the organization can keep track of licenses
    used (for instance, the "center of excellence" can provide regular
    reporting towards the financial departments) and, if the tracking is
    done right, can even suggest improvements in the architecture or
    deployments to further minimize license cost.&lt;/li&gt;
&lt;li&gt;IT teams can freely focus on the solution that they are building for
    the customer without the need to duplicate software installation
    methods and different patching processes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By providing these services from a "center of excellence", you
definitely reduce certain research &amp;amp; development costs - without this
offering, each team would need to develop processes to deploy software
and track its usage. This is independent of the internal customer and as
such, consolidation is a definite win here.&lt;/p&gt;
&lt;p&gt;Once this service is offered, the "center of excellence" can focus on
the services that they can provide and which are &lt;strong&gt;mandatory for all
internal customers&lt;/strong&gt; (not "usable", but mandatory) and for which little
flexibility (in design or development) is possible. There are not that
many cases here, and this is very specific to each technology and
organization in which the software is made available.&lt;/p&gt;
&lt;p&gt;As a hypothetical example, consider an LDAP service. The "center of
excellence" might want to provide auditing (and alignment with an
organization standard auditing system) if the organization has a policy
that auditing is mandatory, regardless of the project for which LDAP is
used. Of course, if the "center of excellence" wants to stop at this
service offering (i.e. the previously mentioned installation/tracking,
and now auditing) then this is most likely a best practice document
geared towards the IT teams that need to implement it.&lt;/p&gt;
&lt;p&gt;The benefit? The IT teams are in this case aware of the requirement
(auditing must be enabled) and do not need to investigate how to do this
anymore (it is already documented).&lt;/p&gt;
&lt;p&gt;The third level of service offering that I see is the &lt;strong&gt;reusable,
customer-independent services&lt;/strong&gt; that one wants to provide on the
software. For a database, this might mean alignment with the
organizations' backup infrastructure. For an LDAP, that might mean
getting feeds from a central source (be it a central LDAP
infrastructure, an Identity Management system, ...).&lt;/p&gt;
&lt;p&gt;When you consider providing this service (which is usually the case in
larger environments), take special care that the service you want to
offer is flexible enough so that any IT team can work with it. A service
that is only applicable to 70% of your internal customers will not do
it. For an LDAP service, this might mean that you provide out-of-the-box
configuration templates, best practice information for its back-end
infrastructure (which includes backup/restore operations), ...&lt;/p&gt;
&lt;p&gt;But make sure that you are not redesigning and re-developing what your
product already provides. I have seen numerous cases where teams develop
tools that should "shield the complexity" from the end user, but in
effect are creating additional layers of clouds and complexity instead.
If you want or need to abstract complexity from the user, make sure that
this is only a single layer that you are introducing. The moment you are
creating tools on top of previously written tools, you should reconsider
your actions.&lt;/p&gt;
&lt;p&gt;Often, "centers of excellence" want to rewrite documentation for the end
users. They feel that the documentation available from the vendor is too
complex for IT teams to use. Although I can relate to that, they should
not underestimate the expertise within the IT teams. If the IT teams do
not want to gain the knowledge or experience through the product guides,
then they are less likely to properly maintain and troubleshoot the
product.&lt;/p&gt;
&lt;p&gt;In such cases, I would wager that it is more beneficial for the
organization to look at their (human) resources and their relation to
the software. In times like these, where cloud solutions play an
important role, my suggestion would be to consolidate the software usage
towards a SaaS principle (&lt;strong&gt;Software-as-a-Service&lt;/strong&gt;) managed by an
experienced team (or teams). This does not mean that the "center of
excellence" has to play this role, but it does sound like a logical step
for them. After all, if the "center of excellence" only defines
additional services without actually consuming them, they might lose
track of the product in real-case scenario's.&lt;/p&gt;
&lt;p&gt;Take a web server for example - Apache. You might have a "center of
excellence" for Apache web servers, which provides easily deployable
packages. They might provide example configuration files as well as
pointers towards Apache's documentation (and best practices). They track
the deployments and ensure that security patches are available as soon
as possible. But they should guard over the rest of the services that
they want to offer.&lt;/p&gt;
&lt;p&gt;Why would that team create a framework for auto-generating configuration
files? What is the benefit of this for the entire organization, for each
customer? If the IT team can take care of the configuration, let them
be. But if those IT teams would rather not manage these web servers
themselves, what is the point in creating additional frameworks to "hide
the complexity"? It is, imo, much more efficient to see if you cannot
provide web hosting services instead, and have the IT teams "buy" these
(internal) web hosting services.&lt;/p&gt;
&lt;p&gt;There are many advantages to this: the web hosting environment is
managed by a team of experts, can be consolidated to keep the TCO low,
can provide default configurations that are fully aligned with the
organization but still offer the flexibility that individual customers
might require.&lt;/p&gt;
&lt;p&gt;Same goes for other software products: java application servers (like
JBoss or IBM WebSphere AS), databases (Oracle, SQL Server, MySQL or
Postgresql), messaging systems, log servers, LDAP services, file share
services, ...&lt;/p&gt;
&lt;p&gt;So what about you - how do you position "centre of excellence" teams?
Can you relate with such a "back to basics" approach, or would you
rather see a fully integrated, standardized solution roll-out where IT
teams only have experience with the organization-only frameworks?&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>SELinux' 2011/07 releases now stable</title><link href="https://blog.siphos.be/2011/10/selinux-201107-releases-now-stable/" rel="alternate"></link><published>2011-10-23T15:07:00+02:00</published><updated>2011-10-23T15:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-10-23:/2011/10/selinux-201107-releases-now-stable/</id><summary type="html">&lt;p&gt;A few minutes ago, I stabilized both the 2.20110726 policies as well as
the SELinux userspace utilities that were stable (upstream) on 20110727.
With the change, I also updated the &lt;a href="http://hardened.gentoo.org/selinux/selinux-handbook.xml"&gt;Gentoo SELinux
Handbook&lt;/a&gt; with
the changes I presented on our
&lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_73ddd74112bef0007f361f3598140a21.xml"&gt;gentoo-hardened&lt;/a&gt;
mailinglist. After some time, I'll remove the now …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few minutes ago, I stabilized both the 2.20110726 policies as well as
the SELinux userspace utilities that were stable (upstream) on 20110727.
With the change, I also updated the &lt;a href="http://hardened.gentoo.org/selinux/selinux-handbook.xml"&gt;Gentoo SELinux
Handbook&lt;/a&gt; with
the changes I presented on our
&lt;a href="http://archives.gentoo.org/gentoo-hardened/msg_73ddd74112bef0007f361f3598140a21.xml"&gt;gentoo-hardened&lt;/a&gt;
mailinglist. After some time, I'll remove the now obsoleted older
policies and userspace utilities to keep the tree in a sane state.&lt;/p&gt;
&lt;p&gt;There are a few policy packages whose stabilized version isn't the
latest (cfr earlier post), those are due within the proper designated
period (about 1 month).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Hardened SELinux policies, rev 5</title><link href="https://blog.siphos.be/2011/10/gentoo-hardened-selinux-policies-rev-5/" rel="alternate"></link><published>2011-10-13T18:30:00+02:00</published><updated>2011-10-13T18:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-10-13:/2011/10/gentoo-hardened-selinux-policies-rev-5/</id><summary type="html">&lt;p&gt;I've pushed out &lt;code&gt;selinux-base-policy&lt;/code&gt; version 2.20110726-r5 to the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=summary"&gt;hardened-dev&lt;/a&gt;
overlay. It does not hold huge changes, most of them are rewrites or
updates on pre-existing patches (on the SELinux policies) to make them
conform the refpolicy naming conventions and other guidelines. It
includes preliminary support for the &lt;a href="http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html"&gt;XDG
Specification …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've pushed out &lt;code&gt;selinux-base-policy&lt;/code&gt; version 2.20110726-r5 to the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=summary"&gt;hardened-dev&lt;/a&gt;
overlay. It does not hold huge changes, most of them are rewrites or
updates on pre-existing patches (on the SELinux policies) to make them
conform the refpolicy naming conventions and other guidelines. It
includes preliminary support for the &lt;a href="http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html"&gt;XDG
Specification&lt;/a&gt;
although changes there are still going to occur (as the policy is still
under development). Other updates are primarily on the policies for user
applications (pan, mozilla, skype), portage and asterisk.&lt;/p&gt;
&lt;p&gt;In related news, the &lt;a href="http://hardened.gentoo.org/selinux-faq.xml"&gt;Gentoo Hardened SELinux
FAQ&lt;/a&gt; is updated with two
entries:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml#recoverportage"&gt;Portage fails to label files because "setfiles" does not work
    anymore&lt;/a&gt;,
    and&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml#nosuid"&gt;Applications do not transition on a nosuid-mounted
    partition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Gentoo"></category></entry><entry><title>Upgrading GCC, revisited</title><link href="https://blog.siphos.be/2011/10/upgrading-gcc-revisited/" rel="alternate"></link><published>2011-10-13T18:23:00+02:00</published><updated>2011-10-13T18:23:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-10-13:/2011/10/upgrading-gcc-revisited/</id><summary type="html">&lt;p&gt;Gentoo has, since long, had a GCC Upgrading guide. A long time ago,
upgrading GCC required quite a lot of side activities and was often
considered a risky upgrade. But times change, and so do the GCC upgrade
cycles. Improved compatibility as well as a better understood impact
made GCC …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Gentoo has, since long, had a GCC Upgrading guide. A long time ago,
upgrading GCC required quite a lot of side activities and was often
considered a risky upgrade. But times change, and so do the GCC upgrade
cycles. Improved compatibility as well as a better understood impact
made GCC upgrades a lot less painful. Sadly, our documentation didn't
reflect that as much - the last update on the GCC Upgrade guide was from
2008.&lt;/p&gt;
&lt;p&gt;Today, after some &lt;a href="http://archives.gentoo.org/gentoo-dev/msg_62dbadfe0175cdf96a7751f69d2f5af7.xml"&gt;clarifications from
gentoo-dev&lt;/a&gt;,
I've &lt;a href="http://www.gentoo.org/doc/en/gcc-upgrading.xml"&gt;updated the GCC Upgrading
Guide&lt;/a&gt; to reflect the
improved situation. I'm hoping this will help clear out the uncertainty
surrounding GCC upgrades. As always, comments and feedback are always
welcome.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Mitigating risks, part 5 - application firewalls</title><link href="https://blog.siphos.be/2011/10/mitigating-risks-part-5-application-firewalls/" rel="alternate"></link><published>2011-10-05T23:38:00+02:00</published><updated>2011-10-05T23:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-10-05:/2011/10/mitigating-risks-part-5-application-firewalls/</id><summary type="html">&lt;p&gt;The last &lt;em&gt;isolation-related&lt;/em&gt; aspect on risk mitigation is called
&lt;strong&gt;application firewalls&lt;/strong&gt;. Like more "regular" firewalls, its purpose is
to be put in front of a service, controlling which data/connections get
through and which don't. But unlike these regular firewalls,
&lt;a href="https://en.wikipedia.org/wiki/Application_firewall"&gt;application
firewalls&lt;/a&gt; work on
higher-level protocols (like HTTP, FTP) that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The last &lt;em&gt;isolation-related&lt;/em&gt; aspect on risk mitigation is called
&lt;strong&gt;application firewalls&lt;/strong&gt;. Like more "regular" firewalls, its purpose is
to be put in front of a service, controlling which data/connections get
through and which don't. But unlike these regular firewalls,
&lt;a href="https://en.wikipedia.org/wiki/Application_firewall"&gt;application
firewalls&lt;/a&gt; work on
higher-level protocols (like HTTP, FTP) that deal with user data rather
than with connection routing. I'm going to call these firewalls "network
firewalls", although most modern network firewalls have some application
firewall functionality as well.&lt;/p&gt;
&lt;p&gt;The purpose and necessity of network firewalls is well known and
understood: make sure that the service is only accessible from the right
location, check if connections aren't abused (or too many connections
are made), etc. But what if the connection itself is valid? After all,
most abuse of services is not because they originate from the wrong
location or try to access the wrong service. Instead, such abuse comes
from valid access to the application, but with less kosher intentions.
So what can application firewalls do in this case?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because they perform inspection of the data that is transferred
    itself, application firewalls can &lt;strong&gt;detect malicious data
    fragments&lt;/strong&gt; or attempts to abuse the service. These detection rules
    can be based on general, heuristic rules (well-known examples are
    detection rules for cross-site scripting attacks (XSS) or
    SQL Injection) but can also be very specific to a
    particular application.&lt;/li&gt;
&lt;li&gt;Because all data is transferred through the firewall and the
    firewall has knowledge of the application itself, these firewalls
    offer &lt;strong&gt;advanced auditing features&lt;/strong&gt; since they can detect
    authentication steps, user data, application-specific transactions
    and more.&lt;/li&gt;
&lt;li&gt;With knowledge of the users' session and
    behavior (application-level) and origin (network level), application
    firewalls can &lt;strong&gt;detect and prevent unauthorized sessions&lt;/strong&gt;, such as
    the case with session hijacking or even man-in-the-middle attacks
    (based on behavior detection)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Implementing an application firewall however doesn't only mean that you
improve access controls on it. It has other advantages that make
application firewalls an important part in many architectures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If all service access is forced through the application firewall
    (for instance through an IP filter on the service that only allows
    connections from the application firewalls) you can implement rules
    that &lt;strong&gt;deter known attacks/vulnerabilities&lt;/strong&gt; without needing to fix
    the code itself (or if fixing is possible, lower the time pressure).
    For instance, for Apache-based services, such an application
    firewall could detect or even change the &lt;code&gt;Range:&lt;/code&gt; header on
    malicious requests to lower the impact of this potentially nasty DoS
    vulnerability&lt;/li&gt;
&lt;li&gt;Depending on the complexity, some &lt;strong&gt;functional application bug
    fixing&lt;/strong&gt; might even be possible. For instance changing content types
    on requests/replies (HTTP), adding a domain on an FTP accounts'
    login statement, ...&lt;/li&gt;
&lt;li&gt;Many application firewalls (or gateways) offer proxy functionality
    which might &lt;strong&gt;improve response times&lt;/strong&gt;. This is not a sure-given,
    since most applications are session-aware so the advantage is only
    for session-agnostic requests (be it static content or specific SQL
    statements in case of a database firewall). But also in case of
    session-aware statements can an improvement be found. Consider a
    database firewall which translates SQL statements from an
    unsupported application towards better defined statements (for
    instance using proper indexes or materialized views).&lt;/li&gt;
&lt;li&gt;In some cases, you might even be able to upgrade a backend of an
    unsupported application (which previously required an outdated
    version of that database) by translating the backend requests when
    they are incompatible with the new backend version. So you can
    &lt;strong&gt;improve integration&lt;/strong&gt; or &lt;strong&gt;support unsupported upgrades&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In case of risk reduction, application firewalls also allow you to
    move a service elsewhere (even in the &lt;strong&gt;public cloud&lt;/strong&gt;) and still
    keep the access under control.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, it would be TGTBT (Too Good To Be True) if there isn't an
(important) downside: &lt;em&gt;maintaining the application firewall is a
daunting task&lt;/em&gt;. Because of its flexibility, you'll need deep knowledge
in the application firewall administration and development, keep track
of all rules you have (and why you have them), do lots and lots of
testing on each rule (since it might affect the functioning of the
application) and still be aware that subtle differences introduced by
the application firewall rules can pop up unexpectedly. Also,
integrating an application firewall is another service between your
customer and his service, which might influence performance but also
makes the underlying architecture more complex. Finally, you'll need to
consider that an application firewall requires lots of resources
(CPU/memory), especially when it needs to perform SSL/TLS termination.
Oh, and they're often expensive too.&lt;/p&gt;
&lt;p&gt;Still, even with these downsides, application firewalls are an important
part of the service isolation strategy, which is a key aspect in the
&lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-1/"&gt;risk mitigation
strategy&lt;/a&gt; which
this series started with. We've focused on three now: &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/"&gt;service
isolation&lt;/a&gt;
(network-wise), process isolation (through &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-4-mandatory-access-control/"&gt;mandatory access
control&lt;/a&gt;)
and now access isolation through application firewalls. And with proper
&lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-3-hardening/"&gt;hardening&lt;/a&gt;
in place, I believe that you have done all you can do to reduce the
risks when running unsupported software (apart from upgrading it or
switching towards supported software).&lt;/p&gt;
&lt;p&gt;If you have other ideas that benefit risk mitigation, with specific
focus on unsupported software, I would be glad to hear about them.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Quickly setup a Gentoo system</title><link href="https://blog.siphos.be/2011/09/quickly-setup-a-gentoo-system/" rel="alternate"></link><published>2011-09-24T15:34:00+02:00</published><updated>2011-09-24T15:34:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-24:/2011/09/quickly-setup-a-gentoo-system/</id><summary type="html">&lt;p&gt;In order to verify if the installation instructions in the Gentoo
Handbook are still valid, and to allow me to quickly seed new Gentoo
installations in a virtual environment, I wrote a &lt;em&gt;very ugly&lt;/em&gt; (really)
script to automatically "stage" a Gentoo Linux installation in a KVM
guest. This is &lt;strong&gt;not …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;In order to verify if the installation instructions in the Gentoo
Handbook are still valid, and to allow me to quickly seed new Gentoo
installations in a virtual environment, I wrote a &lt;em&gt;very ugly&lt;/em&gt; (really)
script to automatically "stage" a Gentoo Linux installation in a KVM
guest. This is &lt;strong&gt;not&lt;/strong&gt; my intention to make this an "unattended"
installation script, it is merely one of the many scripts out there to
help some poor developer in working a bit more agile.&lt;/p&gt;
&lt;p&gt;I decided to &lt;a href="http://dev.gentoo.org/~swift/docs/gensetup-guide.xml"&gt;document
gensetup&lt;/a&gt; as a
first step (cfr my earlier &lt;a href="http://blog.siphos.be/2011/09/catching-up/"&gt;Catching
up&lt;/a&gt; post) in my quest to
document how to setup a virtual Gentoo Hardened (with SELinux) virtual
architecture. The &lt;strong&gt;gensetup&lt;/strong&gt; tool is just to provide a (semi)automated
way to install Gentoo according to the instructions in the Gentoo
Handbook. Later, I'll add documentation for the &lt;code&gt;setup_*.sh&lt;/code&gt; scripts
that I use to upgrade such a base installation to a specific
server/service.&lt;/p&gt;
&lt;p&gt;You want a probably better working installation script, check out
&lt;a href="http://www.agaffney.org/quickstart.php"&gt;Andrew Gaffney's Quickstart for
Gentoo&lt;/a&gt;. And if you know of
other such scripts, I would be glad to hear from them, if not just to
keep track of the various similar projects out there.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt; The quickstart application does not seem to be maintained
anymore. My bad. However, suggestions are made in the comments for more
up-to-date systems ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Power management guide updated</title><link href="https://blog.siphos.be/2011/09/power-management-guide-updated/" rel="alternate"></link><published>2011-09-23T21:57:00+02:00</published><updated>2011-09-23T21:57:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-23:/2011/09/power-management-guide-updated/</id><summary type="html">&lt;p&gt;The &lt;a href="http://www.gentoo.org/doc/en/power-management-guide.xml"&gt;Gentoo Power Management
Guide&lt;/a&gt; is now
updated. It is a full rewrite, focusing currently on two main toolsets:
&lt;a href="http://samwel.tk/laptop_mode/"&gt;Laptop Mode Tools&lt;/a&gt; and
&lt;a href="http://www.linux.it/~malattia/wiki/index.php/Cpufreqd"&gt;cpufreqd&lt;/a&gt;. I was
pleasantly surprised by the number of features that the laptop mode
tools package provided.&lt;/p&gt;
&lt;p&gt;Of course, this does not mean that the guide is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="http://www.gentoo.org/doc/en/power-management-guide.xml"&gt;Gentoo Power Management
Guide&lt;/a&gt; is now
updated. It is a full rewrite, focusing currently on two main toolsets:
&lt;a href="http://samwel.tk/laptop_mode/"&gt;Laptop Mode Tools&lt;/a&gt; and
&lt;a href="http://www.linux.it/~malattia/wiki/index.php/Cpufreqd"&gt;cpufreqd&lt;/a&gt;. I was
pleasantly surprised by the number of features that the laptop mode
tools package provided.&lt;/p&gt;
&lt;p&gt;Of course, this does not mean that the guide is now finished.
Documentation never is, so do keep on suggesting improvements (and
pointing to bugs) in &lt;a href="https://bugs.gentoo.org"&gt;Gentoo's bugzilla&lt;/a&gt;.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Mitigating risks, part 4 - Mandatory Access Control</title><link href="https://blog.siphos.be/2011/09/mitigating-risks-part-4-mandatory-access-control/" rel="alternate"></link><published>2011-09-23T20:16:00+02:00</published><updated>2011-09-23T20:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-23:/2011/09/mitigating-risks-part-4-mandatory-access-control/</id><summary type="html">&lt;p&gt;I've talked about &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/"&gt;service
isolation&lt;/a&gt;
earlier and the risks that it helps to mitigate. However, many
applications still run as highly privileged accounts, or can be abused
to execute more functions than intended. Service isolation doesn't help
there, and system hardening can only go that far. The additional
countermeasures that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've talked about &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/"&gt;service
isolation&lt;/a&gt;
earlier and the risks that it helps to mitigate. However, many
applications still run as highly privileged accounts, or can be abused
to execute more functions than intended. Service isolation doesn't help
there, and system hardening can only go that far. The additional
countermeasures that you can take are application firewalls and
mandatory access control. And now you know what part 5 will talk about
;-)&lt;/p&gt;
&lt;p&gt;Standard access control on most popular operating systems is based on a
limited set of privileges (such as read, write and execute) on a limited
scale (user, group, everyone else). Recent developments are showing an
increase in the privilege flexibility, with the advent of &lt;a href="http://www.gentoo.org/proj/en/hardened/capabilities.xml"&gt;manageable
capabilities&lt;/a&gt;
(Linux/Unix) or &lt;a href="http://technet.microsoft.com/en-us/windowsserver/bb310732"&gt;Group
Policies&lt;/a&gt;
(Windows). However, these still lack some important features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users are still able to &lt;strong&gt;delegate their privilege&lt;/strong&gt; to others. A
    user with read access on a particular file can copy that file to a
    public readable location so others can read it as well. Privileges
    on his own files and directories are fully manageable by the owner.
    For our risk mitigation approach on unsupported software, that means
    that a vulnerability might be exploited so that the service
    "leaks" information. It is especially important in an attack that
    uses a sequence of vulnerabilities (such as in an &lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/Advanced_persistent_threat"&gt;advanced
    persistent
    threat&lt;/a&gt;)
    where low-risk vulnerabilities can be combined into a
    high-risk exploit.&lt;/li&gt;
&lt;li&gt;Privileges are still &lt;strong&gt;user-level privileges&lt;/strong&gt; (including technical
    account users). In case of running services, this almost always
    means that the process has more privileges than it requires. Some
    software titles allow for dropping capabilities when not
    needed anymore. Most however are oblivious of the rights
    they possess. Abuse of the service (which includes use of features
    that the service offers but are not allowed policy-wise by
    the organization) cannot be prevented if
    &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-3-hardening/"&gt;hardening&lt;/a&gt;
    doesn't disable it.&lt;/li&gt;
&lt;li&gt;Privileges are &lt;strong&gt;managed by many actors&lt;/strong&gt; (such as the
    system administrators) and are not that easy to audit. Privilege
    denials are often not audited, causing issues to only come up when
    they occur, rather then when the attempt to provoke issues
    is started. In many cases, a malicious (or "playful, inventive")
    user starts with investigating and trying out long before a way is
    found to abuse the service.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In case of a mandatory access control system, a security administrator
is responsible for writing and managing a security policy which is
&lt;strong&gt;enforced by the operating system&lt;/strong&gt; (well, higher level enforcement
would be even better, but is currently not realistic). Once enforced,
the policy ensures that privileges are not delegated (unless allowed).
Also, in most MAC systems, the policy allows for a much &lt;strong&gt;more detailed
privilege granularity&lt;/strong&gt;. And recent server operating systems have
support for MAC - I personally work with
&lt;a href="http://hardened.gentoo.org/selinux"&gt;SELinux&lt;/a&gt; for the (GNU/)Linux
operating system.&lt;/p&gt;
&lt;p&gt;But this more granular flexibility in privileges comes with some costs.
First of all, it becomes much more &lt;strong&gt;complex to manage the policy&lt;/strong&gt;.
You'll need highly experienced administrators to work with a MAC-enabled
system. Second, a MAC model has a &lt;strong&gt;negative influence on performance&lt;/strong&gt;
since the system has to check many more accesses and access rules. To
make MAC-enabled systems workable, operating systems offer a &lt;em&gt;default
policy&lt;/em&gt; which already covers many services. Also, developers on the MAC
technology are continuously safe-guarding performance - I personally do
not notice a performance degradation when using SELinux, and more
realistic benchmarks suggest that the impact of SELinux is between 3%
and 12% depending on the policy level.&lt;/p&gt;
&lt;p&gt;But what does that mean towards the initial &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-1"&gt;risk
list&lt;/a&gt; that I
identified in the beginning of this article series? Well, directly, very
little: mandatory access control in this case is about reducing the
impact of security vulnerabilities (and abuse of the service). It will
not help you out in other ways. However, there are other things to gain
from a mandatory access control than just threat reduction.&lt;/p&gt;
&lt;p&gt;An advantage is - again - that you get to &lt;strong&gt;know your application&lt;/strong&gt;
well, especially if you had to write a security policy for it. Since you
need to define what files it can access, which kind of accesses it is
allowed to do, which commands it can execute, etc, it will give you
insight in how the application operates. Bugs in the application might
be solved faster and you'll definitely learn more about how the
application is integrated. Another one is that most mandatory access
control systems have much more &lt;strong&gt;detailed auditing&lt;/strong&gt; capabilities.
Attempts to abuse the service will result in denials which are detected
and on which you can then take proper action.&lt;/p&gt;
&lt;p&gt;Taking a higher-level look at mandatory access control will show you
that, in case of risk mitigation, it is much more like service
isolation, but then on the operating system level. You isolate the
processes, governing the accesses they are allowed to do.&lt;/p&gt;
&lt;p&gt;But the one main issue - active exploits on the application service -
cannot be hindered by neither service isolation (since the service is
still accessible), hardening (although it might help) or mandatory
access control (which reduces the actions an exploit can do). To make
sure that vulnerabilities are less likely to be exploited, I'll talk
about &lt;em&gt;application firewalls&lt;/em&gt; in the next post.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Catching up</title><link href="https://blog.siphos.be/2011/09/catching-up/" rel="alternate"></link><published>2011-09-18T16:51:00+02:00</published><updated>2011-09-18T16:51:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-18:/2011/09/catching-up/</id><summary type="html">&lt;p&gt;As
&lt;a href="http://archives.gentoo.org/gentoo-doc/msg_f6d85b4b80e2e147fa09cf669b936b46.xml"&gt;mentioned&lt;/a&gt;
on the gentoo-doc mailinglist, all documentation bugs (that we know of)
related to openrc have been fixed. It was already a week like so, but
the last dependency on our "tracker" bug was an open one (asking if more
needs to be done or not) from which we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As
&lt;a href="http://archives.gentoo.org/gentoo-doc/msg_f6d85b4b80e2e147fa09cf669b936b46.xml"&gt;mentioned&lt;/a&gt;
on the gentoo-doc mailinglist, all documentation bugs (that we know of)
related to openrc have been fixed. It was already a week like so, but
the last dependency on our "tracker" bug was an open one (asking if more
needs to be done or not) from which we haven't received an answer in
over a month. So I guess we're there.&lt;/p&gt;
&lt;p&gt;Now, the OpenRC transition wasn't an easy one documentation-wise. Since
there is no full backwards compatibility, all changes would need to be
done in an atomic way, but due to resource constraints, the
documentation couldn't catch up on the changes in due time. Luckily,
that's over now and we can hopefully start by improving our
documentation once again.&lt;/p&gt;
&lt;p&gt;For &lt;a href="http://hardened.gentoo.org/selinux"&gt;SELinux&lt;/a&gt; too, OpenRC hasn't
been a gift. The latest &lt;code&gt;selinux-base-policy&lt;/code&gt; now in the Portage tree
(20110726-r4) still includes some fixes to get OpenRC support fully
working. However, I'm fairly confident that we will be able to tackle
other bugs (if they arise) quickly now, since the basic policy
definitions (like support for &lt;code&gt;rc_exec_t&lt;/code&gt;) are now in place.&lt;/p&gt;
&lt;p&gt;With the major changes done, let's look at the future. For
documentation, I'm now working on a new &lt;a href="http://dev.gentoo.org/~swift/docs/previews/power-management-guide.xml"&gt;Power Management
Guide&lt;/a&gt;
whereas for SELinux, I'll be focusing on the remaining bugs as well as
documentation updates (the &lt;a href="http://hardened.gentoo.org/selinux/selinux-handbook.xml"&gt;SELinux
Handbook&lt;/a&gt; will
have some major updates in the hope it becomes more useful and
future-proof). Also, for GDP, I'm going to make a suggestion towards the
&lt;a href="http://www.gentoo.org/proj/en/gdp/doc/doc-policy.xml"&gt;Gentoo Documentation
Policy&lt;/a&gt;, taking
into account that the GDP resources are not as high as at the time the
policy was written. Finally, I'm going to update my &lt;a href="https://github.com/sjvermeu/small.coding/tree/HEAD/gensetup"&gt;installation
scripts&lt;/a&gt;
that I use to seed the virtual servers so that I can enhance the SELinux
policy testing.&lt;/p&gt;
&lt;p&gt;I consider this post to be a checklist - after all, now that I promised
that I would do that, I guess I can't excuse myself from that anymore do
I ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Mitigating risks, part 3 - hardening</title><link href="https://blog.siphos.be/2011/09/mitigating-risks-part-3-hardening/" rel="alternate"></link><published>2011-09-13T22:46:00+02:00</published><updated>2011-09-13T22:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-13:/2011/09/mitigating-risks-part-3-hardening/</id><summary type="html">&lt;p&gt;While I'm writing this post, my neighbor is shouting. He's shouting so
hard, that I was almost writing with CAPS on to make sure you could read
me. But don't worry, he's not fighting - it is how he expresses his
(positive) feelings about his religion.&lt;/p&gt;
&lt;p&gt;Security is, for some, also …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While I'm writing this post, my neighbor is shouting. He's shouting so
hard, that I was almost writing with CAPS on to make sure you could read
me. But don't worry, he's not fighting - it is how he expresses his
(positive) feelings about his religion.&lt;/p&gt;
&lt;p&gt;Security is, for some, also a religion. They see risks and
vulnerabilities and what not everywhere. They're always thinking every
system in the world is or will be hacked in the near future and are
frantically trying to secure every service they are running - and more.
But security is also a real-life issue. If you take a look at the
compromised
&lt;a href="http://www.globalsign.com/company/press/090611-security-response.html"&gt;GlobalSign&lt;/a&gt;
website (who mentions that the website is an isolated one - as &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/"&gt;I
described
earlier&lt;/a&gt;)
I hope that you look at security as being a &lt;em&gt;functional&lt;/em&gt; requirement in
architecturing and design (and not a non-functional one as many
frameworks suggest).&lt;/p&gt;
&lt;p&gt;And as you can see from the example, isolating services is not
sufficient to prevent a successful exploit of an insecure or unsupported
software (the reason why I started with this series). One additional
measure that you can take is &lt;strong&gt;hardening&lt;/strong&gt; the server and service.&lt;/p&gt;
&lt;p&gt;The act of hardening a server and service is to configure the system so
that it is as secure as possible, based on configuration entries. Many
vendors and projects offer a security guide (like the &lt;a href="http://www.gentoo.org/doc/en/security/security-handbook.xml"&gt;Gentoo Security
Handbook&lt;/a&gt;
or the &lt;a href="https://docs.fedoraproject.org/en-US/Fedora/16/html/Security_Guide/"&gt;Fedora Security
Guide&lt;/a&gt;)
although most of them add this as part of their standard administrative
documents (like the &lt;a href="http://www.postgresql.org/docs/current/static/runtime.html"&gt;PostgreSQL "Server Setup and
Operation"&lt;/a&gt;
chapter).&lt;/p&gt;
&lt;p&gt;But for some reason, you'll find that default installations - even when
following the instructions of the vendor - are not as secure as you want
it to be. As a matter of fact, if you come in contact with auditors,
you'll probably fail any audit if you use a default installation. To
help administrators to secure their services, you will find lots of
third party sites offering advice on securing the operating system and
the services running on it. These guides are what you will need to
"harden" your system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.owasp.org/index.php/Main_Page"&gt;OWASP&lt;/a&gt;, which stands for
    Open Web Application Security Project,
    &lt;a href="https://www.owasp.org/index.php/OWASP_Backend_Security_Project"&gt;hosts&lt;/a&gt;
    some hardening guides and suggestions together with test scenarios.
    For front-end application servers (mostly web application servers)
    you will find lots of interesting resources in the OWASP site (and
    surrounding community).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.google.com"&gt;Google&lt;/a&gt; is probably the best resource for
    finding hardening guides for your operating system or service. Just
    look for "hardening foo" and you will be reading for a week.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cisecurity.org/"&gt;CISecurity&lt;/a&gt;, or "Center for Internet
    Security", is another one with a larger portfolio on
    hardening guides. Not only does it offer these guides (which it
    calls "benchmarks"), but organizations can also become a member and
    as such benefit from tooling that CISecurity supports for the
    validation of benchmarks (i.e. test if the system/deployment is
    compliant towards a particular benchmark). It does that by
    developing the benchmarks in a open specification called &lt;strong&gt;OVAL&lt;/strong&gt;
    (the &lt;em&gt;Open Vulnerability and Assessment Language&lt;/em&gt;) and &lt;strong&gt;XCCDF&lt;/strong&gt;
    (&lt;em&gt;XML Configuration Checklist Data Format&lt;/em&gt;). And CISecurity is not
    the only one there.&lt;/li&gt;
&lt;li&gt;Another such resource is the &lt;a href="http://web.nvd.nist.gov/view/ncp/repository"&gt;National Vulnerability
    Database&lt;/a&gt; (national for
    US residents, that is ;-) There you can find and download the
    OVAL/XCCDF resources for various software titles and
    operating systems. But as you can imagine from the abbreviations,
    the resources are XML files which are not made to be read by humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although you can use the tool(s) that CISecurity offers, another
possibility is to use
&lt;a href="http://www.open-scap.org/page/Main_Page"&gt;Open-SCAP&lt;/a&gt;, an open source
framework for handling SCAP, OVAL, XCCDF and other such open
specifications on a system. Its
&lt;a href="http://www.open-scap.org/page/Documentation"&gt;documentation&lt;/a&gt; offers a
first glance at what it can support.&lt;/p&gt;
&lt;p&gt;However, this brings on he disadvantages of hardening services...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hardening a system and its services is a &lt;em&gt;time consuming&lt;/em&gt; job. Its
    only purpose is to reduce the impact of exploited vulnerabilities
    and reduce the "attack surface" so that exploits on unused functions
    are not possible.&lt;/li&gt;
&lt;li&gt;Hardening a system and its services &lt;em&gt;can impact the service&lt;/em&gt;. Make
    it too tight, and it might not behave anymore like you want it to.&lt;/li&gt;
&lt;li&gt;Also, since there are many, many resources "out there" on hardening,
    you will have to &lt;em&gt;manage your hardening rules&lt;/em&gt;, document them
    for yourself. It is also advisable to document the rules you are not
    implementing, if not just for future's sake.&lt;/li&gt;
&lt;li&gt;The hardening guides also require quite some &lt;em&gt;expertise on the
    service&lt;/em&gt;. If you are not experienced with the service but you need
    to harden it, you can be lucky and just implement what is suggested
    and hope for the best, but usually you will need to dive deeper in
    the subject and make (tough) choices.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although specifications like SCAP exist to help you in your hardening
exercises, these are still difficult to manage (do &lt;em&gt;not&lt;/em&gt; try to write
OVAL/SCAP/XCCDF content in your favorite text editor). Its adoption
however by Fedora and RedHat is showing a positive effect on the tools
surrounding this specification. I will be writing about SCAP, OVAL and
XCCDF later since I too see good use of it in organizations (or even
free software projects).&lt;/p&gt;
&lt;p&gt;Does that mean that hardening is not beneficial? On the contrary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You &lt;strong&gt;gain lots of knowledge&lt;/strong&gt; in the matter, and also forces you to
    think about integration aspects. Since you are responsible for the
    service (or the damage that could be made if the service
    is exploited) being knowledgeable is definitely a good thing.&lt;/li&gt;
&lt;li&gt;A considerable amount of vulnerabilities that are and will be
    reported on the service (check &lt;a href="http://www.cvedetails.com"&gt;CVE
    details&lt;/a&gt; to find out about publicly known
    vulnerabilities, documented in the CVE specification) will not have
    their effect on a well hardened service. Or put another way, you
    will &lt;strong&gt;reduce the number of real vulnerabilities&lt;/strong&gt; in your service.
    You will not be able to exclude all vulnerabilities, but the
    projected number is high - a fully hardened Windows or Linux system
    can mitigate up to 90% of the exploits on the operating system. It
    will considerably reduce the risks that you and your organization
    are taking.&lt;/li&gt;
&lt;li&gt;A well defined hardening guide will also offer the means to
    &lt;strong&gt;automatically audit&lt;/strong&gt; or check if the &lt;strong&gt;system is still
    compliant&lt;/strong&gt; to the hardening setup you envisioned. Scheduled
    regularly, this will ensure that your configurations are not
    drifting away, back to a more vulnerable setup, for whatever reason.&lt;/li&gt;
&lt;li&gt;By removing the functions that the service should not offer, you
    make sure that the use of the service is per the
    organizations' guidelines. (Internal) abuse of the service is made
    more difficult, so users are forced to take the regular way. Unlike
    service isolation, which allows you to keep track of data/service
    flows, hardening makes sure that &lt;strong&gt;side-functionality is not used&lt;/strong&gt;
    without your consent. Or to put it more blunt, "Yes I know Oracle DB
    can be used to schedule tasks on the operating system, but no,
    you're not allowed to use that function".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And who knows, perhaps by optimizing the configuration, it might run
faster with a lower resource footprint ;-) If it does, that's perfect,
since the next topic on risk mitigation will have a negative influence
on performance: mandatory access control.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Mitigating risks, part 2 - service isolation</title><link href="https://blog.siphos.be/2011/09/mitigating-risks-part-2-service-isolation/" rel="alternate"></link><published>2011-09-09T23:12:00+02:00</published><updated>2011-09-09T23:12:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-09:/2011/09/mitigating-risks-part-2-service-isolation/</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Internet: absolute communication, absolute isolation&lt;br&gt;
 \~Paul Carvel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The quote might be ripped out of its context completely, since it wasn't
made when talking about risks and the assurance you might need to get in
order to reduce risks. But it does give a nice introduction to the
second part of …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Internet: absolute communication, absolute isolation&lt;br&gt;
 \~Paul Carvel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The quote might be ripped out of its context completely, since it wasn't
made when talking about risks and the assurance you might need to get in
order to reduce risks. But it does give a nice introduction to the
second part of this article series on &lt;em&gt;risk mitigation&lt;/em&gt;. After all, if
the unsupported software is offering services to the Internet, you
really want to govern the communication and isolate the service.&lt;/p&gt;
&lt;p&gt;When you are dealing with a product or software that is unsupported (be
it that it will not get any patches and updates from its authors or
vendor, or there is no time/budget to support the environment properly),
it is in my opinion wise to isolate the service from the rest. My &lt;a href="http://blog.siphos.be/2011/09/mitigating-risks-part-1/"&gt;first
post&lt;/a&gt; on the
matter gave a high-level introduction on the risks that you might be
taking when you run unsupported (or out-of-support) systems. Service
isolation helps in reducing the risks that &lt;em&gt;others&lt;/em&gt; have when you run
such software on a shared infrastructure (like in the same network or
even data centre).&lt;/p&gt;
&lt;p&gt;By isolating the unsupported service from the rest, you create a sort-of
quarantine environment where sudden mishaps are shielded from
interfering with other systems. It provides &lt;strong&gt;insurance for others&lt;/strong&gt;,
knowing that their (supported) services cannot be influenced or
jeopardized by issues with the unsupported ones. And if these services
need to interact with the isolated service, the interface used is known
and much more manageable (think about a well-defined TCP connection
versus local communication or even Inter-Process Communication). But it
goes beyond just providing insurance for others.&lt;/p&gt;
&lt;p&gt;Isolation forces you to &lt;strong&gt;learn about the application&lt;/strong&gt; and its
interaction with other services. It is this phase that makes it
extremely important in an environment, because not knowing how an
application works, behaves or interacts creates more problems later when
you need to debug issues, troubleshoot performance problems and more.
Integration failures, as described in my previous post, can only be
dealt with swiftly if you know how the service integrates with others.&lt;/p&gt;
&lt;p&gt;Another advantage of proper service isolation is that you can fix its
dependencies more easily. Remember that I talked about upgrade
difficulties, where a necessary upgrade for one component impacted the
functionalities of the other (unsupported) component? With good
isolation, the &lt;strong&gt;dependencies are more manageable&lt;/strong&gt; and controllable.
Not only are (sub)component upgrades easier to schedule, it is also a
lot easier to provide fall-back scenario's in case problems occur. After
all, the isolated service is the only user so you have little to fear if
you need to roll-back a change.&lt;/p&gt;
&lt;p&gt;But what is proper service isolation?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First of all, it means that you focus on running the (unsupported)
    software alone on an operating system instance. &lt;em&gt;Do not run other
    services on the same OS&lt;/em&gt;, not even if they too are unsupported. The
    only exception here is if the other services are tightly integrated
    with your service and cannot be installed on a separate OS. But
    usually, full service isolation is possible.&lt;/li&gt;
&lt;li&gt;Next, &lt;em&gt;strip the operating system&lt;/em&gt; so it only runs what you need for
    managing the service. Put primary focus on services that are
    accepting incoming connections ("listening") and secondary focus on
    allowed outgoing protocols/sessions (and the tools that
    initiate them).&lt;/li&gt;
&lt;li&gt;See if you can &lt;em&gt;virtualize the environment&lt;/em&gt;. In most cases, the
    service does not require many resources so it would be a waste
    running it on a dedicated system. However, in my opinion, a much
    better reason for virtualization is hardware abstraction. Sure, all
    operating systems tell you that they have some sort of Hardware
    Abstraction Layer in them and that they can deal with hardware
    changes without you noticing it. But if you are an administrator,
    you know this is only partially true. Virtualization offers the
    advantage that the underlying hardware is virtual and can remain the
    same, even if you move the virtualized system to a much more
    powerful host. Another advantage is that you might be able to
    offload certain necessary services from the OS (like backup) to the
    host (snapshotting).&lt;/li&gt;
&lt;li&gt;Shield the operating system, network-wise, from other systems. Yes,
    that means putting &lt;em&gt;a firewall BEFORE the operating system guest&lt;/em&gt;
    (and definitely not on the OS) which governs all traffic coming in
    and out of the environment. Only allow connections that are legit.
    If your organization has a huge network to manage, they might work
    with network segment filtering instead of IP-level filtering. See if
    you can get an exception to that - managing the rules should not
    give too much overhead since the system, being unsupported and all,
    is a lot less likely to get many connectivity updates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But before finishing off, a hint about stripping an operating system.
Stripping is much more than just removing services that are not used. It
also means that you look for services that are needed, and see if you
can externalize them. Common examples here are logging (send your logs
to a remote system rather than keeping them local), e-mail (use simple
"direct-out" mail) and backup (use a locally scheduled backup tool, or
even offload to the host in virtualized systems), but many others exist.&lt;/p&gt;
&lt;p&gt;Of course, service isolation is not unknown to most people. If you run a
large(r) network with Internet-facing services, you probably isolate
those in a DMZ environment. That is quite frankly (also) for the same
"risk mitigation" reason. In case of a security breach, service
unavailability or otherwise, you want to reduce the risk that this fault
spreads to other systems (be it getting to internal documents or putting
more services down).&lt;/p&gt;
&lt;p&gt;Another aspect administrators do with systems in their DMZ is &lt;em&gt;system
hardening&lt;/em&gt;, which I will talk about in the third part.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Mitigating risks, part 1</title><link href="https://blog.siphos.be/2011/09/mitigating-risks-part-1/" rel="alternate"></link><published>2011-09-05T22:05:00+02:00</published><updated>2011-09-05T22:05:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-05:/2011/09/mitigating-risks-part-1/</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;We are running Foobar 2.0 on Tomcat 4. We know that Tomcat 4 isn't
supported, but hey - our (internal) customer is happy that the Foobar
application works and would like to keep it that way. Upgrading to
Tomcat 5 or higher is not possible - Foobar 2.0 only works …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;We are running Foobar 2.0 on Tomcat 4. We know that Tomcat 4 isn't
supported, but hey - our (internal) customer is happy that the Foobar
application works and would like to keep it that way. Upgrading to
Tomcat 5 or higher is not possible - Foobar 2.0 only works on Tomcat
4. If we want to use a higher Tomcat version, we need to upgrade the
application which costs a lot of money (which our (internal) customer
doesn't want to pay) and requires lots of testing as it is a
non-trivial upgrade. So... what can an IT department do to mitigate
the risks here?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not a hypothetical example (well, apart from the software titles
used) for many organizations. Be it the application itself, its
middleware, back-end or operating system: often you'll face an
end-of-support deadline without the means to upgrade the application
(because of budgetary issues, unwillingness of the responsible
department or no alternative). Whatever the reason, you as an IT
department have the responsibility to mitigate the risks involved with
running out-of-support software (and communicate risks to all parties
that are affected by it). So what are your options?&lt;/p&gt;
&lt;p&gt;In this series of posts, I'll cover a set of risk mitigation strategies
that might help you reduce the issues that come up from running
out-of-support software. But first, what are those risks?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security patches&lt;/strong&gt;. It is the first risk that the operations
    department will say when they have to deal with
    unsupported software. Well, the risk isn't the security patches, but
    the result made by the lack of it. Software tends to have bugs. Some
    of these bugs can result in inappropriate functionality, such as
    granting access to unauthorized people or even executing (unwanted)
    commands on the server. Sounds improbable? &lt;a href="http://cvedetails.com/cve/CVE-2011-3190/"&gt;Guess
    again&lt;/a&gt;. Especially when
    running out-of-support software this becomes a nightmare to manage,
    because security patches are not created anymore, and newly
    discovered vulnerabilities might still affect older versions - even
    when the vulnerabilities do not mention the older versions anymore.
    And the worst thing is that you &lt;em&gt;might not even detect it&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional bugs&lt;/strong&gt;. If your customer tries something out and the
    application barfs, then there is little you can do to fix this.
    Either you dive in the code yourself (good luck with that) or you
    hope that a workaround exists. Getting a functional bug fix is not
    that feasible. Also, do not think that functional bugs will not pop
    up anymore "because the application has been running fine
    for years". A change on the system (update of the java runtime,
    kernel upgrade, update on a particular library) might be enough to
    trigger it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-functional bugs&lt;/strong&gt;. The application starts dragging down? You
    notice inflated response times? Can the application only deal with
    10 concurrent users, but your customer just hired 2 additional
    employees? Too bad. You might be able to work around this by
    duplicating the application and putting a load balancer in front of
    it, but with stateful applications that isn't always that easy to&lt;br&gt;
    accomplish. Forget about service level agreements when the software
    is unsupported. You can't guarantee them anymore.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Legal requirements&lt;/strong&gt;. You might not know it, but many institutions
    are governed by specific IT requirements. Especially the financial
    sector (with the recent crisis and all) is and will get more and
    more regulatory compliance requests, and the IT infrastructure will
    not be spared. If you run unsupported software, you might be
    ignoring particular requirements that you have.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upgrade difficulties&lt;/strong&gt;. Eventually you will need to upgrade. If
    the software you are upgrading from is unsupported, chances are very
    low that a good, flexible (and cost-efficient) upgrade
    trajectory exists. Migration scripts will probably not work and
    consultancy will fail. Anyone have experience with upgrading from
    Oracle 7.3 to Oracle 11g?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integration failures&lt;/strong&gt;. Most applications are integrated in a
    larger architecture. Applications probably get authorization feeds
    or send out events to other components. As the external services
    that the application interacts with get updated, their interfaces
    update with them. And eventually you will get into a situation where
    the integration suddenly fails. I've seen an application use
    HTTP/1.0 whereas its external services suddenly only
    supported HTTP/1.1. Have fun explaining that to your customer (who
    might not even know that HTTP is a protocol).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customer support&lt;/strong&gt;. If you use an internal help desk, then you
    might be able to educate them with troubleshooting the (unsupported)
    software. But if the help desk is external, you'll probably be
    facing a "No" after a while - or a nice additional fee.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of these risks not only affect the product itself, but all other
products / softwares that are installed on the server (or even on the
network). If you ever face the request to continue supporting Foobar 2.0
on an unsupported Tomcat, now you have a checklist that you can tell
your (requesting) customer about the risks he is introducing - and don't
forget to tell the other customers about the risks they will be taking
as well then.&lt;/p&gt;
&lt;p&gt;But I promised that I will be talking about risk mitigation... so just
hold on for part 2 -- "service isolation".&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Now using refpolicy 2.20110726</title><link href="https://blog.siphos.be/2011/09/now-using-refpolicy-2-20110726/" rel="alternate"></link><published>2011-09-04T20:38:00+02:00</published><updated>2011-09-04T20:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-09-04:/2011/09/now-using-refpolicy-2-20110726/</id><summary type="html">&lt;p&gt;A few days ago, I committed the SELinux policy modules that are based on
the 2.20110726 set released upstream. For those that are using Gentoo
Hardened with SELinux, you'll find them if you use the \~arch set for
the &lt;code&gt;sec-policy&lt;/code&gt; category.&lt;/p&gt;
&lt;p&gt;When I talk about upstream, it usually is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few days ago, I committed the SELinux policy modules that are based on
the 2.20110726 set released upstream. For those that are using Gentoo
Hardened with SELinux, you'll find them if you use the \~arch set for
the &lt;code&gt;sec-policy&lt;/code&gt; category.&lt;/p&gt;
&lt;p&gt;When I talk about upstream, it usually is the &lt;a href="http://oss.tresys.com/projects/refpolicy"&gt;reference
policy&lt;/a&gt; as maintained by
&lt;a href="http://www.tresys.com/"&gt;Tresys&lt;/a&gt;. This project, often abbreviated to
&lt;em&gt;refpolicy&lt;/em&gt;, tries to maintain a set of SELinux policies that are useful
for the majority of Linux distributions. In fact, most (if not all)
Linux distributions that support SELinux base their policies on the
refpolicy.&lt;/p&gt;
&lt;p&gt;Now maintaining a reference policy for SELinux is not that easy, even
with the contributions of many distributions and developers. Since the
policy is used for many distributions (including &lt;a href="https://www.redhat.com/rhel/server/features/benefits.html"&gt;RedHat Enterprise
Linux&lt;/a&gt;) it is
vital that presented changes are only accepted if truly necessary (and
do not present additional security risks). That means that patches
should be well documented and easy to read. Patches that lack a proper
motivation and that are not trivial are not accepted.&lt;/p&gt;
&lt;p&gt;When distributions want to push updates on the policy to the refpolicy,
they need to send the patches to the &lt;a href="http://oss.tresys.com/pipermail/refpolicy/"&gt;refpolicy
mailinglist&lt;/a&gt;. There they are
picked up and analyzed and eventually added to the release.&lt;/p&gt;
&lt;p&gt;For &lt;a href="http://hardened.gentoo.org/selinux"&gt;Gentoo Hardened's SELinux
project&lt;/a&gt;, getting (the majority of)
our own patches in the reference policy is important, mainly because we
currently lack the manpower to maintain a huge patch set ourselves.
Every time a new release is made by the reference policy, we need to
re-apply (and redevelop) our own patches. For a small set of patches,
this isn't a lot of work, but the more changes you include, the more
time-consuming this "patch forwarding" becomes. Of course, by quickly
pushing out our patches we also get the confirmation (or rejection) of
the patch, allowing us to be certain that we are on the right track.
After all, it is a &lt;em&gt;security policy&lt;/em&gt; that we are talking of.&lt;/p&gt;
&lt;p&gt;Now, the reference policy is just one of "our upstreams". A second
important project - also governed by the Tresys organization - is what
is called the &lt;a href="http://userspace.selinuxproject.org/trac"&gt;SELinux
Userspace&lt;/a&gt;. This project
maintains the tools necessary to build the SELinux policy from readable
text for humans to interpretable binary blobs for the Linux kernel. It
maintains the tools that help us modify the policies' runtime behavior
(using conditionals), manage file contexts and more. As this tool
interacts intimately with the SELinux internals, development of these
tools is discussed on the &lt;a href="http://www.nsa.gov/research/selinux/list.shtml"&gt;SELinux
mailinglist&lt;/a&gt; offered by
the NSA.&lt;/p&gt;
&lt;p&gt;It is the SELinux userspace project that provides tools like
&lt;strong&gt;semanage&lt;/strong&gt;, &lt;strong&gt;semodule&lt;/strong&gt;, &lt;strong&gt;restorecon&lt;/strong&gt;, &lt;strong&gt;chcon&lt;/strong&gt;, etc.&lt;/p&gt;
&lt;p&gt;So next time you hear me talk about upstream, you know what it is.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Use parted for large partitions</title><link href="https://blog.siphos.be/2011/08/use-parted-for-large-partitions/" rel="alternate"></link><published>2011-08-24T23:46:00+02:00</published><updated>2011-08-24T23:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-08-24:/2011/08/use-parted-for-large-partitions/</id><summary type="html">&lt;p&gt;A few bugs that were sitting in Gentoo's bugzilla for the documentation
were related to large partitions (2 TB and higher). Previously, this
wasn't as much as an issue since the number of users that have 2+ TB
partitions are fairly slim. But of course time flies, hardware becomes
cheaper …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few bugs that were sitting in Gentoo's bugzilla for the documentation
were related to large partitions (2 TB and higher). Previously, this
wasn't as much as an issue since the number of users that have 2+ TB
partitions are fairly slim. But of course time flies, hardware becomes
cheaper and I have large partitions myself now too. So we had to update
the docs.&lt;/p&gt;
&lt;p&gt;Since yesterday or so, the Gentoo Handbook now provides instructions on
&lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml?part=1&amp;amp;chap=4#parted"&gt;using
parted&lt;/a&gt;
for partitioning the disk (not for all architectures yet since I can't
validate if it works on those as well). The use of the &lt;strong&gt;parted&lt;/strong&gt;
command, which is in the minimal LiveCDs for some time now, allows users
to create GPT partition labels (instead of the old-style msdos ones).
This partition system, which is supported by other operating systems as
well, does not stop at 4 primary partitions, and supports partitions of
a size I can currently only dream of (somewhere in the exabytes if I am
not mistaken).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Easy documentation updates thanks to the many contributions</title><link href="https://blog.siphos.be/2011/08/easy-documentation-updates-thanks-to-the-many-contributions/" rel="alternate"></link><published>2011-08-22T23:01:00+02:00</published><updated>2011-08-22T23:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-08-22:/2011/08/easy-documentation-updates-thanks-to-the-many-contributions/</id><summary type="html">&lt;p&gt;As mentioned previously, I took a stab at the &lt;a href="http://www.gentoo.org/doc/en/ldap-howto.xml"&gt;Gentoo Guide to OpenLDAP
Authentication&lt;/a&gt;, updating
its configuration settings as well as give an introduction to its
replication mechanism. Although I am no OpenLDAP guru at all, I set up a
similar architecture for testing some SELinux policy changes. This test …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As mentioned previously, I took a stab at the &lt;a href="http://www.gentoo.org/doc/en/ldap-howto.xml"&gt;Gentoo Guide to OpenLDAP
Authentication&lt;/a&gt;, updating
its configuration settings as well as give an introduction to its
replication mechanism. Although I am no OpenLDAP guru at all, I set up a
similar architecture for testing some SELinux policy changes. This test
environment grew (okay, it's all KVM guests, so the only thing that grew
was my resource consumption) and is currently entailing over 24 systems,
ranging from BIND (in master/slave) to Apache/Nginx setups, reverse
proxies to database clusters and what not.&lt;/p&gt;
&lt;p&gt;I'm hoping that I can manage the scripts I use to create those images
(perform unattended installations of all these softwares as well as
configuration aspects) and eventually provide some more documents for
Gentoo on these matters. But until then, I'll focus more on fixing and
helping the publication of documentation (a small list of changes
contributed by various people are in the &lt;a href="http://www.gentoo.org/doc/en/handbook/handbook-amd64.xml"&gt;Gentoo
Handbook&lt;/a&gt;
which finally mentions ext4, has seen a whole slew of OpenRC fixes and
updated kernel configuration information, or the &lt;a href="http://www.gentoo.org/doc/en/guide-to-mutt.xml"&gt;Gentoo Guide to
Mutt&lt;/a&gt; which has been
rewritten from scratch). If you notice any errors or needs for
corrections on Gentoo documentation, don't hesitate to &lt;a href="http://bugs.gentoo.org"&gt;file a
bugreport&lt;/a&gt; or drop by on IRC's #gentoo-doc
channel.&lt;/p&gt;
&lt;p&gt;Speaking of documentation, the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;SELinux
Handbook&lt;/a&gt;
has seen a few updates as well, and I have also started pushing some
module-specific information (for instance on
&lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/modules/portage.xml"&gt;Portage&lt;/a&gt;).
This might help some users with their quest to get a particular software
title to work on their system with the SELinux policies in place.&lt;/p&gt;
&lt;p&gt;Next to the documentation, you'll also find the SELinux policy modules
based on the 2.20110726 version of the reference policy in the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git;a=summary"&gt;hardened-dev&lt;/a&gt;
overlay. The base policy is currently in revision 2 with revision 3 on
the way (asterisk, mutt and mozilla fixes). It now uses a cleaner
patching process, something that is also part of the updated
&lt;a href="http://devmanual.gentoo.org/eclass-reference/selinux-policy-2.eclass/index.html"&gt;selinux-policy-2.eclass&lt;/a&gt;.
I'm also hoping that I can introduce delivery of the SELinux policy
interface documentation (a nicely formatted set of HTML pages showing
which kind of interfaces - calls or privilege "bundles" if you like -
are available), of course based on the availability of USE="doc".&lt;/p&gt;
&lt;p&gt;Once this has been accomplished, I'll see that the new policy modules
are migrated from the hardened-dev overlay to the main tree. Also, the
majority of changes made to the policy are since revision 2 of the base
policy in a more manageable format, allowing for faster pushing of the
changes to the upstream reference policy.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Ready, set, commit!</title><link href="https://blog.siphos.be/2011/08/ready-set-commit/" rel="alternate"></link><published>2011-08-12T22:35:00+02:00</published><updated>2011-08-12T22:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-08-12:/2011/08/ready-set-commit/</id><summary type="html">&lt;p&gt;Yesterday, I have entered the realms of Gentoo Development again. But as
it was getting late then, I had to wait before the first commits
happened. So this evening, things were done. The first couple of
documentation bugs (mostly related to OpenRC) have been committed to the
Gentoo CVS repository …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday, I have entered the realms of Gentoo Development again. But as
it was getting late then, I had to wait before the first commits
happened. So this evening, things were done. The first couple of
documentation bugs (mostly related to OpenRC) have been committed to the
Gentoo CVS repository and I've also committed my first change on the
gentoo-x86 CVS repository (a small change for the SELinux eclass, needed
for the upcoming storm of packages.&lt;/p&gt;
&lt;p&gt;So what does that mean for Gentoo and Gentoo Hardened? Well, that means
that I'll be taking on bugs myself now. You can ping me for
documentation changes as well as SELinux policy changes. Within the next
few hours, a little over 200 packages will be sent to the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git"&gt;hardened-development&lt;/a&gt;
overlay, containing the &lt;em&gt;SELinux policy modules based on the upstream
2.20110726 release&lt;/em&gt;. These will linger there for a while, since I had
some troubles getting them into the shape they are now - so some
additional testing doesn't hurt.&lt;/p&gt;
&lt;p&gt;During the testing, most of the patches applied will also be submitted
upstream for verification and inclusion. Simultaneously, a second set of
patches will be prepared to squeeze out the remaining issues that are
either left, or that have been reported since the push (I am expecting
quite a few still, but luckily many users on #gentoo-hardened are
helping out in testing SELinux).&lt;/p&gt;
&lt;p&gt;While we are on the SELinux policy development, I'll also be handling a
few other documentation bugs. I'm hoping to take a stab at Gentoo's
&lt;a href="http://www.gentoo.org/doc/en/ldap-howto.xml"&gt;OpenLDAP HOWTO&lt;/a&gt; since I've
been running a similar setup here for some time (including SELinux
support of course ;-)&lt;/p&gt;
&lt;p&gt;Speaking about documentation, Anthony G. "blueness" Basile has pushed
some documentation updates that I made in the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-docs.git"&gt;hardened-docs&lt;/a&gt;
repository to the main site. That means that users can now see how
&lt;em&gt;Gentoo Hardened supports MCS&lt;/em&gt; (and even talks about MLS for the brave
ones out there). And since we now support MCS and have the latest
userspace utilities in our repository, we can finally see if we can
support SELinux sandboxing, a functionality that is already available in
Fedora/RedHat but not fully supported through the upstream channels yet.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>checksec kernel security</title><link href="https://blog.siphos.be/2011/07/checksec-kernel-security/" rel="alternate"></link><published>2011-07-24T00:18:00+02:00</published><updated>2011-07-24T00:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-24:/2011/07/checksec-kernel-security/</id><summary type="html">&lt;p&gt;I have
&lt;a href="http://blog.siphos.be/2011/07/high-level-explanation-on-some-binary-executable-security/"&gt;blogged&lt;/a&gt;
about &lt;a href="http://www.trapkit.de/tools/checksec.html"&gt;checksec.sh&lt;/a&gt; earlier
before. Jono, one of the #gentoo-hardened IRC-members, kindly pointed
me to its &lt;code&gt;--kernel&lt;/code&gt; option. So I feel obliged to give its options a
stab as well. So, here goes the next batch of OPE-style (One Paragraph
Explanations).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# checksec.sh --kernel
* Kernel protection information …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I have
&lt;a href="http://blog.siphos.be/2011/07/high-level-explanation-on-some-binary-executable-security/"&gt;blogged&lt;/a&gt;
about &lt;a href="http://www.trapkit.de/tools/checksec.html"&gt;checksec.sh&lt;/a&gt; earlier
before. Jono, one of the #gentoo-hardened IRC-members, kindly pointed
me to its &lt;code&gt;--kernel&lt;/code&gt; option. So I feel obliged to give its options a
stab as well. So, here goes the next batch of OPE-style (One Paragraph
Explanations).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# checksec.sh --kernel
* Kernel protection information:

  Description - List the status of kernel protection mechanisms. Rather than
  inspect kernel mechanisms that may aid in the prevention of exploitation of
  userspace processes, this option lists the status of kernel configuration
  options that harden the kernel itself against attack.

  Kernel config: /proc/config.gz

  GCC stack protector support:            Enabled
  Strict user copy checks:                Enabled
  Enforce read-only kernel data:          Disabled
  Restrict /dev/mem access:               Enabled
  Restrict /dev/kmem access:              Enabled

* grsecurity / PaX: Custom GRKERNSEC

  Non-executable kernel pages:            Enabled
  Prevent userspace pointer deref:        Disabled
  Prevent kobject refcount overflow:      Enabled
  Bounds check heap object copies:        Enabled
  Disable writing to kmem/mem/port:       Enabled
  Disable privileged I/O:                 Enabled
  Harden module auto-loading:             Enabled
  Hide kernel symbols:                    Enabled

* Kernel Heap Hardening: No KERNHEAP

  The KERNHEAP hardening patchset is available here:
    https://www.subreption.com/kernheap/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In-kernel &lt;strong&gt;GCC stack protector support&lt;/strong&gt; is the same as the &lt;strong&gt;Canary&lt;/strong&gt;
explanation I gave earlier, but now for the kernel code. Memory used by
the stack (which contains both function variables as well as return
addresses) is "interleaved" with specific data (canaries) which are
checked before using a return address that is on the stack. If the
canary doesn't match, you'll see a nice kernel panic. This is to prevent
buffer overflows that might influence the in-kernel activity flow or
overwrite data.&lt;/p&gt;
&lt;p&gt;When talking about &lt;strong&gt;Strict user copy checks&lt;/strong&gt;, one can compare this
with the &lt;strong&gt;FORTIFY_SOURCE&lt;/strong&gt; explanation given earlier. Although not the
same implementation-wise (since the latter is gcc/glibc bound, whereas
the Linux kernel does not use glibc) this too enables the compiler to
detect function calls with variable length data arguments to an extend
that it can predict the (should-be) length of the argument. If this is
the case, the function is switched with a(nother in-kernel) function
that either continues the call, or break in case of a length mismatch.
This is to prevent buffer overflows that might corrupt the stack (or
other data locations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enforce read-only kernel data&lt;/strong&gt; marks specific kernel data sections as
read-only to prevent accidental (or malicious) manipulations.&lt;/p&gt;
&lt;p&gt;When selecting &lt;strong&gt;Restrict /dev/mem access&lt;/strong&gt;, the kernel does not allow
applications (even those running as root) to access all of memory.
Instead, they are only allowed to see device-mapped memory (and their
own process memory). The same goes for &lt;strong&gt;Restrict /dev/kmem access&lt;/strong&gt;,
which is specifically for kernel memory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-executable kernel pages&lt;/strong&gt; is similar to the &lt;strong&gt;NX&lt;/strong&gt; explanation
given earlier. It makes sure that pages marked as holding data can not
contain executable code (and will as such never be "jumped" in) and
pages marked as holding code will never be written to.&lt;/p&gt;
&lt;p&gt;To explain &lt;strong&gt;Prevent userspace pointer deref&lt;/strong&gt;, first you need to
understand the difference between a &lt;em&gt;userland address&lt;/em&gt; and a &lt;em&gt;kernel
address&lt;/em&gt;. Each application holds its own, private virtual address space.
Part of that virtual address space is "reserved" for most of the kernel
data (in other words, the kernel data is available in each process'
virtual address space), the rest is for the application. When
interaction with the kernel occurs, a userland address is given to the
kernel, which needs to translate it to a proper address (and treat it as
data). With &lt;strong&gt;Prevent userspace pointer deref&lt;/strong&gt;, specific checks are
made to ensure that the kernel doesn't directly use userspace pointers,
because that could be exploited by (malicious) software to trick the
kernel into doing things it shouldn't.&lt;/p&gt;
&lt;p&gt;Reference counters in the Linux kernel are used to track users of
specific objects or resources. A "popular" way to mistreat reference
counters (or any counter per se) is to increment them that much until
they overflow and wrap around, setting the counter to zero (or a
negative number), leading to unexpected results (such as freeing memory
that is in use). The &lt;strong&gt;Prevent kobject refcount overflow&lt;/strong&gt; detects this
for kobject resources and ensures that no wrap-around happens.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Bounds check heap object copies&lt;/strong&gt; checks if particular memory
copies use memory fragments within proper bounds. If the memory copy is
for a fragment that crosses that bound (for instance because the
fragment is too large) the copy fails. This offers some support against
overflows, similar to (but not the same as) the use of the stack
protector mentioned above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disable writing to kmem/mem/port&lt;/strong&gt; is similar to the &lt;strong&gt;Restrict
/dev/(k)mem access&lt;/strong&gt; settings, plus disable &lt;code&gt;/dev/port&lt;/code&gt; from being
opened.&lt;/p&gt;
&lt;p&gt;By selecting &lt;strong&gt;Disable privileged I/O&lt;/strong&gt;, access to the kernel through
functions like ioperm and iopl is prohibited. These functions are
sometimes used by applications that need direct device access, like
Xorg, but if you do not have such applications, it is wise to disable
privileged I/O access. If not, any vulnerability in such an application
might result in malicious code tampering with your devices.&lt;/p&gt;
&lt;p&gt;When &lt;strong&gt;Harden module auto-loading&lt;/strong&gt; is set, processes that do not run as
root will not be able to have particular kernel modules auto-loaded.
Although this seems strange, it isn't. Suppose you have an application
that wants to perform some IPv6 actions. Such applications can call
&lt;code&gt;request_module&lt;/code&gt; to ask the Linux kernel to load in a particular
service. If the kernel supports IPv6 through a module, then it will load
IPv6 support (you might have seen traces in your logs about &lt;code&gt;net-pf-10&lt;/code&gt;
- well, that's the IPv6 support). You can disable auto-loading
completely, but that might not be what you want. With this setting
enabled, auto-loading is supported but only for root-running
applications.&lt;/p&gt;
&lt;p&gt;The added security of &lt;strong&gt;Hide kernel symbols&lt;/strong&gt; is not to prevent
activities, but to prevent information to be leaked and (ab)used by
malicious users. Kernel symbols are string representations of functions
or variables that the kernel offers to kernel users (such as kernel
modules and drivers). This is needed because the location of these
functions/variables in memory cannot be provided in advance (this is no
different from symbols used as explained in the &lt;strong&gt;RELRO&lt;/strong&gt; security
setting in my previous posting). By hiding these symbols from any user
without sufficiently high privileges (and limit the exposure for high
privileged process to well-known locations so these too can be protected
by other means) it is far more difficult for malicious users to find out
about available functions/variables on your system.&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;Kernel Heap Hardening&lt;/strong&gt; enhances the in-kernel dynamic memory
allocator with additional hardening features (double-free protection,
use-after-free protection, ...). It tries to ensure proper use of the
allocated memory segments and protect against improper access.&lt;/p&gt;
&lt;p&gt;From reading all this, you probably imagine why this isn't all enabled
by default. Well, many of the settings have implications on how the
system behaves. Some restrict functionalities to the root user only
(making it sometimes less user-friendly), some disable functionalities
that are needed (like the I/O access) or are (ab)used (like the user
space pointer deref which is used by many virtualization solutions)
while others add some additional overhead (the more you check, the
longer an action takes before it completes).&lt;/p&gt;
&lt;p&gt;To help users select the proper settings, Gentoo Hardened tries to
differentiate settings based on &lt;em&gt;workstation&lt;/em&gt; and &lt;em&gt;virtualization&lt;/em&gt;
usage. So you get most security settings for "No Workstation, No
Virtualization" and less for each of those you enable. But of course,
like always, Gentoo supports custom settings too so you don't have to
follow the differentiation we suggest ;-)&lt;/p&gt;
&lt;p&gt;Find something incorrect in the above paragraphs? Or too much
sales-speak and too little explanation? Give me a shout (and prove me
wrong) ;-)&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>emerge-webrsync and gpg verification</title><link href="https://blog.siphos.be/2011/07/emerge-webrsync-and-gpg-verification/" rel="alternate"></link><published>2011-07-22T14:33:00+02:00</published><updated>2011-07-22T14:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-22:/2011/07/emerge-webrsync-and-gpg-verification/</id><summary type="html">&lt;p&gt;Gentoo has been working on its
&lt;a href="http://www.gentoo.org/proj/en/glep/glep-0057.html"&gt;security&lt;/a&gt; from very
early on. One of the (many) features it supports is to allow users to
validate the state of the portage tree. Ebuild signing (where developers
sign the Manifest file with their key) is one of the layers offered by
Gentoo, but …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Gentoo has been working on its
&lt;a href="http://www.gentoo.org/proj/en/glep/glep-0057.html"&gt;security&lt;/a&gt; from very
early on. One of the (many) features it supports is to allow users to
validate the state of the portage tree. Ebuild signing (where developers
sign the Manifest file with their key) is one of the layers offered by
Gentoo, but another one is full tree signing.&lt;/p&gt;
&lt;p&gt;When you use &lt;strong&gt;emerge-webrsync&lt;/strong&gt; instead of &lt;strong&gt;emerge --sync&lt;/strong&gt;, an
archive containing a consistent state of the portage tree is downloaded
and unpacked on your system. If you however set
&lt;code&gt;FEATURES="webrsync-gpg"&lt;/code&gt; then this tool will check the GPG signature
attached to the file with the public key used by Gentoo's infrastructure
(0x239C75C4). If the archive does not contain a valid signature, then it
is not used on the system.&lt;/p&gt;
&lt;p&gt;If you want to use this, here are the steps to do so.&lt;/p&gt;
&lt;p&gt;First, set up the location where you keep the key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mkdir -p /etc/portage/gpg
~# gpg --homedir /etc/portage/gpg --keyserver subkeys.pgp.net --recv-keys 0x239C75C4
~# gpg --homedir /etc/portage/gpg --edit-key 0x239C75C4 trust
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, edit &lt;code&gt;/etc/make.conf&lt;/code&gt; and set the following parameters:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;FEATURES=&amp;quot;webrsync-gpg&amp;quot;
PORTAGE_GPG_DIR=&amp;quot;/etc/portage/gpg&amp;quot;
# Disable &amp;#39;emerge --sync&amp;#39; so emerge-webrsync has to be used
SYNC=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that done, you're all set. Just run &lt;strong&gt;emerge-webrsync&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Happy Gentooing!&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Preliminary SELinux MCS support in Gentoo Hardened</title><link href="https://blog.siphos.be/2011/07/preliminary-selinux-mcs-support-in-gentoo-hardened/" rel="alternate"></link><published>2011-07-21T22:04:00+02:00</published><updated>2011-07-21T22:04:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-21:/2011/07/preliminary-selinux-mcs-support-in-gentoo-hardened/</id><summary type="html">&lt;p&gt;Users tracking the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git"&gt;hardened-dev&lt;/a&gt;
overlay for SELinux packages will notice yet another update on the
&lt;code&gt;selinux-base-policy&lt;/code&gt; package. This time however, the change is &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.hardened/4939"&gt;a
little more&lt;/a&gt;
than just a policy update. With this new revision, preliminary support
for &lt;em&gt;Multi-Category Security&lt;/em&gt; (aka MCS) is added.&lt;/p&gt;
&lt;p&gt;MCS is an update on the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Users tracking the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git"&gt;hardened-dev&lt;/a&gt;
overlay for SELinux packages will notice yet another update on the
&lt;code&gt;selinux-base-policy&lt;/code&gt; package. This time however, the change is &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.hardened/4939"&gt;a
little more&lt;/a&gt;
than just a policy update. With this new revision, preliminary support
for &lt;em&gt;Multi-Category Security&lt;/em&gt; (aka MCS) is added.&lt;/p&gt;
&lt;p&gt;MCS is an update on the SELinux policy where domains and resources can
be given a "category". This is especially useful on what is called
multi-tenant systems, where multiple processes (but of the same
application and hence the same domain definition) are running, servicing
requests of different clients (or even customers). With MCS, these
different processes, although using the same domain definitions, can
still be isolated. The use of categories is well accepted for
virtualization hosts (where virtual guests should be run isolated from
each other) and web servers, but other uses can be found easily as well.&lt;/p&gt;
&lt;p&gt;Next to MCS, the update also supports MLS or &lt;em&gt;Multi-Level Security&lt;/em&gt;.
Like MCS, this supports multiple categories, but it also supports
multiple sensitivity levels. On an MLS system, the security
administrator can control how information of a certain sensitivity label
"flows" through the system. Now, the MLS support within Gentoo Hardened
is still very experimental so I don't recommend it yet, unless you are
willing to help us get it in a workable shape.&lt;/p&gt;
&lt;p&gt;In order to use MCS, you need to use the &lt;code&gt;POLICY_TYPES&lt;/code&gt; variable in
/etc/make.conf (which allows Portage to build the policy type(s) you
want) and the &lt;code&gt;SELINUXTYPE&lt;/code&gt; variable in /etc/selinux/config. Whereas
this previously was limited to "strict" or "targeted", they now support
"mls" and "mcs" as well. Of course, this is
&lt;a href="http://goo.gl/DlHJD"&gt;documented&lt;/a&gt; in the Gentoo Hardened SELinux
handbook (currently in the hardened-doc overlay).&lt;/p&gt;
&lt;p&gt;Now, this is still &lt;em&gt;preliminary&lt;/em&gt; support for MCS. A small fix needs to
happen on our eclass and it definitely needs lots of testing before it
can be considered for production use. Also, the majority of development
attention will continue in the "strict" policy type although MCS testing
and support will grow.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>High level explanation on some binary executable security</title><link href="https://blog.siphos.be/2011/07/high-level-explanation-on-some-binary-executable-security/" rel="alternate"></link><published>2011-07-15T22:01:00+02:00</published><updated>2011-07-15T22:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-15:/2011/07/high-level-explanation-on-some-binary-executable-security/</id><summary type="html">&lt;p&gt;One very important functionality offered by &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt; is a specific toolchain (compiler,
libraries and more) that contains patches to make the built binaries a
bit more protected from certain vulnerabilities. Explaining all those in
detail is too much for a simple blog post like this, but some time ago …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One very important functionality offered by &lt;a href="http://hardened.gentoo.org"&gt;Gentoo
Hardened&lt;/a&gt; is a specific toolchain (compiler,
libraries and more) that contains patches to make the built binaries a
bit more protected from certain vulnerabilities. Explaining all those in
detail is too much for a simple blog post like this, but some time ago
the friendly folks of the Gentoo Hardened project told be about a script
called &lt;a href="http://www.trapkit.de/tools/checksec.html"&gt;checksec.sh&lt;/a&gt; that
displays a few of those protections on a binary.&lt;/p&gt;
&lt;p&gt;So what can I find out of such a run? Let me show you the output on two
binaries here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ checksec.sh --file /opt/skype/skype
RELRO           STACK CANARY      NX            PIE                     FILE
No RELRO        Canary found      NX disabled   No PIE                  /opt/skype/skype

~$ checksec.sh --file /bin/bash
RELRO           STACK CANARY      NX            PIE                     FILE
Full RELRO      Canary found      NX enabled    PIE enabled             /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It even comes with pretty colors (the "No RELRO" is red whereas "Full
RELRO" is green). But beyond interpreting those colors (which should be
obvious for the non-colorblind), what does that all mean? Well, let me
try to explain them in one-paragraph entries (yes, I like such
challenges ;-) Note that, if a protection is not found, then it probably
means that the application was not built with this protection (the skype
example, since this is a binary from Skype\^WMicrosoft, versus bash
which is built by the Gentoo Hardened toolchain).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RELRO&lt;/strong&gt; stands for &lt;em&gt;Relocation Read-Only&lt;/em&gt;, meaning that the headers in
your binary, which need to be writable during startup of the application
(to allow the dynamic linker to load and link stuff like shared
libraries) are marked as read-only when the linker is done doing its
magic (but before the application itself is launched). The difference
between &lt;strong&gt;Partial RELRO&lt;/strong&gt; and &lt;strong&gt;Full RELRO&lt;/strong&gt; is that the Global Offset
Table (and Procedure Linkage Table) which act as kind-of
process-specific lookup tables for symbols (names that need to point to
locations elsewhere in the application or even in loaded shared
libraries) are marked read-only too in the &lt;strong&gt;Full RELRO&lt;/strong&gt;. Downside of
this is that lazy binding (only resolving those symbols the first time
you hit them, making applications start a bit faster) is not possible
anymore.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Canary&lt;/strong&gt; is a certain value put on the stack (memory where function
local variables are also stored) and validated before that function is
left again. Leaving a function means that the "previous" address (i.e.
the location in the application right before the function was called) is
retrieved from this stack and jumped to (well, the part right after that
address - we do not want an endless loop do we?). If the &lt;strong&gt;canary&lt;/strong&gt;
value is not correct, then the stack might have been overwritten /
corrupted (for instance by writing more stuff in the local variable than
allowed - called &lt;em&gt;buffer overflow&lt;/em&gt;) so the application is immediately
stopped.&lt;/p&gt;
&lt;p&gt;The abbreviation &lt;strong&gt;NX&lt;/strong&gt; stands for non-execute or non-executable
segment. It means that the application, when loaded in memory, does not
allow any of its segments to be both writable and executable. The idea
here is that writable memory should never be executed (as it can be
manipulated) and vice versa. Having &lt;strong&gt;NX enabled&lt;/strong&gt; would be good.&lt;/p&gt;
&lt;p&gt;The last abbreviation is &lt;strong&gt;PIE&lt;/strong&gt;, meaning &lt;em&gt;Position Independent
Executable&lt;/em&gt;. A &lt;strong&gt;No PIE&lt;/strong&gt; application tells the loader which virtual
address it should use (and keeps its memory layout quite static). Hence,
attacks against this application know up-front how the virtual memory
for this application is (partially) organized. Combined with in-kernel
ASLR (&lt;em&gt;Address Space Layout Randomization&lt;/em&gt;, which Gentoo's
hardened-sources of course support) PIE applications have a more diverge
memory organization, making attacks that rely on the memory structure
more difficult.&lt;/p&gt;
&lt;p&gt;But hold on, the checksec.sh application also supports detection for
&lt;code&gt;FORTIFY_SOURCE&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ checksec.sh --fortify-file /opt/skype/skype
 * FORTIFY_SOURCE support available (libc)    : Yes
* Binary compiled with FORTIFY_SOURCE support: Yes

 ------ EXECUTABLE-FILE ------- . -------- LIBC --------
 FORTIFY-able library functions | Checked function names
 -------------------------------------------------------
 printf                         | __printf_chk
...
SUMMARY:

* Number of checked functions in libc                : 75
* Total number of library functions in the executable: 2468
* Number of FORTIFY-able functions in the executable : 25
* Number of checked functions in the executable      : 0
* Number of unchecked functions in the executable    : 25
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the given example, my system does support &lt;code&gt;FORTIFY_SOURCE&lt;/code&gt; and the
binary is supposedly built with this support as well, but the checks
return that out of the 25 functions identified as &lt;code&gt;FORTIFY&lt;/code&gt;-able, none
of them were successfully verified as using a &lt;code&gt;FORTIFY&lt;/code&gt;-ed library call.
It goes without saying that for the /bin/bash binary, this yielded a bit
more good results (12 out of 30 verified).&lt;/p&gt;
&lt;p&gt;Again, what is &lt;strong&gt;FORTIFY_SOURCE&lt;/strong&gt;? Well, when using &lt;code&gt;FORTIFY_SOURCE&lt;/code&gt;,
the compiler will try to intelligently read the code it is compiling /
building. When it sees a C-library function call against a variable
whose size it can deduce (like a fixed-size array - it is more
intelligent than this btw) it will replace the call with a &lt;code&gt;FORTIFY&lt;/code&gt;'ed
function call, passing on the maximum size for the variable. If this
special function call notices that the variable is being overwritten
beyond its boundaries, it forces the application to quit immediately.
Note that not all function calls that can be fortified are fortified as
that depends on the intelligence of the compiler (and if it is realistic
to get the maximum size).&lt;/p&gt;
&lt;p&gt;If you do not agree with the explanation above, please comment... and
try to explain it in a single paragraph without going too detailed (i.e.
do not assume people are capable of writing their own compiler just for
fun).&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Some people on #selinux are ... dolphins</title><link href="https://blog.siphos.be/2011/07/some-people-on-selinux-are-dolphins/" rel="alternate"></link><published>2011-07-14T20:00:00+02:00</published><updated>2011-07-14T20:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-14:/2011/07/some-people-on-selinux-are-dolphins/</id><summary type="html">&lt;p&gt;A very useful resource for anyone working on or with SELinux policies is
the #selinux chat channel on irc.freenode.net. People like Dominick
Grift and Dan Walsh you would first think are IRC bots (being online all
the time, answering questions), but I recently read that they must be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A very useful resource for anyone working on or with SELinux policies is
the #selinux chat channel on irc.freenode.net. People like Dominick
Grift and Dan Walsh you would first think are IRC bots (being online all
the time, answering questions), but I recently read that they must be
... dolphins.&lt;/p&gt;
&lt;p&gt;Yes, dolphins. Dolphins are known to
&lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/Dolphins"&gt;not&lt;/a&gt;
&lt;a href="http://www.sciencentral.com/articles/view.php3?article_id=218392593"&gt;really&lt;/a&gt;
&lt;a href="http://www.nature.com/news/2004/040920/full/news040920-10.html"&gt;sleep&lt;/a&gt;
&lt;a href="http://newswatch.nationalgeographic.com/2009/05/06/dolphins_sleep_with_half_their_brains/"&gt;fully&lt;/a&gt;
- only one part of their brain goes to a low-wave sleep so they're
sufficiently conscious to keep track of what is happening around them.&lt;/p&gt;
&lt;p&gt;No seriously, thanks guys!&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>On the new SELinux profiles</title><link href="https://blog.siphos.be/2011/07/on-the-new-selinux-profiles/" rel="alternate"></link><published>2011-07-14T19:31:00+02:00</published><updated>2011-07-14T19:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-14:/2011/07/on-the-new-selinux-profiles/</id><summary type="html">&lt;p&gt;Ever since Anthony put in the &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=365483"&gt;new SELinux
profiles&lt;/a&gt; - which was
long due - they have seen quite a few tests and the necessary,
evolutionary updates. No changes that broke things, no oddities that
would give a WTF to whomever is using it. The latest updates were to
remove some obsolete …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever since Anthony put in the &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=365483"&gt;new SELinux
profiles&lt;/a&gt; - which was
long due - they have seen quite a few tests and the necessary,
evolutionary updates. No changes that broke things, no oddities that
would give a WTF to whomever is using it. The latest updates were to
remove some obsolete masks so that our visibility in the &lt;a href="http://qa-reports.gentoo.org/"&gt;Gentoo QA
Reports&lt;/a&gt; is down again.&lt;/p&gt;
&lt;p&gt;However, we are well aware that these profiles are still in the
dev(elopment) phase but would like to stabilize these soon. For this to
happen, we need SELinux users to give the new profiles a go. Become
&lt;code&gt;sysadm_r&lt;/code&gt; &amp;amp; root and switch your profile to whichever SELinux profile
suits you the most (with the new profiles, we support SELinux on
multilib and no-multilib and across various settings).&lt;/p&gt;
&lt;p&gt;All my local servers run with "hardened/linux/amd64/no-multilib/selinux"
whereas my main workstation uses "hardened/linux/amd64/selinux" (since I
still have some need for the multilib setup). We did some tests on
non-hardened profiles too as well as on the x86 architecture with no
problems whatsoever. So although we can't guarantee anything, I'm pretty
convinced that the profiles will work for you too!&lt;/p&gt;
&lt;p&gt;So by all means, see if you can switch from the v2refpolicy/ profiles
and give us your feedback. You're always welcome for a chat on
&lt;code&gt;#gentoo-hardened&lt;/code&gt; (irc.freenode.net) or on our mailinglists.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Gentoo Hardened SELinux state</title><link href="https://blog.siphos.be/2011/07/gentoo-hardened-selinux-state/" rel="alternate"></link><published>2011-07-09T16:39:00+02:00</published><updated>2011-07-09T16:39:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-07-09:/2011/07/gentoo-hardened-selinux-state/</id><summary type="html">&lt;p&gt;Since last post, we've been working on the further stabilization and bug
fixing of the SELinux policies within Gentoo Hardened. You might have
noticed that we started working on the QA of the packages, like I
promised in the last post. The binaries within &lt;code&gt;selinux-base-policy&lt;/code&gt; are
now published somewhere on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since last post, we've been working on the further stabilization and bug
fixing of the SELinux policies within Gentoo Hardened. You might have
noticed that we started working on the QA of the packages, like I
promised in the last post. The binaries within &lt;code&gt;selinux-base-policy&lt;/code&gt; are
now published somewhere on blueness' &lt;a href="http://dev.gentoo.org/~blueness/patchbundle-selinux-base-policy/"&gt;developer
page&lt;/a&gt;
since he's proxy'ing all my commits until recruiters get the chance to
pick up my &lt;a href="https://bugs.gentoo.org/show_bug.cgi?id=176886"&gt;recruitment
bug&lt;/a&gt;. Other patches that
are coming up will be published likewise as well if they get too big to
be within the main Portage tree.&lt;/p&gt;
&lt;p&gt;Next to the binaries, I'm currently checking if the SELinux policy
packages can become &lt;a href="http://devmanual.gentoo.org/ebuild-writing/eapi/index.html"&gt;EAPI-4
compliant&lt;/a&gt;
(they're currently still using EAPI-0). Same for the SELinux-specific
packages, like policycoreutils, libsemanage, libselinux etc.&lt;/p&gt;
&lt;p&gt;During the last few days, I've tried to take a few stabs at supporting
Python 2 and Python 3 simultaneously. It seems to work for
policycoreutils and libsemanage (necessary fixes are in the
&lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-dev.git"&gt;gentoo-hardened
overlay&lt;/a&gt;)
but any attempt to fix libselinux seems to give me hard walls. So for
now, we're still stuck with Python 2 support when using Portage (note
that you can still use Python 3 for all other things, but Portage
requires Python 2 as it calls libselinux). This is currently still
accomplished through a proper &lt;code&gt;use.mask&lt;/code&gt; and &lt;code&gt;use.force&lt;/code&gt; setting against
Portage.&lt;/p&gt;
&lt;p&gt;Of course, the policies themselves are not silent either. I've updated
the &lt;code&gt;selinux-base-policy&lt;/code&gt; package so that Portage can now support
NFS-mounted Portage trees and made quite a few openrc-related fixes as
well (against the policy, not against openrc ;-)&lt;/p&gt;
&lt;p&gt;I promised to take a stab at MCS in the near future, and that's still
the plan. Hopefully in the coming few weeks ;-)&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>What's next after stabilization?</title><link href="https://blog.siphos.be/2011/06/whats-next-after-stabilization/" rel="alternate"></link><published>2011-06-13T20:46:00+02:00</published><updated>2011-06-13T20:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-06-13:/2011/06/whats-next-after-stabilization/</id><summary type="html">&lt;p&gt;The last few weeks have shown quite a few interesting improvements on
Gentoo Hardened's SELinux state. We now have improved (simplified)
Gentoo profile support, supporting SELinux on no-multilib (an often
requested feature, now finally in), we stabilized the 2.20101213
policies that are in the tree and are cleaning up …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The last few weeks have shown quite a few interesting improvements on
Gentoo Hardened's SELinux state. We now have improved (simplified)
Gentoo profile support, supporting SELinux on no-multilib (an often
requested feature, now finally in), we stabilized the 2.20101213
policies that are in the tree and are cleaning up the old ones. The
documentation is continuously updated
(&lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;handbook&lt;/a&gt;
and &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml"&gt;FAQ&lt;/a&gt;) and
we are getting a nice stream of users helping out and reporting stuff on
SELinux.&lt;/p&gt;
&lt;p&gt;So, besides the further stabilization and bug fixing, what else is on
the horizon?&lt;/p&gt;
&lt;p&gt;Well, our first concern now will be to make the ebuilds more... correct.
Some of them still violate a few QA rules and this needs to be fixed. If
possible, we'll also start converting our ebuilds to a more recent EAPI.&lt;/p&gt;
&lt;p&gt;Then, we will take a first stab at MCS within Gentoo Hardened. Our
primary concern here is support for the virtualization technologies
which, if SELinux-aware, are often using MCS to shield off running
guests from each other.&lt;/p&gt;
&lt;p&gt;So interesting times are ahead. And of course, while we're at it, we'll
continue improving policies and submitting our own patches upstream.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Policy 25, 26</title><link href="https://blog.siphos.be/2011/06/policy-25-26/" rel="alternate"></link><published>2011-06-01T21:32:00+02:00</published><updated>2011-06-01T21:32:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-06-01:/2011/06/policy-25-26/</id><summary type="html">&lt;p&gt;Recently I've seen quite a few messages on IRC pop up about &lt;code&gt;policy.25&lt;/code&gt;
or even &lt;code&gt;policy.26&lt;/code&gt; so I harassed the guys in the chat channel to talk
about it. Apparently, these new binary policy formats add support for
filename transitions and non-process role transitions.&lt;/p&gt;
&lt;p&gt;Currently, when you initiate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently I've seen quite a few messages on IRC pop up about &lt;code&gt;policy.25&lt;/code&gt;
or even &lt;code&gt;policy.26&lt;/code&gt; so I harassed the guys in the chat channel to talk
about it. Apparently, these new binary policy formats add support for
filename transitions and non-process role transitions.&lt;/p&gt;
&lt;p&gt;Currently, when you initiate a type transition, you would use something
like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type_transition mysqld_t mysql_db_t:sock_file mysqld_var_run_t;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This statement sais that, if a process running in the &lt;code&gt;mysqld_t&lt;/code&gt; domain
creates a socket in a directory labelled with &lt;code&gt;mysql_db_t&lt;/code&gt;, then this
socket gets the &lt;code&gt;mysqld_var_run_t&lt;/code&gt; label. In other words, the type
transitions from &lt;code&gt;mysql_db_t&lt;/code&gt; (parent label) to &lt;code&gt;mysqld_var_run_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What will be supported from version 25 onwards is that you can add
another argument, the file name (well, actually it is called "last name
component" and should be seen as what &lt;strong&gt;basename /path/to/something&lt;/strong&gt;
returns). That allows processes running in the same domain and writing
files in directories labelled with the same type to still have these
files labelled specifically. A non-existing example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;type_transition puppet_t etc_t:file locale_t timezone;
type_transition puppet_t etc_t:file net_conf_t resolv.conf;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, if the &lt;code&gt;puppet_t&lt;/code&gt; domain creates files in
&lt;path&gt;/etc&lt;/path&gt; (which is labelled &lt;code&gt;etc_t&lt;/code&gt;) then based on the file it
is creating, this file gets a different label (&lt;code&gt;/etc/timezone&lt;/code&gt; gets
labelled &lt;code&gt;locale_t&lt;/code&gt; whereas &lt;code&gt;/etc/resolv.conf&lt;/code&gt; gets labelled
&lt;code&gt;net_conf_&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The second change (valid since policy version 26) is that role
transitions now also support non-process class transitions. &lt;a href="http://permalink.gmane.org/gmane.comp.security.selinux/15079"&gt;A lengthy
post&lt;/a&gt; that
Harry Ciao made helps to describe it. The &lt;code&gt;role_transition&lt;/code&gt; support in
SELinux was previously used in the following way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;role_transition roleA_r some_exec_t roleB_r;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What this statement indicates is that a domain running within &lt;code&gt;roleA_r&lt;/code&gt;
and that is executing &lt;code&gt;some_exec_t&lt;/code&gt; will change its runtime role to
&lt;code&gt;roleB_r&lt;/code&gt;. If by calling &lt;code&gt;some_exec_t&lt;/code&gt; a domain transition occurs as
well (which is most common when a role transition is supported as well)
then this domain will run with the &lt;code&gt;roleB_r&lt;/code&gt; runtime role.&lt;/p&gt;
&lt;p&gt;The added functionality is now that this isn't limited to processes
anymore. You can now define non-process classes as well. If the source
domain creates something new of a particular class and a role transition
is declared for that, then the resulting new object will have the
specified role assigned to it (rather than the default &lt;code&gt;object_r&lt;/code&gt;). So
for instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;role_transition sysadm_r cron_spool_t:file sysadm_r;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If a domain running within the &lt;code&gt;sysadm_r&lt;/code&gt; role creates a file in a
directory labelled &lt;code&gt;cron_spool_t&lt;/code&gt;, then the resulting file will have the
role &lt;code&gt;sysadm_r&lt;/code&gt; rather than &lt;code&gt;object_r&lt;/code&gt;. This opens up more support for
role-based access controls (similar to the UBAC functionality that I
described earlier, but in some cases more flexible). I'm pretty sure
that the crontab management for vixie-cron will be one of the first ones
that can benefit greatly from this ;-)&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>SELinux file contexts</title><link href="https://blog.siphos.be/2011/05/selinux-file-contexts/" rel="alternate"></link><published>2011-05-15T13:39:00+02:00</published><updated>2011-05-15T13:39:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-05-15:/2011/05/selinux-file-contexts/</id><summary type="html">&lt;p&gt;If you have been working with SELinux for a while, you know that file
contexts are an important part of the policy and its enforcement. File
contexts are used to inform the SELinux tools which type a file,
directory, socket, ... should have. These types are then used to manage
the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you have been working with SELinux for a while, you know that file
contexts are an important part of the policy and its enforcement. File
contexts are used to inform the SELinux tools which type a file,
directory, socket, ... should have. These types are then used to manage
the policy itself, which is based on inter-type permissions.&lt;/p&gt;
&lt;p&gt;When dealing with file contexts, you either use&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;chcon&lt;/strong&gt; (mostly) if you are trying out stuff as a &lt;strong&gt;chcon&lt;/strong&gt;-set
    security context doesn't stick after a file system relabel operation
    (customizable types notwithstanding, and even then)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;restorecon&lt;/strong&gt; if you want to reset the file context of a file or
    set of files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;semanage&lt;/strong&gt; through the &lt;strong&gt;semanage fcontext -a -t your_type
    "regular_expression"&lt;/strong&gt; method, which enhances the SELinux known
    file contexts with the appropriate information so that relabel
    operations are survived&lt;/li&gt;
&lt;li&gt;policy improvements by editing and enhancing the &lt;code&gt;*.fc&lt;/code&gt; files that
    take part in the policy definition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When you look at the policy, or the output of &lt;strong&gt;semanage fcontext -l&lt;/strong&gt;,
you'll notice that the policy uses regular expressions very often. Of
course, without regular expression support, the file context rules
themselves would be impossible to manage. However, it immediately brings
up the question about what SELinux does when two or more lines are
appropriate for a particular file. Let's look at a few lines for
configuration related locations...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/etc/.*                     all files          system_u:object_r:etc_t
/etc/HOSTNAME               regular file       system_u:object_r:etc_runtime_t
/etc/X11/[wx]dm/Xreset.*    regular file       system_u:object_r:xsession_exec_t 
/etc/X11/wdm(/.*)?          all files          system_u:object_r:xdm_rw_etc_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above examples, you'll notice that there is quite some overlap.
To start, the first line already matches all other lines as well. So how
does SELinux handle this?&lt;/p&gt;
&lt;p&gt;Well, SELinux uses the following logic to find the most specific match,
and uses the most specific match then (extract taken from a pending
update to the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux-faq.xml"&gt;Gentoo Hardened SELinux
FAQ&lt;/a&gt;):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If line A has a regular expression, and line B doesn't, then line B
    is more specific.&lt;/li&gt;
&lt;li&gt;If the number of characters before the first regular expression in
    line A is less than the number of characters before the first
    regular expression in line B, then line B is more specific&lt;/li&gt;
&lt;li&gt;If the number of characters in line A is less than in line B, then
    line B is more specific&lt;/li&gt;
&lt;li&gt;If line A does not map to a specific SELinux type, and line B does,
    then line B is more specific&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So in case of &lt;code&gt;/etc/HOSTNAME&lt;/code&gt;, the second line is most specific because
it does not contain a regular expression.&lt;/p&gt;
&lt;p&gt;In case of &lt;code&gt;/etc/X11/wdm/Xreset.sh&lt;/code&gt;, SELinux will use the
xdm_rw_etc_t type and not the xsession_exec_t one. This is because
the first regular expression in the xsession_exec_t line (&lt;code&gt;[wx]&lt;/code&gt;)
comes sooner than the first regular expression in the xdm_rw_etc_t
line (&lt;code&gt;(/.*)?&lt;/code&gt;). You can validate this - even if you do not have such
file - with &lt;strong&gt;matchpathcon&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# matchpathcon /etc/X11/wdm/Xreset.sh
/etc/X11/wdm/Xreset.sh   system_u:object_r:xdm_rw_etc_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to know which line in the &lt;strong&gt;semanage fcontext -l&lt;/strong&gt; output is
used, you can use &lt;strong&gt;findcon&lt;/strong&gt; to show which lines match. That together
with the output of matchpathcon can help you deduce which line is
causing the label to be set:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# matchpathcon /etc/X11/wdm/Xreset.sh
/etc/X11/wdm/Xreset.sh   system_u:object_r:xdm_rw_etc_t
~# findcon /etc/selinux/strict/contexts/files/file_contexts -p /etc/X11/wdm/Xreset.sh
/.*             system_u:object_r:default_t
/etc/.*         system_u:object_r:etc_t
/etc/X11/[wx]dm/Xreset.*        --      system_u:object_r:xsession_exec_t
/etc/X11/wdm(/.*)?              system_u:object_r:xdm_rw_etc_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In many cases, the last output line of &lt;strong&gt;findcon&lt;/strong&gt; is the line you are
looking for, but I have not find a source that confirms this behavior so
do not trust this.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>SELinux Gentoo profile updates</title><link href="https://blog.siphos.be/2011/05/selinux-gentoo-profile-updates/" rel="alternate"></link><published>2011-05-03T23:17:00+02:00</published><updated>2011-05-03T23:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-05-03:/2011/05/selinux-gentoo-profile-updates/</id><summary type="html">&lt;p&gt;The SELinux support within Gentoo Hardened is continuing to go forward.
Anthony G. Basile has been working on the new SELinux Gentoo profiles
which were in dire need of updates. With the rework, we'll also support
the AMD64 no-multilib environment properly. With the new profiles we'll
also make &lt;em&gt;USE="open_perms …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;The SELinux support within Gentoo Hardened is continuing to go forward.
Anthony G. Basile has been working on the new SELinux Gentoo profiles
which were in dire need of updates. With the rework, we'll also support
the AMD64 no-multilib environment properly. With the new profiles we'll
also make &lt;em&gt;USE="open_perms"&lt;/em&gt; enabled by default. This will enable the
"open" permission within the SELinux policies. And of course we'll
remove that &lt;em&gt;FEATURES="loadpolicy"&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Not in git yet, but close, are further updates on the policy ebuilds.
Revision 14 of our &lt;code&gt;selinux-base-policy&lt;/code&gt; package will fail when the base
policy couldn't be loaded properly. This will ensure that a successful
installation means that the policies are loaded successfully as well. If
we wouldn't do this, then users might assume that the policies are
constantly being updated while in reality their system has always been
working with older policies. I am also going to, based on the &lt;a href="http://qa-reports.gentoo.org"&gt;QA
reports&lt;/a&gt;, update the &lt;code&gt;sec-policy/*&lt;/code&gt;
packages so they are not mentioned in those reports anymore.&lt;/p&gt;
&lt;p&gt;In other related news, the &lt;a href="http://goo.gl/uaaf4"&gt;Gentoo Hardened SELinux
FAQ&lt;/a&gt; will get updates on UBAC, cron issues and a
few errors (cosmetic or not) that you might have when working with
Gentoo Hardened SELinux. I'm also constantly updating the &lt;a href="http://goo.gl/DlHJD"&gt;Gentoo
Hardened SELinux Handbook (PDF)&lt;/a&gt; with the latest
information and developments.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>SELinux User-Based Access Control</title><link href="https://blog.siphos.be/2011/05/selinux-user-based-access-control/" rel="alternate"></link><published>2011-05-02T22:14:00+02:00</published><updated>2011-05-02T22:14:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-05-02:/2011/05/selinux-user-based-access-control/</id><summary type="html">&lt;p&gt;Within the reference policy, support is given to a feature called &lt;em&gt;UBAC
constraints&lt;/em&gt;. Here, UBAC stands for &lt;em&gt;User Based Access Control&lt;/em&gt;. The
idea behind the constraint is that any activity between two types (say
&lt;code&gt;foo_t&lt;/code&gt; and &lt;code&gt;bar_t&lt;/code&gt;) can be prohibited if the user contexts of the
resources that are using …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Within the reference policy, support is given to a feature called &lt;em&gt;UBAC
constraints&lt;/em&gt;. Here, UBAC stands for &lt;em&gt;User Based Access Control&lt;/em&gt;. The
idea behind the constraint is that any activity between two types (say
&lt;code&gt;foo_t&lt;/code&gt; and &lt;code&gt;bar_t&lt;/code&gt;) can be prohibited if the user contexts of the
resources that are using those types are different. So even though
&lt;code&gt;foo_t&lt;/code&gt; can read files with label &lt;code&gt;bar_t&lt;/code&gt;, a process running as
&lt;code&gt;user1:user_r:foo_t&lt;/code&gt; will not be able to read a file labeled
&lt;code&gt;user2:user_r:bar_t&lt;/code&gt;. The policy defines the constraint like so (taken
from &lt;code&gt;policy/constraints&lt;/code&gt; and rewritten in a more readable code):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Action is okay if
  user1 == user2, or
  user1 == system_u, or
  user2 == system_u, or
  type1 is not a UBAC constrained type, or
  type2 is not a UBAC constrained type
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So the constraint only denies an activity if the users involved are not
&lt;code&gt;system_u&lt;/code&gt; (that would render your system useless), not the same, and
&lt;em&gt;both types are ubac constrained types&lt;/em&gt;. The latter is, within the
policy, set using type attributes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ seinfo -aubac_constrained_type -x
ubac_constrained_type
   screen_var_run_t
   admin_crontab_t
   links_input_xevent_t
   ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Some domains are also UBAC exempt (currently I know of the &lt;code&gt;sysadm_t&lt;/code&gt;
domain - cfr the &lt;code&gt;ubacproc&lt;/code&gt; and &lt;code&gt;ubacfile&lt;/code&gt; attributes), meaning that
activities started from the &lt;code&gt;sysadm_t&lt;/code&gt; domain will not trigger the
constraint.&lt;/p&gt;
&lt;p&gt;UBAC gives some additional control on information flow between
resources. But it isn't perfect. One major downside is that the error
you get when the constraint is hit is a simple AVC denial where most
users would just check the inter-type privileges, without paying
attention to the difference in SELinux user identities. Another is that
it might be difficult for users or administrators that use different
SELinux user identities to still work properly with UBAC constrained
domains. Work is on the way in the SELinux development to improve the
role-based access control (RBAC) by allowing files and directories to
have a role as well (rather than the &lt;code&gt;object_r&lt;/code&gt; placeholder used
currently) and then work on those roles. You can then grant the users
that need access to a particular resource the necessary role rather than
requiring those users to use the same SELinux user id. This would take
at least one major downside of UBAC away and I'm hoping that the logging
will improve on this as well.&lt;/p&gt;
&lt;p&gt;Of course, I do not ramble about UBAC here because it is fun (well yes,
yes it is fun) but because in Gentoo, we've hit one UBAC-related issue.
When a user starts vixie-cron, the root crontab would fail to be loaded.
What gives? The root crontab has the SELinux identity of &lt;code&gt;staff_u&lt;/code&gt; (as
it is created by a regular staff user that su(do)'ed) whereas the
&lt;code&gt;cronjob_t&lt;/code&gt; process would have the SELinux identity of &lt;code&gt;root&lt;/code&gt;. Bang.
Dead. No error beyond what vixie-cron gives.&lt;/p&gt;
&lt;p&gt;Of course this can be easily worked around. &lt;strong&gt;chcon -u root
/var/spool/cron/crontabs/root&lt;/strong&gt; works, or you can recreate the crontab
as a console-logged-on root user. We could also change the default
context used by &lt;code&gt;cronjob_t&lt;/code&gt; to use &lt;code&gt;staff_u:sysadm_r:cronjob_t&lt;/code&gt; for
root. But we can also take a look at how other distributions do this.
What gives: most distributions &lt;em&gt;disable&lt;/em&gt; UBAC within the policy. Their
reasons might vary, but manageability of the policy comes to mind, as
well as reducing the number of (difficult to debug) problems. Most are
keen to include the RBAC at some point in the future though. Some
discussion on #gentoo-hardened and #selinux later, and I decided to
use a USE flag called "ubac" to optionally enable UBAC within the
policy. How very Gentoo, isn't it? At least users have the choice of
using UBAC or not (I know I'm going to enable it) and when RBAC is
available, we'll definitely make sure that support for RBAC is available
too.&lt;/p&gt;
&lt;p&gt;Currently in the hardened overlay,
&lt;code&gt;sec-policy/selinux-base-policy-2.20101213-r13&lt;/code&gt;. Take your pick on it,
give it a try and report any bugs you have on
&lt;a href="https://bugs.gentoo.org"&gt;Bugzilla&lt;/a&gt;. And if you enable USE="ubac", you
get user based access control for free.&lt;/p&gt;
&lt;p&gt;PS I'm also going to reapply for Gentoo developer-ship and, amongst
other things, help out the hardened team with SELinux policies and
documentation.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>SELinux and noatsecure, or why portage complains about LD_PRELOAD and libsandbox.so</title><link href="https://blog.siphos.be/2011/04/selinux-and-noatsecure-or-why-portage-complains-about-ld_preload-and-libsandbox-so/" rel="alternate"></link><published>2011-04-22T21:00:00+02:00</published><updated>2011-04-22T21:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-04-22:/2011/04/selinux-and-noatsecure-or-why-portage-complains-about-ld_preload-and-libsandbox-so/</id><summary type="html">&lt;p&gt;If you're fiddling with SELinux policies, you will eventually notice
that the reference policy by default hides certain privilege requests
(which are denied). One of them is noatsecure. But what is noatsecure?
To describe noatsecure, I first need to describe what atsecure is. And
to describe what that is, we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're fiddling with SELinux policies, you will eventually notice
that the reference policy by default hides certain privilege requests
(which are denied). One of them is noatsecure. But what is noatsecure?
To describe noatsecure, I first need to describe what atsecure is. And
to describe what that is, we first need to give a small talk about ELF
auxiliary vectors.&lt;/p&gt;
&lt;p&gt;As you probably know, when an application instantiates another
application, it calls the &lt;code&gt;execve&lt;/code&gt; function right after fork'ing to load
the new application in memory. The actual task to load the new
application in memory is done by the C library, more specifically the
binary loader. For Linux, this is the ELF loader. Now, &lt;em&gt;ELF auxiliary
vectors&lt;/em&gt; are parameters or flags that are set (or at least managed) by
the ELF loader to allow the application and program interpreter to get
some OS-specific information. Examples of such vectors are &lt;code&gt;AT_UID&lt;/code&gt; and
&lt;code&gt;AT_EUID&lt;/code&gt; (real uid and effective uid) and &lt;code&gt;AT_PAGESZ&lt;/code&gt; (system page
size).&lt;/p&gt;
&lt;p&gt;One of the vectors that glibc supports is &lt;code&gt;AT_SECURE&lt;/code&gt;. This particular
parameter (which is either "0" (default) or "1") tells the ELF dynamic
linker to unset various &lt;a href="http://sourceware.org/git/?p=glibc.git;a=blob_plain;f=sysdeps/generic/unsecvars.h;hb=HEAD"&gt;environment
variables&lt;/a&gt;
that are considered potentially harmful for your system. One of these is
&lt;code&gt;LD_PRELOAD&lt;/code&gt; (I mention this one specifically because it was the source
of my small investigation). Normally, this environment sanitation is
done when a setuid/setgid application is called (to prevent the obvious
vulnerabilities). However, SELinux enhances the use of this
sanitation...&lt;/p&gt;
&lt;p&gt;Whenever an application is called which triggers a domain transition in
SELinux (say &lt;code&gt;sysadm_t&lt;/code&gt; to &lt;code&gt;mozilla_t&lt;/code&gt; through a binary labelled
&lt;code&gt;mozilla_exec_t&lt;/code&gt;), SELinux sets the &lt;code&gt;AT_SECURE&lt;/code&gt; flag for the loaded
application (in the example, mozilla/firefox). In other words, every
time a domain transition occurs, the environment for this application is
sanitized.&lt;/p&gt;
&lt;p&gt;As you can imagine now the &lt;code&gt;noatsecure&lt;/code&gt; permission disables the
environment sanitation activity for a particular transition. You can do
this through the following allow statement (applied to the above
example):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow sysadm_t mozilla_t:process { noatsecure };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;if every domain transition for which this permission isn't allowed would
log its denial, our audit logs would be filled with noise. That is why
the reference policy by default hides (&lt;code&gt;dontaudit&lt;/code&gt;) these calls. But
knowing what they are for is important, because you might sometimes come
into contact with it, like I did:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Installing (1 of 1) net-dns/host-991529
&amp;gt;&amp;gt;&amp;gt; Setting SELinux security labels
ERROR: ld.so: object &amp;#39;libsandbox.so&amp;#39; from LD_PRELOAD cannot be preloaded: ignored.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This error message is when Portage (running in &lt;code&gt;portage_t&lt;/code&gt;) wants to
relabel the files that it just placed on the system through setfiles
(which will run in &lt;code&gt;setfiles_t&lt;/code&gt;). As this involves a domain transition,
&lt;code&gt;AT_SECURE&lt;/code&gt; is set for setfiles, but &lt;code&gt;LD_PRELOAD&lt;/code&gt; was set as part of
Portage' sandboxing feature. This environment variable is disabled, and
the loader warns the user that it cannot preload &lt;code&gt;libsandbox.so&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although we can just set &lt;em&gt;noatsecure&lt;/em&gt; here, it would open up a (small)
window for exploits (although they would need to be provided through
Portage, because when a user calls Portage, a domain transition is done
there as well so the user-provided environment variables are already
sanitized by then). By not allowing &lt;em&gt;noatsecure&lt;/em&gt;, we are disabling a few
functionalities provided by the libsandbox.so library &lt;em&gt;for the file
labeling activity&lt;/em&gt; (this is &lt;strong&gt;very important&lt;/strong&gt; to understand: it does
not disable the sandboxing for the builds and merges, only for the file
relabeling). As we already run setfiles in its own, confined domain, I
believe that we are best served by keeping the secure environment
sanitation here. That does mean that the warning will stay as we cannot
control that from within SELinux.&lt;/p&gt;
&lt;p&gt;If you want to allow &lt;em&gt;noatsecure&lt;/em&gt; here, create a simple module and load
it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# cat &amp;gt; portage_noatsecure.te &amp;lt;&amp;lt; EOF
module portage_noatsecure 1.0;
require {
  type portage_t;
  type setfiles_t;
  class process { noatsecure };
}
allow portage_t setfiles_t:process { noatsecure };
EOF
~# checkmodule -m -o portage_noatsecure.mod portage_noatsecure.te
~# semodule_package -o portage_noatsecure.pp -m portage_noatsecure.mod
~# semodule -i portage_noatsecure.pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category></entry><entry><title>cvechecker 3.0</title><link href="https://blog.siphos.be/2011/04/cvechecker-3-0/" rel="alternate"></link><published>2011-04-12T22:47:00+02:00</published><updated>2011-04-12T22:47:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-04-12:/2011/04/cvechecker-3-0/</id><summary type="html">&lt;p&gt;I'm pleased to announce the immediate availability of &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
3.0&lt;/a&gt;. It contains two major feature
enhancements: watchlists and MySQL support.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;watchlists&lt;/em&gt; allow cvechecker to track and report on CVEs for software
that cvechecker didn't detect on the system (or perhaps even isn't
installed on the system). You can use …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm pleased to announce the immediate availability of &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
3.0&lt;/a&gt;. It contains two major feature
enhancements: watchlists and MySQL support.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;watchlists&lt;/em&gt; allow cvechecker to track and report on CVEs for software
that cvechecker didn't detect on the system (or perhaps even isn't
installed on the system). You can use watchlists to stay informed of
potential vulnerabilities in software used at work on servers where you
are not allowed (or do not want) to run cvechecker on. To use
watchlists, create a text file containing the CPE identifiers for the
software that you want to watch out for, and add it to the database:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ cat watchlist.txt
cpe:/a:microsoft:excel:2003:::

~$ cvechecker -d -w watchlist.txt
Adding CPE entries
  - Added watch for cpe:/a:microsoft:excel:2003:::
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The second major feature is support for MySQL. This is the first
server-oriented RDBMS that cvechecker supports (earlier versions worked
with sqlite only) although sqlite support remains available as well. I
hope to extend the number of supported databases in the future (say
PostgreSQL, Oracle, SQL Server, ...). With support for server RDBMSes
came of course the requirement that multiple cvechecker clients are able
to use the same server (as the CVE and CPE data itself can be shared).
With the 3.0 release, this is supported as each client now "adds" to the
data both his hostname as well as an (optional) user defined value
(which can be anything you like). If unset, this user value is set to
the hostname, but you can use things like the systems' serial ID or
asset ID.&lt;/p&gt;
&lt;p&gt;I'm hoping all users have fun with it - I know I have while writing it.
Feedback, remarks, feature requests, bugs and other criticism is always
very much appreciated.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker updates</title><link href="https://blog.siphos.be/2011/03/cvechecker-updates/" rel="alternate"></link><published>2011-03-27T22:20:00+02:00</published><updated>2011-03-27T22:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-03-27:/2011/03/cvechecker-updates/</id><summary type="html">&lt;p&gt;The in-svn version of cvechecker has seen quite a few changes in the
last few days. I'm adding support for MySQL to it. This support will be
added in three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;support the same features as cvechecker currently does using sqlite&lt;/li&gt;
&lt;li&gt;streamline the database code so that duplicate code in …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;The in-svn version of cvechecker has seen quite a few changes in the
last few days. I'm adding support for MySQL to it. This support will be
added in three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;support the same features as cvechecker currently does using sqlite&lt;/li&gt;
&lt;li&gt;streamline the database code so that duplicate code in the sqlite
    implementation and mysql implementation is removed&lt;/li&gt;
&lt;li&gt;support multi-node systems with a single master database&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The latter is something I've been meaning to implement for quite some
time: have a single system dedicated to download and store the latest
CVE entries in the database (as well as CPE definitions) whereas several
systems can use the database by storing their own system information and
getting a mapping from that information against the CVE database. Even
more so, it would allow you to query the database asking on which
systems a particular software was detected, or which systems still have
vulnerable software installed.&lt;/p&gt;
&lt;p&gt;When the MySQL support is implemented, I'm going to work a bit on the
&lt;code&gt;versions.dat&lt;/code&gt; file, because it doesn't really support many services
currently. I'm going to use it against my "virtual network" (a
combination of KVM guests running bind (master/slave), ldap
(multi-master), postfix, apache, squirrelmail, courier, postgresql,
mysql and more) and enhance it so that it detects all those components
as well.&lt;/p&gt;
&lt;p&gt;Oh, btw, I had a request to include support for just telling cvechecker
which components/software to look for (rather than it scanning the files
and deducing it from regular expressions and the like). The in-svn
version supports it, so it will definitely be part of the 3.0 release.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Restoring configuration files on Gentoo</title><link href="https://blog.siphos.be/2011/03/restoring-configuration-files-on-gentoo/" rel="alternate"></link><published>2011-03-19T16:32:00+01:00</published><updated>2011-03-19T16:32:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-03-19:/2011/03/restoring-configuration-files-on-gentoo/</id><summary type="html">&lt;p&gt;If you work with Gentoo, you're probably aware of tools like
&lt;strong&gt;etc-update&lt;/strong&gt; and &lt;strong&gt;dispatch-conf&lt;/strong&gt;. If you use &lt;strong&gt;dispatch-conf&lt;/strong&gt;, you
might know that it supports &lt;strong&gt;rcs&lt;/strong&gt; for version control of the changes
it makes. But if you have enabled it, you might be wondering how to
actually restore configuration files with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you work with Gentoo, you're probably aware of tools like
&lt;strong&gt;etc-update&lt;/strong&gt; and &lt;strong&gt;dispatch-conf&lt;/strong&gt;. If you use &lt;strong&gt;dispatch-conf&lt;/strong&gt;, you
might know that it supports &lt;strong&gt;rcs&lt;/strong&gt; for version control of the changes
it makes. But if you have enabled it, you might be wondering how to
actually restore configuration files with it.&lt;/p&gt;
&lt;p&gt;Well, &lt;strong&gt;dispatch-conf&lt;/strong&gt; stores its version control information in
&lt;code&gt;/etc/config-archive&lt;/code&gt;. To restore a configuration file to a previous
version, first find out what versions there are in the version control
system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ rlog -zLT /etc/config-archive/etc/protocols,v
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output of the &lt;strong&gt;rlog&lt;/strong&gt; command should allow you to find the revision
you are interested in. The -zLT option displays date/time in the current
timezone (instead of UTC). Once you have found the revision you are
looking for, restore the file by redirecting the output of the &lt;strong&gt;co&lt;/strong&gt;
command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ co -p -r1.1.1 /etc/config-archive/etc/protocols,v &amp;gt; /etc/protocols
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Gentoo"></category></entry><entry><title>Updates on SELinux docs, added FAQ</title><link href="https://blog.siphos.be/2011/03/updates-on-selinux-docs-added-faq/" rel="alternate"></link><published>2011-03-09T22:17:00+01:00</published><updated>2011-03-09T22:17:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-03-09:/2011/03/updates-on-selinux-docs-added-faq/</id><summary type="html">&lt;p&gt;As you're probably noticing from my &lt;a href="https://twitter.com/#!/sjvermeu"&gt;twitter
feed&lt;/a&gt; and the various posts earlier in
my blog, I'm helping out with the Gentoo Hardened folks to get the
SELinux support state up to par. Today, the &lt;a href="http://goo.gl/DlHJD"&gt;Gentoo Hardened/SELinux
Handbook&lt;/a&gt; had a few updates, but the most important
change is that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As you're probably noticing from my &lt;a href="https://twitter.com/#!/sjvermeu"&gt;twitter
feed&lt;/a&gt; and the various posts earlier in
my blog, I'm helping out with the Gentoo Hardened folks to get the
SELinux support state up to par. Today, the &lt;a href="http://goo.gl/DlHJD"&gt;Gentoo Hardened/SELinux
Handbook&lt;/a&gt; had a few updates, but the most important
change is that there is now a &lt;a href="http://goo.gl/uaaf4"&gt;Gentoo Hardened SELinux
FAQ&lt;/a&gt; (in draft). I'm hoping that, at the next IRC
meeting, we can vote on having it pushed to the main site. Also, the
latest changes I made to various SELinux policy ebuilds have been pushed
to the main tree.&lt;/p&gt;
&lt;p&gt;I'm now focusing on running various servers in KVM guests to test the
SELinux policies. Following the &lt;a href="http://www.gentoo.org/doc/en/virt-mail-howto.xml"&gt;Gentoo Virtual Mailhosting
HOWTO&lt;/a&gt; creates a
working system, although a few SELinux-specific steps had to be added
(if you follow the guide exactly to the letter, you won't finish it).
The issues are minor though: &lt;code&gt;selinux-sasl&lt;/code&gt; needs to be installed
manually (it isn't pulled in as a dependency), the PIDFILE and
SSLPIDFILE variables in /etc/courier-imap/* need to point to
/var/run/courier (and that location needs to be created) to match the
file context definitions as suggested by upstream, you need to run
&lt;strong&gt;postalias /etc/mail/aliases&lt;/strong&gt; instead of &lt;strong&gt;newaliases&lt;/strong&gt; and during the
installation of Apache, you might need to &lt;strong&gt;chcon -t bin_t
/usr/share/build-1/mkdir.sh&lt;/strong&gt; as you'll get a permission denied
otherwise.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Portage fails to build due to SELinux?</title><link href="https://blog.siphos.be/2011/03/portage-fails-to-build-due-to-selinux/" rel="alternate"></link><published>2011-03-03T00:26:00+01:00</published><updated>2011-03-03T00:26:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-03-03:/2011/03/portage-fails-to-build-due-to-selinux/</id><summary type="html">&lt;p&gt;If you're having troubles getting Portage to build packages due to
SELinux, then the reason usually is that it is unable to transition to
the proper portage domains. You'll get a nice OSError back with an ugly
backtrace, saying somewhere that "setexeccon" is misbehaving.&lt;/p&gt;
&lt;p&gt;Now, the real issue (not being …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're having troubles getting Portage to build packages due to
SELinux, then the reason usually is that it is unable to transition to
the proper portage domains. You'll get a nice OSError back with an ugly
backtrace, saying somewhere that "setexeccon" is misbehaving.&lt;/p&gt;
&lt;p&gt;Now, the real issue (not being able to transition) means that the
current domain you are in (check &lt;strong&gt;id -Z&lt;/strong&gt;) has no right to transition
to the &lt;em&gt;portage_fetch_t&lt;/em&gt;, &lt;em&gt;portage_t&lt;/em&gt; or &lt;em&gt;portage_sandbox_t&lt;/em&gt;
domains. You can verify that with &lt;strong&gt;seinfo&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# id -Z
unconfined_u:unconfined_r:unconfined_t
~# seinfo -runconfined_r -x | grep portage
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above example shows it for the &lt;em&gt;unconfined_t&lt;/em&gt; domain, but the same
is true if your current domain is a more illogical &lt;em&gt;local_login_t&lt;/em&gt;
(hint: check your PAM settings) or &lt;em&gt;initrc_t&lt;/em&gt;. Now, if you want to fix
these things, we will eventually ask you to reemerge some things - which
was the first reason why you came asking how to fix things.&lt;/p&gt;
&lt;p&gt;There are two ways to handle this situation: the proper way (disabling
SELinux and reenabling later) or the ugly way (hack Portage to ignore).&lt;/p&gt;
&lt;p&gt;In the first way, you need to edit &lt;em&gt;/etc/selinux/config&lt;/em&gt;, set
&lt;em&gt;SELINUX=disabled&lt;/em&gt;, reboot, emerge whatever you need, edit the file
again restoring SELINUX to what you had before, reboot, relabel your
entire filesystem (&lt;strong&gt;rlpkg -a -r&lt;/strong&gt;) and perhaps even reboot again.&lt;/p&gt;
&lt;p&gt;In the second method, edit /usr/lib(64)/portage/pym/portage/_selinux.py
and go to line 79. It reads:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;        if selinux.setexeccon(ctx) &amp;lt; 0:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Comment out that line (so it isn't lost) and substitute it with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;        if selinux.setexeccon(&amp;quot;\n&amp;quot;) &amp;lt; 0:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you should be able to install software without hitting the error.
But note that &lt;em&gt;this is only to help you fix the real problem&lt;/em&gt; as we're
circumventing SELinux integration in Portage a bit.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Updates on the Gentoo Hardened SELinux state</title><link href="https://blog.siphos.be/2011/03/updates-on-the-gentoo-hardened-selinux-state/" rel="alternate"></link><published>2011-03-02T23:09:00+01:00</published><updated>2011-03-02T23:09:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-03-02:/2011/03/updates-on-the-gentoo-hardened-selinux-state/</id><summary type="html">&lt;p&gt;For those following the progress of SELinux support in Gentoo
Hardened...&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;hardened-development&lt;/em&gt; overlay, the &lt;code&gt;selinux-base-policy&lt;/code&gt; package
has been updated, hopefully fixing a nasty issue with support for the
targeted policy (up to today, I only tested strict policies so I missed
that). It also fixes an issue with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For those following the progress of SELinux support in Gentoo
Hardened...&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;hardened-development&lt;/em&gt; overlay, the &lt;code&gt;selinux-base-policy&lt;/code&gt; package
has been updated, hopefully fixing a nasty issue with support for the
targeted policy (up to today, I only tested strict policies so I missed
that). It also fixes an issue with dhcpcd not functioning properly. If
you use SELinux and don't have the &lt;em&gt;hardened-development&lt;/em&gt; overlay yet,
please use &lt;strong&gt;layman -a hardened-development&lt;/strong&gt;, synchronize (&lt;strong&gt;layman
-S&lt;/strong&gt;) and update your system to get the latest base policy. Also, please
report bugs on &lt;a href="https://bugs.gentoo.org"&gt;Gentoo Bugzilla&lt;/a&gt; (and perhaps
immediately add &lt;em&gt;selinux@gentoo.org&lt;/em&gt; to the Cc-list.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://goo.gl/DlHJD"&gt;Gentoo Hardened SELinux Handbook&lt;/a&gt;, still in
draft, has gotten a few updates. It now documents the use of the
&lt;strong&gt;gentoo_try_dontaudit&lt;/strong&gt; boolean which the Gentoo Hardened SELinux
developers use to hide potential cosmetic denials. If they are truly
cosmetic, they will be reported upstream later to be included in the
reference policy. If they are not, then users can simple toggle the
boolean (&lt;strong&gt;setsebool gentoo_try_dontaudit off&lt;/strong&gt;) to see the denials
that the developers hid.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://goo.gl/2U0Zr"&gt;Gentoo Hardened SELinux Policy&lt;/a&gt; now includes
the naming convention on the SELinux policy packages with a very short
explanation why this particular convention was chosen. The discussion on
it can be found on the
&lt;a href="http://news.gmane.org/gmane.linux.gentoo.hardened"&gt;gentoo-hardened&lt;/a&gt;
mailing list and the last &lt;a href="http://article.gmane.org/gmane.linux.gentoo.hardened/4765"&gt;online
meeting&lt;/a&gt;.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Temporary script for Gentoo Hardened SELinux users</title><link href="https://blog.siphos.be/2011/02/temporary-script-for-gentoo-hardened-selinux-users/" rel="alternate"></link><published>2011-02-27T17:37:00+01:00</published><updated>2011-02-27T17:37:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-02-27:/2011/02/temporary-script-for-gentoo-hardened-selinux-users/</id><summary type="html">&lt;p&gt;If you are currently using Gentoo Hardened with SELinux, you might have
noticed that we are currently lacking the proper dependencies within our
Portage tree upon the SELinux policies (or, in other words, installing a
package doesn't guarantee that the SELinux policy needed for that
package is pulled in as …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are currently using Gentoo Hardened with SELinux, you might have
noticed that we are currently lacking the proper dependencies within our
Portage tree upon the SELinux policies (or, in other words, installing a
package doesn't guarantee that the SELinux policy needed for that
package is pulled in as well). As the current SELinux policy is still in
\~arch phase, it is also not really feasible to ask other package
maintainers to add the proper dependency information as that might stall
potential stability requests in general.&lt;/p&gt;
&lt;p&gt;So, for the time being, I'm using a simple script (which I call
&lt;a href="https://github.com/sjvermeu/small.coding/tree/master/genmodoverview"&gt;genmodoverview&lt;/a&gt;)
which tells me on my systems which SELinux policy modules i might still
be missing. Based on the output of that script, I can then continue to
install the &lt;code&gt;sec-policy/selinux-*&lt;/code&gt; package(s) for those modules.&lt;/p&gt;
&lt;p&gt;It's usage is simple. Download the genmodoverview.sh and LISTING files,
make the first one executable and run &lt;strong&gt;./genmodoverview.sh -c
LISTING&lt;/strong&gt;.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>About time...</title><link href="https://blog.siphos.be/2011/02/about-time/" rel="alternate"></link><published>2011-02-24T21:44:00+01:00</published><updated>2011-02-24T21:44:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-02-24:/2011/02/about-time/</id><summary type="html">&lt;p&gt;I was just wondering why "UTC" stood for "Coordinated Universal Time".
Apparently (okay, citing
&lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/UTC"&gt;Wikipedia&lt;/a&gt; here, so
be critical), it's of two main reasons: English and French speaking
folks that were participating in that discussion wanted their language
to be presented in the abbreviation (English wants "CUT - Coordinated
Universal Time …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was just wondering why "UTC" stood for "Coordinated Universal Time".
Apparently (okay, citing
&lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/UTC"&gt;Wikipedia&lt;/a&gt; here, so
be critical), it's of two main reasons: English and French speaking
folks that were participating in that discussion wanted their language
to be presented in the abbreviation (English wants "CUT - Coordinated
Universal Time", French "TUC" - Temps Universel Coordonné), so a
consensus was taken to use "UTC" which gladly followed another time
convention (UT1, UT2, ... for universal time notations - which is the
second reason).&lt;/p&gt;
&lt;p&gt;Little did I know that it is UTC that has leap seconds (sometimes you
hear in the news that the last minute of a year lasts 61 seconds)
whereas UT1 (which is the modern notation for GMT) doesn't.&lt;/p&gt;
&lt;p&gt;All that because I read about a meeting taking place at 2000 UTC...&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>cvechecker update</title><link href="https://blog.siphos.be/2011/02/cvechecker-update/" rel="alternate"></link><published>2011-02-19T16:31:00+01:00</published><updated>2011-02-19T16:31:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-02-19:/2011/02/cvechecker-update/</id><summary type="html">&lt;p&gt;A while ago, I got the request to enhance
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; with support for
providing a list of installed software (or software you want to watch
over with cvechecker) even if cvechecker isn't able to detect that
software on your system. I've implemented this and it is currently
available in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while ago, I got the request to enhance
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; with support for
providing a list of installed software (or software you want to watch
over with cvechecker) even if cvechecker isn't able to detect that
software on your system. I've implemented this and it is currently
available in the SVN repository. The next release of cvechecker will
support this, but I'm hoping to add support for other databases with it
as well (currently, it uses a local sqlite database but I'm hoping to
support at least MySQL and postgresql too).&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>File System Labels in Linux Sea</title><link href="https://blog.siphos.be/2011/02/file-system-labels-in-linux-sea/" rel="alternate"></link><published>2011-02-12T20:42:00+01:00</published><updated>2011-02-12T20:42:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-02-12:/2011/02/file-system-labels-in-linux-sea/</id><summary type="html">&lt;p&gt;I have added some information on file system labels in &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;
(&lt;a href="http://swift.siphos.be/linux_sea/linux_sea.pdf"&gt;PDF&lt;/a&gt;). If you don't
know what labels are (or UUIDs), here is a quick summary.&lt;/p&gt;
&lt;p&gt;Most, if not all file systems, assign a universally unique identifier
(UUID) which looks like a random hexadecimal string to each file system …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have added some information on file system labels in &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;
(&lt;a href="http://swift.siphos.be/linux_sea/linux_sea.pdf"&gt;PDF&lt;/a&gt;). If you don't
know what labels are (or UUIDs), here is a quick summary.&lt;/p&gt;
&lt;p&gt;Most, if not all file systems, assign a universally unique identifier
(UUID) which looks like a random hexadecimal string to each file system.
On a Gentoo system, you can get an overview of all UUIDs detected using
a simple &lt;strong&gt;ls -l /dev/disk/by-uuid&lt;/strong&gt;, courtesy of the gentoo udev rules.
Users can also assign a specific label to a file system, either when
they create it (like &lt;strong&gt;mkfs.ext4 -L ROOT /dev/sda2&lt;/strong&gt;) or afterwards
(&lt;strong&gt;e2label /dev/sda2 ROOT&lt;/strong&gt;). This is also not limited to the content
file systems - you can also assign a label to a swap file system.&lt;/p&gt;
&lt;p&gt;This information can then be used to uniquely identify the file system,
even if you don't know what the device file (/dev/sda2) is called. A
huge advantage is for those devices that often change device file
(removable media, but also SATA or SCSI disks on systems where the admin
loves adding and removing disks ;-) as you can keep your fstab
configuration static: the fstab file doesn't need to be changed, even
when the device files themselves change.&lt;/p&gt;
&lt;p&gt;A simple fstab line (&lt;code&gt;/dev/sda2  /  ext4  defaults,noatime  0 0&lt;/code&gt;) can
then easily be transformed to use the LABEL="..." syntax (like
&lt;code&gt;LABEL=ROOT /  ext4  defaults,noatime  0 0&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Some people might also think they can use this mechanism for their
kernel boot parameter (like using &lt;code&gt;root=LABEL=ROOT&lt;/code&gt; instead of
&lt;code&gt;root=/dev/sda2&lt;/code&gt;). Sadly, the Linux kernel doesn't offer this
functionality. It is possible, but only when you use an initramfs (which
I don't).&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>SELinux for Gentoo Hardened</title><link href="https://blog.siphos.be/2011/02/selinux-for-gentoo-hardened/" rel="alternate"></link><published>2011-02-06T23:26:00+01:00</published><updated>2011-02-06T23:26:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-02-06:/2011/02/selinux-for-gentoo-hardened/</id><summary type="html">&lt;p&gt;Recently, most of the SELinux-related ebuilds from the hardened overlay
have been moved to the official Portage tree. Hopefully, this will
trigger more people / organizations to try Gentoo Hardened with SELinux
and help us improve the ebuilds. They're still marked as \~arch (as they
should be). The draft &lt;a href="http://goo.gl/DlHJD"&gt;SELinux handbook …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently, most of the SELinux-related ebuilds from the hardened overlay
have been moved to the official Portage tree. Hopefully, this will
trigger more people / organizations to try Gentoo Hardened with SELinux
and help us improve the ebuilds. They're still marked as \~arch (as they
should be). The draft &lt;a href="http://goo.gl/DlHJD"&gt;SELinux handbook&lt;/a&gt; has been
updated to reflect this and I'm currently performing a few greenfield
installations using the documentation again to verify if everything
still works as it is supposed to. In the meantime, I'm scripting these
things so that I can automate these tests and run those at night in the
future ;-)&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>"Gentoo in production?" Oh no, not again...</title><link href="https://blog.siphos.be/2011/01/gentoo-in-production-oh-no-not-again/" rel="alternate"></link><published>2011-01-21T21:59:00+01:00</published><updated>2011-01-21T21:59:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-01-21:/2011/01/gentoo-in-production-oh-no-not-again/</id><summary type="html">&lt;p&gt;I think it is that time of the year again, where people get some crazy
ideas. Again I discussed the what must be the gazillion-th time I've
been asked "Do you think Gentoo is ripe for use in production?".
Honestly, I always tell myself to ignore those discussions but I've …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I think it is that time of the year again, where people get some crazy
ideas. Again I discussed the what must be the gazillion-th time I've
been asked "Do you think Gentoo is ripe for use in production?".
Honestly, I always tell myself to ignore those discussions but I've
never managed to actually do that - ignore, that is. So to give me some
leverage the gazillion-th plus one time I get that same question, let me
vent my opinion about the subject right here and allow me to hurdle the
permalink to whomever tries to start another heated discussion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your question is wrong&lt;/strong&gt;. It is never about a technology being "ripe
enough" or "stable enough". What you need to ask yourself (or get
acquainted with) is what you, your organization or your company expects
from a technology to be introduced in your (organization/company)
infrastructure. This includes, but is not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What kind of &lt;em&gt;bugfixing&lt;/em&gt; and &lt;em&gt;security fixing&lt;/em&gt; support do you want
    for the technology?&lt;/li&gt;
&lt;li&gt;What kind of &lt;em&gt;knowledge support&lt;/em&gt; do you want for the technology?&lt;/li&gt;
&lt;li&gt;How important is &lt;em&gt;certification&lt;/em&gt; of (other) technologies with
    respect to the technology or operating system?&lt;/li&gt;
&lt;li&gt;How far do you implement an operating systems' &lt;em&gt;release cycle&lt;/em&gt;?&lt;/li&gt;
&lt;li&gt;What level of experience do you expect from your &lt;em&gt;internal support
    team&lt;/em&gt; (or yourself)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, the questions are not about technology itself or pretty
features. It is about how you work with that technology. And one
shouldn't look at these questions as having a single phrase as answer.
To properly answer the first question alone, you'll need to take a look
at delivery times (how fast do you want a bug to be fixed), follow-up
(how fast does the technology issue security announcements, do they
follow CVE closely, ...), responsibilities, eventual legal or
contractual obligations you might need to cover your ___, the ability
of the provider to reproduce issues etc.&lt;/p&gt;
&lt;p&gt;Internal experience is also not to be underestimated. How quick do you
(organization/company) want to be able to resolve problems? How
experienced are you with analyzing logs? How well are you able to
integrate a technology within your existing architecture? What auditing
does you(r organization/company) require and do you know how to get that
from the technology?&lt;/p&gt;
&lt;p&gt;I mean, come on, you're talking about "production". That's not the same
as saying "I've installed it for my parents".&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Confining user applications</title><link href="https://blog.siphos.be/2011/01/confining-user-applications/" rel="alternate"></link><published>2011-01-16T16:23:00+01:00</published><updated>2011-01-16T16:23:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2011-01-16:/2011/01/confining-user-applications/</id><summary type="html">&lt;p&gt;Ever since I started using SELinux, I'm getting more and more fond of
what it can do for (security) administrators. Lately, I've started
confining user applications (like &lt;strong&gt;skype&lt;/strong&gt;) in the idea that I do not
want any application connecting to the Internet or working with content
received from untrusted sources …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever since I started using SELinux, I'm getting more and more fond of
what it can do for (security) administrators. Lately, I've started
confining user applications (like &lt;strong&gt;skype&lt;/strong&gt;) in the idea that I do not
want any application connecting to the Internet or working with content
received from untrusted sources to work inside the main user domain
(&lt;code&gt;user_t&lt;/code&gt; or &lt;code&gt;staff_t&lt;/code&gt; in my case). This particular exercise has been
quite interesting, not only to learn more on SELinux, not only to get
acquainted with the reference policy which Gentoo basis its policies
upon. No, it's been interesting because you learn how applications work
underneith...&lt;/p&gt;
&lt;p&gt;Take the skype application for example. Little did I know it read stuff
from my firefox configuration (like the &lt;code&gt;sec8.db&lt;/code&gt; and &lt;code&gt;prefs.js&lt;/code&gt; file),
most likely to see if the skype firefox plugin is installed. With
SELinux, I saw that it did all that - and also denied it. But it isn't
easy to find out why an application behaves as it does. After all, these
aren't questions that average joe asks. It also isn't easy to deduce if
you want to allow it or not. If it was purely for my own system, I
wouldn't hesitate for long, but the idea is that the modules should work
for the majority of people - and who knows, perhaps even be included in
the reference policy in the future.&lt;/p&gt;
&lt;p&gt;Perhaps Gentoo Hardened can write up some rules on the SELinux policies
and how they should be made for the distribution. Do we want to deny as
much as possible, only allowing those things developers can safely
verify need to be allowed? Or do we want to allow everything that the
application already does (but nothing more) so that no AVC denials are
shown anymore? And if Gentoo Hardened chooses "deny as much as
possible", do we configure the policy to not audit those things we don't
think we need (hiding it) or do we expect the security administrator to
manage his own &lt;em&gt;dontaudit&lt;/em&gt; rules? Well, guess I'll ask the hardened
folks and see what they think ;-)&lt;/p&gt;
&lt;p&gt;During the quest, I'll try to update the &lt;a href="http://git.overlays.gentoo.org/gitweb/?p=proj/hardened-docs.git;a=blob_plain;f=pdf/selinux-handbook.pdf;hb=HEAD"&gt;Gentoo Hardened SELinux
handbook
draft&lt;/a&gt;.
It's far from finished, but should be usable for most interested
parties. If you're interested in SELinux and want to give it a try with
Gentoo Hardened, this might be the document you are looking for.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Why I have backups</title><link href="https://blog.siphos.be/2010/12/why-i-have-backups/" rel="alternate"></link><published>2010-12-30T20:06:00+01:00</published><updated>2010-12-30T20:06:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-12-30:/2010/12/why-i-have-backups/</id><summary type="html">&lt;p&gt;You often read stories about people who have data loss and did not keep
any (recent) backups, and are now fully equipped with a state-of-the-art
backup mechanism. So no - no such failure story here but an example why
backups are important.&lt;/p&gt;
&lt;p&gt;Yesterday I had a vicious RAID/LVM failure. Due …&lt;/p&gt;</summary><content type="html">&lt;p&gt;You often read stories about people who have data loss and did not keep
any (recent) backups, and are now fully equipped with a state-of-the-art
backup mechanism. So no - no such failure story here but an example why
backups are important.&lt;/p&gt;
&lt;p&gt;Yesterday I had a vicious RAID/LVM failure. Due to my expeditions in the
world of SELinux, for some odd reason when I booted with SELinux
enforcing on, my RAID-1 where an LVM volume group (with &lt;code&gt;/usr&lt;/code&gt;, &lt;code&gt;/var&lt;/code&gt;,
&lt;code&gt;/opt&lt;/code&gt; and &lt;code&gt;/home&lt;/code&gt;) was hosted (coincidentally the only RAID-1 with 1.2
metadata version, the others are at 0.90) suddenly decided to split
itself into two (!) degraded RAID-1 systems: &lt;code&gt;/dev/md126&lt;/code&gt; and
&lt;code&gt;/dev/md127&lt;/code&gt;. During detection, LVM found two devices (the two RAID
metadevices) but with the same meta data on it (same physical volume
signature), so randomly picked one as its physical volume.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Found duplicate PV Lgrl5nNfenRUg9bIwM20q1hfMrWylyyL: using /dev/md126 not /dev/md127&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Result: after a few reboots (no, I didn't notice it at first - why would
I, everything seemed to work well so I didn't look at the dmesg output)
I started noticing that changes I made were suddenly gone (for instance,
ebuild updates that I made) which almost immediately triggers for me a
"&lt;em&gt;remount read-only, check logs and take emergency backup&lt;/em&gt;"-adrenaline
surge. And then I noticed that there were I/O errors in my logs,
together with the previously mentioned error message. So I quickly made
an emergency backup of my most critical file system locations (&lt;code&gt;/home&lt;/code&gt;
as well as &lt;code&gt;/etc&lt;/code&gt; and some files in &lt;code&gt;var&lt;/code&gt;) and then tried to fix the
problem (without having to reinstall everything).&lt;/p&gt;
&lt;p&gt;The first thing I did - and that might have been the trigger for real
pandemonium - was to try and found out which RAID (md126 or md127) is
being used. The &lt;strong&gt;vgdisplay&lt;/strong&gt; and other commands showed me that only
&lt;code&gt;md127&lt;/code&gt; was used at that time. Also, &lt;code&gt;/proc/mdstats&lt;/code&gt; showed that &lt;code&gt;md126&lt;/code&gt;
was in a &lt;em&gt;auto read-only&lt;/em&gt; state, meaning it wasn't touched since my
system booted. So I decided to drop &lt;code&gt;md126&lt;/code&gt; and add its underlying
partitions to the &lt;code&gt;md127&lt;/code&gt; RAID device. Once added, I would expect that
the degraded array would start syncing, but no: the moment the partition
was added, the RAID was shown to be fully operational.&lt;/p&gt;
&lt;p&gt;So I rebooted my system, only to find out it couldn't mount &lt;code&gt;md127&lt;/code&gt;.
File system checks, duplicate inodes, deleted blocks, the whole shebang.
Even running multiple &lt;strong&gt;fsck -y&lt;/strong&gt; commands didn't help. The volume group
was totally corrupted and my system almost totally gone. At that time,
it was around 1am and knowing I wouldn't be able to sleep before my
system is back operational - and knowing that I cannot sleep long as my
daughter will wake up at her usual hour - I decided to remove the array,
recreate it and pull back my original backup (not the one I just took as
it might already have corrupted files). As I take daily backups (they
are made at 6 o'clock or during first boot, whatever comes first) I
quickly had most of my &lt;code&gt;/home&lt;/code&gt; recovered (the backup doesn't take
caches, git/svn/cvs snapshots etc. into account). A quick delta check
between the newly restored &lt;code&gt;/home&lt;/code&gt; and the backup I took yielded a few
files which I have changed since, so those were recovered as well. But
it also showed lost changes, lost files and just corrupted files so I'm
glad I have my original backups.&lt;/p&gt;
&lt;p&gt;I don't take backups of my &lt;code&gt;/usr&lt;/code&gt; as it is only a system-rebuild away.
As &lt;code&gt;/etc&lt;/code&gt; wasn't corrupted, I recovered my &lt;code&gt;/var&lt;/code&gt;, threw in a Gentoo
stage snapshot (but not the full tarball as that would overwrite clean
files) and ran a &lt;strong&gt;emerge -pe world --keep-going&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When I woke up, my system was almost fully recovered with only a few
failed installs - which were identified and fixed in the next hour.&lt;/p&gt;
&lt;p&gt;Knowing that my backup system is rudimentary (an &lt;strong&gt;rsync&lt;/strong&gt; command which
uses hardlinks for incremental updates towards a second system plus a
secure file upload to a remote system for really important files) I was
quite happy to have only lost a few changes which I neglected to
commit/push. So, what did I learn?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep taking backups (and perhaps start using binpkg for fast
    recovery),&lt;/li&gt;
&lt;li&gt;Use 0.90 raid metadata version,&lt;/li&gt;
&lt;li&gt;Commit often, and&lt;/li&gt;
&lt;li&gt;Install a log checking tool that warns me the moment something weird
    might be occurring&lt;/li&gt;
&lt;/ul&gt;</content><category term="Gentoo"></category></entry><entry><title>cvechecker 2.0 released</title><link href="https://blog.siphos.be/2010/12/cvechecker-2-0-released/" rel="alternate"></link><published>2010-12-01T22:29:00+01:00</published><updated>2010-12-01T22:29:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-12-01:/2010/12/cvechecker-2-0-released/</id><summary type="html">&lt;p&gt;Okay, enough play - time for a new release. Since &lt;strong&gt;cvechecker 1.0&lt;/strong&gt; was
released, a few important changes have been made to the &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
tools&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can now tell cvechecker to only check newly added files, or
    remove a set of files from its internal database. Previously, you
    had to …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Okay, enough play - time for a new release. Since &lt;strong&gt;cvechecker 1.0&lt;/strong&gt; was
released, a few important changes have been made to the &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
tools&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can now tell cvechecker to only check newly added files, or
    remove a set of files from its internal database. Previously, you
    had to have cvechecker scan the entire system again.&lt;/li&gt;
&lt;li&gt;cvechecker can now also report if vulnerabilities have been found in
    software versions that are higher than the version you currently
    have installed. This can help you find seriously outdated software,
    but also help you identify possible vulnerabilities if the CVE
    itself doesn't contain all vulnerable versions, just the "latest"
    vulnerable version.&lt;/li&gt;
&lt;li&gt;The toolset now contains a command called &lt;strong&gt;cverules&lt;/strong&gt; which, on a
    Gentoo system, will attempt to generate version matching rules for
    software that is currently not detected by cvechecker yet. Very
    useful as I myself cannot install every possible software on my
    system to enhance the version matching rules. If you want to help
    out, run the &lt;strong&gt;cverules&lt;/strong&gt; command and send me the output.&lt;/li&gt;
&lt;li&gt;Some needed performance enhancements have been added as well&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One thing I wanted to include as well was a tool that validates
&lt;strong&gt;cvechecker&lt;/strong&gt; output against the distribution security information.
Some distributions patch software (to fix a vulnerability) rather than
ask the user to upgrade to a non-vulnerable software. The cvechecker
tools often cannot differentiate between the vulnerable and
non-vulnerable binaries (as they both mention the same version), but
often you can check against some meta data files of the distribution if
and which CVEs have been resolved in which versions of a distribution
package.&lt;/p&gt;
&lt;p&gt;The cvechecker tarball contains a script (see the &lt;code&gt;scripts/&lt;/code&gt; folder for
&lt;strong&gt;cvepkgcheck_gentoo&lt;/strong&gt;) for Gentoo that tries to get this information
from the GLSAs, but it is far from ready. I should try setting up a KVM
instance with an "old" Gentoo installation just to validate if the
command works, but even if it does, I'm not happy with how it is
written. Seems to me a lot of trouble, and if it cannot be done simply,
I'm afraid I'm doing it wrong ;-)&lt;/p&gt;
&lt;p&gt;Anyhow, I hope you enjoy version 2.0 of cvechecker.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Helping with version detection rules in cvechecker</title><link href="https://blog.siphos.be/2010/11/helping-with-version-detection-rules-in-cvechecker/" rel="alternate"></link><published>2010-11-27T17:59:00+01:00</published><updated>2010-11-27T17:59:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-11-27:/2010/11/helping-with-version-detection-rules-in-cvechecker/</id><summary type="html">&lt;p&gt;The new development snapshot, available from the &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker project
site&lt;/a&gt;, contains a helper script that
returns potential version detection rules for your system if the current
cvechecker database doesn't detect your software. The script is
currently available for Gentoo (called &lt;strong&gt;cverules_gentoo&lt;/strong&gt;) but other
distributions can be easily added. The actual …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The new development snapshot, available from the &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker project
site&lt;/a&gt;, contains a helper script that
returns potential version detection rules for your system if the current
cvechecker database doesn't detect your software. The script is
currently available for Gentoo (called &lt;strong&gt;cverules_gentoo&lt;/strong&gt;) but other
distributions can be easily added. The actual logic for detection is
distribution-agnostic (the script &lt;strong&gt;cvegenversdat&lt;/strong&gt;) so it shouldn't be
too much of a problem for other distributions to be supported as well.&lt;/p&gt;
&lt;p&gt;Note that the script isn't very fast (it's not intended to be) nor has a
very high accuracy rate. After all, it does use generic regular
expressions to try. The idea is that deployments on systems that have
software I don't have on my system can help me with the development of
the version detection rules by sending me the output of the helper
script.&lt;/p&gt;
&lt;p&gt;Next up: tool to auto-generate (part of) the acknowledgements file for
reporting purposes - getting information from distribution-specific
information. Once that is in, I'll tag it version 2.0 of cvechecker.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Delta processing in cvechecker</title><link href="https://blog.siphos.be/2010/11/delta-processing-in-cvechecker/" rel="alternate"></link><published>2010-11-02T00:30:00+01:00</published><updated>2010-11-02T00:30:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-11-02:/2010/11/delta-processing-in-cvechecker/</id><summary type="html">&lt;p&gt;The &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; application will
support delta file processing as well as higher version matching with
its next release. The functionality is currently in version control and
I still have to work out quite a few things before they can go live, but
the functionality is there.&lt;/p&gt;
&lt;p&gt;Now why would these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; application will
support delta file processing as well as higher version matching with
its next release. The functionality is currently in version control and
I still have to work out quite a few things before they can go live, but
the functionality is there.&lt;/p&gt;
&lt;p&gt;Now why would these functions be interesting?&lt;/p&gt;
&lt;p&gt;Well, first of all, by supporting &lt;strong&gt;delta file processing&lt;/strong&gt; I am able to
use &lt;strong&gt;cvechecker&lt;/strong&gt; with Portage' hooks. Every time a package is
unmerged, &lt;strong&gt;cvechecker&lt;/strong&gt; will remove the files from its database (so
that it doesn't get picked up in vulnerability reports anymore).
Similarly, every time a package is emerged, the files are stored in the
database. There is no need to perform a full system scan every time the
system has been updated.&lt;/p&gt;
&lt;p&gt;Second, being able to report on &lt;strong&gt;higher version vulnerabilities&lt;/strong&gt; the
tool can now also trap potential issues with reports that do not contain
the exact version as detected by &lt;strong&gt;cvechecker&lt;/strong&gt; but &lt;em&gt;can&lt;/em&gt; be relevant.
For instance, a version detection of &lt;code&gt;Linux 2.6.35-hardened-r1&lt;/code&gt; might
otherwise not be noticed (for instance because no CVE is reported on the
hardened-r1 release) yet a CVE report on &lt;code&gt;2.6.35&lt;/code&gt; or even &lt;code&gt;2.6.36-rc4&lt;/code&gt;
might be of interest. By using the higher version reporting, you'll be
notified of this as well. Same goes for vulnerability reports on an
entire branch (say &lt;code&gt;Python 2.4&lt;/code&gt;), especially when those branches are not
actively being developed anymore (so the vulnerability remains). And
another benefit is that you might be informed about higher versions of
particular software being available ;-)&lt;/p&gt;
&lt;p&gt;Now, a very quick warning before everybody cheers and does the penguin
dance: enabling higher version reports will give you &lt;em&gt;lots&lt;/em&gt; of false
hits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First of all, detecting if a version is higher than another version
    isn't easy. The tool is able to put &lt;code&gt;0.9.8 - 0.9.8a - 0.9.8b&lt;/code&gt; in the
    right order, as well as &lt;code&gt;0.5.1_alpha - 0.5.1_beta - 0.5.1&lt;/code&gt;, but the
    same algorithm will make &lt;code&gt;2.6.35-hardened-r1&lt;/code&gt; be less than &lt;code&gt;2.6.35&lt;/code&gt;,
    and a secure &lt;code&gt;0.9.8&lt;/code&gt; version will be seen vulnerable when
    &lt;code&gt;1.0.0_alpha&lt;/code&gt; has a vulnerability.&lt;/li&gt;
&lt;li&gt;Second of all, official CVE entries don't always provide a good
    version match themselves. For instance,
    &lt;a href="http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2008-4609"&gt;CVE-2008-4609&lt;/a&gt;
    has been configured that &lt;code&gt;Linux Kernel 390&lt;/code&gt; and &lt;code&gt;Linux Kernel 3.25&lt;/code&gt;
    (I know those are not correct version numbers - my point exactly)
    are vulnerable. So yes, &lt;code&gt;390&lt;/code&gt; is (a lot) higher than &lt;code&gt;2.6.35&lt;/code&gt;...&lt;/li&gt;
&lt;li&gt;Third, many tools use parallel development branches. Take Python for
    instance: even when version 2.6.5 would have no vulnerabilities and
    2.7 or 3.2 alpha releases do, it will still report the 2.6.5 one as
    having a potential vulnerability. This seems to give (for me
    at least) the most false positives of all.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I still don't know how to deal with this huge amount of false positives
- comments are always welcome.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>SELinux enforcing for console activity</title><link href="https://blog.siphos.be/2010/10/selinux-enforcing-for-console-activity/" rel="alternate"></link><published>2010-10-30T21:30:00+02:00</published><updated>2010-10-30T21:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-10-30:/2010/10/selinux-enforcing-for-console-activity/</id><summary type="html">&lt;p&gt;I'm now able to boot into my system with SELinux in enforcing mode
(without unconfined domains), do standard system administration tasks as
root / sysadm_r (including the relevant Portage activities) and work as
a regular user as long as I don't want to run in Xorg. I'm not going to
focus …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm now able to boot into my system with SELinux in enforcing mode
(without unconfined domains), do standard system administration tasks as
root / sysadm_r (including the relevant Portage activities) and work as
a regular user as long as I don't want to run in Xorg. I'm not going to
focus on Xorg pretty soon now as there is a bunch of other things to do
(like other applications, writing policies, patching etc.), but here is
a very quick summary on the activities I had to do (apart from those in
the &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/selinux-handbook.xml"&gt;Gentoo Hardened SELinux
Handbook&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a more recent reference policy to start from. I fiddled with a
    live ebuild first, but am now falling back to the latest reference
    policy release of &lt;a href="http://oss.tresys.com"&gt;Tresys&lt;/a&gt;, versioned
    &lt;code&gt;2.20100524&lt;/code&gt;. The implementing package
    (&lt;code&gt;sec-policy/selinux-base-policy&lt;/code&gt;) can be found in my
    &lt;a href="http://github.com/sjvermeu/gentoo.overlay"&gt;overlay (sjvermeu)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I use a meta package &lt;code&gt;sec-policy/selinux-policy&lt;/code&gt; which pulls in the
    base policy as well as policies that you definitely need, but seem
    to work well when loaded as a module. Currently, that only includes
    &lt;code&gt;sec-policy/selinux-portage&lt;/code&gt; but others may follow later. The main
    reason is that I like the modular approach and this way, I can
    update/patch these modules without requiring a base rebuild/reload&lt;/li&gt;
&lt;li&gt;Speaking of patching, the &lt;code&gt;sec-policy/selinux-portage&lt;/code&gt; ebuild
    contains a patch for those who have &lt;code&gt;/tmp&lt;/code&gt; and/or &lt;code&gt;/var/tmp&lt;/code&gt; as a
    tmpfs filesystem&lt;/li&gt;
&lt;li&gt;I had to update &lt;code&gt;/lib64/rcscripts/addons/lvm-start.sh&lt;/code&gt; so that the
    lvm locks are placed in &lt;code&gt;/etc/lvm/lock&lt;/code&gt; rather than &lt;code&gt;/dev/.lvm&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;I also had to update &lt;code&gt;/lib/dhcpcd/dhcpcd-hooks/50-dhcpcd-compat&lt;/code&gt; to
    put the &lt;code&gt;*.info&lt;/code&gt; files in &lt;code&gt;/var/lib/dhcpcd&lt;/code&gt; rather than &lt;code&gt;/var/lib&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Many binaries in /bin (part of &lt;code&gt;sys-apps/net-tools&lt;/code&gt;) are hard links
    (same inode) but different name. This gives issues with SELinux'
    file contexts. Quick fix is to copy rather than hardlink (for
    instance, &lt;strong&gt;cp hostname hostname.old&lt;/strong&gt;). After this, I ran &lt;strong&gt;rlpkg
    net-tools&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Many packages need to be unmasked (from &lt;code&gt;~amd64&lt;/code&gt;) as the current
    stable packages either don't work or are too old. The "unstable"
    ones seem to work pretty well though.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I know much development is being put in the SELinux state of Gentoo
Hardened (just visit #gentoo-hardened if you have questions) so I'm
sure things will be improving soon.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Risk identification</title><link href="https://blog.siphos.be/2010/10/risk-identification/" rel="alternate"></link><published>2010-10-14T20:18:00+02:00</published><updated>2010-10-14T20:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-10-14:/2010/10/risk-identification/</id><summary type="html">&lt;p&gt;Risk identification is a difficult subject. Analysts need it to defend
mitigation strategies or to suggest investments. Yet risk identification
is often a subjective method, especially in the IT industry. How do you
give a number on a certain risk? When do you believe that that number
exceeds a threshold …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Risk identification is a difficult subject. Analysts need it to defend
mitigation strategies or to suggest investments. Yet risk identification
is often a subjective method, especially in the IT industry. How do you
give a number on a certain risk? When do you believe that that number
exceeds a threshold? Because of the ambiguous definition of risks, it is
often overlooked or substituted with impact analysis.&lt;/p&gt;
&lt;p&gt;Now impact analysis is not much better, but it is easier to comprehend.
Impact analysis describes what happens when something has occurred. It
doesn't state how often it can occur, or what the chances are of the
event to occur, but gives an estimation on how big the impact would be
when it occurs. Most of the time one describes an impact with financial
loss. The highest impact a company has is that its survival is
threatened. If you're a simple shop and you don't take an insurance on
your stock, the impact of a fire or explosion in your shop might be that
you're put out of business. If you do have insurance, your impact is
more limited.&lt;/p&gt;
&lt;p&gt;In the majority of cases, impact analysis is more straightforward.&lt;/p&gt;
&lt;p&gt;IT risks are a prime example of difficult exercises. Quantifying the IT
risks that a company is taking is difficult. In this post, I'm
introducing a more straightforward method - even if it isn't fail-safe,
it might still give some interesting sights on the matter. It is of
course not something I invented myself (there's enough information on
the internet about risk analysis or risk identification), merely a
combination of several methods and ideas which I find useful (and
decided to write up about).&lt;/p&gt;
&lt;p&gt;The method identifies a risk within four levels: low, medium, high and
very high. To get inside a level, it uses two metrics: &lt;em&gt;impact analysis&lt;/em&gt;
which we've discussed before, and &lt;em&gt;chance of occurrence&lt;/em&gt;. The latter is
the most difficult to identify, but let's first show how to map the two
onto the risk level:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;chance of occurrence
   ^
 4 +  M  M  VH VH
 3 +  M  M  VH VH
 2 +  L  L  H  H
 1 +  L  L  H  H
   x--+--+--+--+--&amp;gt; impact analysis
      1  2  3  4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, I give each axis four values, going from low (low
impact, or low probability) onto high. The resulting point lays within
one of four quadrants - Low, Medium, High or Very High. I identify an
event as having a high(er) risk when its probability is low but impact
high, whereas an event that has a high probability but low impact is
considered to be less of a risk: when something with a somewhat low(er)
impact occurs, you should still have some breathing space to make sure
it won't happen again (or find a way to reduce the impact) whereas an
occurrence of something with a high(er) impact will most likely leave
you hurt, fragile and licking your wounds.&lt;/p&gt;
&lt;p&gt;So, how to measure the chance of occurrence of an event? Well, let's do
this in two stages: do an initial assessment, and then elevate the
chance using particular checks. Within IT, an often described threat is
the threat of someone trying to achieve personal gain (financial or
public) from the system(s). Note that, for every possible threat, one
will need to make a risk identification - you can't just say that a
system has a risk. It is always a particular threat which is assigned a
risk. So how large is the chance of such a "hack" attempt occurring?&lt;/p&gt;
&lt;p&gt;First, how "wide" is the access vector towards your system? Can the
entire world (read: Internet) access the system (level = 4), is it
everyone in the company (level = 3), everyone within an affiliated
department (in most cases it means "IT department", level = 2) or
limited to a very small set of people (level = 1)?&lt;/p&gt;
&lt;p&gt;Second, if one of the people identified earlier would want to perform
malicious activity with eye on personal gain (financial or PR), would he
succeed by his own (level + 1) or would he need at least one accomplice?&lt;/p&gt;
&lt;p&gt;Third, if the activity was performed, would it be traceable to the
person or not (if not: level + 1)?&lt;/p&gt;
&lt;p&gt;As an example (purely hypothetical): read access to the access logs of a
web application server which contains HTTP session information (logging
of SESSIONID) as well as username (authentication) and origin (IP
address) as well as other information. The threat: using this
information to hijack an active session.&lt;/p&gt;
&lt;p&gt;First, if a hijack would be successful, the impact would be considered a
level-3 (with 1 being low and 4 being very high): the company might
suffer huge financial losses or PR would be a difficult beast to tame
(because the application is an internal stock application with features
to perform financial operations). But how high is the chance of
occurrence?&lt;/p&gt;
&lt;p&gt;Well, say that the log files themselves can only be read by IT staff
(level = 2), but that someone of the IT staff cannot hijack the session
easily with this information alone as he would either require firewall
changes (for instance because the application can only be reached
through trusted middleware components) or have access to the machine the
user is working on, and such changes or access require more than a
single person in the situation. Also, if he did, audit trails would lead
the changes (firewall changes or machine access) to the person. As a
result, the chance of this event occurring given the circumstances is
considered a level 2. At the quadrant, this would yield a level of
"High". The risk of occurrence is relatively low, but the impact is too
high to ever consider this a "Medium" or "Low".&lt;/p&gt;
&lt;p&gt;Now if we were to reduce the risk, we could focus on lowering the chance
of occurrence (only few people access to the given information - say
keep SESSIONID information in a separate, inaccessible logfile only to
be used for general, automated metric collection - after all, if there's
no point in keeping track of SESSIONID's, they wouldn't be logged
anyhow). This is a lot easier to accomplish than to try and lower the
impact analysis. On the other hand, if the impact analysis could be
lowered (say by requesting a stronger authentication method for
validating particular steps within the application, such as approvals) a
session hijack would give less impact - say level 2 or even 1. In that
case, the current risk would be lowered from "High" to "Low".&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker 1.0 released</title><link href="https://blog.siphos.be/2010/10/cvechecker-1-0-released/" rel="alternate"></link><published>2010-10-01T21:34:00+02:00</published><updated>2010-10-01T21:34:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-10-01:/2010/10/cvechecker-1-0-released/</id><summary type="html">&lt;p&gt;With only a few small bugfixes between this release and the previous
one, &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker 1.0&lt;/a&gt; has finally
been released. It runs fine on my few systems and I have not gotten any
bugreports from other users anymore. It can definitely need more rules
to identify installed software (those rules …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With only a few small bugfixes between this release and the previous
one, &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker 1.0&lt;/a&gt; has finally
been released. It runs fine on my few systems and I have not gotten any
bugreports from other users anymore. It can definitely need more rules
to identify installed software (those rules are released separately)
which is what I will focus on the few upcoming weeks. Once that has been
accomplished, I will start with the alpha releases for the 2.0 series
using the &lt;a href="http://cvechecker.sourceforge.net/docs/featurerequests.html"&gt;feature
requests&lt;/a&gt;
as a guideline.&lt;/p&gt;
&lt;p&gt;I plan on maintaining the earlier versions of cvechecker for two
consecutive releases - one including small functional enhancements (as
long as they can be applied on the development release and the stable
release easily) and one just for security and bug fixes.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>SELinux quicky</title><link href="https://blog.siphos.be/2010/09/selinux-quicky/" rel="alternate"></link><published>2010-09-14T23:44:00+02:00</published><updated>2010-09-14T23:44:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-14:/2010/09/selinux-quicky/</id><summary type="html">&lt;p&gt;I've been using SELinux for a few days now (in permissive mode, just to
get to know things) and have learned a few interesting commands (or
other nice-to-know's) for using SELinux. Since I'm going to forget those
the moment all is running well, I'll "document" them here ;-) I'm not
going …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been using SELinux for a few days now (in permissive mode, just to
get to know things) and have learned a few interesting commands (or
other nice-to-know's) for using SELinux. Since I'm going to forget those
the moment all is running well, I'll "document" them here ;-) I'm not
going to talk about the &lt;strong&gt;-Z&lt;/strong&gt; switches in &lt;strong&gt;ps&lt;/strong&gt; or &lt;strong&gt;ls&lt;/strong&gt;, that has
been documented sufficiently on the Internet already.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;sesearch&lt;/strong&gt; you can query through the loaded policy. For instance,
you want to know why you can execute &lt;strong&gt;sudo&lt;/strong&gt; as a user (and not just
due to the DAC permissions):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sesearch -s user_t -t sudo_exec_t -p execute -c file -A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, this is only one of the three requirements for a transition
from &lt;code&gt;user_t&lt;/code&gt; to &lt;code&gt;user_sudo_t&lt;/code&gt;, for that you still need process
transition and entrypoint:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sesearch -s user_t -t user_sudo_t -p transition -A 
~$ sesearch -s user_sudo_t -t sudo_exec_t -p entrypoint -A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, sometimes you find a rule that you didn't expect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sesearch -s user_t -t dmesg_exec_t -p execute -A
Found 1 semantic av rules:
  allow user_t application_exec_type : file { ioctl read getattr lock execute execute_no_trans open } ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because &lt;code&gt;dmesg_exec_t&lt;/code&gt; has the &lt;code&gt;application_exec_type&lt;/code&gt; attribute
set. You can see the list of types that have an attribute set (or vice
versa) with &lt;strong&gt;seinfo&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(Show list of types that have the application_exec_type attribute)
~$ seinfo -aapplication_exec_type -x
(Show list of attributes given to the dmesg_exec_t type)
~$ seinfo -tdmesg_exec_t -x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, during my browsing through the SELinux activities on my system, I
noticed that I could run &lt;code&gt;/usr/sbin/dnsmasq&lt;/code&gt; as root, without generating
an error in the avc log. Yet &lt;strong&gt;sesearch&lt;/strong&gt; didn't give any results. I've
almost killed a few kittens by searching for possibilities (perhaps
types with &lt;code&gt;exec_type&lt;/code&gt; automatically have &lt;code&gt;application_exec_type&lt;/code&gt; - not,
or perhaps the domain transitions to another domain first without me
knowing - not, I would see that the process runs as a different domain
then, which wasn't the case). Luckily, dgrift on &lt;code&gt;#selinux&lt;/code&gt; gave me the
hint of checking the &lt;em&gt;dontaudit&lt;/em&gt; rules as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sesearch --dontaudit -s sysadm_t -t dnsmasq_exec_t
...
   dontaudit sysadm_t exec_type : file { execute execute_no_trans } ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So there we had it - it was being denied, just not logged. And because I
ran in permissive mode, it gets executed anyhow. I disabled the
dontaudit rules and got the avc denial I was expecting:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(Disable dontaudit rules)
~$ semodule -DB
(Reenable dontaudit rules)
~$ semodule -B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category></entry><entry><title>Switching to hardened</title><link href="https://blog.siphos.be/2010/09/switching-to-hardened/" rel="alternate"></link><published>2010-09-12T13:41:00+02:00</published><updated>2010-09-12T13:41:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-12:/2010/09/switching-to-hardened/</id><summary type="html">&lt;p&gt;Yesterday (and this night) I successfully converted my system to a
&lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; system. In my case, this
currently means that
&lt;a href="http://www.gentoo.org/proj/en/hardened/pax-quickstart.xml"&gt;PaX&lt;/a&gt; has
been enabled and I am currently running the system (which is an x86_64
laptop) with &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/"&gt;SELinux&lt;/a&gt;
in permissive mode (so it won't enforce the policies yet, but …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday (and this night) I successfully converted my system to a
&lt;a href="http://hardened.gentoo.org"&gt;Gentoo Hardened&lt;/a&gt; system. In my case, this
currently means that
&lt;a href="http://www.gentoo.org/proj/en/hardened/pax-quickstart.xml"&gt;PaX&lt;/a&gt; has
been enabled and I am currently running the system (which is an x86_64
laptop) with &lt;a href="http://www.gentoo.org/proj/en/hardened/selinux/"&gt;SELinux&lt;/a&gt;
in permissive mode (so it won't enforce the policies yet, but report
violations so I can see in my logs if enforcement is possible or not).
The permissive mode will be on for quite some time I would assume, as
getting SELinux active on the system involved quite a few \~amd64
packages and I'm not too fond of using that branch (I'm more of a
stability guy).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>prezi presentations</title><link href="https://blog.siphos.be/2010/09/prezi-presentations/" rel="alternate"></link><published>2010-09-10T10:40:00+02:00</published><updated>2010-09-10T10:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-10:/2010/09/prezi-presentations/</id><summary type="html">&lt;p&gt;While doing some research on current rich internet applications / web
application platforms, I discovered an online presentation site/tool
called &lt;a href="http://www.prezi.com"&gt;Prezi&lt;/a&gt;. This online application allows you
to make dynamic presentations differently from the standard presentation
software like &lt;a href="http://www.openoffice.org/product/impress.html"&gt;OpenOffice.org's
Impress&lt;/a&gt;. A nice example
can be found
&lt;a href="http://prezi.com/hgjm18z36h75/why-should-you-move-beyond-slides/"&gt;online&lt;/a&gt;
as well of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While doing some research on current rich internet applications / web
application platforms, I discovered an online presentation site/tool
called &lt;a href="http://www.prezi.com"&gt;Prezi&lt;/a&gt;. This online application allows you
to make dynamic presentations differently from the standard presentation
software like &lt;a href="http://www.openoffice.org/product/impress.html"&gt;OpenOffice.org's
Impress&lt;/a&gt;. A nice example
can be found
&lt;a href="http://prezi.com/hgjm18z36h75/why-should-you-move-beyond-slides/"&gt;online&lt;/a&gt;
as well of course. I think I'm going to try this out in the near future
;-)&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>cvechecker 0.6 released</title><link href="https://blog.siphos.be/2010/09/cvechecker-0-6-released/" rel="alternate"></link><published>2010-09-08T21:41:00+02:00</published><updated>2010-09-08T21:41:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-08:/2010/09/cvechecker-0-6-released/</id><summary type="html">&lt;p&gt;This release makes me quite happy, because it resolves one major PITA I
had (performance), but you know how things go. If it works fine for the
developer, it's probably an abomination for the rest of the world.
Anyhow, &lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt; version 0.6 is now
available. It improves reporting performance …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This release makes me quite happy, because it resolves one major PITA I
had (performance), but you know how things go. If it works fine for the
developer, it's probably an abomination for the rest of the world.
Anyhow, &lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt; version 0.6 is now
available. It improves reporting performance tremendously if your sqlite
library is sufficiently up-to-date, now supports reporting on found
software (regardless if it matches a CVE entry or not) and adds quite a
few bug fixes along the way.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Linux Sea last content chapter</title><link href="https://blog.siphos.be/2010/09/linux-sea-last-content-chapter/" rel="alternate"></link><published>2010-09-04T22:42:00+02:00</published><updated>2010-09-04T22:42:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-04:/2010/09/linux-sea-last-content-chapter/</id><summary type="html">&lt;p&gt;The last chapter in &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;
focuses on &lt;a href="http://swift.siphos.be/linux_sea/ch19.html"&gt;Using A Shell&lt;/a&gt;.
This seems to me like a nice last chapter, as it confronts the user with
the exciting world of shell scripts. I hope that the chapters in the
book are sufficiently stuffed so that beginners (who are not afraid …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The last chapter in &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;
focuses on &lt;a href="http://swift.siphos.be/linux_sea/ch19.html"&gt;Using A Shell&lt;/a&gt;.
This seems to me like a nice last chapter, as it confronts the user with
the exciting world of shell scripts. I hope that the chapters in the
book are sufficiently stuffed so that beginners (who are not afraid to
learn) can more easily start off with (Gentoo) Linux. Of course, this is
just the beginning. The existing content needs to be sharpened, extended
where needed, updated, etc. so expect a few updates coming along!&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>devops - how hard can it/it can be</title><link href="https://blog.siphos.be/2010/09/devops-how-hard-can-itit-can-be/" rel="alternate"></link><published>2010-09-04T09:17:00+02:00</published><updated>2010-09-04T09:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-04:/2010/09/devops-how-hard-can-itit-can-be/</id><summary type="html">&lt;p&gt;Dieter made a good reference to &lt;a href="http://dieter.plaetinck.be/what_the_open_source_community_can_learn_from_devops"&gt;devops and the open source
community&lt;/a&gt;
and (correctly) points out that, even in a more collaborative scene such
as the free software communities', there is still distinction between
development and operations. And it isn't hard to see commonalities
between enterprise organizations and free software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Dieter made a good reference to &lt;a href="http://dieter.plaetinck.be/what_the_open_source_community_can_learn_from_devops"&gt;devops and the open source
community&lt;/a&gt;
and (correctly) points out that, even in a more collaborative scene such
as the free software communities', there is still distinction between
development and operations. And it isn't hard to see commonalities
between enterprise organizations and free software communities in that
respect.&lt;/p&gt;
&lt;p&gt;But is the comparison correct? If you look at a distribution as an
enterprise, then surely the distinction between upstream (project
development) and "downstream" (distribution) should be compared with the
relations between an enterprise and its ISVs, not its internal
development / operational divisions. If we look at internal divisions,
then distributions tend to provide better integration between (internal)
projects and the distribution. I cannot talk for every distribution, but
in those I do know, the infrastructure team ("operations") has a firm
grip on the infrastructure, yet leaves out sufficient space for
development to do their releases/production activity: uploading files,
changing documentation, ...&lt;/p&gt;
&lt;p&gt;This works, if the provided interface does not allow for developers to
harm the principles that infrastructure has. This is what many
(enterprise) organizations are still lacking, but there is no simple
solution for this. Often, the operations team has principles that are
difficult to match with the goals of development. Finding the correct
balance between development and operations in that respect is quite a
challenge - usually, free software communities can get there faster,
often because their mass is sufficiently low. With a total 'employee'
count of a few hundreds it is statistically easier to find a balance
than within enterprises of a few thousand employees.&lt;/p&gt;
&lt;p&gt;I believe that both teams should write down their principles, policies
and standards, and see if they can find matches (which is good) and
mutually exclusive distinctions (which is challenging) where more
investigation can be done. Both teams should be allowed to question
decisions made by the other (but without pretending to know better) and
make suggestions. This should lead to the emergence of interfaces where
a team has sufficient freedom to get to their own goals autonomously.&lt;/p&gt;
&lt;p&gt;With such interfaces, people will start thinking that devops is growing
apart (after all, they're starting to work autonomously and
independently of each other). That isn't true. In my opinion, devops is
about interacting on a high level (which is less time-delimited) so that
interactions on a low level (which is very time-limited and focused on
releasing, releasing, releasing) aren't necessary. Less interaction
means that the teams that are responsible for getting to a specific,
short time-framed goal, can cooperate closely and have a better grip on
resources and requirements.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Linux Sea: log file management and backups</title><link href="https://blog.siphos.be/2010/09/linux-sea-log-file-management-and-backups/" rel="alternate"></link><published>2010-09-02T14:31:00+02:00</published><updated>2010-09-02T14:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-02:/2010/09/linux-sea-log-file-management-and-backups/</id><summary type="html">&lt;p&gt;I've added two more chapters to the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; book. The first one is about &lt;a href="http://swift.siphos.be/linux_sea/ch17.html"&gt;Log
file management&lt;/a&gt;, the second
one about &lt;a href="http://swift.siphos.be/linux_sea/ch18.html"&gt;Taking Backups&lt;/a&gt;.
They're far from finished, but I thought that those two topics are
important for day-to-day Gentoo usage and shouldn't be left out of the
Linux Sea …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've added two more chapters to the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; book. The first one is about &lt;a href="http://swift.siphos.be/linux_sea/ch17.html"&gt;Log
file management&lt;/a&gt;, the second
one about &lt;a href="http://swift.siphos.be/linux_sea/ch18.html"&gt;Taking Backups&lt;/a&gt;.
They're far from finished, but I thought that those two topics are
important for day-to-day Gentoo usage and shouldn't be left out of the
Linux Sea saga.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>cvechecker 0.5 released</title><link href="https://blog.siphos.be/2010/09/cvechecker-0-5-released/" rel="alternate"></link><published>2010-09-02T00:57:00+02:00</published><updated>2010-09-02T00:57:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-09-02:/2010/09/cvechecker-0-5-released/</id><summary type="html">&lt;p&gt;A new intermediate release of
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; is now released. The
tool is reported to build properly on NetBSD and FreeBSD as well
(although much user experience there is still welcome), introduces a
&lt;strong&gt;cvereport&lt;/strong&gt; command (&lt;a href="http://cvechecker.sourceforge.net/example/report.html"&gt;example
output&lt;/a&gt;), has
lowered its initial dependency requirements and &lt;strong&gt;pullcves&lt;/strong&gt; now only
loads the CVE XML …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A new intermediate release of
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; is now released. The
tool is reported to build properly on NetBSD and FreeBSD as well
(although much user experience there is still welcome), introduces a
&lt;strong&gt;cvereport&lt;/strong&gt; command (&lt;a href="http://cvechecker.sourceforge.net/example/report.html"&gt;example
output&lt;/a&gt;), has
lowered its initial dependency requirements and &lt;strong&gt;pullcves&lt;/strong&gt; now only
loads the CVE XML changes in the database, rather than iterating across
all CVE XML entries.&lt;/p&gt;
&lt;p&gt;Many thanks to Nigel Horne for his continuous testing/hammering on the
tool.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>qemu monitor cd change</title><link href="https://blog.siphos.be/2010/08/qemu-monitor-cd-change/" rel="alternate"></link><published>2010-08-30T21:38:00+02:00</published><updated>2010-08-30T21:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-30:/2010/08/qemu-monitor-cd-change/</id><summary type="html">&lt;p&gt;I've been playing around with kvm (which uses qemu) to try out other
operating systems and Linux distributions. Up until now, little progress
on that part (not because it is difficult, just little time) but there
are a few things worth mentioning. For this post, let's start with a
quicky …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been playing around with kvm (which uses qemu) to try out other
operating systems and Linux distributions. Up until now, little progress
on that part (not because it is difficult, just little time) but there
are a few things worth mentioning. For this post, let's start with a
quicky on CD changes.&lt;/p&gt;
&lt;p&gt;qemu's integrated monitor is very nice and powerful. To go to the
monitor from inside the vnc session, press Ctrl+Alt+2 (to go back, use
Ctrl+Alt+1). Then you can query for attached hardware, add new devices,
remove others, cpu's, etc. Something I found necessary was to switch
CD/DVD images. With &lt;code&gt;info block&lt;/code&gt; I found the device. I then ran
&lt;code&gt;eject ide1-cd0&lt;/code&gt; followed by &lt;code&gt;change ide1-cd0 /path/to/new/image&lt;/code&gt; et
voila, new CD available.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Added "iw" support to Linux Sea</title><link href="https://blog.siphos.be/2010/08/added-iw-support-to-linux-sea/" rel="alternate"></link><published>2010-08-26T01:42:00+02:00</published><updated>2010-08-26T01:42:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-26:/2010/08/added-iw-support-to-linux-sea/</id><summary type="html">&lt;p&gt;The wireless driver developers are actively working on a &lt;a href="http://wireless.kernel.org/en/users/Documentation/iw"&gt;new wireless
toolset called
"iw"&lt;/a&gt;, slowly
deprecating the older wireless-tools toolset (which contains the
"iwconfig" command). Kasumi_Ninja reported to me in the &lt;a href="https://forums.gentoo.org/viewtopic-p-6310061.html#6310061"&gt;Gentoo
Forums&lt;/a&gt; that
it would be nice to add information on iw to &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;, so I did. I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The wireless driver developers are actively working on a &lt;a href="http://wireless.kernel.org/en/users/Documentation/iw"&gt;new wireless
toolset called
"iw"&lt;/a&gt;, slowly
deprecating the older wireless-tools toolset (which contains the
"iwconfig" command). Kasumi_Ninja reported to me in the &lt;a href="https://forums.gentoo.org/viewtopic-p-6310061.html#6310061"&gt;Gentoo
Forums&lt;/a&gt; that
it would be nice to add information on iw to &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;, so I did. I must admit though
that my systems don't (or hardly) support iw, so I had to be lead by
examples throughout the Internet.&lt;/p&gt;
&lt;p&gt;Apart from the iw addition, I've reorganized the sections on &lt;a href="http://swift.siphos.be/linux_sea/ch09.html"&gt;Software
Management&lt;/a&gt; as it was
becoming too crowded. I've also started a
&lt;a href="http://github.com/sjvermeu/Linux-Sea/raw/master/ChangeLog"&gt;ChangeLog&lt;/a&gt;
for those who want to track the changes to the document.&lt;/p&gt;
&lt;p&gt;Now if anyone can recommend me a good spell- and grammar checker...&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>cvechecker 0.4 released</title><link href="https://blog.siphos.be/2010/08/cvechecker-0-4-released/" rel="alternate"></link><published>2010-08-25T23:55:00+02:00</published><updated>2010-08-25T23:55:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-25:/2010/08/cvechecker-0-4-released/</id><summary type="html">&lt;p&gt;Albeit with less updates than 0.3 had, &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
0.4&lt;/a&gt; brings in internal project files
reorganization (more to the liking of the GNU autoconf/automake
standards - I think), fixes a databaseleak (instead of memoryleak ;-)
bug and introduces a teenie weenie bit more intelligent pullcves command
(with multiple return code …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Albeit with less updates than 0.3 had, &lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker
0.4&lt;/a&gt; brings in internal project files
reorganization (more to the liking of the GNU autoconf/automake
standards - I think), fixes a databaseleak (instead of memoryleak ;-)
bug and introduces a teenie weenie bit more intelligent pullcves command
(with multiple return code behavior to improve automation efforts) as
was mentioned in the feature request list. All documentation is also
updated and a pullcves manual page has been added.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>I remain impressed by the free software community</title><link href="https://blog.siphos.be/2010/08/i-remain-impressed-by-the-free-software-community/" rel="alternate"></link><published>2010-08-25T00:42:00+02:00</published><updated>2010-08-25T00:42:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-25:/2010/08/i-remain-impressed-by-the-free-software-community/</id><summary type="html">&lt;p&gt;My current personal projects, &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; and
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt;, are actively being
watched by the free software community. For the Linux Sea book, I get
nice feedback and ideas on the &lt;a href="https://forums.gentoo.org/viewtopic-t-812252.html"&gt;Gentoo
Forums&lt;/a&gt; and on the
cvechecker application, people such as Nigel Horne are helping out in
various ways - including &lt;a href="http://cvechecker.sourceforge.net/docs/featurerequests.html"&gt;feature …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;My current personal projects, &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; and
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt;, are actively being
watched by the free software community. For the Linux Sea book, I get
nice feedback and ideas on the &lt;a href="https://forums.gentoo.org/viewtopic-t-812252.html"&gt;Gentoo
Forums&lt;/a&gt; and on the
cvechecker application, people such as Nigel Horne are helping out in
various ways - including &lt;a href="http://cvechecker.sourceforge.net/docs/featurerequests.html"&gt;feature
requests&lt;/a&gt;
of all sorts.&lt;/p&gt;
&lt;p&gt;I must admit, I remain impressed.&lt;/p&gt;
&lt;p&gt;Small changes have already been squeezed in in the Linux Sea document. A
larger change (the use of the &lt;a href="http://wireless.kernel.org/en/users/Documentation/iw"&gt;iw
tools&lt;/a&gt; for
wireless connectivity) is being investigated (sadly, &lt;a href="http://blog.siphos.be/2010/08/new-laptop-time-to-play/"&gt;my broadcom-sta
device&lt;/a&gt; doesn't
support the new nl80211 API so the documentation change is slower to
integrate than expected). I'm also planning to make some updates on the
software management chapter as it is currently becoming quite crowded.&lt;/p&gt;
&lt;p&gt;In the cvetool, most
&lt;a href="http://cvechecker.svn.sourceforge.net/viewvc/cvechecker/ChangeLog?view=markup"&gt;changes&lt;/a&gt;
are bugfixes and output enhancements as expected. I'm not going to add
more functionality now - I first want to get a stable 1.0 release out
there. But first continue to squash bugs and add rules to the
versions.dat file so that it is usable on various systems (release 0.4
is around the corner).&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>cvechecker userguide</title><link href="https://blog.siphos.be/2010/08/cvechecker-userguide/" rel="alternate"></link><published>2010-08-22T17:37:00+02:00</published><updated>2010-08-22T17:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-22:/2010/08/cvechecker-userguide/</id><content type="html">&lt;p&gt;Just a quick note, I've created and uploaded the &lt;a href="http://cvechecker.sourceforge.net/documentation.html"&gt;cvechecker
userguide&lt;/a&gt;.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker 0.3 released</title><link href="https://blog.siphos.be/2010/08/cvechecker-0-3-released/" rel="alternate"></link><published>2010-08-20T22:15:00+02:00</published><updated>2010-08-20T22:15:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-20:/2010/08/cvechecker-0-3-released/</id><content type="html">&lt;p&gt;Time for a new intermediate
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt; release, so here it is.
Changes include (beyond the usual bugfixes) different CSV output (with
some sort of version support) so that it can be easily used for
reporting purposes, removal of debugging/verbose items and added example
files for reporting.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker 0.2 released</title><link href="https://blog.siphos.be/2010/08/cvechecker-0-2-released/" rel="alternate"></link><published>2010-08-16T21:35:00+02:00</published><updated>2010-08-16T21:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-16:/2010/08/cvechecker-0-2-released/</id><summary type="html">&lt;p&gt;I've made version 0.2 available of
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt;. It fixes some build
warnings and also supports the normal "make install" step. The
&lt;strong&gt;pullcves&lt;/strong&gt; command now also pulls in the latest &lt;code&gt;versions.dat&lt;/code&gt; file.
Special thanks to Per Andersson for reporting that the &lt;code&gt;./configure&lt;/code&gt;
didn't fail if sqlite3 or libconfig wasn't …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've made version 0.2 available of
&lt;a href="http://cvechecker.sourceforge.net"&gt;cvechecker&lt;/a&gt;. It fixes some build
warnings and also supports the normal "make install" step. The
&lt;strong&gt;pullcves&lt;/strong&gt; command now also pulls in the latest &lt;code&gt;versions.dat&lt;/code&gt; file.
Special thanks to Per Andersson for reporting that the &lt;code&gt;./configure&lt;/code&gt;
didn't fail if sqlite3 or libconfig wasn't available - that should be
fixed now as well.&lt;/p&gt;
&lt;p&gt;In my &lt;a href="http://github.com/sjvermeu/gentoo.overlay"&gt;Gentoo overlay&lt;/a&gt;,
&lt;code&gt;cvechecker-0.2.ebuild&lt;/code&gt; has been put available. Thanks there to
webkiller71 for helping out with a more sane approach to
&lt;code&gt;cvechecker-0.1.ebuild&lt;/code&gt; - I hope I didn't screw up his changes in 0.2
too much ;-)&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker 0.1 released</title><link href="https://blog.siphos.be/2010/08/cvechecker-0-1-released/" rel="alternate"></link><published>2010-08-14T22:03:00+02:00</published><updated>2010-08-14T22:03:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-14:/2010/08/cvechecker-0-1-released/</id><summary type="html">&lt;p&gt;cvechecker &lt;a href="https://sourceforge.net/projects/cvechecker/files/"&gt;version
0.1&lt;/a&gt; is out. This is
the first publicly available development release, so it's still far from
production-ready yet. However, it is usable so it can now be publicly
analyzed to remove all icky bugs and such. I'm not planning (m)any new
features (apart from the reporting …&lt;/p&gt;</summary><content type="html">&lt;p&gt;cvechecker &lt;a href="https://sourceforge.net/projects/cvechecker/files/"&gt;version
0.1&lt;/a&gt; is out. This is
the first publicly available development release, so it's still far from
production-ready yet. However, it is usable so it can now be publicly
analyzed to remove all icky bugs and such. I'm not planning (m)any new
features (apart from the reporting script as mentioned on the tools'
&lt;a href="http://cvechecker.sourceforge.net"&gt;homepage&lt;/a&gt;) before the first general
available release, but any request will be gladly documented and taken
in scope in future versions.&lt;/p&gt;
&lt;p&gt;What is cvechecker? Well, it is a tool that strives to scan your system
for installed software. For each software it detects, it attempts to
discover which version you have. This information is stored in a local
database. The tool then matches this information with the (publicly
available) CVE data (security vulnerability information). If a CVE entry
mentions the software/version you have, the tool will report this to
you.&lt;/p&gt;
&lt;p&gt;Who needs cvechecker? Noone needs it, but it can be interesting
nevertheless. Users that install lots of software themselves and don't
use the Linux distribution's package manager might benefit from this as
the tool will then help them verify if a security update is needed or
not. Users of &lt;a href="http://www.linuxfromscratch.org"&gt;LinuxFromScratch&lt;/a&gt; can do
some security validation tests on their installations. Developers of
particular packages (or even tools) can use the tool to be notified when
one of their software's has a CVE (which most likely results in a new
version of the software to be made available).&lt;/p&gt;
&lt;p&gt;Who needs cvechecker? No, this is not a duplicate paragraph - cvechecker
needs input. Most of the work goes in detecting the available software
and version. The method cvechecker uses is very rudimentary: run a
(predefined) regular expression against the file (which is parsed with
the &lt;strong&gt;strings&lt;/strong&gt; command as this command understands ELF structures) and
if the expression matches, it will extract the version (which is found
using the expressions' groups) and store this in the database. The rules
are defined in the &lt;code&gt;versions.dat&lt;/code&gt; file (also available from
&lt;a href="https://sourceforge.net/projects/cvechecker/files/"&gt;sourceforge&lt;/a&gt;), but
this file is currently microscopicly filled - so lots and lots of
additional rules need to be added. I'll be adding more and more rules as
I encounter them (or have immediate need for), but I can definitely use
additional help here.&lt;/p&gt;
&lt;p&gt;If you are interested in enhancing the &lt;code&gt;versions.dat&lt;/code&gt; file, check out
the cvechecker manual page - it describes the format and how it is used
as well as some examples.&lt;/p&gt;
&lt;p&gt;And yes, an ebuild is available in my
&lt;a href="http://github.com/sjvermeu/gentoo.overlay"&gt;overlay&lt;/a&gt;, but I'm no ebuild
developer (it's just easy to have them so Portage can track it) and the
ebuild is butt-ugly (and probably also violates all QA policies).&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>HP webcam on Linux</title><link href="https://blog.siphos.be/2010/08/hp-webcam-on-linux/" rel="alternate"></link><published>2010-08-13T18:18:00+02:00</published><updated>2010-08-13T18:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-13:/2010/08/hp-webcam-on-linux/</id><summary type="html">&lt;p&gt;Okay, getting the HP webcam running on Linux wasn't hard at all. Enable
Video For Linux (CONFIG_VIDEO_DEV) which can be found in the Linux
kernel configuration at Device Drivers, Multimedia Support. Then, select
Video capture adapters and inside that menu, select V4L USB devices and
then USB Video Class (UVC …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Okay, getting the HP webcam running on Linux wasn't hard at all. Enable
Video For Linux (CONFIG_VIDEO_DEV) which can be found in the Linux
kernel configuration at Device Drivers, Multimedia Support. Then, select
Video capture adapters and inside that menu, select V4L USB devices and
then USB Video Class (UVC). Once installed, reboot (or load the kernel
module) and ensure that your user is in the video group. You'll then be
able to activate the webcam (for instance, using &lt;strong&gt;mplayer tv://&lt;/strong&gt; or
using skype).&lt;/p&gt;
&lt;p&gt;The integrated microphone is also no problem. It is by default muted, so
using &lt;strong&gt;alsamixer&lt;/strong&gt;, press F4 (to get to the capture menu) and unmute
the channels + enable capturing (press spacebar).&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>New laptop, time to play</title><link href="https://blog.siphos.be/2010/08/new-laptop-time-to-play/" rel="alternate"></link><published>2010-08-13T01:33:00+02:00</published><updated>2010-08-13T01:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-08-13:/2010/08/new-laptop-time-to-play/</id><summary type="html">&lt;p&gt;I gave myself a nice treat and bought a new laptop. After some
consideration, I decided to go with the HP Pavilion DV7 3150EB. Years
ago, I didn't take an HP laptop as the reviews were not that satisfying.
However, it looks as if that is past. So I first …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I gave myself a nice treat and bought a new laptop. After some
consideration, I decided to go with the HP Pavilion DV7 3150EB. Years
ago, I didn't take an HP laptop as the reviews were not that satisfying.
However, it looks as if that is past. So I first took a stab at the
&lt;a href="http://www.gentoo.org/doc/en/gentoo-x86+raid+lvm2-quickinstall.xml"&gt;Gentoo Quickinstall for LVM2 and
RAID&lt;/a&gt;
and gave myself a (software) RAID-1 system with everything except / and
/boot using LVM2. To my satisfaction, following the guide was a breeze
and it worked out just fine.&lt;/p&gt;
&lt;p&gt;The real hurdle, that I just won, was to get the wireless up and running
on WPA2. I noticed earlier (before I bought the laptop) that getting the
Broadcom 43255 wifi (Broadcom Corporation Device 4357 (rev 01)) might be
a challenge. Well, the open-source b43 driver didn't detect the wifi
card, but the (closed-source) Broadcom STA driver (as supported by
Broadcom itself) does. To install it on Gentoo, it was as easy as
unmasking &lt;em&gt;broadcom-sta&lt;/em&gt; and installing it. It worked immediately, but
not for WPA/WPA2 networks (and I am not going to put my wireless in
non-WPA2 mode). Luckily, it was easy to discover that it was
&lt;em&gt;wpa_supplicant&lt;/em&gt; itself that was giving the card a hard time as non-WPA
networks worked flawlessly. A quick stab at the &lt;code&gt;wpa_supplicant.conf&lt;/code&gt;
file gave me the final success I needed: &lt;em&gt;ap_scan=2&lt;/em&gt; did the trick.&lt;/p&gt;
&lt;p&gt;Tomorrow: getting the webcam working...&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Linux Sea sources online, cvechecker still in development</title><link href="https://blog.siphos.be/2010/07/linux-sea-sources-online-cvechecker-still-in-development/" rel="alternate"></link><published>2010-07-23T20:59:00+02:00</published><updated>2010-07-23T20:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-07-23:/2010/07/linux-sea-sources-online-cvechecker-still-in-development/</id><summary type="html">&lt;p&gt;First of all, I've put the sources for &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; online at
&lt;a href="http://github.com/sjvermeu/Linux-Sea"&gt;GitHub&lt;/a&gt;. Not only does that
safeguard any latest changes from not hitting my backup in time before
my laptop dies (it's terminal, but I can't let him go yet ;-) but it
also allows people who want to help …&lt;/p&gt;</summary><content type="html">&lt;p&gt;First of all, I've put the sources for &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; online at
&lt;a href="http://github.com/sjvermeu/Linux-Sea"&gt;GitHub&lt;/a&gt;. Not only does that
safeguard any latest changes from not hitting my backup in time before
my laptop dies (it's terminal, but I can't let him go yet ;-) but it
also allows people who want to help with it (or translate it) to pull in
the sources.&lt;/p&gt;
&lt;p&gt;Note that it is still not finished (no spelling and grammar check done
yet, still need to add some exercises, etc); once it is, I will tag the
sources appropriately.&lt;/p&gt;
&lt;p&gt;On the &lt;a href="http://cvechecker.sf.net"&gt;cvechecker&lt;/a&gt; state, it is also still
under development, but progress is going nicely. Most of the work now is
in updating the &lt;code&gt;versions.dat&lt;/code&gt; file with information on how to obtain
the current version of a package/tool. It is an easy activity - most of
the work is in finding out how CVE entries would label a tool (what
vendor and product name would be chosen) and because I am too lazy, I am
currently only adding those that already have CVE entries assigned to
them (so I can just take a look at the correct values).&lt;/p&gt;
&lt;p&gt;It is also my first attempt at using autotools. Quite some overkill for
such a small project, but why not. At least it allows me to try to do
some new things here ;-)&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>cvechecker in development mode</title><link href="https://blog.siphos.be/2010/07/cvechecker-in-development-mode/" rel="alternate"></link><published>2010-07-12T20:31:00+02:00</published><updated>2010-07-12T20:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-07-12:/2010/07/cvechecker-in-development-mode/</id><summary type="html">&lt;p&gt;A while ago I had the idea to create a simple tool that checks the CVE
database against my current system. It would allow me to check if my
system is somewhat up to date (no pending security vulnerabilities), but
also to get an automated overview of the various software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while ago I had the idea to create a simple tool that checks the CVE
database against my current system. It would allow me to check if my
system is somewhat up to date (no pending security vulnerabilities), but
also to get an automated overview of the various software packages (and
versions) using a distribution-agnostic method.&lt;/p&gt;
&lt;p&gt;So I started coding. The idea is to have a tool which can interprete CVE
data, gather current version information from the system and match the
CVEs against these versions and report the results to me.&lt;/p&gt;
&lt;p&gt;I have created a &lt;a href="http://cvechecker.sourceforge.net"&gt;sourceforge&lt;/a&gt;
project to host the source code and preliminary documentation for the
tool. Although the tool runs, it is still far from finished. On the
site, you can check out the progress of the development (there's a first
todo-list on the main page).&lt;/p&gt;
&lt;p&gt;Do you think this is a good idea? I'd be happy to hear it.&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>OVAL, SCAP, CVE, CPE, ...</title><link href="https://blog.siphos.be/2010/06/oval-scap-cve-cpe/" rel="alternate"></link><published>2010-06-05T15:13:00+02:00</published><updated>2010-06-05T15:13:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-06-05:/2010/06/oval-scap-cve-cpe/</id><summary type="html">&lt;p&gt;For a personal &lt;abbr title="Proof Of Concept"&gt;POC&lt;/abbr&gt; I wanted to see
if it is possible to generate, based on the collection of CVE entries
publicly available, a report informing a system administrator about
possible vulnerabilities. Nothing fancy, just based upon versions.&lt;/p&gt;
&lt;p&gt;A simple example: tool detects Perl, acquires installed Perl version,
then matches …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For a personal &lt;abbr title="Proof Of Concept"&gt;POC&lt;/abbr&gt; I wanted to see
if it is possible to generate, based on the collection of CVE entries
publicly available, a report informing a system administrator about
possible vulnerabilities. Nothing fancy, just based upon versions.&lt;/p&gt;
&lt;p&gt;A simple example: tool detects Perl, acquires installed Perl version,
then matches the collection of CVE entries against this Perl version. If
at least one CVE is found, report it. The idea is then to make this as
generic as possible (not specific for an operating system or Linux
distribution), so not use a package version but really the tool version
(or library version).&lt;/p&gt;
&lt;p&gt;Of course, whenever I am planning such minor POCs, I search the Internet
for possible existing tools (just like &lt;a href="http://www.kev009.com/wp/2010/05/but-first-write-no-code/"&gt;kev009 describes - "But First,
Write No
Code"&lt;/a&gt;). And
I found out that there are already quite some "foundation components"
available...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cpe.mitre.org"&gt;CPE&lt;/a&gt; is a structured way of naming software
    (vendor, title, version ...)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://oval.mitre.org"&gt;OVAL&lt;/a&gt; is a method for performing structured
    tests (like regular expression matches in text) for reporting
    purposes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many more of these efforts are linked through the Mitre sites. The above
two are the most important ones though - it seems that it might be
possible to use OVAL to describe the tests I wanted for the POC.&lt;/p&gt;
&lt;p&gt;To be continued...&lt;/p&gt;</content><category term="Security"></category></entry><entry><title>Listing files of (not) installed software</title><link href="https://blog.siphos.be/2010/06/listing-files-of-not-installed-software/" rel="alternate"></link><published>2010-06-05T10:54:00+02:00</published><updated>2010-06-05T10:54:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-06-05:/2010/06/listing-files-of-not-installed-software/</id><summary type="html">&lt;p&gt;Everyone that has been using Gentoo for a while now knows about tools
such as &lt;strong&gt;qlist&lt;/strong&gt; that show you the list of files installed by an
(installed) package, or &lt;strong&gt;qfile&lt;/strong&gt; that allows you to find which package
provided a particular file on your system.&lt;/p&gt;
&lt;p&gt;One thing lacking is to be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Everyone that has been using Gentoo for a while now knows about tools
such as &lt;strong&gt;qlist&lt;/strong&gt; that show you the list of files installed by an
(installed) package, or &lt;strong&gt;qfile&lt;/strong&gt; that allows you to find which package
provided a particular file on your system.&lt;/p&gt;
&lt;p&gt;One thing lacking is to be able to find out which package &lt;em&gt;would&lt;/em&gt;
provide a file. Unlike the previous tools, this tool cannot rely on the
information found on your system as the package isn't installed yet.&lt;/p&gt;
&lt;p&gt;There have been projects in the past that attempted to provide such
functionality, almost always through an online queryable database. Many
haven't survived, due to too high expectations or little server
infrastructure resources. But it seems like
&lt;a href="http://www.portagefilelist.de"&gt;PortageFileList&lt;/a&gt; is to stay for a while.&lt;/p&gt;
&lt;p&gt;The project not only offers an online interface for querying
information, it also provides a package (&lt;code&gt;app-portage/pfl&lt;/code&gt;) that allows
you to query their infrastructure from the command line. The package
provides a tool called &lt;strong&gt;e-file&lt;/strong&gt; which supports SQL-like syntax for the
queries.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;~$ e-file '%bin/xdm'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The above command will then display, using the well-known emerge/Portage
output, which package provides the file (as well as which file was
matched by the query).&lt;/p&gt;
&lt;p&gt;Definitely a nice tool to have around. Thanks guys of
&lt;a href="http://www.portagefilelist.de"&gt;PortageFileList&lt;/a&gt;!&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category></entry><entry><title>GSE TWS BeLux 2010</title><link href="https://blog.siphos.be/2010/06/gse-tws-belux-2010/" rel="alternate"></link><published>2010-06-03T23:21:00+02:00</published><updated>2010-06-03T23:21:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-06-03:/2010/06/gse-tws-belux-2010/</id><summary type="html">&lt;p&gt;Today, IBM generously hosted the &lt;a href="http://www.gsebelux.com/?q=node/101"&gt;GSE TWS BeLux 2010
conference&lt;/a&gt;. Although it was
organized together with the GSE DB2 conference (which I would also have
loved to attend) I must say I was pretty impressed with the topics
given, especially those after the lunch.&lt;/p&gt;
&lt;p&gt;For me, personally, the topic on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, IBM generously hosted the &lt;a href="http://www.gsebelux.com/?q=node/101"&gt;GSE TWS BeLux 2010
conference&lt;/a&gt;. Although it was
organized together with the GSE DB2 conference (which I would also have
loved to attend) I must say I was pretty impressed with the topics
given, especially those after the lunch.&lt;/p&gt;
&lt;p&gt;For me, personally, the topic on TWS 8.5.1 with broker functionality was
most impressive. Not that the features are extremely innovative, but
they are very useful (especially in an enterprise context). I don't know
if you have (production) experience with TWS Broker (or even 8.5.1's
broker functionality) - if you do, I'd love to hear about it!&lt;/p&gt;</content><category term="Misc"></category><category term="gse"></category><category term="TWS"></category></entry><entry><title>Question yourself v3</title><link href="https://blog.siphos.be/2010/05/question-yourself-v3/" rel="alternate"></link><published>2010-05-19T22:11:00+02:00</published><updated>2010-05-19T22:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-05-19:/2010/05/question-yourself-v3/</id><content type="html">&lt;p&gt;Another update to &lt;a href="http://swift.siphos.be/tools-quizzer.html"&gt;Quizzer&lt;/a&gt;,
now at version 3. But more importantly, updates to the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; related chapters are made
available online - get a taste for it at the &lt;a href="http://swift.siphos.be/tools/quizzer/quizzer.html?category=linuxsea"&gt;online quizzer
set&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feedback is, as always, very much appreciated.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Question yourself v2</title><link href="https://blog.siphos.be/2010/05/question-yourself-v2/" rel="alternate"></link><published>2010-05-11T20:59:00+02:00</published><updated>2010-05-11T20:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-05-11:/2010/05/question-yourself-v2/</id><summary type="html">&lt;p&gt;A new version of the
&lt;a href="http://swift.siphos.be/tools-quizzer.html"&gt;Quizzer&lt;/a&gt; webscript is
available. The &lt;a href="http://swift.siphos.be/tools/quizzer/quizzer.html"&gt;demo&lt;/a&gt;
has also been updated with quick tests on the first few chapters of
Linux Sea.&lt;/p&gt;
&lt;p&gt;More exercises on the following chapters will follow soon.&lt;/p&gt;
&lt;p&gt;Updates to the script include visual accept/reject of single-choice and
multiple choice answers and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A new version of the
&lt;a href="http://swift.siphos.be/tools-quizzer.html"&gt;Quizzer&lt;/a&gt; webscript is
available. The &lt;a href="http://swift.siphos.be/tools/quizzer/quizzer.html"&gt;demo&lt;/a&gt;
has also been updated with quick tests on the first few chapters of
Linux Sea.&lt;/p&gt;
&lt;p&gt;More exercises on the following chapters will follow soon.&lt;/p&gt;
&lt;p&gt;Updates to the script include visual accept/reject of single-choice and
multiple choice answers and improved support for Internet Explorer
(which I don't have at home to validate).&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Question yourself</title><link href="https://blog.siphos.be/2010/05/question-yourself/" rel="alternate"></link><published>2010-05-02T23:58:00+02:00</published><updated>2010-05-02T23:58:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-05-02:/2010/05/question-yourself/</id><summary type="html">&lt;p&gt;Do you ever write down things in the hope you never forget them, but
still think it would be better if you could somehow take a test of that
subject from time to time to make sure you don't forget?&lt;/p&gt;
&lt;p&gt;I do, and I found it quite difficult to keep …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Do you ever write down things in the hope you never forget them, but
still think it would be better if you could somehow take a test of that
subject from time to time to make sure you don't forget?&lt;/p&gt;
&lt;p&gt;I do, and I found it quite difficult to keep the knowledge live without
having to reread the things every now and then. For that purpose, I
started writing a simple JavaScript/XML/XSL fileset that allowed me to
present questions (randomly if necessary) from a structured set of
questions. In the beginning, it was too simple to share (string
matching) but quickly grew to something more elaborate: regular
expression support, multiple string-answer support, in-paragraph answer
boxes and single/multiple choice answers.&lt;/p&gt;
&lt;p&gt;With this fileset in place, I can keep track of things I would most
likely otherwise forget: just select the category which I want to take a
test from, and start with a (lot of) random question(s).&lt;/p&gt;
&lt;p&gt;I've decided to put this fileset
&lt;a href="http://swift.siphos.be/tools-quizzer.html"&gt;online&lt;/a&gt; (including
&lt;a href="http://swift.siphos.be/tools/quizzer/quizzer.html"&gt;demo&lt;/a&gt; files) and
will extend the demo file with questions regarding my book, &lt;a href="http://swift.siphos.be/linux_sea/"&gt;Linux
Sea&lt;/a&gt;, allowing readers of the book to
take online tests after they've finished a chapter.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>SAI and N-O-SQL</title><link href="https://blog.siphos.be/2010/04/sai-and-n-o-sql/" rel="alternate"></link><published>2010-04-22T01:02:00+02:00</published><updated>2010-04-22T01:02:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-04-22:/2010/04/sai-and-n-o-sql/</id><summary type="html">&lt;p&gt;Yesterday (argh, the day before yesterday) I went to a
&lt;a href="http://www.sai.be"&gt;SAI&lt;/a&gt; conference on nosql. In Belgium, SAI is a
non-profit organization for IT people which focuses on knowledge
sharing.&lt;/p&gt;
&lt;p&gt;The conference that day was on nosql. The presentation given by
&lt;a href="http://www.outerthought.be"&gt;OuterThought&lt;/a&gt; was very good and offered a
nice introduction to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday (argh, the day before yesterday) I went to a
&lt;a href="http://www.sai.be"&gt;SAI&lt;/a&gt; conference on nosql. In Belgium, SAI is a
non-profit organization for IT people which focuses on knowledge
sharing.&lt;/p&gt;
&lt;p&gt;The conference that day was on nosql. The presentation given by
&lt;a href="http://www.outerthought.be"&gt;OuterThought&lt;/a&gt; was very good and offered a
nice introduction to the "new types of database architectures" that are
being actively developed as we speak.&lt;/p&gt;
&lt;p&gt;Although the use of these nosql databases within
&lt;a href="http://www.kbc.com"&gt;KBC&lt;/a&gt; (where I work for) is limited (I'm not aware
of any application that is already using this technology) it would be
plain wrong to discard the technology as "too immature". With the recent
developments that we face in the IT industry, applications are nowadays
quick in adopting such new technologies and I suspect that off-the-shelf
applications will soon come with such nosql database technology as part
of the solution.&lt;/p&gt;
&lt;p&gt;For large enterprises, this does face some (hard?) challenges: how do
you control your network usage (some of the technologies are easy to
use, but hard to tune), how do you design your architecture, where is
your data, how can you ensure that you do not "lock in" into a single
nosql technology (i.e. how do you ensure interoperability and migrations
between technologies), do you still need SAN-based replication or will
you now let the technology handle this for you, etc.&lt;/p&gt;
&lt;p&gt;So yes, the nosql technology is nice to look into (and definitely
something to follow up on) but make sure you don't introduce it in your
organization without thinking about the entire integration and
management aspect.&lt;/p&gt;</content><category term="Databases"></category><category term="Databases"></category><category term="enterprise-architecture"></category><category term="nosql"></category></entry><entry><title>A dozen pages added</title><link href="https://blog.siphos.be/2010/04/a-dozen-pages-added/" rel="alternate"></link><published>2010-04-22T00:53:00+02:00</published><updated>2010-04-22T00:53:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-04-22:/2010/04/a-dozen-pages-added/</id><content type="html">&lt;p&gt;Just a quick heads-up that a dozen pages in the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; book have been added. Nothing
spectacular, just a few more paragraphs on services/runlevels, a few
updates on software management and on boot failure resolutions.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>License support in Gentoo</title><link href="https://blog.siphos.be/2010/02/license-support-in-gentoo/" rel="alternate"></link><published>2010-02-16T00:10:00+01:00</published><updated>2010-02-16T00:10:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-02-16:/2010/02/license-support-in-gentoo/</id><summary type="html">&lt;p&gt;It's a bit sad that Gentoo didn't promote this more, but Gentoo users
now have support for license-based masking.&lt;/p&gt;
&lt;p&gt;What does this mean? Well, previously, Gentoo already supported various
masking reasons (like stable versus staging - the x86 versus \~x86 saga,
package.mask'ing - for security reasons or critical bugs, ...). Now, a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's a bit sad that Gentoo didn't promote this more, but Gentoo users
now have support for license-based masking.&lt;/p&gt;
&lt;p&gt;What does this mean? Well, previously, Gentoo already supported various
masking reasons (like stable versus staging - the x86 versus \~x86 saga,
package.mask'ing - for security reasons or critical bugs, ...). Now, a
new feature is added: license masking.&lt;/p&gt;
&lt;p&gt;By default, Portage accepts all non-EULA licenses. If a package uses a
EULA license, you'll get a failure message stating that the license is
'masked'.&lt;/p&gt;
&lt;p&gt;Now, what good does this do for users? Well, you can now ask Portage
only to accept certain licenses (like &lt;code&gt;@FSF-APPROVED&lt;/code&gt;, which is a list
of all FSF-approved licenses) and deny the installation of others. Nice,
isn't it?&lt;/p&gt;
&lt;p&gt;I've added information regarding package license states (and the global
as well as per-package unmasking support through
/etc/portage/package.license) to the &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt; document.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Executing, but only when you're home</title><link href="https://blog.siphos.be/2010/01/executing-but-only-when-youre-home/" rel="alternate"></link><published>2010-01-18T23:48:00+01:00</published><updated>2010-01-18T23:48:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2010-01-18:/2010/01/executing-but-only-when-youre-home/</id><summary type="html">&lt;p&gt;Sometimes you want to execute a particular command, but only when you're
at home. Examples would be running fetchmail (or fetchnews) through
cron, but you don't want this to run when you're in the train, connected
to the Internet through GPRS...&lt;/p&gt;
&lt;p&gt;My idea here would be to create a script …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes you want to execute a particular command, but only when you're
at home. Examples would be running fetchmail (or fetchnews) through
cron, but you don't want this to run when you're in the train, connected
to the Internet through GPRS...&lt;/p&gt;
&lt;p&gt;My idea here would be to create a script (say "athome.sh") which returns
0 if you're at home, and 1 otherwise. The key of the script is that the
MAC address of your (default) gateway is unique.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nv"&gt;GW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;/sbin/ip route &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;/default/ {print $3}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;MGW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;/sbin/arp -e &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;GW&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{print $3}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;MGW&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;00:11:22:33:44:55&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;With this script, you can then run &lt;code&gt;athome.sh &amp;amp;&amp;amp; fetchmail&lt;/code&gt;. If you
aren't home, &lt;code&gt;athome.sh&lt;/code&gt; will return 1 and the fetchmail command will
never be executed. When you are, the command returns 0 and fetchmail is
launched.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Switching to database architecture</title><link href="https://blog.siphos.be/2009/12/switching-to-database-architecture/" rel="alternate"></link><published>2009-12-11T00:34:00+01:00</published><updated>2009-12-11T00:34:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-12-11:/2009/12/switching-to-database-architecture/</id><summary type="html">&lt;p&gt;It's finally committed: I'm going to dive into the realms of database
architecture. It's with some sentiment that I'm leaving the expertise
field of Apache, J(2)EE and WebSphere, but seeing the database
architecture field makes it up well. I'm starting to get acquainted with
Oracle DB as first …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's finally committed: I'm going to dive into the realms of database
architecture. It's with some sentiment that I'm leaving the expertise
field of Apache, J(2)EE and WebSphere, but seeing the database
architecture field makes it up well. I'm starting to get acquainted with
Oracle DB as first platform and IBM DB2 + Microsoft SQL Server are
pending. Exciting times are coming!&lt;/p&gt;</content><category term="Databases"></category></entry><entry><title>Translations to "Linux Sea"</title><link href="https://blog.siphos.be/2009/12/translations-to-linux-sea/" rel="alternate"></link><published>2009-12-02T17:38:00+01:00</published><updated>2009-12-02T17:38:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-12-02:/2009/12/translations-to-linux-sea/</id><summary type="html">&lt;p&gt;A few people have contacted me if they were allowed to translate the
online book I'm writing (&lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;).
Of course they are, the license allows it. However, I recommend to wait
a bit. At this moment, I'm not going to release the docbook sources (I'm
not writing it in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few people have contacted me if they were allowed to translate the
online book I'm writing (&lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;).
Of course they are, the license allows it. However, I recommend to wait
a bit. At this moment, I'm not going to release the docbook sources (I'm
not writing it in DocBook, but I'm generating from another XML into
DocBook) until I'm happy with the final result.&lt;/p&gt;
&lt;p&gt;I'm glad to see that the document is well received. There is still lots
of work on it (more excercises, a thorough spelling / grammar check,
elaborate on certain topics, ...) so stay tuned for further updates. Why
are those updates "slow"? Well, let's say that I use a "fair share
scheduling" principle on all my hobbies ;-)&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Small updates on Linux Sea</title><link href="https://blog.siphos.be/2009/10/small-updates-on-linux-sea/" rel="alternate"></link><published>2009-10-19T21:36:00+02:00</published><updated>2009-10-19T21:36:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-10-19:/2009/10/small-updates-on-linux-sea/</id><content type="html">&lt;p&gt;A few updates have made it to the &lt;a href="http://swift.siphos.be/linux_sea" title="Linux Sea (HTML)"&gt;Linux
Sea&lt;/a&gt; book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Information regarding ndiswrapper&lt;/li&gt;
&lt;li&gt;Some information about udev and the symlinks that it creates&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The
&lt;a href="http://swift.siphos.be/linux_sea/linux_sea.pdf" title="Linux Sea (PDF)"&gt;PDF&lt;/a&gt;
version has been updated as well.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Online image gallery</title><link href="https://blog.siphos.be/2009/10/online-image-gallery/" rel="alternate"></link><published>2009-10-05T21:48:00+02:00</published><updated>2009-10-05T21:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-10-05:/2009/10/online-image-gallery/</id><content type="html">&lt;p&gt;If you're not up to the various free image gallery sites, you might want
to try out &lt;a href="http://www.zenphoto.org/"&gt;ZenPhoto&lt;/a&gt;. Quite powerful, easy to
use and well themeable. Requires PHP / MySQL.&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Added quota information</title><link href="https://blog.siphos.be/2009/09/added-quota-information/" rel="alternate"></link><published>2009-09-01T23:19:00+02:00</published><updated>2009-09-01T23:19:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-09-01:/2009/09/added-quota-information/</id><summary type="html">&lt;p&gt;I've added quota support information to the &lt;a href="http://swift.siphos.be/linux_sea/"&gt;Linux
Sea&lt;/a&gt; book as well as information
about the eclean command for cleaning distfiles and packages. The part
on building a Linux kernel has been moved into its own
&lt;a href="http://swift.siphos.be/linux_sea/ch07.html"&gt;chapter&lt;/a&gt;, the chapter on
&lt;a href="http://swift.siphos.be/linux_sea/ch08.html"&gt;hardware support&lt;/a&gt; now has a
bit more information about dealing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've added quota support information to the &lt;a href="http://swift.siphos.be/linux_sea/"&gt;Linux
Sea&lt;/a&gt; book as well as information
about the eclean command for cleaning distfiles and packages. The part
on building a Linux kernel has been moved into its own
&lt;a href="http://swift.siphos.be/linux_sea/ch07.html"&gt;chapter&lt;/a&gt;, the chapter on
&lt;a href="http://swift.siphos.be/linux_sea/ch08.html"&gt;hardware support&lt;/a&gt; now has a
bit more information about dealing with sound cards (ALSA support) and
will contain information about sound servers in the near future. This
chapter will also be used to configure the various other hardware things
as they come by (printers, scanners, ...).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Draft PDF for Linux Sea</title><link href="https://blog.siphos.be/2009/08/draft-pdf-for-linux-sea/" rel="alternate"></link><published>2009-08-10T22:27:00+02:00</published><updated>2009-08-10T22:27:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-08-10:/2009/08/draft-pdf-for-linux-sea/</id><content type="html">&lt;p&gt;I've added a draft &lt;a href="http://swift.siphos.be/linux_sea/linux_sea.pdf"&gt;PDF&lt;/a&gt;
version of my Linux Sea document. If you don't mind the A4 papersize and
the bad typesetting of the text boxes (I still have lots of overflows to
correct) it is quite usable.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Darwin Information Typing Architecture</title><link href="https://blog.siphos.be/2009/04/darwin-information-typing-architecture/" rel="alternate"></link><published>2009-04-18T09:59:00+02:00</published><updated>2009-04-18T09:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-04-18:/2009/04/darwin-information-typing-architecture/</id><summary type="html">&lt;p&gt;Having documented a lot in LaTeX (back in the old days at the
university), &lt;a href="http://www.gentoo.org/doc/en/xml-guide.xml"&gt;GuideXML&lt;/a&gt;
(Gentoo's document markup language) and DocBook (&lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;) I'm now pointing my arrows at
DITA, the &lt;a href="http://en.wikipedia.org/wiki/Darwin_Information_Typing_Architecture"&gt;Darwin Information Typing
Architecture&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DITA "forces" the technical writer in separating the content of his
document in specialized subjects …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Having documented a lot in LaTeX (back in the old days at the
university), &lt;a href="http://www.gentoo.org/doc/en/xml-guide.xml"&gt;GuideXML&lt;/a&gt;
(Gentoo's document markup language) and DocBook (&lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux
Sea&lt;/a&gt;) I'm now pointing my arrows at
DITA, the &lt;a href="http://en.wikipedia.org/wiki/Darwin_Information_Typing_Architecture"&gt;Darwin Information Typing
Architecture&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DITA "forces" the technical writer in separating the content of his
document in specialized subjects: reference, task or concept, or a
specialized version of any of those which you can create/define
yourself.&lt;/p&gt;
&lt;p&gt;By separating content in those three subjects, you can more easily
manage your technical documentation (write concepts as individual
topics, tasks as end-user procedures and references for affiliated
topics or command information).&lt;/p&gt;
&lt;p&gt;Once all these documents are written, you bind them together using a
DITA map (a metadocument which holds references to all related
concepts/tasks/references) et voila: your documentation is ready.&lt;/p&gt;
&lt;p&gt;Well, not really, you need to build it to something end users can read -
you can use &lt;a href="http://dita-ot.sourceforge.net/"&gt;dita-ot&lt;/a&gt; for that. It
supports building for Eclipse Infocenter, RTF, XHTML and PDF out of the
box.&lt;/p&gt;</content><category term="Documentation"></category></entry><entry><title>Linux Sea is progressing slowly but surely</title><link href="https://blog.siphos.be/2009/02/linux-sea-is-progressing-slowly-but-surely/" rel="alternate"></link><published>2009-02-10T23:33:00+01:00</published><updated>2009-02-10T23:33:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2009-02-10:/2009/02/linux-sea-is-progressing-slowly-but-surely/</id><summary type="html">&lt;p&gt;My everlasting document, &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;,
is progressing slowely but surely. I've started a few new chapters and
also initiated a chapter on &lt;a href="http://swift.siphos.be/linux_sea/ch15.html"&gt;Installing
Gentoo&lt;/a&gt; (which is more a
shortlist of tasks with pointers to earlier chapters).&lt;/p&gt;
&lt;p&gt;I also took a different CSS (docbook.css file used by the FreeBSD
handbook …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My everlasting document, &lt;a href="http://swift.siphos.be/linux_sea"&gt;Linux Sea&lt;/a&gt;,
is progressing slowely but surely. I've started a few new chapters and
also initiated a chapter on &lt;a href="http://swift.siphos.be/linux_sea/ch15.html"&gt;Installing
Gentoo&lt;/a&gt; (which is more a
shortlist of tasks with pointers to earlier chapters).&lt;/p&gt;
&lt;p&gt;I also took a different CSS (docbook.css file used by the FreeBSD
handbook) as it looks a lot better.&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Extremely simple task manager</title><link href="https://blog.siphos.be/2008/12/extremely-simple-task-manager/" rel="alternate"></link><published>2008-12-18T22:46:00+01:00</published><updated>2008-12-18T22:46:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2008-12-18:/2008/12/extremely-simple-task-manager/</id><summary type="html">&lt;p&gt;At work, I am often busy with quite a few projects. Yet, at times, I
have no outstanding tasks because all of my tasks can only start when an
event has occurred (like a server which is made available, or a budget
that is approved) or another task has finished …&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, I am often busy with quite a few projects. Yet, at times, I
have no outstanding tasks because all of my tasks can only start when an
event has occurred (like a server which is made available, or a budget
that is approved) or another task has finished.&lt;/p&gt;
&lt;p&gt;To keep track of my work, I write an extremely simple task manager: an
XML file (for the data), XSL file (for the rendering) and HTML/CSS file
(to render and use the browsers' XSL capabilities). I call it
&lt;a href="http://swift.siphos.be/tools-taskviewer.html"&gt;taskviewer&lt;/a&gt; due to lack
of more imagination ;-)&lt;/p&gt;
&lt;p&gt;It is a simple manager with no user interface for managing it at all -
so you'll need to edit the XML file yourself. However, the HTML/CSS
file, together with the XSL file, renders the content of the XML file in
such a way that you have a nice overview of your tasks.&lt;/p&gt;
&lt;p&gt;It's "features" are simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;keep track of events you are waiting for&lt;/li&gt;
&lt;li&gt;keep track of a tasks' dependencies (events or other tasks)&lt;/li&gt;
&lt;li&gt;get an overview of tasks that can immediately start versus that are
    blocked, waiting for its dependencies to finish&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an &lt;a href="http://swift.siphos.be/tools/taskviewer/taskviewer.html"&gt;example available
online&lt;/a&gt; with
some hypothetical data.&lt;/p&gt;
&lt;p&gt;If you know of a simple program (preferably java or one available for
both Windows and Linux) that has similar features (especially tracking
tasks depending on certain events), please do tell me. I've looked at
tools like &lt;a href="http://www.taskjuggler.org"&gt;taskjuggler&lt;/a&gt; but couldn't find
one that remains simple yet has these features.&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>hex2passwd, a password generator</title><link href="https://blog.siphos.be/2008/09/hex2passwd-a-password-generator/" rel="alternate"></link><published>2008-09-25T19:34:00+02:00</published><updated>2008-09-25T19:34:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2008-09-25:/2008/09/hex2passwd-a-password-generator/</id><summary type="html">&lt;p&gt;I know that repeatable password generators are less secure than random
character generators. After all, if you want a strong password, you can
simply perform &lt;strong&gt;head -c 8 /dev/urandom | mimencode&lt;/strong&gt; to obtain a nice,
random password string.&lt;/p&gt;
&lt;p&gt;However, in certain cases you might want to generate passwords given a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I know that repeatable password generators are less secure than random
character generators. After all, if you want a strong password, you can
simply perform &lt;strong&gt;head -c 8 /dev/urandom | mimencode&lt;/strong&gt; to obtain a nice,
random password string.&lt;/p&gt;
&lt;p&gt;However, in certain cases you might want to generate passwords given a
particular entry which always returns the same password. For instance,
for low-profile web sites. Most people use mneumonics (such as username
reversed and appended with domainname abbreviation to give an example)
but mneumonics can be quite insecure, especially if you use a mneumonic
that, once someone sees one of your passwords, he can deduce all
passwords.&lt;/p&gt;
&lt;p&gt;An example would be the above-given algorithm, which yields for the
following sites:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bugs.gentoo.org, user foobar, password raboofbgo
forums.gentoo.org, user bleh, password helbfgo
www.sourceforge.net, user mynick, password kcinymwsn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I'm sure you can find the password for other sites I would show you, so
this kind of passwords are not that secure.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href="http://dev.gentoo.org/~swift/tools-hex2passwd.html"&gt;hex2passwd&lt;/a&gt;,
a tool which generates (the same) password for the same input over and
over again. With the tool you can make your mneumonic a bit more secure
as it uses hashfunctions to create a pseudorandom sequence and a
character mapping to convert the hash result into a possible password.&lt;/p&gt;
&lt;p&gt;An example for the above sites / mneumonic would yield:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;For bugs.gentoo.org, user foobar
$ echo raboofbgo | sha1sum | hex2passwd -n 8
XqXgOYce
For forums.gentoo.org, user bleh
$ echo helbfgo | sha1sum | hex2passwd -n 8
l8U.tdzg
For www.sourceforge.net, user mynick
$ echo kcinymwsn | sha1sum | hex2passwd -n 8
70z4Bu3k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, the tool offers some more flexibility, such as choosing your
own character maps or scrambling the maps before you use them. In any
case, if you think such a tool is useful for you as well, don't hesitate
to download, compile and install it - it's a simple C program, probably
too ugly to show ;-)&lt;/p&gt;</content><category term="Free-Software"></category></entry><entry><title>Adding exercises and resources</title><link href="https://blog.siphos.be/2008/09/adding-exercises-and-resources/" rel="alternate"></link><published>2008-09-15T22:59:00+02:00</published><updated>2008-09-15T22:59:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2008-09-15:/2008/09/adding-exercises-and-resources/</id><summary type="html">&lt;p&gt;As stated earlier, I'm now focusing on the existing content of my
(work-in-progress) ebook called Linux Sea
(&lt;a href="http://dev.gentoo.org/~swift/linux_sea.pdf"&gt;PDF&lt;/a&gt;,
&lt;a href="http://dev.gentoo.org/~swift/linux_sea/"&gt;HTML&lt;/a&gt;). I'm going to add more
text where appropriate, add exercises to each chapter as well as
references to online resources.&lt;/p&gt;
&lt;p&gt;When that's finished, I'll probably be writing a chapter on installing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As stated earlier, I'm now focusing on the existing content of my
(work-in-progress) ebook called Linux Sea
(&lt;a href="http://dev.gentoo.org/~swift/linux_sea.pdf"&gt;PDF&lt;/a&gt;,
&lt;a href="http://dev.gentoo.org/~swift/linux_sea/"&gt;HTML&lt;/a&gt;). I'm going to add more
text where appropriate, add exercises to each chapter as well as
references to online resources.&lt;/p&gt;
&lt;p&gt;When that's finished, I'll probably be writing a chapter on installing
Gentoo Linux as that's a major end-user topic that isn't discussed yet
(but luckily, there's still the &lt;a href="http://www.gentoo.org/doc/en/handbook/"&gt;Gentoo
Handbook&lt;/a&gt;).&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Linux Sea - Updates on graphical environment chapter</title><link href="https://blog.siphos.be/2008/08/linux-sea-updates-on-graphical-environment-chapter/" rel="alternate"></link><published>2008-08-21T22:08:00+02:00</published><updated>2008-08-21T22:08:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2008-08-21:/2008/08/linux-sea-updates-on-graphical-environment-chapter/</id><summary type="html">&lt;p&gt;I've updated the chapter on &lt;a href="http://dev.gentoo.org/~swift/linux_sea/c2990.htm"&gt;graphical
environments&lt;/a&gt; a bit to
reflect how applications, window managers, X server and widget toolkits
work together. Hopefully it isn't a big lie that I wrote there ;-)&lt;/p&gt;
&lt;p&gt;I'll probably be doing a bit of clean ups the coming days before I start
out with more …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've updated the chapter on &lt;a href="http://dev.gentoo.org/~swift/linux_sea/c2990.htm"&gt;graphical
environments&lt;/a&gt; a bit to
reflect how applications, window managers, X server and widget toolkits
work together. Hopefully it isn't a big lie that I wrote there ;-)&lt;/p&gt;
&lt;p&gt;I'll probably be doing a bit of clean ups the coming days before I start
out with more chapters...&lt;/p&gt;</content><category term="Gentoo"></category></entry><entry><title>Playing with gqview</title><link href="https://blog.siphos.be/2008/08/playing-with-gqview/" rel="alternate"></link><published>2008-08-18T15:48:00+02:00</published><updated>2008-08-18T15:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2008-08-18:/2008/08/playing-with-gqview/</id><summary type="html">&lt;p&gt;Some time ago I received a digital camera; however, due to diskspace
shortage I need to clean up my home directory. One of the directories
that eats most of my sectors is one where I store all my pictures.&lt;/p&gt;
&lt;p&gt;I know I have a lot of duplicate pictures, pictures deduced …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Some time ago I received a digital camera; however, due to diskspace
shortage I need to clean up my home directory. One of the directories
that eats most of my sectors is one where I store all my pictures.&lt;/p&gt;
&lt;p&gt;I know I have a lot of duplicate pictures, pictures deduced from master
pictures (lower resolution, some editing) and similar pictures (same
scene taken 4 or 5 times with different camera settings, hoping to get
at least one good shot) but managing them wasn't easy.&lt;/p&gt;
&lt;p&gt;I now played a bit with &lt;a href="http://gqview.sourceforge.net/"&gt;gqview&lt;/a&gt; and
this tool seems to provide some features I find very interesting; one of
them is the "find duplicates" where you can even search for pictures
with "similar" content and I must say that it does work. Of course,
nothing is perfect, but I've managed to clean up the picture directory
so it works for me.&lt;/p&gt;</content><category term="Free-Software"></category></entry></feed>