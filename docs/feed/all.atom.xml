<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art...</title><link href="https://blog.siphos.be/" rel="alternate"></link><link href="https://blog.siphos.be/feed/all.atom.xml" rel="self"></link><id>https://blog.siphos.be/</id><updated>2025-01-27T21:10:00+01:00</updated><entry><title>Is IT a DORA CIF?</title><link href="https://blog.siphos.be/2025/01/is-it-a-dora-cif/" rel="alternate"></link><published>2025-01-27T21:10:00+01:00</published><updated>2025-01-27T21:10:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2025-01-27:/2025/01/is-it-a-dora-cif/</id><content type="html">&lt;p&gt;Core to the &lt;a href="https://blog.siphos.be/2025/01/digital-operational-resilience-act/"&gt;Digital Operational Resilience
Act&lt;/a&gt; is the notion of
a &lt;em&gt;critical or important function&lt;/em&gt;. When a function is deemed critical or
important, DORA expects the company or group to take precautions and measures
to ensure the resilience of the company and the markets in which it is active.&lt;/p&gt;
&lt;p&gt;But what exactly is a function? When do we consider it critical or important?
Is there a differentiation between critical and important? Can an IT function
be a critical or important function?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining functions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's start with the definition of a &lt;em&gt;function&lt;/em&gt;. Surely that is defined in the
documents, right? Right?&lt;/p&gt;
&lt;p&gt;Eh... no. The &lt;a href="https://eur-lex.europa.eu/eli/reg/2022/2554/oj/eng"&gt;DORA
regulation&lt;/a&gt; does not seem
to provide a definition for a function. It does however refer to the
definition of &lt;em&gt;critical function&lt;/em&gt; in the &lt;a href="https://eur-lex.europa.eu/eli/dir/2014/59/oj/eng"&gt;Bank Recovery and Resolution
Directive (BRRD), aka Directive
2014/59/EU&lt;/a&gt;. That's one of
the regulations that focuses on the resolution in case of severe disruptions,
bankrupcy or other failures of banks at a national or European level. A
&lt;a href="https://eur-lex.europa.eu/eli/reg_del/2016/778/oj/eng"&gt;Delegated regulation EU/
2016/778&lt;/a&gt; further
defines several definitions that inspired the DORA regulation as well.&lt;/p&gt;
&lt;p&gt;In the latter document, we do find the definition of a &lt;em&gt;function&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘function’ means a structured set of activities, services or operations that
are delivered by the institution or group to third parties irrespective from
the internal organisation of the institution;&lt;/p&gt;
&lt;p&gt;Article 2, (2), of Delegated regulation 2016/778&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So if you want to be blunt, you could state that an IT function which is only
supporting the own group (as in, you're not insourcing IT of other companies)
is not a function, and thus cannot be a "critical or important function" in
DORA's viewpoint.&lt;/p&gt;
&lt;p&gt;That is, unless you find that the definition of previous regulations do not
necessarily imply the same interpretation within DORA. After all, DORA does
not amend the EU 2016/778 regulation. It amends &lt;a href="https://eur-lex.europa.eu/eli/reg/2009/1060/oj/eng"&gt;EC
1060/2009&lt;/a&gt;, &lt;a href="https://eur-lex.europa.eu/eli/reg/2012/648/oj/eng"&gt;EU
2012/648&lt;/a&gt;, &lt;a href="https://eur-lex.europa.eu/eli/reg/2014/600/oj/eng"&gt;EU
2014/600 aka MiFIR&lt;/a&gt;, &lt;a href="https://eur-lex.europa.eu/eli/reg/2014/909/oj/eng"&gt;EU
2014/909 aka CSDR&lt;/a&gt; and &lt;a href="https://eur-lex.europa.eu/eli/reg/2016/1011/oj/eng"&gt;EU
2016/1011 aka Benchmark
Regulation&lt;/a&gt;. But none of
these have a definition for 'function' at first sight.&lt;/p&gt;
&lt;p&gt;So let's humor ourselves and move on. What is a &lt;em&gt;critical function&lt;/em&gt;? Is that
defined in DORA? Not really, sort-of. DORA has a definition for &lt;em&gt;critical or
important function&lt;/em&gt;, but let's first look at more distinct definitions.&lt;/p&gt;
&lt;p&gt;In the BRRD regulation, this is defined as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘critical functions’ means activities, services or operations the
discontinuance of which is likely in one or more Member States, to lead to
the disruption of services that are essential to the real economy or to
disrupt financial stability due to the size, market share, external and
internal interconnectedness, complexity or cross-border activities of an
institution or group, with particular regard to the substitutability of
those activities, services or operations;&lt;/p&gt;
&lt;p&gt;Article 2, (35), of BRRD 2014/59&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This extends on the use of &lt;em&gt;function&lt;/em&gt;, and adds in the evaluation if it is
crucial for the economy, especially when it would be suddenly discontinued.
The extension on the definition of &lt;em&gt;function&lt;/em&gt; is also confirmed by guidance
that the &lt;a href="https://www.srb.europa.eu/en/content/critical-functions"&gt;European Single Resolution
Board&lt;/a&gt; published,
namely that "the function is provided by an institution to third parties not
affiliated to the institution or group".&lt;/p&gt;
&lt;p&gt;The preamble of the Delegated regulation also mentions that its focus is at
the safeguarding of the financial stability and the real economy. It gives
examples of potential critical functions such as deposit taking, lending and
loan services, payment, clearing, custody and settlement services, wholesale
funding markets activities, and capital markets and investments activities.&lt;/p&gt;
&lt;p&gt;Of course, your IT is supporting your company, and in case of financial
institutions, IT is a very big part of the company. Is IT then not involved in
all of this?&lt;/p&gt;
&lt;p&gt;It sure is... &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://eur-lex.europa.eu/eli/reg_del/2016/778/oj/eng"&gt;Delegated regulation EU
2016/778&lt;/a&gt; in its
preamble already indicates that functions are supported by services:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Critical services should be the underlying operations, activities and
services performed for one (dedicated services) or more business units or
legal entities (shared services) within the group which are needed to
provide one or more critical functions. Critical services can be performed
by one or more entities (such as a separate legal entity or an internal
unit) within the group (internal service) or be outsourced to an external
provider (external service). A service should be considered critical where
its disruption can present a serious impediment to, or completely prevent,
the performance of critical functions as they are intrinsically linked to
the critical functions that an institution performs for third parties. Their
identification follows the identification of a critical function.&lt;/p&gt;
&lt;p&gt;Preamble, (8), Delegated regulation 2016/778&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;IT within an organization is certainly offering services to one or more of the
business units within that financial institution. Once the company has defined
its critical functions (or for DORA, "critical or important functions"), then
the company will need to create a mapping of all assets and services that are
needed to realize that function.&lt;/p&gt;
&lt;p&gt;Out of that mapping, it is very well possible that several IT services will be
considered &lt;em&gt;critical services&lt;/em&gt;. I'm myself involved in the infrastructure side
of things, which are often shared services. The delegated regulation already
points to it, and a somewhat older guideline from the &lt;a href="https://www.fsb.org/2013/07/r_130716a/"&gt;Financial Stability
Board&lt;/a&gt; has the following to say about
critical shared services:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a critical shared service has the following elements:
(i) an activity, function or service is performed by either an internal unit, a separate legal
entity within the group or an external provider;
(ii) that activity, function or service is performed for one or more business units or legal
entities of the group;
(iii) the sudden and disorderly failure or malfunction would lead to the collapse of or
present a serious impediment to the performance of, critical functions.&lt;/p&gt;
&lt;p&gt;FSB guidance on identification of critical functions and critical shared
services&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For IT organizations, it is thus most important to focus on the services they
offer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition of critical or important function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within &lt;a href="https://eur-lex.europa.eu/eli/reg/2022/2554/oj/eng"&gt;DORA&lt;/a&gt;, the
definition of &lt;em&gt;critical or important function&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(22) ‘critical or important function’ means a function, the disruption of
which would materially impair the financial performance of a financial
entity, or the soundness or continuity of its services and activities, or
the discontinued, defective or failed performance of that function would
materially impair the continuing compliance of a financial entity with the
conditions and obligations of its authorisation, or with its other
obligations under applicable financial services law;&lt;/p&gt;
&lt;p&gt;Article 3, (22), DORA&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we compare this definition with the previous ones about critical functions,
we notice that it is extended with an evaluation of the impact towards the
company - rather than the market. I think it is safe to say that this is the
&lt;em&gt;or important&lt;/em&gt; part of the &lt;em&gt;critical or important function&lt;/em&gt;: whereas a
function is critical if its discontinuance has market impact, a function is
important if its discontinuance causes material impairment towards the company
itself.&lt;/p&gt;
&lt;p&gt;Hence, we can consider a critical or important function as being either market
impact (critical) or company impact (important), but retaining externally
offered (function).&lt;/p&gt;
&lt;p&gt;This more broad definition does mean that DORA's regulation puts more
expectations forward than previous regulation, which is one of the reasons
that DORA is that impactful to financial institutions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Implications towards IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From the above, I'd wager that IT itself is not a "critical or important
function", but IT offers services which could be supporting critical or
important functions. Hence, it is necessary that the company has a good
mapping of the functions and their underlying services, operations and
systems. From that mapping, we can then see if those underlying services are
crucial for the function or not. If they are, then we should consider those as
critical or important systems.&lt;/p&gt;
&lt;p&gt;This mapping is mandated by DORA as well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Financial entities shall identify all information assets and ICT assets,
including those on remote sites, network resources and hardware equipment,
and &lt;em&gt;shall map those considered critical&lt;/em&gt;. They shall map the configuration of
the information assets and ICT assets and the links and interdependencies
between the different information assets and ICT assets.&lt;/p&gt;
&lt;p&gt;Article 8, (4), DORA&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;as well as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As part of the overall business continuity policy, financial entities shall
conduct a business impact analysis (BIA) of their exposures to severe
business disruptions. Under the BIA, financial entities shall assess the
potential impact of severe business disruptions by means of quantitative and
qualitative criteria, using internal and external data and scenario
analysis, as appropriate. The BIA shall consider the criticality of
identified and mapped business functions, support processes, third-party
dependencies and information assets, and their interdependencies. Financial
entities shall ensure that ICT assets and ICT services are designed and used
in full alignment with the BIA, in particular with regard to adequately
ensuring the redundancy of all critical components.&lt;/p&gt;
&lt;p&gt;Article 11, paragraph 2, DORA&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In more complex landscapes, it is very well possible that the mapping is a
multi-layered view with different types of systems or services in between,
which could make the effort to identify services as being critical or
important quite challenging. &lt;/p&gt;
&lt;p&gt;For instance, it could be that the IT organization has a service catalog, but
that this service catalog is too broadly defined to use the indication of
critical or important. Making a more fine-grained service catalog will be
necessary to properly evaluate the dependencies, but that also implies that
your business (who has defined their critical or important functions) will
need to indicate which fine-grained service they are depending on, rather than
the high-level services.&lt;/p&gt;
&lt;p&gt;In later posts, I'll probably dive deeper into this layered view.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to get in touch on
&lt;a href="https://discuss.systems/@infrainsight"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;
</content><category term="Regulation"></category><category term="dora"></category></entry><entry><title>Digital Operational Resilience Act</title><link href="https://blog.siphos.be/2025/01/digital-operational-resilience-act/" rel="alternate"></link><published>2025-01-12T22:12:00+01:00</published><updated>2025-01-12T22:12:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2025-01-12:/2025/01/digital-operational-resilience-act/</id><summary type="html">&lt;p&gt;One of the topics that most financial institutions are (still) currently
working on, is their compliance with a European legislation called
&lt;a href="https://eur-lex.europa.eu/eli/reg/2022/2554/oj/eng"&gt;DORA&lt;/a&gt;.  This abbreviation,
which stands for "Digital Operational Resilience Act", is a European
regulation. European regulations apply automatically and uniformly across all EU
countries. This is unlike another recent legislation called
&lt;a href="https://eur-lex.europa.eu/eli/dir/2022/2555/oj/eng"&gt;NIS2&lt;/a&gt;, the "Network and
Information Security" directive. As a EU directive, NIS2 requires the EU
countries to formulate the directive into local law. As a result, 
different EU countries can have a slightly different implementation.&lt;/p&gt;
&lt;p&gt;The DORA regulation applies to the EU financial sector, and has some strict
requirements in it that companies' IT stakeholders are affected by. It doesn't
often sugar-coat things like some frameworks do. This has the advantage that
its "interpretation flexibility" is quite reduced - but not zero of course.
Yet, that advantage is also a disadvantage: financial entities might have
had different strategies covering their resiliency, and now need to adjust their
strategy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;One of the topics that most financial institutions are (still) currently
working on, is their compliance with a European legislation called
&lt;a href="https://eur-lex.europa.eu/eli/reg/2022/2554/oj/eng"&gt;DORA&lt;/a&gt;.  This abbreviation,
which stands for "Digital Operational Resilience Act", is a European
regulation. European regulations apply automatically and uniformly across all EU
countries. This is unlike another recent legislation called
&lt;a href="https://eur-lex.europa.eu/eli/dir/2022/2555/oj/eng"&gt;NIS2&lt;/a&gt;, the "Network and
Information Security" directive. As a EU directive, NIS2 requires the EU
countries to formulate the directive into local law. As a result, 
different EU countries can have a slightly different implementation.&lt;/p&gt;
&lt;p&gt;The DORA regulation applies to the EU financial sector, and has some strict
requirements in it that companies' IT stakeholders are affected by. It doesn't
often sugar-coat things like some frameworks do. This has the advantage that
its "interpretation flexibility" is quite reduced - but not zero of course.
Yet, that advantage is also a disadvantage: financial entities might have
had different strategies covering their resiliency, and now need to adjust their
strategy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;History of DORA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Officially called the Regulation (EU) 2022/2554, DORA was proposed as a new
regulatory framework in September 2020. It aimied to further strengthen the
digital operational resilience of the financial sector. "Operational resilience"
here focuses strongly on cyber threat resilience and operational risks
like IT disasters. On January 16th 2023, the main DORA regulation
&lt;a href="https://www.esma.europa.eu/esmas-activities/digital-finance-and-innovation/digital-operational-resilience-act-dora"&gt;entered into
force&lt;/a&gt; 
and will apply as of January 17th 2025. Yes, that's about now.&lt;/p&gt;
&lt;p&gt;Alongside the main DORA text, additional standards are being developed too.
These are &lt;a href="https://www.eba.europa.eu/regulation-and-policy/operational-resilience"&gt;Regulatory Technical
Standards&lt;/a&gt;
that detail requirements of one or more articles within DORA. The one I
currently come into contact with the most is the &lt;a href="https://www.eba.europa.eu/activities/single-rulebook/regulatory-activities/operational-resilience/regulatory-technical-standards-ict-risk-management-framework-and-simplified-ict-risk-management"&gt;RTS on ICT risk management
framework&lt;/a&gt;.
This RTS elaborates on various requirements close to my own expertise. However,
other RTS documents are also on my visor to read through, such as the technical
standard on &lt;a href="https://www.eba.europa.eu/activities/single-rulebook/regulatory-activities/operational-resilience/regulatory-technical-standards-policy-ict-services-supporting-critical-or-important-functions"&gt;ICT services supporting critical or important functions provided by
ICT third-party service
providers&lt;/a&gt;
and &lt;a href="https://www.eba.europa.eu/activities/single-rulebook/regulatory-activities/operational-resilience/joint-regulatory-technical-subcontracting"&gt;subcontracting ICT
services&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During the development of the DORA regulation and these technical standards,
various stakeholders were consulted. The European Supervisory Authorities (ESAs) were 
of course primary stakeholders here, but other stakeholders could also provide
their feedback. This feedback, and the answers or reactions of it by the
legislative branch, help out in understanding parts of the regulation ("am I
reading this right") as well as conveying the understanding of the regulatory
branch about what is stated ("does the regular understand what they are
asking").&lt;/p&gt;
&lt;p&gt;DORA is not a first of course. The moment you start reading the regulation, you
notice that it amends previous regulations. These were predecessors, so should
also be seen as part of the "history" of DORA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://eur-lex.europa.eu/eli/reg/2009/1060/oj/eng"&gt;Regulation (EC) No
  1060/2009&lt;/a&gt;, which
  regulates the credit rating agencies.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eur-lex.europa.eu/eli/reg/2012/648/oj/eng"&gt;Regulation (EU) No
  648/2012&lt;/a&gt;, covering
  regulation on derivatives, central counter parties, and trade repositories.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eur-lex.europa.eu/eli/reg/2014/600/oj/eng"&gt;Regulation (EU) No
  600/2014&lt;/a&gt;, regulating the
  markets in financial instruments.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eur-lex.europa.eu/eli/reg/2014/909/oj/eng"&gt;Regulation (EU) No
  909/2014&lt;/a&gt;, which focuses on
  securities settlements and central securities depositories.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eur-lex.europa.eu/eli/reg/2016/1011/oj/eng"&gt;Regulation (EU)
  2016/1011&lt;/a&gt;, focusing on
  benchmarks in financial instruments and financial contracts, and measuring
  performance of investment funds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, these are all market oriented regulations, and while many of these do
sporadically refer to an operational resilience aspect, they require a
significant understanding of that financial market to begin with, which isn't
the case for DORA. But DORA wasn't the first to be more IT oriented.&lt;/p&gt;
&lt;p&gt;The first part of the DORA regulation provides context into the actual
legislative articles (which only start one-third into the document). It provides
references to previous publications or legislation that are more IT (or cyber
threat) oriented. This first part is called the "preamble" in EU legislation.&lt;/p&gt;
&lt;p&gt;In this preamble, paragraph 15 references &lt;a href="https://eur-lex.europa.eu/eli/dir/2016/1148/oj/eng"&gt;Directive (EU)
2016/1148&lt;/a&gt; as the
first broad cybersecurity framework enacted at EU level. It covers a high common
level of security of network and information systems. Yes, that's the "NIS" directive
that recently got a new iteration: &lt;a href="https://eur-lex.europa.eu/eli/dir/2022/2555/oj/eng"&gt;Directive (EU)
2022/2555&lt;/a&gt;, aka NIS2.
Plenty of other references exist as well. Sometimes these refer to the
legislation that covers certain markets, as listed before. Other references
focus on the supervisory bodies. Many references are towards other legislation
that provides definitions for the used terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure of DORA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main DORA legislation (so excluding the regulatory technical standards)
covers 64 articles, divided into 9 chapters. But as mentioned earlier, it 
has a quite sizeable preamble that covers context, motivation and interpretation
details for the legislative text. This allows for improved interpretation
of the articles themselves.&lt;/p&gt;
&lt;p&gt;More specifically, such a preamble covers the legal basis of the legislation,
and the objectives why the legislation came to be. I found &lt;a href="https://fabianbohnenberger.com/2024/04/10/how-to-read-eu-law/comment-page-1/"&gt;How to read EU
law&lt;/a&gt;
by Fabian Bohnenberger to be a very good and quick-to-read overview of how an EU
legislative text is structured. The DORA preamble covers 106 paragraphs already,
and they're not even the actual legislative articles.&lt;/p&gt;
&lt;p&gt;So, how are the legislative articles themselves structured like?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter I - General provisions&lt;/em&gt; defines what the purpose of the legislation is
  and its structure (art 1), where the legislation applies to (art 2), 65
  definitions used throughout the legislation (art 3) and the notion that
  proportionality applies (art 4).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter II - ICT risk management&lt;/em&gt; tells the in-scope markets and institutions
  how to govern and organize themselves to cover risk management (art 5), what
  the risk management framework should do (art 6), that they need to have
  fit-for-purpose tooling and procedures to cover risk management (art 7), that
  they need to properly identify risks towards their assets (art 8), need to
  take sufficient measures to protect themselves against various threats (art
  9), be able to detect anomalous activities (art 10), have a response and
  recovery process in place (art 11), have backup/restore processes in place
  (art 12), ensure knowledge of the employees is sound (art 13), be able to
  communicate properly (art 14) and follow regulatory technical standards for
  ICT risk management (art 15). In the end, art 16 covers the requirements for
  smaller financial institutions (as DORA differentiates requirements based on
  the impact, size and some other criteria).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter III - ICT-related incident management, classification and reporting&lt;/em&gt;
  describes how to handle ICT incidents (art 17), how to classify these
  incidents and threats (art 18), what reporting expectations exist (art 19),
  and the standardization on the reporting (art 20). The reported incidents are
  centralized (art 21), supervisors will provide answers to the reports (art
  22), and art 23 informs us for which incidents the above is all applicable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter IV - Digital operational resilience testing&lt;/em&gt; covers the testing of
  the operational resilience. First, general requirements are provided (art 24),
  after which DORA covers the testing of tools and systems (art 25), mandatory
  use of penetration testing (art 26), and how these threat-led penetration
  tests (TLPTs) are carried out (art 27).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter V - Managing of ICT third-party risk&lt;/em&gt; further divulges in managing
  threats related to outsourcing and use of third parties. Art 28 covers the
  general principles, whereas art 29 covers the potential concentration risk
  (aka "if everyone depends on this third party, then..."). Contractual
  expectations are covered in art 30. Further, this chapter covers the
  introduction of an oversight framework for large, critical third-party service
  providers. Art 31 designates when a third-party service provider is deemed
  critical, art 32 covers the oversight structure, art 33 introduces the
  role of the Lead Overseer(s), their operational coordination (art 34), their
  power (art 35), and what their capabilities are outside of the EU (art 36).
  Art 37 covers how the overseer can receive information, if and how
  investigations take place (art 38), how inspections are handled (art 39), how
  this relates to existing oversights (art 40), how the conduct of oversight
  activities is handled (art 41), how authorities follow-up on overseer
  activities (art 42), who will pay for the oversight activities (art 43) and
  the international cooperation amongst regulatory/supervisory bodies (art 44).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter VI - Information-sharing arrangements&lt;/em&gt; has a single article, art 45,
  on threat/intelligence information sharing amongst the financial institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter VII - Competent authorities&lt;/em&gt; assigns the appropriate authorities
  towards the various financial institutions (art 46), how these authorities
  cooperate with others (art 47), and how they cooperate amongst themselves (art
  48). Cross-sector exercises are covered in art 49, and the penalties and
  remedial measures are covered in art 50. Art 51 is how/when administrative
  penalties/measures are imposed, art 52 is when criminal liabilities where
  found. Art 53 requires EU member states to notify the EU institutions about
  related legislation/provisions, art 54 documents how administrative penalties
  are published. Art 55 confirms the professional secrecy of the authorities,
  and art 56 covers the data protection provisions for the authorities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter VIII - Delegated acts&lt;/em&gt; has one article (art 57) covering by whom this
  legislation is exercised (role of the Commission, Parliament, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter IX - Transitional and final provisions&lt;/em&gt; is a "the rest" chapter. It
  covers a review of the law and implementations by January 17th 2028 (art 58),
  and then many amendments to existing regulations to make them aligned and
  consistent with the DORA legislation (art 59 - 63). The last article, art 64,
  describes when DORA comes into force and when it shall apply.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For me, chapters II (risk management), IV (resilience testing) and V (third
party risk) are the most interesting as they cover expectations for many IT
processes or controls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regulatory Technical Standards&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within the DORA legislation, references are made to regulatory technical
standards that need to be drafted up. The intention of the regulatory technical
standards is to further elaborate on the expectations and requirements of DORA.
These RTS documents also have legislative power (hence the "regulatory" in the 
name) and are important to track too.&lt;/p&gt;
&lt;p&gt;The RTS that covers the ICT risk framework from article 15 is one that has a
strong IT orientation with it. Like the EU legislative texts, it holds 
a lot of context to begin with. The draft publications also cover the
feedback received and the answers/results from that feedback. It is unlikely
that these will be found in the final published RTS' though.&lt;/p&gt;
&lt;p&gt;The current &lt;a href="https://www.eba.europa.eu/sites/default/files/2024-01/bf5a2976-1a48-44f3-b5a7-56acd23ba55c/JC%202023%2086%20-%20Final%20report%20on%20draft%20RTS%20on%20ICT%20Risk%20Management%20Framework%20and%20on%20simplified%20ICT%20Risk%20Management%20Framework.pdf"&gt;JC 2023 76 - Final report on draft RTS on ICT Risk Management
Framework and on simplified ICT Risk Management
Framework&lt;/a&gt;
has the actual technical standard between pages 45 and 89. It too uses
chapters to split the text up a bit. After art 1, covering the overall risk
profile and complexity, we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter I - ICT security policies, procedures, protocols, and tools&lt;/em&gt; contains
  significant input for various IT processes and domains. It is further subdivided
  into sections:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Section I&lt;/em&gt; covers what should be in the ICT security policies (art 2),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section II&lt;/em&gt; describes the details of the ICT risk management framework (art 3), &lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section III - ICT Asset Management&lt;/em&gt; covers the ICT asset management expectations (art
  4) and ICT asset management process (art 5), &lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section IV - Encryption and cryptography&lt;/em&gt; covers cryptography expectations (art 6 and
  7), &lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section V - ICT Operations Security&lt;/em&gt; handles the ICT operations policies
    (art 8), capacity and performance management (art 9), vulnerability and
    patch management (art 10), data and system security (art 11), logging
    expectations (art 12), &lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section VI - Network security&lt;/em&gt; is about the network security expectations
    (art 13), and in-transit data protection measures (art 14),&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Section VII - ICT project and change management&lt;/em&gt; covers the ICT project
    management (art 15), ICT development and maintenance activities (art 16),
    and change management (art 17),&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Section VIII&lt;/em&gt; handles physical security measures (art 18)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter II - Human Resources Policy and Access Control&lt;/em&gt; handles HR policies (art 19), identity management
  (art 20), access control (art 21).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Chapter III - ICT-related incident detection and response&lt;/em&gt; , incident management (art 22), anomalous
  activity detection and response (art 23), business continuity (art 24 and 25),
  ICT response and recovery (26)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can read from the titles, these are more specific. Don't think "Oh, it is
just a single article" about a subject. Some articles span more than a full
page. For instance, Article 13 on network security has 13 sub-paragraphs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DORA for architects&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I think that the DORA legislation is a crucial authority to consider when
you are developing internal policies for EU-based financial institutions. I've
mentioned the use of
&lt;a href="https://blog.siphos.be/2022/08/getting-lost-in-the-frameworks/"&gt;frameworks&lt;/a&gt; in
the past, which can inspire companies in the development of their own policies.
Companies should never blindly copy these frameworks (or legislative
requirements) into a policy, because then your policy becomes a mess of
overlapping or sometimes even contradictory requirements. Instead, policies
should refer to these authorities when relevant, allowing readers to understand
which requirements are triggered by which source.&lt;/p&gt;
&lt;p&gt;When you're not involved in the development of policies, having a read through
some of the DORA texts might be still sensible as it gives a grasp on what
requirements are pushed to your company. And while we're at it, do the same for
the NIS2 documents, because even if your company is in scope of DORA, NIS2 still
applies (DORA is a specialized law, so takes precedence over what NIS2 asks, but
if DORA doesn't cover a topic and NIS2 does, then you still have to follow
NIS2).&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to get in touch on
&lt;a href="https://discuss.systems/@infrainsight"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;</content><category term="Regulation"></category><category term="dora"></category></entry><entry><title>Diagrams are no communication channel</title><link href="https://blog.siphos.be/2024/09/diagrams-are-no-communication-channel/" rel="alternate"></link><published>2024-09-05T22:00:00+02:00</published><updated>2024-09-05T22:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2024-09-05:/2024/09/diagrams-are-no-communication-channel/</id><summary type="html">&lt;p&gt;IT architects generally use architecture-specific languages or modeling
techniques to document their thoughts and designs. &lt;a href="https://www.opengroup.org/archimate-forum/archimate-overview"&gt;ArchiMate&lt;/a&gt;,
the framework I have the most experience with, is a specialized enterprise
architecture modeling language. It is maintained by The Open Group, an organization
known for its broad architecture framework titled TOGAF.&lt;/p&gt;
&lt;p&gt;My stance, however, is that architects should not use the diagrams from their
architecture modeling framework to convey their message to every stakeholder
out there...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT architects generally use architecture-specific languages or modeling
techniques to document their thoughts and designs. &lt;a href="https://www.opengroup.org/archimate-forum/archimate-overview"&gt;ArchiMate&lt;/a&gt;,
the framework I have the most experience with, is a specialized enterprise
architecture modeling language. It is maintained by The Open Group, an organization
known for its broad architecture framework titled TOGAF.&lt;/p&gt;
&lt;p&gt;My stance, however, is that architects should not use the diagrams from their
architecture modeling framework to convey their message to every stakeholder
out there...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;An enterprise framework for architects&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Certainly, using a single modeling language like ArchiMate is important.
It allows architects to use a common language, a common framework,
in which they can convey their ideas and designs. When collaborating on
the same project, it would be unwise to use different modeling techniques
or design frameworks among each other.&lt;/p&gt;
&lt;p&gt;By standardizing on a single framework &lt;em&gt;for a particular purpose&lt;/em&gt;, a company
can optimize their efforts surrounding education and documentation. If several 
architecture frameworks are used for the same purpose, inefficiencies arise.
Supporting tooling can also be selected (such as 
&lt;a href="https://www.archimatetool.com/"&gt;Archi&lt;/a&gt;), which has specialized features to support
this framework.  The more architects are fluent in a common framework, the less
likely ambiguity or misunderstandings occur about what certain architectures or
designs want to present.&lt;/p&gt;
&lt;p&gt;Now, I highlighted "&lt;em&gt;for a particular purpose&lt;/em&gt;", because that architecture
framework isn't the goal, it's a means.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Domain-specific language, also in architecture&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In larger companies, you'll find architects with different specializations
and focus areas. A common set is to have architects at different levels
or layers of the architecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enterprise architects focus on the holistic and strategic level, &lt;/li&gt;
&lt;li&gt;domain architects manage the architecture for one or more domains (a means of splitting the complexity
of a company, often tied to business domains), &lt;/li&gt;
&lt;li&gt;solution or system architects focus on the architecture for specific projects or solutions, &lt;/li&gt;
&lt;li&gt;security architects concentrate on the cyber threats and protection measures, &lt;/li&gt;
&lt;li&gt;network architects look at the network design, flows and flow controls,
etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Architecture frameworks are often not meant to support all levels. ArchiMate
for instance, is tailored to enterprise and domain level in general. It also
supports solution or system architecture well when it focuses
on applications. Sure, other architecture layers can be expressed as well, but
after a while, you'll notice that the expressivity of the framework lacks the
details or specifics needed for those layers.&lt;/p&gt;
&lt;p&gt;It is thus not uncommon that, at a certain point, architects drop one framework 
and start using another. Network architecture and design is expressed differently
than the ICT domain architecture. Both need to 'plug into each other',
because network architects need to understand the larger picture they operate
in, and domain architects should be able to read network architecture design
and relate it back to the domain architecture.&lt;/p&gt;
&lt;p&gt;Such a transition is not only within IT. Consider city planning and
housing units, where architects design new community areas and housing.
These designs need to be well understood by the architects, who are
responsible for specific specializations such as utilities, transportation,
interior, landscaping, and more. They use different ways of designing, but
make sure it is understandable (and often even standardized) by the others.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your schematic is not your presentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've seen architects who are very satisfied with their architectural
design: they want nothing more than to share this with their
(non-architect) stakeholders in all its glory. And while I do agree that lead
engineers, for instance, should be able to understand architecture drawings, the
schematics themselves shouldn't be the presentation material.&lt;/p&gt;
&lt;p&gt;And definitely not towards higher management.&lt;/p&gt;
&lt;p&gt;When you want to bring a design to a broader community, or to stakeholders
with different backgrounds or roles, it is important to tell your story
in an easy-to-understand way. Just like building architects would create
physical mock-ups at scale to give a better view of a building, IT
architects should create representative material to expedite
presentations and discussions.&lt;/p&gt;
&lt;p&gt;Certainly, you will lose a lot of insight compared to the architectural
drawings, but you'll get much better acceptance by the community.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category></entry><entry><title>Sustainability in IT</title><link href="https://blog.siphos.be/2022/09/sustainability-in-IT/" rel="alternate"></link><published>2022-09-25T13:00:00+02:00</published><updated>2022-09-25T13:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-09-25:/2022/09/sustainability-in-IT/</id><summary type="html">&lt;p&gt;For one of the projects I'm currently involved in, we want to have a better
view on sustainability within IT and see what we (IT) can contribute in light
of the sustainability strategy of the company. For IT infrastructure, one would
think that selecting more power-efficient infrastructure is the way to go, as
well as selecting products whose manufacturing process takes special attention
to sustainability. &lt;/p&gt;
&lt;p&gt;There are other areas to consider as well, though. Reusability of IT
infrastructure and optimal resource consumption are at least two other
attention points that deserve plenty of attention. But let's start at the
manufacturing process...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;For one of the projects I'm currently involved in, we want to have a better
view on sustainability within IT and see what we (IT) can contribute in light
of the sustainability strategy of the company. For IT infrastructure, one would
think that selecting more power-efficient infrastructure is the way to go, as
well as selecting products whose manufacturing process takes special attention
to sustainability. &lt;/p&gt;
&lt;p&gt;There are other areas to consider as well, though. Reusability of IT
infrastructure and optimal resource consumption are at least two other
attention points that deserve plenty of attention. But let's start at the
manufacturing process...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Certifications for products and companies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Eco certifications are a good start in the selection process. By selecting
products with the right certification, companies can initiate their sustainable
IT strategy with a good start. Such certifications look at the product and
manufacturing, and see if they use proper materials, create products that
can have extended lifetimes in the circular (reuse) economy, ensure the
manufacturing processes use renewable energy and do not have harmful 
emissions, safeguard clean water, etc.&lt;/p&gt;
&lt;p&gt;In the preliminary phase I am right now, I do not know yet which 
certifications make most sense to pursue and request. Sustainability is
becoming big business, so plenty of certifications exist as well. From a
cursory search, I'd reckon that the following certifications are worth more
time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ecovadis.com/"&gt;EcoVadis&lt;/a&gt; provides business sustainability ratings
  that not only cover the ecological aspect, but also social and ethical
  performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.iso.org/iso-14001-environmental-management.html"&gt;ISO 14001&lt;/a&gt;
  covers environmental management, looking at organizations' processes and
  systematic improvements contributing to sustainability.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.carbonneutral.com/"&gt;Carbon Neutral&lt;/a&gt; focus on transparency
  in measurements and disclosure of emissions, and how the company is
  progressing in their strategy to reduce the impact on the environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tcocertified.com/"&gt;TCO Certified&lt;/a&gt; attempts to address all
  stages of a manufacturing process, from material selection over social 
  responsibility and hazardous substances up to electronic waste and 
  circular economy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.energystar.gov/"&gt;Energy Star&lt;/a&gt; focuses on energy efficiency,
  and tries to use standardized methods for scoring appliances (including
  computers and servers).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Power efficiency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A second obvious part is on power efficiency. Especially in data center
environments, which is the area that I'm interested in, power efficiency
also influences the data center's capability of providing sufficient
cooling to the servers and appliances. Roughly speaking, a 500 Watt
server generates twice as much heat as a 250 Watt server. Now, that's
oversimplifying, but for calculating heat dissipation in a data center,
the maximum power of infrastructure is generally used for the calculations.&lt;/p&gt;
&lt;p&gt;Now, we could start looking for servers with lower power consumption. But
a 250 Watt server is most likely going to be less powerful (computing-wise)
than a 500 Watt server. Hence, power efficiency should be considered in
line with the purpose of the server, and thus also the workloads that it
would have to process.&lt;/p&gt;
&lt;p&gt;We can use benchmarks, like &lt;a href="https://www.spec.org/cpu2017/"&gt;SPEC's CPU 2017&lt;/a&gt;
or &lt;a href="https://www.spec.org/benchmarks.html"&gt;SPEC's Cloud IaaS 2018&lt;/a&gt; benchmarks,
to compare the performance of systems. Knowing the server's performance
for given workloads and the power consumption, allows architects to optimize
the infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heat management (and more) in the data center&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A large consumer of power in a data center environment are the environmental
controls, with the cooling systems taking a big chunk out of the total
power consumption. Optimizing the heat management in the data center has a
significant impact on the power consumption. Such optimizations are not solely
about reducing the electricity bill, but also about reusing the latent heat
for other purposes. For instance, data center heat can be used to heat up
nearby buildings.&lt;/p&gt;
&lt;p&gt;A working group of the European Commission, the European Energy Efficiency
Platform (E3P), publishes an annual set of best practices in the &lt;a href="https://e3p.jrc.ec.europa.eu/publications/2022-best-practice-guidelines-eu-code-conduct-data-centre-energy-efficiency"&gt;EU Code
of Conduct on Data Center Energy Efficiency&lt;/a&gt;
which covers areas such as airflow design patterns, operating temperature
and humidity ranges, power management features in servers and appliances, 
infrastructure design aspects (like virtualization and appropriate, but no
over-engineered redundancy), etc.&lt;/p&gt;
&lt;p&gt;This practice goes much beyond the heat management alone (and is worth
a complete read), covering the complete data center offering. Combining
these practices with other areas of data center design (such as redundancy
levels, covered by data center tiering) allows for companies that are looking
at new data centers to overhaul their infrastructure and be much better
prepared for sustainable IT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Circular ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another part that often comes up in sustainability measures is how reusable
the infrastructure components are after their "first life". Infrastructure
systems, which frequently renew after 4 to 5 years of activity, can be resold
rather than destroyed. The same can be said for individual components.&lt;/p&gt;
&lt;p&gt;Companies that deal with sensitive data regularly employ "Do Not Return" clauses
in the purchases of storage devices. Disks are not returned if they are
faulty, or just swapped for higher density disks. Instead, they are routinely
destroyed to make sure no data leakage occurs.&lt;/p&gt;
&lt;p&gt;Instead of destroying otherwise perfect disks (or disks that still have
reusable components) companies could either opt for degaussing (which still
renders the disk unusable, but has better recyclability than destroyed
disks) or data wiping (generally through certified methods that guarantee
the data cannot be retrieved).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extended lifecycle&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Systems are often working perfectly beyond their 4 to 5 year lifespans.
Still, these systems are process-wise automatically renewed to get more
efficient and powerful systems in place. But that might not always be necessary
- beyond even the circular ecosystem remarks above (where such systems could be
resold), these systems can even get extended lifecycle within the company.&lt;/p&gt;
&lt;p&gt;If there is no need for a more powerful system, and the efficiency of
the system is still high (or the efficiency can be improved through
minor updates), companies can seek out ways to prolong the use of the
systems. In previous projects, I advised that big data nodes can
perfectly remain inside the cluster after their regular lifetime, as the
platform software (Hadoop) can easily cope with failures if those would
occur.&lt;/p&gt;
&lt;p&gt;Systems can also be used to host non-production environments or support
lab environments. Or they can be refurbished to ensure maximal efficiency
while still being used in production. Microsoft for instance has a program
called &lt;a href="https://customers.microsoft.com/en-us/story/1431789627332547010-microsoft-circular-centers"&gt;Microsoft Circular Centers&lt;/a&gt;
which aims at a zero-waste sustainability within the data center, through
reuse, repurpose and recycling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Right-sizing the infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Right-sizing is to select and design infrastructure to deal with the
workload, but not more. Having a set of systems at full capacity is
better than having twice as many systems at half capacity, as this leads
to power inefficiencies.&lt;/p&gt;
&lt;p&gt;To accomplish right-sizing isn't as easy as selecting the right server
for a particular workload. Workload is distributed, and systems are
virtualized. Virtualization allows for much better right-sizing as you
can distribute workload more optimally.&lt;/p&gt;
&lt;p&gt;Companies with large amounts of systems can more efficiently distribute
workload across their systems, making it easier to have a good consumption
pattern. Smaller companies will notice that they need to design for
the burst and maximum usage, whereas the average usage is far, far below
that threshold. &lt;/p&gt;
&lt;p&gt;Using cloud resources can help to deal with bursts and higher demand, while
still having resources on-premise to deal with the regular workload. Such
hybrid designs, however, can be complex, so make sure to address this with
the right profiles (yes, I'm making a stand for architects here ;-)&lt;/p&gt;
&lt;p&gt;Standardizing your infrastructure also makes this easier to accomplish.
If the vast majority of servers are of the same architecture, and you
standardize on as few operating systems, programming languages and what
not, you can more easily distribute workload than when these systems
have different architectures and purposes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Automated workload and power management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Large environments will regularly have servers and infrastructure that is not
continuously used at near full capacity. Workloads are frequently following a
certain curve, such as higher demand during the day and lower at night.
Larger platforms use this curve to schedule appropriate workload (like
running heavy batch workload at night while keeping the systems available
for operational workload during the day) so that the resources are more
optimally used.&lt;/p&gt;
&lt;p&gt;By addressing workload management and aligning power management, companies
can improve their power usage by reducing active systems when there are less
resource needs. This can be done gradually, such as putting CPUs in lower
power modes (CPU power takes roughly 30% of a system's total power usage),
but can expand to complete hosts being put in idle state.&lt;/p&gt;
&lt;p&gt;We can even make designs where servers are shut down when unused. While
this is frequently frowned upon, citing possible impact on hardware
failures as well as reduced reactivity to sudden workload demand, proper
shutdown techniques do offer significant power savings (as per a
research article titled &lt;a href="https://www.researchgate.net/publication/323356951_Quantifying_the_Impact_of_Shutdown_Techniques_for_Energy-Efficient_Data_Centers"&gt;Quantifying the Impact of Shutdown Techniques
for Energy-Efficient Data Centers&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sustainability within IT focuses on several improvements and requirements.
Certification helps in finding and addressing these, but this is not
critical in any company's strategy. Companies can address sustainability
easily without certification, but with proper attention and design.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1573941352844464128"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="sustainability"></category></entry><entry><title>Getting lost in the frameworks</title><link href="https://blog.siphos.be/2022/08/getting-lost-in-the-frameworks/" rel="alternate"></link><published>2022-08-26T13:00:00+02:00</published><updated>2022-08-26T13:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-08-26:/2022/08/getting-lost-in-the-frameworks/</id><summary type="html">&lt;p&gt;The IT world is littered with frameworks, best practices, reference
architectures and more. In an ever-lasting attempt to standardize IT,
we often get lost in too many standards or specifications. For consultants,
this is a gold-mine, as they jump in to support companies - for a fee, 
naturally - in adopting one or more of these frameworks or specifications.&lt;/p&gt;
&lt;p&gt;While having references and specifications isn't a bad thing, there are
always pros and cons.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;The IT world is littered with frameworks, best practices, reference
architectures and more. In an ever-lasting attempt to standardize IT,
we often get lost in too many standards or specifications. For consultants,
this is a gold-mine, as they jump in to support companies - for a fee, 
naturally - in adopting one or more of these frameworks or specifications.&lt;/p&gt;
&lt;p&gt;While having references and specifications isn't a bad thing, there are
always pros and cons.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The benefits of frameworks and specifications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main benefit in having a proper framework or specification, for whatever
task is ahead of you, is that you don't have to reinvent the wheel. If a
framework or specification is well-written, with a good scope aligned with
what you need, then using this will speed up implementations.&lt;/p&gt;
&lt;p&gt;Some organizations will even require their outsourcing activities to be
according to a certain framework or specification. This will ensure that the
development results are aligned with how the organization works, improving
integration and validation.&lt;/p&gt;
&lt;p&gt;The integration part is a major aspect of standardization as well. As we've
learned from the more physical equivalents (like paper standards, or standards
in construction) having standardized deliverables ensures for efficient
integration, economical comparisons, and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The downsides of frameworks and specifications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An often cited downside of standardization (and thus also reflecting the use
of frameworks or specifications) is that it stifles innovation, and if the
framework or specification addresses the end product itself, it might result
in the company struggling to find a unique selling proposal.&lt;/p&gt;
&lt;p&gt;Frameworks are also often incomplete. Most of the time, that is by design, as
the framework's authors do not want to remove all of the organization's
inputs. I've talked about &lt;a href="https://en.wikipedia.org/wiki/ITIL"&gt;ITIL&lt;/a&gt; and 
&lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT&lt;/a&gt; in the past already. Both
are frameworks which leave implementation details open to the organization.
For some companies, that means the benefit of quick adoption and efficiency
might become more challenging to attain.&lt;/p&gt;
&lt;p&gt;One of the issues I'm often facing is that there are too many frameworks and
specifications. While any decent one will try to show how it aligns with
other frameworks (while expressing its own benefits, of course), it is very
likely that an organization who adopted one framework will be confronted with
the need (be it through a merger and acquisition, market expansion, insourcing
and outsourcing endeavors or other) to adopt another one. Tailoring an
organization to multiple frameworks is a hassle, especially if there are
requirements for certifications.&lt;/p&gt;
&lt;p&gt;Many companies also easily fall in the trap that they have to adhere to
a framework or specification completely and company-wide. This not only
jeopardizes innovation (as mentioned earlier) but also has the downside of
becoming the target by itself. Rather than focusing on attaining the benefits,
the company focuses on attaining alignment with the framework. This not only
impacts the responsiveness of the organization, but can also lead to employee
dissatisfaction.&lt;/p&gt;
&lt;p&gt;Finally, plenty of frameworks or specifications can only be accessed after
paying serious amounts of money. For documents that are meant to provide
efficiency and improved integrations, this is a major nuisance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There are too damn many&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let me elaborate a bit on the "too many frameworks". I'm currently working
on a resource (that I'll make public later, when it has a bit more body to it)
where I also consider including a maturity approach. Maturity, or maturity
levels, are a way for an organization to measure the current state of something
(in my case, of an IT process) as well as define where the organization wants
to be, and at what pace.&lt;/p&gt;
&lt;p&gt;The maturity model approach that I grew up with is &lt;a href="https://cmmiinstitute.com/"&gt;ISACA's
CMMI&lt;/a&gt; or &lt;em&gt;Capability Maturity Model Integration&lt;/em&gt;.
CMMI prescribes five maturity levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Incomplete (work may or may not be completed, no established goals, processes
  are only partially defined)&lt;/li&gt;
&lt;li&gt;Initial (unpredictable and reactive, work gets completed but often delayed
  and over budget)&lt;/li&gt;
&lt;li&gt;Managed (some management is applied, processes are planned, performed,
  measured and controlled, but there are still lots of issues to address)&lt;/li&gt;
&lt;li&gt;Defined (more proactive than reactive, there are organization-wide standards,
  and guidance exists across projects, programs and portfolios)&lt;/li&gt;
&lt;li&gt;Quantitatively managed (higher degree of measurement and control,
  using quantitative data to determine predictable processes)&lt;/li&gt;
&lt;li&gt;Optimizing (stable and flexible processes, constant state of improving)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While this framework also expresses other things than just the five maturity
levels, this seems to be the most common starting point for maturity
efforts.&lt;/p&gt;
&lt;p&gt;But CMMI isn't the only one out there. Process maturity can also be evaluated
by OMG's &lt;a href="https://www.omg.org/spec/BPMM/1.0/About-BPMM/"&gt;Business Process Maturity Model
(BPMM)&lt;/a&gt; which also defines
five maturity levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Level 1 isn't explicitly defined further, let's keep it at &lt;em&gt;Initial&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Managed&lt;/li&gt;
&lt;li&gt;Standardized&lt;/li&gt;
&lt;li&gt;Predictable&lt;/li&gt;
&lt;li&gt;Innovating&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What about the &lt;a href="https://www.tmmi.org/tmmi-model/"&gt;TMMi model&lt;/a&gt;, which has - 
you guessed it - five maturity levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initial&lt;/li&gt;
&lt;li&gt;Managed&lt;/li&gt;
&lt;li&gt;Defined&lt;/li&gt;
&lt;li&gt;Measured&lt;/li&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also frameworks that specialize for specific domains. Given that
this blog is about IT, I have to mention &lt;a href="https://en.wikipedia.org/wiki/ISO/IEC_33001"&gt;ISO/IEC
33001&lt;/a&gt;, which is a revision of
ISO/IEC 15504, aka the &lt;em&gt;Software Process Improvement and Capability
 Determination (SPICE)&lt;/em&gt; methodology. It has five (well, six if you also
count the "you have not even started" level) maturity models for the
processes as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Incomplete (level 0)&lt;/li&gt;
&lt;li&gt;Performed&lt;/li&gt;
&lt;li&gt;Managed&lt;/li&gt;
&lt;li&gt;Established&lt;/li&gt;
&lt;li&gt;Predictable&lt;/li&gt;
&lt;li&gt;Optimizing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ITIL also has a capability maturity model in it, which also is expressed in
five levels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many frameworks are paywalled&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many larger IT frameworks are also paywalled, making it harder for individuals
to easily get access to them. CMMI v2.0 for instance is not publicly accessible,
whereas its previous version seems to be. ISO is notorious for being hard to
find. ITIL and CObIT are also paywalled.&lt;/p&gt;
&lt;p&gt;While there is plenty of information "out there" on all these standards, 
frameworks and specifications, you need to grab, collect and disseminate all
information separately which is a major nuisance. Many online resources are
also presented by companies (including consultancy firms) which means you
need to take care and know that these resources are opinionated.&lt;/p&gt;
&lt;p&gt;Personally, I try to learn from the publicly available resources in the
following order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get access to as many public resources from the framework owner as possible.
   While this often only gives a general feeling on the framework, it is defined
   by the framework owner itself and the least opinionated (well, the framework
   itself is opinionated, but the resources about it will try to give a full
   feeling on what the framework is about).&lt;/li&gt;
&lt;li&gt;Learn more from online resources that are attempting to combine knowledge
   from multiple sources (like Wikipedia), and from sources that are specialized
   in the domain itself without being commercially affiliated with one specific
   framework.&lt;/li&gt;
&lt;li&gt;Learn more from academic research on the matter (through &lt;a href="https://scholar.google.com"&gt;Google
   Scholar&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Read up on the matter in communities from people that are already
   knowledgeable in the matter.&lt;/li&gt;
&lt;li&gt;Finish with resources available from commercially affiliated companies or
   consultants.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;It is not about the framework but about the benefits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I want to conclude here with the notion that adherence to the frameworks
should be to attain the benefits. I'm currently very strongly involved in
IT asset management endeavors, and while ensuring that the IT asset management
process is as good as possible, I know that there are subdivisions in the 
company where this adherence with a highly optimized IT asset management
process isn't beneficial (especially when this subdivision is completely
separate from the rest and has a much more specific, specialized purpose).&lt;/p&gt;
&lt;p&gt;The scope of the IT asset management process is also not something you want
to address for all possible assets. Some assets are much more involved in risk
processes and cost structures than others, so the "maturity" to attain is
also specific to the scope.&lt;/p&gt;
&lt;p&gt;Always keep in mind what you want to reach and why. &lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1563143657586585603"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="framework"></category><category term="CMMI"></category><category term="ISO"></category></entry><entry><title>Containers are the new IaaS</title><link href="https://blog.siphos.be/2022/05/containers-are-the-new-iaas/" rel="alternate"></link><published>2022-05-21T13:00:00+02:00</published><updated>2022-05-21T13:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-05-21:/2022/05/containers-are-the-new-iaas/</id><summary type="html">&lt;p&gt;At work, as with many other companies, we're actively investing in new
platforms, including container platforms and public cloud. We use Kubernetes
based container platforms both on-premise and in the cloud, but are also very
adamant that the container platforms should only be used for application
workload that is correctly designed for cloud-native deployments: we do not
want to see vendors packaging full operating systems in a container and
then shouting they are now container-ready.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At work, as with many other companies, we're actively investing in new
platforms, including container platforms and public cloud. We use Kubernetes
based container platforms both on-premise and in the cloud, but are also very
adamant that the container platforms should only be used for application
workload that is correctly designed for cloud-native deployments: we do not
want to see vendors packaging full operating systems in a container and
then shouting they are now container-ready.&lt;/p&gt;


&lt;p&gt;Sadly, we notice more and more vendors abusing containerization to wrap their
products in and selling it as 'cloud-ready' or 'container-ready'. For many
vendors, containers allow them to bundle everything as if it were an
appliance, but without calling it an appliance - in our organization, we
have specific requirements on appliances to make sure they aren't just
pre-build systems that lack the integration, security, maintainability and
supportability capabilities that we would expect from an appliance.&lt;/p&gt;
&lt;p&gt;Even developers are occasionally tempted to enlarge container images with a
whole slew of middleware and other services, making it more monolithic
solutions than micro-services, just running inside a container because they
can. I don't feel that this evolution is beneficial (or at least not yet),
because the maintainability and supportability of these images can be very
troublesome.&lt;/p&gt;
&lt;p&gt;This evolution is similar to the initial infrastructure-as-a-service
offerings, where the focus was on virtual machines: you get a platform on top
of which your virtual machines run, but you remain responsible for the virtual
machine and its content. But unlike virtual machines, where many organizations
have standardized management and support services deployed for, containers are
often shielded away or ignored. But the same requirements should be applied to
containers just as to virtual machines.&lt;/p&gt;
&lt;p&gt;Let me highlight a few of these, based on my &lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;Process view of
infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost and licensing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Be it on a virtual machine or in a container, the costs and licensing of the
products involved must be accounted for. For virtual machines, this is often
done through license management tooling that facilitates tracking of software
deployments and consumption. These tools often use agents running on the
virtual machines (and a few run at the hypervisor level so no in-VM agents are
needed).&lt;/p&gt;
&lt;p&gt;Most software products also use licensing metrics that are tailored to
(virtual) hardware (like processors) or deployments (aka nodes, i.e. a
per-operating-system count). Software vendors often have the right to audit
software usage, to make sure companies do not abuse their terms and
conditions. &lt;/p&gt;
&lt;p&gt;Now let's tailor that to a container environment, where platforms like
Kubernetes can dynamically scale up the number of deployments based on the
needs. Unlike more static virtual machine-based deployments, we now have a
more dynamic environment. How do you measure software usage here? Running
software license agents inside containers isn't a good practice. Instead, we
should do license scanning in the images up-front, and tag resources
accordingly. But not many license management tooling is already
container-aware, let alone aligned with a different way of working.&lt;/p&gt;
&lt;p&gt;But "our software license management tooling is not container-ready yet" is
not an adequate answer to software license audits, nor will the people in the
organization that are responsible for license management be happy with such
situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Product lifecycle&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the licensing part, companies also want to track which software
versions are being used: not just for vulnerability management purposes, but
also to make sure the software remains supported and fit for purpose.&lt;/p&gt;
&lt;p&gt;On virtual machines, regular software scanning and inventory setup can be done
to report on the software usage. And while on container environments this can
be easily done at the image level (which software and versions are available in
which containers) this often provides a pre-deployment view, and doesn't tell
us if a certain container is being used or not, nor if additional deployments
have been triggered since the container is launched.&lt;/p&gt;
&lt;p&gt;Again, deploying in-container scanning capabilities seems to be
contra-productive here. Having an end-to-end solution that detects and
registers software titles and products based on the container images, and then
provides insights into runtime deployments (and history) seems to be a better match.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authorization management (and access control)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When support teams need to gain access to the runtime environment (be it for
incident handling, problem management, or other operational tasks) most
companies will already have a somewhat finer-grained authorization system in
place: you don't want to grant full system administrator rights if they aren't
needed.&lt;/p&gt;
&lt;p&gt;For containers, this is often not that easy to accomplish: the design of
container platforms is tailored to situations where you don't want to
standardize on in-container access: runtimes are ephemeral, and support is
handled through logging and metric, with adaptation to the container images
and rolling out new versions. If containers are starting to get used for more
classical workloads, authorization management will become a more active field
to work out.&lt;/p&gt;
&lt;p&gt;Consider a database management system within the container alongside the
vendor software. Managing this database might become a nightmare, especially
if it is only locally accessible (within the container or pod). And before you
yell how horrible such a setup would be for a container platform... yes, but
it is still a reality for some.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Auditing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Auditing is a core part of any security strategy, logging who did what, when,
from where, on what, etc. For classical environments, audit logging, reporting
and analysis are based upon static environment details: IP addresses,
usernames, process names, etc.&lt;/p&gt;
&lt;p&gt;In a container environment, especially when using container orchestration,
these classical details are not as useful. Sure, they will point to the
container platform, but IP addresses are often shared or dynamically assigned.
Usernames are dynamically generated or are pooled resources. Process
identifiers are not unique either.&lt;/p&gt;
&lt;p&gt;Auditing for container platforms needs to consider the container-specific
details, like namespaces. But that means that all the components involved in
the auditing processes (including the analysis frameworks, AI models, etc.)
need to be aware of these new information types.&lt;/p&gt;
&lt;p&gt;In the case of monolithic container usage, this can become troublesome as the
in-container logging often has no knowledge of the container-specific nature,
which can cause problems when trying to correlate information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I only touched upon a few processes here. Areas such as quality assurance and
vulnerability management are also challenges for instance, as is data
governance. None of the mentioned processes are impossible to solve, but
require new approaches and supporting services, which make the total cost of
ownership of these environments higher than your business or management might
expect.&lt;/p&gt;
&lt;p&gt;The rise of monolithic container usage is something to carefully consider. In
the company I work for, we are strongly against this evolution as the enablers
we would need to put in place are not there yet, and would require significant
investments. It is much more beneficial to stick to container platforms for
the more cloud-native setups, and even in those situations dealing with ISV
products can be more challenging than when it is only for internally developed
products.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1527975405730336768"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="kubernetes"></category><category term="container"></category><category term="iaas"></category><category term="infrastructure"></category><category term="virtual-machine"></category></entry><entry><title>Defining what an IT asset is</title><link href="https://blog.siphos.be/2022/02/defining-what-an-it-asset-is/" rel="alternate"></link><published>2022-02-13T13:00:00+01:00</published><updated>2022-02-13T13:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-02-13:/2022/02/defining-what-an-it-asset-is/</id><summary type="html">&lt;p&gt;One of the main IT processes that a company should strive to have in place
is a decent IT asset management system. It facilitates knowing what assets
you own, where they are, who the owner is, and provides a foundation for
numerous other IT processes.&lt;/p&gt;
&lt;p&gt;However, when asking "what is an IT asset", it gets kind off fuzzy...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;One of the main IT processes that a company should strive to have in place
is a decent IT asset management system. It facilitates knowing what assets
you own, where they are, who the owner is, and provides a foundation for
numerous other IT processes.&lt;/p&gt;
&lt;p&gt;However, when asking "what is an IT asset", it gets kind off fuzzy...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Searching for a definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I went on to search for a definition of an "IT asset", my first
thought was to check on the more common reference frameworks out there.
Surely, if they tout the benefits of a well-functioning IT asset management
system and process, they will declare what an IT asset is, right?&lt;/p&gt;
&lt;p&gt;Many of these frameworks do not have their resources publicly available. They do have
enough material online to provide me with the necessary insights.&lt;/p&gt;
&lt;p&gt;The first focus I had was on ITIL, formerly known as the Information
Technology Infrastructure Library. There are quite a few articles on the &lt;a href="https://www.itil-docs.com/blogs/asset-management"&gt;ITIL
Docs&lt;/a&gt; site on IT asset
management, but while they do have some breadcrumbs, they do not offer a full
definition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;as financial management of IT assets is an important sub-process,
  I can derive that assets are either expected to have a financial
  value, or that such economical assets are a large subset of the
  IT assets in general&lt;/li&gt;
&lt;li&gt;assets always have a lifecycle to follow (although I am uncertain
  if this will help in deriving a definition)&lt;/li&gt;
&lt;li&gt;assets support the IT in the strategic and financial decision-making&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I found a &lt;a href="https://itil.it.utah.edu/downloads/ITILv2_Terms_and_Definitions_r2.0_0808.pdf"&gt;Terms and Definitions&lt;/a&gt;
extract that has an asset defined as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"[An IT asset is a] component of a business process. Assets can include
people, accommodation (facilities), computer systems, networks, paper records,
fax machines, etc."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, that definition is too broad for IT asset management.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1800-5.pdf"&gt;NIST 1800-5 IT Asset Management publication&lt;/a&gt;
focuses on technologies that ensure a smooth business operation.
It includes computers, mobile devices, operating systems, applications,
data, and network resources. It later mentions that IT assets include items
such as servers, desktops, laptops, and network appliances. And when it
touches upon the value of IT asset management, it too mentions that it is
about informed business-driven decision-making.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.isaca.org/resources/cobit"&gt;ISACA's CObIT&lt;/a&gt;, IT assets are
frequently mentioned. Here as well, there is a strong indication that they
relate to a financial part and a value delivery part. For instance, in the
objective "Ensured Benefits Delivery", we find:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Create and maintain portfolios of I&amp;amp;T-enabled investment programs, IT
services and IT assets, which form the basis for the current IT budget and
support the I&amp;amp;T tactical and strategic plans"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is an entire objective called "Managed Assets" which covers the bulk of
the IT asset management practice. It again refers to the assets, but focuses
less on the definition of what an IT asset is, and more on what it supports.
For instance:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Manage I&amp;amp;T assets [...] to make sure that their use delivers value at optimal
cost, they remain operational (fit for purpose), and they are accounted for
and physically protected."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the practice of managing an inventory, the references are towards IT assets
that are "required to deliver services and that are owned or controlled by the
organization with an expectation of future benefit".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.techopedia.com/definition/16946/it-asset"&gt;Techopedia&lt;/a&gt; has it
defined as "a piece of software or hardware within an information technology
environment."&lt;/p&gt;
&lt;p&gt;On &lt;a href="https://en.wikipedia.org/wiki/Asset_(computer_security)"&gt;Wikipedia's "Asset (computer
security)"&lt;/a&gt; page, 
the definition refers to the ISO/IEC 13335-1:2004 publication, and summarizes it
as "any data, device, or other component of the environment that supports
information-related activities." It also refers to an older ENISA page
(archived), and looking at the current &lt;a href="https://www.enisa.europa.eu/topics/threat-risk-management/risk-management/current-risk/risk-management-inventory/glossary"&gt;ENISA
Glossary&lt;/a&gt;
it covers the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"[An IT asset is] anything that has value to the organization, its business
operations and their continuity, including Information resources that support
the organization's mission."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ENISA appears to quote ISO/IEC PDTR 13335-1 as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deriving a definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From the search, I think I have some grasp of what an IT asset is. Or at
least, how I look at it. Two main attributes exist that define if something in
an IT environment is an asset or not.&lt;/p&gt;
&lt;p&gt;First, it has to offer or realize an IT value to the environment. Often, 
IT value is about automating (a part of a) business process.&lt;/p&gt;
&lt;p&gt;Second, the asset has to have a value of its own. That value is typically 
economical, but can also be intangible, like intellectual property.&lt;/p&gt;
&lt;p&gt;Through these two attributes, I would create the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An IT asset is anything that provides visible IT value towards the
organization, and has an intrinsic value on its own. A visible IT value is
generally in support of IT automation. An intrinsic value can be economical,
but also a capital asset like intellectual property tied to the IT asset
itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some processes make a distinction between regular IT assets and more critical
(or important) assets. In most cases, I read that it focuses on the impact that
a defect or unavailability of the asset has on the organization. So, a critical
IT asset gets the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A critical (or important) IT asset is an IT asset that directly realizes a
(business or IT) service, where the business impact (in case of exploited risk
or unavailability) is non-trivial.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This distinction is visible in CObIT for instance, which is the framework that
I'm most used to working with. CObIT has different activities related to
critical/important IT assets compared to general IT assets. A computer mouse
might be an IT asset, but it is not a critical/important asset. Or, at least, I
hope your company isn't going to feel it if one mouse is malfunctioning.&lt;/p&gt;
&lt;p&gt;While having a definition to work with is essential to confirm the scope for
the activities a company needs to attend to, the next step isn't to run off
and tag everything that seems to fit the definition. Instead, I recommend
first starting by classifying the IT assets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifying the assets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next you want to classify for your organization what you consider to be IT
assets. This clarifies which types you want to follow the IT asset management
processes for. We've had examples mentioned earlier, like software, operating
systems, servers, but there are plenty more assets out there.&lt;/p&gt;
&lt;p&gt;For instance, a leased line you have between yourself and a third party is an IT
asset. Leased lines are network lines with a certain quality associated with
them and which you lease from a network provider. Their quality attributes
include line performance, expected availability, recovery capabilities, etc.
Such lines have economical value, and they offer support to the IT processes
that use that network.&lt;/p&gt;
&lt;p&gt;To create a classification, you might want to start with the conceptual
data model that I've &lt;a href="https://blog.siphos.be/2022/01/an-it-conceptual-data-model/"&gt;shared
previously&lt;/a&gt;. You can, of
course, also use an existing classification model. The &lt;a href="https://www.ungm.org/Public/UNSPSC"&gt;United Nations Standard
Products and
Services Code (UNSPSC)&lt;/a&gt; has a class list
published which you can use to tailor your classification.&lt;/p&gt;
&lt;p&gt;The classification shouldn't limit itself to IT asset management.
It is often better to work out this model for the company in general first
(including requirements from other processes like solution build and
configuration management) and then derive what classes should be part of the IT
asset management process.&lt;/p&gt;
&lt;p&gt;A well-maintained classification will also make it easier for the company
to work out how to improve the asset management processes. It enables the search
for more tangible solutions, a more structured analysis of the current
situation, and easier discussions with the stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While there does not appear to be a commonly accepted definition of what an IT
asset is, most frameworks do have similar concepts attributed to IT assets.
Common concepts are the value the assets deliver to the organization, and the
value the assets hold themselves. Trying to create a good definition helps in
trying to grasp at what level an IT asset should exist.&lt;/p&gt;
&lt;p&gt;Yet, working out a definition is only a first step. After the definition,
building out a good classification system helps to make IT asset management
more tangible for the organization. Most online resources use such classes
(like servers, desktops, operating systems, ...) to clarify what they consider
as IT assets, rather than attempting to derive a workable definition.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1492833029907173380"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="asset-management"></category><category term="cobit"></category><category term="itil"></category></entry><entry><title>An IT conceptual data model</title><link href="https://blog.siphos.be/2022/01/an-it-conceptual-data-model/" rel="alternate"></link><published>2022-01-17T10:00:00+01:00</published><updated>2022-01-17T10:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-01-17:/2022/01/an-it-conceptual-data-model/</id><summary type="html">&lt;p&gt;This time a much shorter post, as I've been asked to share this information
recently and found that it, by itself, is already useful enough to publish. It
is a conceptual data model for IT services.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;This time a much shorter post, as I've been asked to share this information
recently and found that it, by itself, is already useful enough to publish. It
is a conceptual data model for IT services.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The IT model, and why it is useful&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A conceptual data model for IT services supports several IT processes, with a
strong focus on &lt;em&gt;asset management&lt;/em&gt; and &lt;em&gt;configuration management&lt;/em&gt;. Many IT
vendors that have solutions active within those processes will have their own
data model in place, but I often feel that their models have room for
improvement.&lt;/p&gt;
&lt;p&gt;Some of these models are too fine-grained, others are limited to server
infrastructure. And while most applications allow for further customization, I
feel that an IT architect should have a conceptual model in mind for their
actions and projects.&lt;/p&gt;
&lt;p&gt;The conceptual data model that I'm currently working on looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="An IT CDM, first version" src="https://blog.siphos.be/images/202201/it-cdm-v1.png"&gt;&lt;/p&gt;
&lt;p&gt;My intention is to update this CDM with new insights when I capture those. It is
not my intention to further develop the data model into a physical data model,
but perhaps in the long term I could make it a conceptual one (explaining what
the attributes are of each concept).&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1483002656478093315"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="cdm"></category><category term="asset-management"></category><category term="configuration-management"></category></entry><entry><title>Ownership and responsibilities for infrastructure services</title><link href="https://blog.siphos.be/2022/01/ownership-and-responsibilities-for-infrastructure-services/" rel="alternate"></link><published>2022-01-13T09:00:00+01:00</published><updated>2022-01-13T09:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-01-13:/2022/01/ownership-and-responsibilities-for-infrastructure-services/</id><summary type="html">&lt;p&gt;In a perfect world, using infrastructure or technology services would be
seamless, without impact, without risks. It would auto-update, tailor to
the user needs, detect when new features are necessary, adapt, etc. But
while this is undoubtedly what vendors are saying their product delivers,
the truth is way, waaaay different.&lt;/p&gt;
&lt;p&gt;Managing infrastructure services implies that the company or organization
needs to organize itself to deal with all aspects of supporting a service.
What are these aspects? Well, let's go through those that are top-of-mind
for me...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In a perfect world, using infrastructure or technology services would be
seamless, without impact, without risks. It would auto-update, tailor to
the user needs, detect when new features are necessary, adapt, etc. But
while this is undoubtedly what vendors are saying their product delivers,
the truth is way, waaaay different.&lt;/p&gt;
&lt;p&gt;Managing infrastructure services implies that the company or organization
needs to organize itself to deal with all aspects of supporting a service.
What are these aspects? Well, let's go through those that are top-of-mind
for me...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Operational support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you have a service running, then you need to ensure operational
support is in place in case there are issues. Be it to resolve a
malfunction, a security issue, or a performance degradation - you need
(human) resources to ensure that the service remains running adequately.&lt;/p&gt;
&lt;p&gt;In many organizations, this is handled in a three- or sometimes 
even four-level support structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;First line&lt;/em&gt; is generally a call center that procedurally validates if
  an issue is service-bound, or if they can assist the user to correctly
  or better use the service. &lt;em&gt;First line&lt;/em&gt; often does not require any
  knowledge of the customer base nor target infrastructure, and is strongly
  procedure-oriented. They do not have operational duties on the services
  themselves, and are an important part to weed out unstructured or invalid
  service requests. They then escalate the issues to &lt;em&gt;second line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Second line&lt;/em&gt; is an organization that has knowledge on the customer
  base and the services themselves. They are also often the last line
  that has a wide view of all services within the company, as subsequent
  support levels are more specialized. &lt;em&gt;Second line&lt;/em&gt; has the ability to
  take actions on the services themselves (like restarting services)
  if this is agreed upon in the past with the main stakeholders, and when
  this is executed in a controlled manner. If &lt;em&gt;second line&lt;/em&gt; isn't able to
  resolve an issue, it moves to &lt;em&gt;third line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Third line&lt;/em&gt; support is generally the team that is operationally involved
  in the service itself. If the problem lays with a customer portal for
  instance, then &lt;em&gt;third line support&lt;/em&gt; is often the team that maintains the
  customer portal. They know the service and its usage in detail, and are
  in many organizations the last line of support.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some organizations even continue this layering, with a fourth line of
support being a technical expert in a particular component used by the
service. If the customer portal has problems, and it is within the database,
and the DBAs of the customer portal team do not have the knowledge to
resolve it, they might escalate further to a dedicated DBA team (which
is not assigned to any particular business service but specifically
organized to be experts in database and database administration).&lt;/p&gt;
&lt;p&gt;Not all companies have a technology-oriented support team though, and
many companies will consider this as part of 3rd line support as well,
if not just to be more aligned with market terminology on support structures.
Still, organizing and optimizing this third line of support is often
something that infrastructure service support is heavily involved in.&lt;/p&gt;
&lt;p&gt;Regardless of the structure approached by the organization, these teams
will need the knowledge and supporting tools and procedures to do their job.
You need people that can develop support procedures, create simplified
automation (for second line to execute), and continuously update that
information. And if a vendor is involved, then the support line will need to
have knowledge on how to approach the vendor: what are the procedures and
processes for raising incidents, what is the priority queue like? Does
the vendor have certain SLAs that the team should know about?&lt;/p&gt;
&lt;p&gt;The several layers of support will need continuous training, even if it
is just refreshing past information. It is also wise to involve these
support lines in information sharing, like when you know there is a growth
in database reliance in the business services, or when you know many
databases are being migrated from one technology to another. Second line
for instance might be able to use that information, together with their
cross-organizational knowledge, to better triage issues.&lt;/p&gt;
&lt;p&gt;Finally, this support structure is often tied to service level agreements that
the company makes either internally, or with its customers. Hence, the support
must be guaranteed within those SLAs. That implies that the support must be
driven through multiple people, as the organization has to provide support
during off-hours, during holidays, during absence... heck, even during global
pandemics. I use the "3 FTE per technology" principle: you need three people
with enough knowledge to support the technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintenance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As mentioned in the support part, teams exist that are responsible for
the service itself, including patching and updates. This is part of the
maintenance requirement on services, and infrastructure services are not
different. Perhaps even more so than business services, infrastructure
services have a wider impact if they are hit with a bug or with
performance degradation, as many business services rely on the infrastructure
to be up and running, highly available, well-performing, and secure.&lt;/p&gt;
&lt;p&gt;Maintenance tasks for services include, alongside the participation in the
operational support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Executing security- and stability patch validation and roll-out (updates)&lt;/li&gt;
&lt;li&gt;Proactively assessing the state of the service to see if improvements
  or mitigations are needed, before these result in actual issues&lt;/li&gt;
&lt;li&gt;Ensuring sufficient capacity is available for now and in the immediate
  future&lt;/li&gt;
&lt;li&gt;Resolving performance issues, be it by increasing resources, moving
  services to different locations or hosts, fine-tuning configuration
  parameters, offloading workload to different services, etc.&lt;/li&gt;
&lt;li&gt;Executing planned maintenance activities, which can include upgrades
  to higher versions. When the service cannot be transparently upgraded,
  this will involve a thorough alignment with all stakeholders as part of
  the change management processes.&lt;/li&gt;
&lt;li&gt;In case of larger, wide-spanning incidents (or even disasters), the teams
  play an active role in the orchestrated recovery together with all other
  teams.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Designing and architecting the integrations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To ensure that the services are well supported and can be maintained in
an efficient manner, there is often a strong focus on the proper design of
the infrastructure services and their role in the architecture. This
design does not just include pointing out which components exist where,
but also how the service integrates within the larger landscape:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are administration tasks executed? How do administrators authenticate
  themselves?&lt;/li&gt;
&lt;li&gt;Where are the monitoring metrics found and stored? Do you use trend analysis,
  and in which system?&lt;/li&gt;
&lt;li&gt;How are the assets related to the service tracked? What is the lifecycle for
  the individual assets, and how does that impact the maintainability of the
  service?&lt;/li&gt;
&lt;li&gt;What is the best way to use the service, and what will not be allowed?&lt;/li&gt;
&lt;li&gt;How are backups taken, and what does a restore procedure look like? Do you
  support both in-place restores, side restores, etc.&lt;/li&gt;
&lt;li&gt;What underlying resource requirements exist? Do you require certain
  tiered storage, or network performance requirements? If you allow your
  service to be instantiated by others (rather than maintain a platform
  yourself), what are the target resources that you allow? Are there
  minimal resource requirements?&lt;/li&gt;
&lt;li&gt;How do users interact with the service? Do they access it directly,
  or do you require intermediate gateways (like reverse proxies)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, design and architecture go beyond integrations. I focus strongly on
integrations here as it is a part of design and architecture that has strong
dependencies and relations with other teams and technologies. To work out
the integration side of a service, you can't do this autonomously without
assistance from the other service teams. Of course, if those teams have
everything well documented and architected, and easy to consume, then the
team responsible for the design and architecture can do a large part of the
work independently, but having a final validation or confirmation is always
beneficial.&lt;/p&gt;
&lt;p&gt;One of the values of a proper integration design is a higher quality of the
service delivery. If integrations are missed, you'll find that a service 
might not be ready to be activated in production, and then suddenly there
is a stronger focus on timely delivery than on quality. Situations where
firewall rules need to be quickly pushed and opened up because a project
failed to assess their integrations, resulting in security risks, is
sadly enough all too common.&lt;/p&gt;
&lt;p&gt;Larger organizations will often have architects and designers within the teams
or directorates to support this endeavor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secure setup and deployment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that some integrations might result in heightened risks, and
infrastructure services are often widely used or consumed, the organization
will need to make sure that the services are designed to be secure.&lt;/p&gt;
&lt;p&gt;Security of a service is more than just ensuring it is up-to-date. You
will also need to make sure it is configured correctly (as misconfigurations
are a frequent occurrence of security incidents), that the authentication
and authorizations are properly designed (and where needed or possible,
using multi-factor authentication), that the deployment considers the
placement and interactions (firewalls, zoning, etc.), that the service
provides functional (or perhaps even physical) segregation, that the
data governance is appropriate and aligned with regulatory and company
requirements, that the service is continuously validated by the security
tooling available (patch indications, vulnerability management, ...), etc.&lt;/p&gt;
&lt;p&gt;As services also evolve when they are alive, secure setup and deployment
is not a one-off (but the initial thoughts and designs are not to be
underestimated): the teams will need to assess the impact of new
insights (like security notifications, vulnerabilities, global changes
by the organization, new threats in the wider IT world) which implies
that the team has a continuous security and risk focus.&lt;/p&gt;
&lt;p&gt;In many cases, there is even a coding and development component with
a service. Not many services can be installed and deployed without requiring
any integration scripting or even in-depth coding (such as custom plugins).
And when there is coding, there is the need for secure coding: following
a Secure Development LifeCycle (SDLC) approach to get assurance about
the secure state of the developed code.&lt;/p&gt;
&lt;p&gt;When the deployment uses infrastructure-as-code methods, follows a
GitOps approach, or similar, then there should also be sufficient attention
to the secure setup of these pipelines and the platforms on which they
run. The code (or configurations) hosted should also follow appropriate
security guidelines&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robust and reliable services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Designing for a trustworthy, secure environment is one thing. The other
major focus area for infrastructure services is the availability,
robustness, and resilience of the service. While not all services
require to be up and running 24/7, nowadays it is hard to imagine
many services to still have significant downtimes.&lt;/p&gt;
&lt;p&gt;So, you often need to architecture and design for a resilient service,
which can take a beating if it has to. Organize load- and stress testing.
Organize disruptive testing if needed, and learn from the results to build
a better service. Design for a service that you can do technical maintenance
on without disrupting the customers themselves as much as possible - but
don't overshoot.&lt;/p&gt;
&lt;p&gt;If the service is set up in multiple locations, make sure that there is
independence between these locations (often across different regions) so
that failures in one region do not affect the other regions. Consider a setup
as used in many public cloud environments: high availability across availability
zones in the same region, and regional independence while allowing for
cross-region usage scenarios for the customers.&lt;/p&gt;
&lt;p&gt;Assess what can go wrong, and either try to update the architectures and
designs to be more resilient against these failures, or establish procedures
and processes to quickly recover. A common focus area here is to recover
from disasters (using so-called Disaster Recovery Procedures), and there 
are plenty of disasters to assess: data center failures, large Internet
outages, ransomware or other cyberattacks, worldwide epidemic outbreaks,
etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quality assurance at all stages&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The organization has to be able to provide fixes for defects and problems that
were raised, or to preemptively fix issues before they reach the customers. That
doesn't mean that the organization needs to be able to develop the fixes itself:
especially with off-the-shelf products the development is done by the
independent software vendor (ISV). However, the organization does have the
responsibility to track and put their weight on this so that the issues are
indeed properly resolved (and in case of a third-party product, preferably
through a fix that is applied to the main product, and not a one-off for that
particular company). Of course, if the service is developed in-house, then the
development of the fixes has to be guaranteed by the organization as well.&lt;/p&gt;
&lt;p&gt;To be able to provide secure and reliable services, it is vital to have
good change management processes and tools in place so that you can
approach the various stages of quality assurance before reaching production.
In &lt;a href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/"&gt;The pleasures of having
DTAP&lt;/a&gt; I mention
the benefits of having four environments for the various stages of a development
lifecycle (development, testing, acceptance, and production) and that is
perfectly applicable to infrastructure services as well, even when the
environments for infrastructure services might be isolated from those of the
more business-oriented development stages: you want to make sure that the
business-oriented development has production-grade services for its processes,
and not the intermediate and possibly less reliable in-development infrastructure
services.&lt;/p&gt;
&lt;p&gt;Throughout these environments, testing can (should) be introduced to provide
guidance to the engineers and developers to improve on the service. Testing can
take several forms: unit testing within the frameworks, integration testing of
code in larger environments, integration testing of new products in those
environments, sanity testing with simple use cases to ensure nothing blows
up (figuratively... hopefully), regression tests to ensure no fixed defects
creep back in, acceptance tests for specific features or use cases, load
testing to ensure the product stays up under the expected loads (and the
individual components or services have the right performance profile), stress testing
to ensure the product is resilient against higher loads or bursts, destructive
tests to see the product behaves as expected when unauthorized usage is
performed, security testing and penetration testing to provide assurance
on the secure state of the product deployment, etc.&lt;/p&gt;
&lt;p&gt;In many cases, the QA part is driven by organizational requirements, and not all
products require the same intensity on the tests and assurance levels. But, by
introducing the right automation, it is far easier to include multiple test
scenarios and test types in your pipeline.&lt;/p&gt;
&lt;p&gt;Products then pass through the various stages according to the change
management principles and processes that are in effect in the organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Strategy and roadmap&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The team responsible for an infrastructure service will also need to consider
the service in the long term: is the current technology (or set of products)
still state-of-the-art, mature, and following market practices? Or does the team
consider the technology to be relatively outdated and in need of an update? When
would the right time be to address this update?&lt;/p&gt;
&lt;p&gt;Perhaps the currently used technology is nearing its end-of-sale, end-of-support
(EOS), or even end-of-life (EOL). In that case, the team has to be ready to
address these lifecycle stages accordingly, be it through migrations, or
refactoring of current usages. Perhaps the teams find that it is more sensible
to get an extended support contract in place, or that they have the ability to
take the support (including code development) completely internally. Whatever
the choice is, it has to be made clear for the wider organization, and the
support team has to be ready to take on its commitments.&lt;/p&gt;
&lt;p&gt;There is a distinction between the service (or better yet, "capability") that is
offered, versus the products and technologies that support and realize it.
Capabilities will be needed by the organization for a long, long time, whereas
the products that realize these capabilities can have much shorter timeframes.&lt;/p&gt;
&lt;p&gt;The team will need to consider alternatives for the products, and see when those
alternatives make sense to address and implement. Perhaps the organization might
benefit from multiple implementations using different technologies because they
serve different usage scenarios, and the costs of keeping multiple technologies
in the air is less than the value it provides. Or perhaps an alternative needs
to be quickly realizable in case of a sudden exit scenario of the current
technology (or vendor). In that case, while the alternative doesn't need to be
up and running, the team must be able to address the change in due time.&lt;/p&gt;
&lt;p&gt;The need to have a proper roadmap on the capability and products that are being
used also reflects in the relationship that that team has with the vendor. For
strategically important products, an organization might even want to participate
in that vendor's Customer Advisory Board (CAB) or equivalent programs. The
team should have the time and resources to collaborate with that vendor to build
a partnership, participate in the conferences and other events, as that provides
input to the team on the progress and future of that product. Those insights are
primordial to properly design and organize an internal roadmap.&lt;/p&gt;
&lt;p&gt;Once the internal roadmap has taken shape, then the responsible teams are
involved in supporting the organization through updates and upgrades, by
communicating the need for these upfront (so that the internal customers can
plan around it), and to track the progress so there are no lingering risks for
the wide organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More elaborate (internal) customer support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The support for the infrastructure service often has to extend beyond the
operational support that I started with in this article. Teams that provide
certain capabilities within the organization don't do this as a vendor, but as
part of that organization. So when an internal customer needs help in migrating
away from a service, the team is still involved in this process.&lt;/p&gt;
&lt;p&gt;Hence, more elaborate support such as migrations (both towards a new technology,
service or capability, as well as migration away from it), finding
suitable alternatives that are not properly handled by the capability (and thus
might be best provided by a different team), ... helps in building out the trust
that the wider organization has in IT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost, licenses, and contractual obligations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many services have certain contractual obligations associated with them, often
known as the Terms and Conditions of the contract and product usage.
Infrastructure teams will need to make sure that these T&amp;amp;Cs are known and that
the organization adheres to them.&lt;/p&gt;
&lt;p&gt;The cost of an infrastructure service usage also needs to be correctly devised
and accounted for. Teams have to make sure the product usage remains within the
allocated licenses (or, if there is no capping in place, that the usage is
sufficiently constrained that the organization does not get any surprises), and
is often involved in defining a chargeback towards the rest of the organization.&lt;/p&gt;
&lt;p&gt;I tend to make a distinction between showback (show the organization how much a
service costs to the company or usage group) and chargeback (a governance-driven
or tax-driven requirement for charging usage to the organization), as the latter
is more a company decision on how to approach this, whereas the showback is the
actual, factual cost. Showback is needed to support conscious decisions on next
steps or consumption patterns, whereas chargeback might be necessary for
tax reasons in larger corporations where IT is considered as part of a different
legal entity.&lt;/p&gt;
&lt;p&gt;Addressing cost, licenses, and T&amp;amp;Cs is not to be underestimated. Many vendors
make this very difficult, as that allows for many interpretations during license
audits that can give a nice bonus to the vendor if he can show that his
interpretation is more appropriate than how you thought that the contract or
license was structured.&lt;/p&gt;
&lt;p&gt;The cost of a product or technology is not just the cost of the purchase, and
even that cost is still somewhat variable: many things are open for negotiation,
and you also don't need to tackle this directly with the vendor, as there is a
large market of middle parties that facilitate purchasing technologies. These
can often provide better rates as they can bundle purchases of multiple
customers and thus negotiate better deals with the main vendor.&lt;/p&gt;
&lt;p&gt;The cost also depends on the support contract associated with it, as well as
the costs of other depending technologies (such as capacity requirements) that come
from its implementation. This is often neglected in SaaS purchases: even though
you have correctly negotiated a good price for the SaaS service, you might be
jeopardizing your internet connectivity and need to upgrade the bandwidth, the
anti-DDoS service, your firewall capabilities, etc. because the consumption of
that SaaS service is significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The responsibilities for managing and tracking infrastructure services are large
and not to be underestimated. It is not a matter of deploying a new service and
assuming everybody can deal with it, nor are all responsibilities equally
visible to the end-user.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1481535279861280769"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="RACI"></category><category term="responsibilities"></category></entry><entry><title>The pleasures of having DTAP</title><link href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/" rel="alternate"></link><published>2021-12-30T12:00:00+01:00</published><updated>2021-12-30T12:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-12-30:/2021/12/the-pleasures-of-having-DTAP/</id><summary type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What are these four environments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's go over the four environments one by one with a small introduction
to their purpose. I'll cover more specific use cases further down in this
post.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Development&lt;/em&gt; environment is a functionally complete environment on
which the development of products or code is done. It should have the same
technologies in place and very similar setups (deployments) so that
developers are not facing a too different environment. Too much difference
might imply different behavior, which is contra-productive. The environment
is accessible by developers and testers, and is mainly development-oriented.&lt;/p&gt;
&lt;p&gt;Products or code that are being developed will be visible and used in this
environment. Developers and engineers hardly have any threshold to reach for
deploying or modifying code here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Testing&lt;/em&gt; environment is used when the development has reached a phase
where the product or code has passed a minimum of quality. Unit tests
succeed, the code builds fine, and the developer has indicated that the
code or product is ready for wider testing (hence the name).&lt;/p&gt;
&lt;p&gt;A testing environment generally applies a multitude of tests, most of
them (hopefully) automated, but with a strong dependency on testers
(also known as Quality Assurance engineers or QA engineers) to find issues.&lt;/p&gt;
&lt;p&gt;The automated tests focus on integrations, regression testing, security
testing, etc., and provide insights into the new builds or products which the
development team can take up and iterate over to improve the product.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Acceptance&lt;/em&gt; environment is a production-like environment, not
just for one product, but for the entire business or business unit: the same
setup, the same infrastructure, the same foundations, the same application
portfolio, the same integrations, etc. This environment intends to 
validate that the product is fully ready to be released. It is often
also abbreviated as the &lt;em&gt;User Acceptance Testing (UAT)&lt;/em&gt; environment.&lt;/p&gt;
&lt;p&gt;While the testing environment is generally approached by QA engineers, the
acceptance environment should be more tailored to business testers. When
developers or engineers introduce a feature, the stakeholders that
requested that feature use the acceptance environment to accept if the
coming release fulfills their request or not.&lt;/p&gt;
&lt;p&gt;As the environment is production-like in its entirety (and not just for
a single product), it is a prime target for executing performance tests
as well. Cross-product dependencies and processes (like migrations) are
validated here too.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Production&lt;/em&gt; environment is the environment in which the product or
code "goes live", where the customers use it. That does not mean that
a product put in production is immediately accessible for the customers:
there is still a difference between deployment (bring to production),
activation (enable usage), and release (use by customers).&lt;/p&gt;
&lt;p&gt;While DTAP is well-known in larger organizations, there are some challenges
or misconceptions that I would like to point out, and which I discuss
further down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environments are more than just the systems where the code is deployed to.
  Each environment has production services associated with it.&lt;/li&gt;
&lt;li&gt;A prime challenge to implementing DTAP is the cost associated with it. But
  it does not need to be as expensive as you think, and DTAP implementations
  often have a positive business case.&lt;/li&gt;
&lt;li&gt;Agile methodologists might find DTAP to be old-style. They are correct
  that many implementations are prohibitive towards fast deployment and
  release strategies, but that isn't because DTAP is conceptually wrong.&lt;/li&gt;
&lt;li&gt;Not all environments need the same data. On the contrary, a proper DTAP
  design likely uses separate datasets in each environment to deal with 
  the security and regulatory requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conceptual environments still require production services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The purpose of each environment is strongly tied to the 'phase' in which
the code or the product resides in the development lifecycle. That means
that these environments have a strong focus on that product or code, and
not on the services that are needed.&lt;/p&gt;
&lt;p&gt;Indeed, a development environment also entails services that are already
'in production', like a usable workstation, development services like
code repositories and build systems, ticketing services, and more. A testing
environment requires test automation engines, regression test frameworks,
security tools, and more. All these services are production-ready - and they
often have their own DTAP environments as well.&lt;/p&gt;
&lt;p&gt;It is a common misconception that a development-oriented system or service
has a lower SLA or lower risk profile than production, and infrastructure
architecture should make clear that there is a difference between the
systems that host the products or code under review and the systems that
facilitate the functionality needed within the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Static cost is a major inhibitor for implementing DTAP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaller companies or organizations might be hesitant to introduce DTAP
environments as it might be cost-prohibitive. While it is true
that, from a 'static' view, a DTAP environment costs more than a
production-only environment, you need to consider the impact of implementing
DTAP in the processes.&lt;/p&gt;
&lt;p&gt;The main purpose of DTAP is not to make your CFO angry, but to improve the
quality of your production environment (and usage). Ask yourself: how costly
is it when your production systems go down, or when your customers complain
and you need to fix things... Do you update code directly on the server(s)?
What if a security patch is rolled out and suddenly prevents your
customer-facing application from working?&lt;/p&gt;
&lt;p&gt;While DTAP mentions four environments, some companies settle with three, and
others with five or more. Perhaps your testing and acceptance are done by the
same people, and you do not have many automated testing facilities at hand.
Splitting your pre-production environments into multiple environments doesn't
make sense yet, and you might first want to focus on improving your testing
maturity in general.&lt;/p&gt;
&lt;p&gt;If you want to make the case for DTAP, consider the use cases or scenarios that
have visibly disrupted your business and how/where the environments would have
helped. In many cases, you'll notice that there is a positive business case
for a step-wise move towards DTAP.&lt;/p&gt;
&lt;p&gt;Furthermore, a proper design of these environments will facilitate an
economical view towards DTAP. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can use pay-as-you-use environments (as is commonly the case in the 
  public cloud) which you only activate when you do your testing. If you have
  a 24/7 customer-facing service in production, that doesn't mean that your
  acceptance environment has to be 24/7. Yes, it should be as much
  production-like as possible, but if it isn't doing anything outside
  business hours, then you don't need it running outside business hours.&lt;/li&gt;
&lt;li&gt;Commercial products often have distinct terms and conditions for
  non-production usage. You can have databases in production with a
  premium, gold service level agreement, while having the same database in
  the other environments with a low, bronze service level agreement
  towards that vendor: cheaper, but technically the same.&lt;/li&gt;
&lt;li&gt;Abstraction and virtualization technologies allow for better control
  of the resources that are being used. For instance, you can have
  an acceptance environment that is only at 20% of the resources
  of production for day-to-day validation, and then increase its resources
  to 100% for load testing periods. If these environments are not in a
  pay-as-you-use model, shifting resources from one environment to
  another allows for controlling the costs.&lt;/li&gt;
&lt;li&gt;Security controls in these environments might be different as well,
  assuming that these environments have different data needs: if you
  use fictitious data in development and testing, and anonymized data
  in acceptance, then the investments on, say, data leakage controls
  might be different for these environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, while DTAP is at first glance a costly approach, the actual case is
that it is positive for the company.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DTAP is not inefficient, but some implementations are&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a world where "Release fast, release often" is the norm, having a rigid
DTAP strategy might be contra-productive. However, this is more an
implementation concern than a conceptual one. If I consider the downsides
that &lt;a href="https://medium.com/the-liberators/want-to-be-agile-drop-your-dtap-pipeline-7dcb496fe9e3"&gt;Christiaan Verwijs&lt;/a&gt;
mentions in his post, I feel that I can safely state that this is not due
to the lifecycle of the code or product, but rather the choices that a
company has made while assessing and implementing a release strategy.&lt;/p&gt;
&lt;p&gt;There is no need to bundle and make bulk releases with DTAP. You can
perfectly design an environment where DevOps teams can release easily
to production. More so, the bulk release strategy is frequently the result
of an application design constraint, not a deployment constraint.&lt;/p&gt;
&lt;p&gt;Development methodologies and DTAP environments do need to be tailored
to each other. The purpose of DTAP is to facilitate the quality of products
and code, and thus should be tailored towards efficient and qualitative
development processes. In many environments, DTAP is synonymous with
"infrastructure operations" and that's a wrong approach. 
Operations-oriented teams (like &lt;em&gt;Site Reliability Engineers (SREs)&lt;/em&gt;),
development-oriented teams, and DevOps teams all have the same benefits from
DTAP.&lt;/p&gt;
&lt;p&gt;Some might state that an acceptance environment is no longer suitable in
the modern age, as they can deploy products and code to production without
losing the benefits of the acceptance environment. With blue/green
deployments or canary releases, you can enable business testers and
stakeholders to validate new features or code before releasing it
to the wider public.&lt;/p&gt;
&lt;p&gt;To accomplish this properly, however, the platform that is used will
balance resources accordingly, and you're conceptually implementing an
(albeit temporary) acceptance environment in an automated way. This is
an implementation choice and has to be balanced against the requirements
that the organization has.&lt;/p&gt;
&lt;p&gt;For instance, if you work with sensitive data, you might not be allowed
to use this data during testing. In Europe, the &lt;em&gt;General Data Protection
Regulation (GDPR)&lt;/em&gt; is a strong regulatory requirement for dealing with
sensitive data. It isn't a playbook though: companies need to evaluate
how and where data is used, and perhaps the balance made by the company
allows, if not with explicit consent, to use data unmodified for
acceptance testing. But if that isn't the case and your acceptance tests
need to use sanitized data, then having separate environments is likely
more sensible (although different implementations exist that allow
for anonymization in production as well - they're, however, not as easy
to implement).&lt;/p&gt;
&lt;p&gt;Plus, DTAP does not imply that production is doing everything in a single
unit of work: Deploy, Activate and Release. You can still perfectly position
those tasks in production while having an explicit acceptance environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Separate datasets in each environment make sense&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For regulated companies and organizations, security officers might want
to use the DTAP distinction to focus on data minimization strategies as well.
As mentioned before, the GDPR is a strong regulatory requirement whose
alignment can be facilitated by a well-designed DTAP approach.&lt;/p&gt;
&lt;p&gt;You can use fictitious data in development and testing, with development
using datasets that developers use for validating the specific functionality
they are working on (and preferably share and put alongside the code
and products), whereas testing uses a coherent but still fictitious
dataset. I use "coherent" here as an indication that the data should
be functionally correct and integer: a (fictitious) person record in the
customer database in the testing environment should be mapped to the
(fictitious) calls or other interactions that are stored in the support
database (also in the testing environment) and the (fictitious) portfolio
that this (fictitious) person has in the product database (in the testing
environment).&lt;/p&gt;
&lt;p&gt;Don't underestimate how powerful, but also how challenging a good fictitious
dataset is.&lt;/p&gt;
&lt;p&gt;For acceptance testing, perhaps the company decided that anonymized data
is to be used. Or it uses pseudonymized data (which is a weaker form)
with additional technical controls to prevent leakage and attacks (including
inference) that try to deduce the origin of the data.&lt;/p&gt;
&lt;p&gt;Again, these are choices by the company or organization, which need to be
taken with the risk and business stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DTAP is a sensible approach for improving quality in products and code.
While it isn't the holy grail for quality assurance, it has solid
foundations that many larger companies and organizations feel comfortable
with. The implementation details make or break how well-adjusted the DTAP
approach is for modern development processes and regulatory requirements.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1476505537047109635"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="DTAP"></category><category term="environments"></category><category term="zoning"></category><category term="development"></category><category term="test"></category><category term="acceptance"></category><category term="production"></category></entry><entry><title>Creating an enterprise open source policy</title><link href="https://blog.siphos.be/2021/11/creating-an-enterprise-open-source-policy/" rel="alternate"></link><published>2021-11-20T15:00:00+01:00</published><updated>2021-11-20T15:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-20:/2021/11/creating-an-enterprise-open-source-policy/</id><summary type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Assess the current situation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When drafting a policy, make sure you know what the current situation already
is. Especially when the policy might be very restrictive, you might be facing a
huge backlash from the organization if the policy is not reflecting the reality.
If that is the case, and the policy still needs to go through, proper
communication and grooming will be needed (and of course, the "upper management
hammer" can help out as well).&lt;/p&gt;
&lt;p&gt;Often, higher management is not aware of the current situation either. They
might think that open source is hardly in use. Presenting them with facts and
figures not only makes it more understandable, it will also support the need for
a decent open source policy.&lt;/p&gt;
&lt;p&gt;When you have a good view on the current usage, you can use that to track where
you want to go to. For instance, if your company wants to adopt open source more
actively, and pursue open source contributions, you might want to report on the
currently detected contributions, and use that for follow-up later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get HR and compliance involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before you embark on the journey of developing a decent open source policy, make
sure you have management support on this, as well as people from HR and your
compliance department (unless your policy will be extremely restrictive, but
let's hope that is not the case).&lt;/p&gt;
&lt;p&gt;You will need (legal &amp;amp;) compliance involved in order to draft and assess the
impact of internal developers and engineers working on open source projects, as
well as the same people working on open source projects in their free time. Both
are different use cases but have to be assessed regardless.&lt;/p&gt;
&lt;p&gt;HR is generally involved at a later stage, so they know how the company wants to
deal with open source development. This could be useful for recruitment, but
also for HR to understand what the policy is about in case of issues.&lt;/p&gt;
&lt;p&gt;An important consideration to assess is how the company, and the contractual
obligations that the employees have, deals with intellectual property. In some
companies, the contract allows for the employees to retain the intellectual
property rights for their creations outside of company projects. However, that
is not always the case, and in certain sectors intellectual property might be
assumed to be owned by the company whenever the creation is something in which
the company is active. And that might be considered very broadly (such as
anything IT related for employees of an IT company).&lt;/p&gt;
&lt;p&gt;The open source policy that you develop should know what the contractual
stipulations say, and clarify for engineers and developers how the company
considers the intellectual property ownership. This is important, as it defines
who can decide to contribute something to open source.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Understand and simplify license requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many of the decisions that the open source policy has to clarify will be related
to the open source licenses in use. Moreover, it might even be relevant to
define what open source is to begin with.&lt;/p&gt;
&lt;p&gt;A good source to use is the &lt;a href="https://opensource.org/osd"&gt;Open Source Definition&lt;/a&gt;
as published and maintained by the &lt;a href="https://opensource.org/"&gt;Open Source Initiative
(OSI)&lt;/a&gt;. Another definition is the one by the &lt;a href="https://www.fsf.org/"&gt;Free
Software Foundation&lt;/a&gt; titled "&lt;a href="https://www.fsf.org/about/what-is-free-software"&gt;What is free software and
why is it so important for society&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;The license is the agreement that the owner of the software puts out that
declares how users can use that software. Most, if not all software that a
company uses, will have a license - open source or not. But most commercial
software titles have specific licenses that you need to go through for each
specific product, as the licenses are not reused. In the open source world,
licenses are reused so that end users do not need to go through product-specific
terms.&lt;/p&gt;
&lt;p&gt;The OSI organization has a list of &lt;a href="https://opensource.org/licenses"&gt;approved
licenses&lt;/a&gt;. However, even amongst these
licenses, you will find different types of licenses out there. While they are
commonly grouped into &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;copyleft&lt;/a&gt; and
&lt;a href="https://fossa.com/blog/all-about-permissive-licenses/"&gt;permissive&lt;/a&gt; open source
licenses, there are two main categories within the copyleft licenses that you
need to understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strong copyleft licenses that require making all source code available upon
  distribution, or sometimes even disclosure of the application base&lt;/li&gt;
&lt;li&gt;"scoped" copyleft licenses that require making only the source code available of the
  modules or libraries that use the open source license (especially if you
  modified them) without impacting the entire application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the term "strong copyleft" is something that I think is somewhat generally
accepted (such as in the Snyk article "&lt;a href="https://snyk.io/learn/open-source-licenses/"&gt;Open Source Licenses: Types and
Comparison&lt;/a&gt;" or in &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;Wikipedia's
article&lt;/a&gt;), I do not like to use its
opposite "weak" term, as the licenses themselves do not reduce the open source
identity from the code. Instead, they make sure the scope of the license is
towards a particular base (such as a library) and not the complete application
that uses the license.&lt;/p&gt;
&lt;p&gt;Hence, open source policies might want to focus on those three license types for
each of the use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;permissive licenses, like Apache License 2.0 or MIT&lt;/li&gt;
&lt;li&gt;scoped copyleft licenses, like LGPL or EPL-2.0&lt;/li&gt;
&lt;li&gt;strong copyleft licenses, like GPL or AGPL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Differentiate on the different open source use cases&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several use cases that the policy will need to tackle. These are, in
no particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using off-the-shelf, ready-to-use open source products&lt;/li&gt;
&lt;li&gt;Using off-the-shelf libraries and modules for development&lt;/li&gt;
&lt;li&gt;Using open source code&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for company purposes&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for personal/private purposes&lt;/li&gt;
&lt;li&gt;Launching and maintaining open source projects from the company&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these use cases might have their specific focuses. Combine that with the
license categories listed earlier, and you can start assessing how to deal with
these situations.&lt;/p&gt;
&lt;p&gt;For instance, you might want to have a policy that generally boils down to the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When using off-the-shelf, ready-to-use open source products, all types of
  products are allowed, assuming the organization remains able to support the
  technologies adopted. Furthermore, the products have to be known by the
  inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using libraries or modules in development projects, only open source
  products with permissive or scoped copyleft licenses can be used. Furthermore,
  the libraries or modules have to be well managed (kept up-to-date) and known
  by the inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using open source code, only code that is published with a permissive
  license can be used. At all times, a reference towards the original author
  has to be retained.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for company purposes, approval has
  to be given by the hierarchical manager of the team. Contributions have to be
  tagged appropriately as originating from the company (e.g. using the company
  e-mail address as author). Furthermore, employees are not allowed to
  contribute code or intellectual property that is deemed a competitive
  advantage for the company.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for personal/private purposes,
  employees are prohibited to use code from the company or to do contributions
  using their company's e-mail address. However, the company does not claim
  ownership on the contributions an employee does outside the company's projects
  and hours.&lt;/li&gt;
&lt;li&gt;When creating new projects or publishing internal projects as open source,
  sufficient support for the project has to be granted from the company, and the
  publications are preferentially done within the same development services
  (like version control) under management of the company. This ensures
  consistency and control over the company's assets and liability. Projects have
  to use a permissive license (and perhaps even a single, particular license).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, if the company actively pursues an open source first strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Off-the-shelf, ready-to-use open source products are preferred over
  propriatary products. Internal support teams must be able to deal with
  general maintenance and updates. The use of commercially backed products is
  not mandatory, but might help when there is a need for acquiring short-term
  support (such as through independent consultants).&lt;/li&gt;
&lt;li&gt;Development projects must use projects that use permissive or scoped copyleft
  licenses for the libraries and dependencies of that project. Only when the
  development project itself uses a strong copyleft license are dependencies
  with (the same) strong copyleft license allowed. Approval to use a strong
  copyleft license is left to the management board.&lt;/li&gt;
&lt;li&gt;Engineers and developers retain full intellectual property rights to their
  contributions. However, a Contributor License Agreement (CLA) is used to grant
  the company the rights to use and distribute the contributions under the
  license mentioned, as well as initiate or participate in legal actions related
  to the contributed code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Clarify what is allowed to be contributed and what not&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example I already indicated a "do not contribute code that is
deemed a competitive advantage" statement. While it would be common sense,
companies will need to clarify this (if they follow this principle) in their
policies so they are not liable for problems later on.&lt;/p&gt;
&lt;p&gt;A competitive advantage primarily focuses on a company's crown jewels, but can
be extended with code or other intellectual property (like architectural
information, documentation, etc.) that refers to indirect advantageous
solutions. If a company is a strong data-driven company that gains massive
insights from data, it might refuse to share its artificial intelligence related
code.&lt;/p&gt;
&lt;p&gt;There are other principles that might decide if code is contributed or not. For
instance, the company might only want to contribute code that has received all
the checks and controls to ensure it is secure, it is effective and efficient,
and is understandable and well-written. After all, when such contributions are
made in name of the company, the quality of that code reflects upon the company
as well.&lt;/p&gt;
&lt;p&gt;I greatly suggest to include examples in the open source policy to clarify or
support certain statements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assess the maturity of an open source product&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When supporting the use of open source products, the policy will also have to
decide which open source products can be used and which ones can't. Now, it is
it possible to create an exhaustive list (as that would defeat the purpose of an
open source policy). Instead, I recommend to clarify how stakeholders can assess
if an open source product can be used or not.&lt;/p&gt;
&lt;p&gt;Personally, I consider this from a "maturity" point of view. Open source
products that are mature are less likely to become a liability within a larger
company, whereas products that only have a single maintained (like my own
&lt;a href="https://github.com/sjvermeu/cvechecker"&gt;cvechecker&lt;/a&gt; project) are not to be used
without understanding the consequences.&lt;/p&gt;
&lt;p&gt;So, what is a mature open source project? There are online resources that could
help you out (like the Qualipso-originated &lt;a href="https://en.wikipedia.org/wiki/OpenSource_Maturity_Model"&gt;Open Source Maturity Model
(OSMM)&lt;/a&gt;), but
personally I tend to look at the following principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The project has an active development, with more than 5 active contributors in
  the last three months.&lt;/li&gt;
&lt;li&gt;The project is visibly used by several other projects or products.&lt;/li&gt;
&lt;li&gt;The project has well-maintained documentation, both for developers and for
  users. This can very well be a decent wiki site.&lt;/li&gt;
&lt;li&gt;The project has an active support community, with not only an issue system,
  but also interactive services like forums, IRC, Slack, Discord, etc.&lt;/li&gt;
&lt;li&gt;The project supports more than one major version in parallel, and has a clear
  lifecycle for its support (such as "major version is supported up to at least
  1 year after the next major version is released").&lt;/li&gt;
&lt;li&gt;The project publishes its artefacts in a controlled and secure manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A policy is just the beginning, not the end&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As always, there will be situations where a company wants to allow a one-off
case to deviate from the policy. Hence, make clear how deviations can be
targeted.&lt;/p&gt;
&lt;p&gt;For instance, you might want to position an architecture review board to support
deviations from the license usage. When you do, make sure that this governance
body knows how to deal with such deviations - understanding what licenses are,
what the impact might be towards the organization, etc.&lt;/p&gt;
&lt;p&gt;Furthermore, once the policy is ready to be made available, make sure you have
support for that policy in the organization, as well as supporting tools and
processes.&lt;/p&gt;
&lt;p&gt;You might want to include an internal community to support open source/free
software endeavors. This community can help other stakeholders with the
assessment of a product's maturity, or with the license identification.&lt;/p&gt;
&lt;p&gt;You might want to make sure you can track license usage in projects and
deployments. For software development projects, there are plenty of commercial
and free services that scan and present license usage (and other details) for a
project. Inventory and asset management utilities often also include
identification of detected software. Validate that you can report on open source
usage if the demand comes up, and that you can support development and
engineering teams in ensuring open source usage is in line with the company's
expectations.&lt;/p&gt;
&lt;p&gt;The company might want to dedicate resources in additional leakage detection and
prevention measures for the open source contributions. While the company might
already have code scanning techniques in place in their on-premise version
control system, it might be interesting to extend this service to the public
services (like &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt; and
&lt;a href="https://about.gitlab.com/"&gt;GitLab&lt;/a&gt;). And with that, I do not want to imply
using the same tools and integrations, but more on a functional level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing off&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few companies, and most governmental organizations, publish their open source
policies online. The &lt;a href="https://todogroup.org/"&gt;TODO Group&lt;/a&gt; has graceously drafted
a &lt;a href="https://github.com/todogroup/policies"&gt;list of examples and templates&lt;/a&gt; to
use. They might be a good resource to use when drafting up your own.&lt;/p&gt;
&lt;p&gt;Having a clear and understandable open source policy simplifies discussions, and
with the appropriate support within the organization it might jumpstart
initiatives even further. Assuming the policy is sufficiently supportive of open
source, having it published might eliminate the fear of engineers and developers
to suggest certain open source projects.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1462043477835976705"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="opensource"></category><category term="enterprise"></category><category term="legal"></category><category term="compliance"></category></entry><entry><title>Hybrid cloud can be very complex</title><link href="https://blog.siphos.be/2021/11/hybrid-cloud-can-be-very-complex/" rel="alternate"></link><published>2021-11-08T20:00:00+01:00</published><updated>2021-11-08T20:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-08:/2021/11/hybrid-cloud-can-be-very-complex/</id><summary type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Hybrid cloud complexity starts with the definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing that I've already learned is not to use "hybrid cloud" without
defining what I mean. And if somebody else uses it (or a research article), I
will frantically try to get a definition on what the person or article implies
with the term.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://azure.microsoft.com/en-us/overview/what-is-hybrid-cloud-computing/"&gt;Microsoft&lt;/a&gt;
for instance defines hybrid cloud as "a computing environment that combines an
on-premises datacenter with a public cloud, allowing data and applications to
be shared between them."&lt;/p&gt;
&lt;p&gt;This definition isn't unambiguous. What does Microsoft mean with "sharing"? If
I expose an application and/or its data through APIs that are shielded and
independently managed in each environment, and allow for interaction between
those APIs, does that still entail a hybrid cloud architecture? Because if that
is the case, then I want to know what other cloud interaction architectures
Microsoft thinks exist besides hybrid cloud.&lt;/p&gt;
&lt;p&gt;I do think that the intention of Microsoft's definition is that the cloud
hosting environments are considered as "similar enough" to the on premise
environment, and managed in the same way (some cloud specifics
notwithstanding), as inspired by their claim that hybrid cloud allow
"businesses [to] use the cloud to instantly scale capacity up or down to handle
excess capacity" and that organizations using hybrid cloud architectures "are
able to use many of the same security mreasures that they use in their existing
on-premises infrastructure".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gartner.com/en/information-technology/glossary/hybrid-cloud-computing"&gt;Gartner&lt;/a&gt;
defines hybrid cloud computing as "policy-based and coordinated service
provisioning, use and management across a mixture of internal and external
cloud services." That doesn't narrow things down, and in &lt;a href="https://www.gartner.com/document/3956442"&gt;(paywalled) research
articles&lt;/a&gt;, Gartner does express that
"hybrid cloud is a vague term that does not allow enough granularity for
implementation planning by cloud and infrastructure professionals".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://go.forrester.com/blogs/13-08-02-cloud_management_in_a_hybrid_cloud_world/"&gt;Forrester&lt;/a&gt;
follows a definition that is very generic, as "a cloud service connected to any
other corporate resource" makes it a hybrid cloud service. Regardless of how it
is infrastructurally or application-wise integrated.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ibm.com/cloud/learn/hybrid-cloud"&gt;IBM&lt;/a&gt; declares hybrid cloud as
"integrates public cloud services, private cloud services and on-premises
infrastructure and provides orchestration, management and application
portability across all three. The result is a single, unified and flexible
distributed computing environment [...]"&lt;/p&gt;
&lt;p&gt;This definition does seem to imply an architecture that considers all hosting
environments as infrastructurally equal (as they see the sum of all
environments as a single, unified environment).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud"&gt;Wikipedia&lt;/a&gt;
mentions that hybrid cloud "is a composition of a public cloud and a private
environment [...] that remain distinct entities but are bound together,
offering the benefits of multiple deployment models." Such a definition leaves
the implementation details open, as it boils down to any architecture where you
mix such hosting environments.&lt;/p&gt;
&lt;p&gt;For me, a &lt;strong&gt;hybrid cloud on infrastructure level&lt;/strong&gt; implies that the different
environments are similarly or equally managed, using the same processes,
principles and most often even tooling, and that application teams have little
impact on where their application (or application components) are hosted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The promise of hybrid cloud towards business&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When business decision makers hear (or are confronted with) hybrid cloud, they
are often told that it a perfect way to deal with capacity management. Whereas
a pure on-premise deployment model requires you to purchase and deploy enough
capacity to deal with your maximum workload (and even more, if you need to
consider disaster recovery situations), a hybrid cloud could simply "add more
resources as needed, without requiring the application to be refactored for
cloud-native or cloud hosting".&lt;/p&gt;
&lt;p&gt;Let's make this more tangible with an example: a simple ticket sales service,
which consists out of a website (frontend) and an API (which is backend-alike).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales service overview" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-overview.png"&gt;&lt;/p&gt;
&lt;p&gt;The company that manages this ticket sales application is currently fully
on-premise, with a simple deployment model where the front and backend are
hosted on a web application hosting cluster (which could be a Kubernetes
cluster), and the backend also uses a database.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level infrastructure" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-hlinfra.png"&gt;&lt;/p&gt;
&lt;p&gt;Ticket sales are seasonally bound, and often ticket sales platforms are
services offered to the specific events. Suppose a major event wants to sell
its tickets through this ticket sale application, and you are afraid that the
website part will not be able to deal with the load, then you could use a
hybrid cloud setup to enable bursting on the front-end side.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level bursting" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-burstfrontend.png"&gt;&lt;/p&gt;
&lt;p&gt;Of course, this is just one of many target architectures that might solve the
capacity challenge, and there is no reason to believe the API itself wouldn't
be overloaded as well. But let's stick to this simple example.&lt;/p&gt;
&lt;p&gt;From a business perspective, this all sounds very fun and promising. There
seems to be no initial investment needed, and the capacity of the cloud is
limitless, not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network investments needed for such hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, as always, the devil is in the details. If we were to pursue this
architecture, we need to have the public cloud and the on premise environment
properly connected. You don't want to use regular Internet access, because the
intention is to see these environments as a single, unified environment, and
you don't open up your internal systems directly to the Internet either do you?&lt;/p&gt;
&lt;p&gt;So you need to architect the public cloud usage to be as private as possible,
and then connect that environment with your on premise network, preferably
through a high speed private link. Sure, you can use VPN over internet, but
with a private link you have more guarantees on the latency for instance, and
for many cloud environments such private link interactions are also bringing
benefits for data ingress/egress (cheaper for you). They also generally have
better SLAs (although larger environments will have very high internet-related
SLAs) and do not require the same security protection measures (like anti-DDoS
protection).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales network connectivity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-connectivity.png"&gt;&lt;/p&gt;
&lt;p&gt;In the design, we assume that the end users still first go through your on
premise environment, as the perimeter protections you have in place for
instance still need to apply. Perimeter protections are not just simple
firewall capabilities. It includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anti-DDoS measures&lt;/li&gt;
&lt;li&gt;context gathering for, and applying coarse-grained access controls&lt;/li&gt;
&lt;li&gt;intrusion detection and prevention&lt;/li&gt;
&lt;li&gt;anti-malware protection&lt;/li&gt;
&lt;li&gt;application attack prevention&lt;/li&gt;
&lt;li&gt;network traffic filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A perimeter is meant to act as a first line of defense. However, when
integrating networks from external sites, you still need some protection
measures applied (shown as external site protection in the diagram), as you are
handing off some ownership to other parties and thus want to have some
safeguards in place.&lt;/p&gt;
&lt;p&gt;Because we still have all traffic through the perimeter, burst loads can still
jeopardize the service offering itself. If the perimeter, internet line, or the
load balancer that spreads the load across the frontends is saturated, then
your service will go down. The hybrid cloud setup used in this example wont
help out here.&lt;/p&gt;
&lt;p&gt;Second, the high speed private link will need to be able to deal with not only
the load of the user to the frontend (and back), but also between the frontend
and backend. And if you were to support bursting the backend application to the
public cloud as well, then it needs to deal with the load between this
application and the database.&lt;/p&gt;
&lt;p&gt;The link is also often not something you set up yourself between you and the
public cloud. You will need intermediate parties to support this, as often this
first requires you to have private links to certain larger networks, and then
have this larger network set up a private link to the point of presence where
you want to 'attach' to that public cloud environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Management complexity rises with hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The investments do not stop just at the network connectivity. You will need to
look into managing the web application (such as deployment and releases),
servers (bootstrapping, updating, maintaining), and other network areas.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hybrid cloud management complexity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-management.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's start with the web application management. Your existing management
systems will now need to deal with the public cloud as well. Your application
needs to be deployed on multiple clusters, and you will need to reconfigure the
load balancers in front to know where these clusters are. Unlike the
pre-installed environments on premise, public cloud is more dynamic (you want
to use it for bursting after all), so the target IP addresses might change (or
you set up fixed IP addresses, but that costs money even when you don't use
them).&lt;/p&gt;
&lt;p&gt;You will need to deal with deployments that can succeed left and fail right, or
vice-versa. While this is not impossible to deal with, the current way of
working might not support that yet because, you know, you never had to deal
with it.&lt;/p&gt;
&lt;p&gt;What about tracking performance and user experience? Your application
management suite might not know about public cloud setups yet, and once
included, it might find that there is latency impact. But can you just use this
APM suite in the public cloud? Perhaps your company has a site license which
does not include other locations. Or it requires per-node licenses, and require
each node to be assigned a license for 30 days at least. In case of burst
situations, you might only have these systems up for a few hours, and with the
next action, these will be new nodes with new licenses.&lt;/p&gt;
&lt;p&gt;Also on the server management level you might find many obstacles. Your on
premise system might use a certain hypervisor integration (e.g. using VMware
vCenter API) which you don't have in the public cloud. So you need to adapt the
management system anyway, which means you need to develop your management
systems to create a hybrid cloud, rather than reap the benefits of hybrid cloud
directly.&lt;/p&gt;
&lt;p&gt;Your servers might use many control systems that have the same licensing issues
as mentioned earlier. Or they are latency-bound, causing either reachability
issues, or requiring you to adapt the infrastructure architecture of these
management systems to be aware of the public cloud.&lt;/p&gt;
&lt;p&gt;Also on network management level it isn't just about connectivity. Your
firewall management might not see the public cloud firewalls automatically (or
doesn't support it), or your current network design doesn't allow for the
bursting of network environments (subnets) in a sufficient dynamic manner.&lt;/p&gt;
&lt;p&gt;The more you consider this hybrid cloud situation, the more you find out that
you will need to revisit all management, support and control systems for this
setup. And you know what is hard to do in an IT environment? Reassessing &lt;em&gt;all&lt;/em&gt;
management, support and control systems. You are effectively redesigning your
entire IT environment, and that is exactly what the promise of "hybrid cloud"
wanted to take away. Or at least, the promise that is done to certain decision
makers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendors know that it is complex (and are happy because of it)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Most of the IT vendors that are related to the management, support and control
of your infrastructure, will say that management in hybrid cloud is hard: &lt;a href="https://www.redhat.com/en/blog/operating-hybrid-architecture-and-managing-complexity"&gt;Red
Hat&lt;/a&gt;
for instance mentions in its container-related article that "Saying that
Kubernetes makes it possible to build a cross-environment management layer
doesn't mean it's easy". &lt;a href="https://blogs.arubanetworks.com/solutions/easing-the-complexity-of-hybrid-cloud/"&gt;Aruba
Networks&lt;/a&gt;
(part of PHE) mentions that "the major drawback is complex management of these
platforms". Consulting firms concur as well, with for instance David Linthicum,
Chief Cloud Strategy Officer of Deloitte Consulting, mentioning in &lt;a href="https://techbeacon.com/enterprise-it/4-things-you-need-know-about-managing-complex-hybrid-clouds"&gt;4 things
you need to know about managing complex hybrid
clouds&lt;/a&gt;
that "hybrid clouds are usually complex, hard to build, and hard to manage".&lt;/p&gt;
&lt;p&gt;Of course, IT vendors will be happy to tell you that it is hard (and for once I
concur), because they will then follow up with how their shiny tool supports
hybrid cloud much better than your existing ones. IT vendors know that an
infrastructural hybrid cloud means a redesign of (many parts of) your IT
architecture, so there is big money involved.&lt;/p&gt;
&lt;p&gt;Hence, it is important that hybrid cloud endeavours are assessed completely,
and that your business decision makers hold off with the decision until the
full scope of this exercise is known (or at least, you have a decent ballpark
estimate on the impact, not just financially, but also time-to-market). &lt;/p&gt;
&lt;p&gt;So, what are all the areas you need to consider? That's difficult to state, but
hopefully the list below can help you out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are your servers bootstrapped (initial deployment)? Can this interact
  with the cloud APIs to do the same in the cloud, or will you need to adapt
  your processes?&lt;/li&gt;
&lt;li&gt;Once your server is bootstrapped, how do you add software, libraries and
  other artefacts to it?&lt;/li&gt;
&lt;li&gt;What security services do you need on your systems? Anti-malware? Behavior
  analytics? Intrusion detection and prevention? Privileged access management
  utilities? &lt;/li&gt;
&lt;li&gt;How do your engineers and administrators follow-up on the systems? Monitoring
  approaches? Application performance management? Trace capabilities? Logging?&lt;/li&gt;
&lt;li&gt;Are there any control systems in place that manage the infrastructure? Are
  these control systems latency-sensitive?&lt;/li&gt;
&lt;li&gt;How do you deal with applicative releases? If you have CI/CD infrastructure
  in place, how would it deal with a burst environment? Does it have any
  dynamical detection capabilities?&lt;/li&gt;
&lt;li&gt;Can your support systems deal with more ephemeral infrastructure (capacity that
  is added for a few hours and then removed again)?&lt;/li&gt;
&lt;li&gt;Can your processes deal with ephemeral infrastructure?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Perhaps your current environment is already capable of dealing with such hybrid
clouds. While the situations I am confronted with at work support my view that
we need to apply a different 'hybrid' approach than the 'seamless
infrastructure' one (I like to see the environments progress more
independently, gradually move towards a &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;zero
trust&lt;/a&gt; model), I do
believe that such hybrid cloud setups can work in certain situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions and feedback&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do come across an intention declaration to move to a hybrid cloud (which
follows the infrastructural 'seamless' setup as implied in this post), make
sure you inform the stakeholders of the consequences. Show that a majority of
management, support and control systels are not designed nor capable of dealing
with this bursting out-of-the-box, and that this will require a significant IT
investment which might not be visible to the decision maker currently.&lt;/p&gt;
&lt;p&gt;If you have the time and resources, try to already build up the arguments for
it by focusing on your management, support and control systems, validating how
ready they would be, and what types of investments would be needed. Compare
this with a setup where the infrastructure side of the hybrid cloud still uses
separate environments, and where you manage each environment using the
strengths of that environment.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1457745672304840709"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="hybrid"></category><category term="cloud"></category></entry><entry><title>Transparent encryption is not a silver bullet</title><link href="https://blog.siphos.be/2021/10/transparent-encryption-is-not-a-silver-bullet/" rel="alternate"></link><published>2021-10-19T08:20:00+02:00</published><updated>2021-10-19T08:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-19:/2021/10/transparent-encryption-is-not-a-silver-bullet/</id><summary type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Listing the threats to protect against&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's first list the threats you want to protect against. It is beneficial that
these threats are also scored in the organization for their likelihood of
occurrence and effect, so that you can optimize and prioritize the measures
appropriately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data leakage through theft or loss of storage media&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (OS level)&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (middleware/database level)&lt;/li&gt;
&lt;li&gt;Data leakage through application vulnerability (including injection attacks)&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through data-in-transit interception&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through local privilege escalation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all the "data leakage" threats are also about loss of confidentiality,
and any loss of confidentiality can also result in data leakage, I made the
distinction in name as the data intercepted through that threat is generally
not as 'bulky' as the others.&lt;/p&gt;
&lt;p&gt;To visualize the threats, consider the situation of an application that has a
database as its backend. The application is hosted on a different system than
the database. In the diagram, the blue color indicates an application-specific
focus. This does not mean it isn't infrastructure oriented anymore, but more
that it can't be transparently implemented without the application supporting
it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application and database interaction" src="https://blog.siphos.be/images/202110/te-accesspatterns.png"&gt;&lt;/p&gt;
&lt;p&gt;There are eight roles listed (well, technically seven roles but let's keep it simple and make "physical access" also a role), ranging from the application user to the physical access:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;application user&lt;/em&gt; interacts with the application itself, for instance
  from a browser to the web application.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;application administrator&lt;/em&gt; also interacts with the application, but has more privileges. The user might also have access to the system on which the application itself resides (but that isn't further modelled here).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;network poweruser&lt;/em&gt; is a user that has access to the network traffic
  between the client and application, as well as to the network traffic between
  the application and the database. Depending on the privileges of the users,
  these powerusers can be administrators on systems that reside in the same
  network.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware user&lt;/em&gt; is a role that has access to the application
  data in the database directly (so not (only) through the application). This
  can commonly be a supporting function in the organization.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware administrator&lt;/em&gt; is the administrator of the
  database engine (or other middleware component that is used).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system administrator&lt;/em&gt; is the administrator for the server on which the
  database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system user&lt;/em&gt; is an unprivileged user that has access to the server on
  which the database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;physical access&lt;/em&gt; is a role that has physical access to the server and
  storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, while the example is easiest to understand with a database system, be
aware that there exist many other middleware services that manage data (like
queueing systems) and the same threats and measurements apply to them as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent encryption is a physical medium data protection measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Transparent encryption, such as through LUKS (with DM-Crypt) on Linux, will
encrypt the data on the disks, while still presenting the data unencrypted to
the users. All users. Its purpose hence is not to prevent unauthorized users
from accessing the data directly, but to prevent the storage media to expose
the data if the media is leaked or lost.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Disk Encryption" src="https://blog.siphos.be/images/202110/te-tde.png"&gt;&lt;/p&gt;
&lt;p&gt;In the diagram, you notice that the transparent disk encryption only takes
effect between the server and its storage. Hence, the only 'inappropriate'
access that it is mitigating is the physical access to the server storage. Note
that physical access to the server itself is still an important attack vector
that isn't completely mitigated here - attackers with physical access to
servers will not have a too hard time to find an entrypoint to the system.
Advanced attackers might even be able to capture the data from memory without
being detected.&lt;/p&gt;
&lt;p&gt;Transparent disk encryption is very sensible when dealing with removable media
(like USB sticks), especially if they contain any (possible) confidential data
and the method for transparent encryption is supported on all systems where you
are going to use the removable media. In larger enterprises, it also makes sense
to apply as well when multiple teams or even companies have physical access and
could attempt to maliciously access the systems.&lt;/p&gt;
&lt;p&gt;For server disks or SAN storage for instance, this has to be balanced against
the downsides of the encryption. You can do disk encryption from the storage
array for instance, but this might impact the array's capability for
deduplication and compression. If your data centers are highly secured, and you
do not allow the storage media to leave the premises without being properly
wiped or destroyed, then such transparent encryption imo has little value.&lt;/p&gt;
&lt;p&gt;Of course, when you have systems hosted in third party locations, then you do
have a higher risk that the media are being removed or stolen, especially if
those locations are accessed by many others, and your own space isn't
physically further protected. So while a company-controlled data center with
tight access requirements, policies and controls that no media leaves the
premises and what not could easily evaluate to not apply transparent disk
encryption, using a public cloud service or a non-private colocation facility
should assess encryption capabilities on disk (and higher).&lt;/p&gt;
&lt;p&gt;Furthermore, a properly configured database system will not expose its data to
unauthorized users to start with, so the &lt;em&gt;system user&lt;/em&gt; role should not have
access to the data. But once you have local access to a system, there is always
the threat that a privilege escalation bug is triggered that allows the
(previously lower privileged) user to access protected files.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent database encryption isn't that much better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies (or more general middleware) offer transparent
encryption themselves. In this case, the actual database files on the system
are encrypted by the database engine, but the database users still see the data
as it is unencrypted.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Database Encryption" src="https://blog.siphos.be/images/202110/te-tdbe.png"&gt;&lt;/p&gt;
&lt;p&gt;Here again, it is important to know what you are protecting yourself from.
Transparent database/middleware encryption does prevent the non-middleware
administrators from directly viewing the data through the files. However,
system administrators generally have the means to become the database (or
middleware) administrator, so while the threat is not direct, it is still
indirectly there.&lt;/p&gt;
&lt;p&gt;The threat of privilege escalation on the system level is partially mitigated.
While a full system compromise will lead to the system user getting system
administrator privileges, partial compromise (such as receiving access to the
data files, but not to the encryption key itself, or not being able to
impersonate users but just access data) will be mitigated by the transparent
database encryption.&lt;/p&gt;
&lt;p&gt;Important to see here is that the threats related to the physical access are
also mostly mitigated by the transparent database encryption, with the
exception that database-only encryption might result in the encryption key
being leaked if it is situated on the system storage.&lt;/p&gt;
&lt;p&gt;Most of the threats however are still not mitigated: network interception (if
it doesn't use a properly configured TLS channel), admin access, database user
access, application admin and application users (through application
vulnerability) can still get access to all that data. The only focus these
measures have is data loss through physical access.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Database or middleware supported, application-driven encryption is somewhat better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies support out-of-the-box data encryption through the
appropriate stored procedures or similar. In this case, the application itself
is designed to use these encryption methods from the database (or middleware),
and often holds the main encryption key itself (rather than in the database).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Database or middleware supported data encryption" src="https://blog.siphos.be/images/202110/te-dmsde.png"&gt;&lt;/p&gt;
&lt;p&gt;While this prevents some of the attack vectors (for instance, some attacks
against the application will not result in getting a context that is able to
decrypt the data) and mitigates the attack vectors related to direct database
user access, there are still plenty of issues here.&lt;/p&gt;
&lt;p&gt;System administrators and database administrators are still able to control the
encryption/decryption process. Sure, it becomes harder and requires more
thought and expertise (like modifying the stored procedures to also store the
key or the data in a different table for them to access), but it remains possible.&lt;/p&gt;
&lt;p&gt;Because of the attack complexity, this measure is one that starts to meet
certain expectations. And because the database or middleware is still
responsible for the encryption/decryption part, it can still use its knowledge
of the data for things like performance tuning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application-managed data encryption is a highly appreciated measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With application-managed data encryption, the application itself will encrypt
and decrypt the data even before it is sent over to the database or middleware.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application-managed data encryption" src="https://blog.siphos.be/images/202110/te-amde.png"&gt;&lt;/p&gt;
&lt;p&gt;With this measure, many of the threats are mitigated. Even network interception
is partially prevented, as the network interception now is only still possible
to obtain data between the client and the application, and not between the
application and database. Also, all roles that are not application-related will
no longer be able to get to the data.&lt;/p&gt;
&lt;p&gt;Personally, I think that application-managed data encryption is always
preferred over the database- or middleware supported encryption methods.
Not only does it remove many threats, it is also much more portable, as you do
not need a database or middleware that supports it (and thus have to include
logic for that in the application).&lt;/p&gt;
&lt;p&gt;Of course, applications will need to ensure that they can still use the
functionalities of the database and middleware appropriately. If you store
names in the database in an encrypted fashion, it is no longer possible to do a
select on its content appropriately.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Client-managed data encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest level of protection against the threats listed, but of course also
the most impactful and challenging to implement, is to use client-managed data
encryption.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Client-managed data encryption" src="https://blog.siphos.be/images/202110/te-cmde.png"&gt;&lt;/p&gt;
&lt;p&gt;A web application might for instance have a (properly designed) encryption
method brought to the browser (e.g. using javascript), allowing the end user to
have sensitive data be encrypted even before it is transmitted over the
network.&lt;/p&gt;
&lt;p&gt;In that case, none of the attack vectors will be able to obtain the data. Of
course, there are plenty of other attack vectors (protecting web applications
is an art by itself), but for those we covered, client-managed encryption does
tick many of the boxes.&lt;/p&gt;
&lt;p&gt;However, client-managed data encryption is also very complex to do securely
while still being able to fully support the users. Most applications that
employ this focus already on sensitive material (like password managers) and
use end user provided information to generate the encryption keys. You need to
be able to deal with stale versions (old javascript libraries), multitude of
browsers (if it is browser-based), vulnerabilities within browsers themselves
and the web application, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Network encryption (as in the use of TLS encrypted communications) only focuses
on the confidentiality and integrity of the communication, in our example
towards the network poweruser that might be using network interception.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Network encryption" src="https://blog.siphos.be/images/202110/te-ne.png"&gt;&lt;/p&gt;
&lt;p&gt;While the majority of other threats are still applicable, I do want to point
out that network encryption is an important measure against other threats. For
instance, with network encryption, attackers cannot easily inject code or data
in existing flows. In case of the client-managed data encryption approach for
instance, the use of network encryption is paramount, as otherwise an 'in the
middle' attacker can just remove the client-side encryption part of the code
that is transmitted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I hope that this article provides better insights in when transparent
encryption is sensible, and when not. With the above assessment, it should be
obvious that transparent (and thus without any application support) encryption
methods do not cover all the threats out there, and it is likely that your
company already has other means to cover the threats that it does handle.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Full overview" src="https://blog.siphos.be/images/202110/te-full.png"&gt;&lt;/p&gt;
&lt;p&gt;The above image shows all the different encryption levels and where in the
application, database and system interactions they are situated.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1450345580778110980"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="encryption"></category><category term="transparent"></category><category term="luks"></category><category term="dm-crypt"></category></entry><entry><title>Evaluating the zero trust hype</title><link href="https://blog.siphos.be/2021/10/evaluating-the-zero-trust-hype/" rel="alternate"></link><published>2021-10-05T00:00:00+02:00</published><updated>2021-10-05T00:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-05:/2021/10/evaluating-the-zero-trust-hype/</id><summary type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the zero trust hype, two sources are driving (or aggregating) most of the
content that exists for zero trust: &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;NIST's Zero Trust Architecture
publication&lt;/a&gt; (report
800-207) and &lt;a href="https://cloud.google.com/beyondcorp/"&gt;Google's BeyondCorp Zero Trust Enterprise
Security&lt;/a&gt; resources.&lt;/p&gt;
&lt;p&gt;The NIST publication is a "dry" consolidation of what zero trust entails, and
focuses on the architecture and design principles for a zero trust environment.
It defines a zero trust architecture as an architecture that "assumes there is
no implicit trust granted to assets or users accounts based solely on their
physical or network location". &lt;/p&gt;
&lt;p&gt;The principles that it applies are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All data sources and computing services are considered resources&lt;/li&gt;
&lt;li&gt;All communication is secured regardless of network location&lt;/li&gt;
&lt;li&gt;Access to individual enterprise resources is granted on a per-session basis&lt;/li&gt;
&lt;li&gt;Access to resources is determined by dynamic policy [...] and may include
  other behavioral and environmental attributes&lt;/li&gt;
&lt;li&gt;The enterprise monitors and measures the integrity and security posture of all
  owned and associated assets&lt;/li&gt;
&lt;li&gt;All resource authentication and authorization are dynamic and strictly
  enforced before access is allowed&lt;/li&gt;
&lt;li&gt;The enterprise collects as much information as possible about the current
  state of assets, network infrastructure, and communications, and uses it to
  improve its security posture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within the publication, a common view is used to explain zero trust and the
components that take an active role within the architecture. This view is
happily shared by vendors to show where in the zero trust architecture their
component(s) are positioned.&lt;/p&gt;
&lt;p&gt;&lt;img alt="NIST core view on zero trust" src="https://blog.siphos.be/images/202110/zerotrust-core.png"&gt;&lt;/p&gt;
&lt;p&gt;The publication further evaluates a few possible architectural approaches (or
patterns if you will) for zero trust, with specific focus on the network side.
It ends with a chapter on migrating to a zero trust architecture.&lt;/p&gt;
&lt;p&gt;The Google resources through its BeyondCorp publication are more loosely written
and have a stronger focus on the cultural and principle aspects of zero trust.
One could see these publications more as an introduction to the value that zero
trust provides to a company and its users, with the focus on exposing services
everywhere, providing dynamic access controls through proxy services, and
eliminating classical patterns like using Virtual Private Networks (VPN) to bind
everything together.&lt;/p&gt;
&lt;p&gt;The main motivation beyond the zero trust principles in Google's publication is
to eliminate the perimeter-style protection where all controls are on the
perimeter, after which users have nearly free rein across the internally
exposed infrastructure.&lt;/p&gt;
&lt;p&gt;The principles it applies are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Access to services must not be determined by the network from which you
  connect&lt;/li&gt;
&lt;li&gt;Access to services is granted based on contextual factors from the user and
  their device&lt;/li&gt;
&lt;li&gt;Access to services must be authenticated, authorized, and encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While these two main resources embody the bulk of what zero trust is, it does
not determine it completely. Many vendors and consultancy firms have
their view of zero trust, which largely coincides with the above, but often
has specific attention points or even foundations that are not part of the
previously mentioned resources.&lt;/p&gt;
&lt;p&gt;The term "zero trust" implies a "trust nothing and nobody" approach to
architecture and design, which you can fill in and apply everywhere. Of course,
you eventually will need to apply some level of trust somewhere, and how this is
done can depend on so many factors that it is unlikely that we will ever settle
down in the zero trust hype on what is and isn't proper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Focus areas in zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While evaluating zero trust, I read through many other resources out there.
Besides the paywalled analyst resources from Gartner and Forrester, it also
included resources from vendors to learn how they see zero trust evolve.&lt;/p&gt;
&lt;p&gt;In most of these resources, there are commonalities that everybody seems to
agree on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approach authentication and authorization at all layers in the stack: device,
  operating system, network, communication path (next-hop), communication
  session, application, etc.&lt;/li&gt;
&lt;li&gt;Enforce high maturity in asset management and inventory management. Asset
  management is more than just devices (it also entails applications, cloud
  services, etc.) and you should not only focus on those you own, but also those
  that are associated with your architecture (such as Bring Your Own Device
  (BYOD) assets)&lt;/li&gt;
&lt;li&gt;Ensure data classification and data management are applied and continuously
  evaluated and updated.&lt;/li&gt;
&lt;li&gt;Contain workloads within sufficiently small logical bounds. This could be
  through micro-segmentation (but that is not the sole method out there).&lt;/li&gt;
&lt;li&gt;Expose services globally (as in, globally reachable), but that does not
  imply that all services are accessible by each and every one.&lt;/li&gt;
&lt;li&gt;Use dynamic access policies and policy enforcement. Dynamic includes
  context-based accesses (access decisions are taken by more than just the
  authentication side of things) as well as authorizations that can change as
  new insights are passed on (such as threat intelligence).&lt;/li&gt;
&lt;li&gt;Perform continuous monitoring, including behavioral assessments.&lt;/li&gt;
&lt;li&gt;Encrypt everything (or more soundly put, cryptographically protect resources
  at all layers of the stack).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="https://www.cisa.gov"&gt;Cybersecurity and Infrastructure Security Agency&lt;/a&gt; has
recently also released the first draft of its &lt;a href="https://www.cisa.gov/publication/zero-trust-maturity-model"&gt;Zero Trust Maturity
Model&lt;/a&gt; that
companies can use to evaluate their posture against the zero trust principles.
It is strongly based upon the NIST explanation of zero trust, with attention to
five pillars (identity, device, network/environment, application workload, and
data) and three foundations (visibility and analytics, automation and
orchestration, and governance). Again, we observe some interpretation of what
zero trust could entail, in this particular case how the US government would
like to approach this towards its agencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why zero trust isn't exactly new&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attentive readers will already understand that most of the principles or focus
areas in zero trust are not new. Let's take a few of the core components and
principles and see how novel these are.&lt;/p&gt;
&lt;p&gt;One of the core components in the zero trust architecture is a policy
enforcement methodology, one that detaches enforcement from declaration.
Separating the mechanism from a policy isn't new. &lt;a href="https://ieeexplore.ieee.org/document/502679"&gt;Decentralized trust
management&lt;/a&gt;, published in 1996,
attempted to implement the necessary abstractions for it. The &lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xacml"&gt;Extensible Access
Control Markup
Language&lt;/a&gt;,
published by OASIS in 2003, is an open standard for integrating the different policy
components.&lt;/p&gt;
&lt;p&gt;The ability to perform authentication at all levels of a stack is also not new.
We can execute device authentication using the &lt;a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module"&gt;Trusted Platform
Module&lt;/a&gt; for instance,
whose first publication was in 2009. The use of certificates for authenticating
websites is common since &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security"&gt;SSL v3 came
about&lt;/a&gt; in 1996.
Authenticating end users through passwords is as old as IT itself, and
multi-factor authentication has had plenty of research since 2005. It is very
popular nowadays since the introduction of the &lt;a href="https://datatracker.ietf.org/doc/html/rfc6238"&gt;Time-based One-time Password
(T-OTP)&lt;/a&gt; as published in 2011.&lt;/p&gt;
&lt;p&gt;Even the use of user profiling for security analytics isn't novel. In 2004, the
paper on &lt;a href="https://ieeexplore.ieee.org/abstract/document/1386699"&gt;User profiling for computer
security&lt;/a&gt; was the start
of what became a very active market in cybersecurity nowadays: User Entity and
Behavior Analytics (UEBA).&lt;/p&gt;
&lt;p&gt;The dismissal of the perimeter-only security architecture seems to be the most
specific 'new' principle, although the foundations for security have long been
to not just consider security from a network point of view: starting with the
layered architecture and requirement tracking by Peter G. Neumann's &lt;a href="http://www.csl.sri.com/users/neumann/survivability.pdf"&gt;Practical
Architectures for Survivable Systems and Networks&lt;/a&gt;
published in 2000, we have seen the market take up more and more traction on
securing the different layers and assessing security not just based on the
perimeter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Personal observations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Zero trust is energizing the cybersecurity ecosystem, allowing both active
research and commercial evolutions/improvements. With the further
digitization of our environment, the significant increase in exposed services (think
IoT), and users that are always online, companies should indeed ensure that their
services (both external-facing and internal ones) are secure. The
increase in attention through the "zero trust" hype is positive, but should not
be considered completely new. Instead, it is an aggregation of already existing
best practices and designs.&lt;/p&gt;
&lt;p&gt;The lack of a common architecture (despite NISTs efforts) is to be expected, as
each company, organization or government has a different architecture and
vision. This, of course, means that decision-makers will need to understand that
"zero trust" is not a pattern to apply blindly. Vendors will attempt to
influence businesses, but without a good understanding of the current
environment and understanding the direction a company wants to go, these will
just be tools. And as the saying goes, "A fool with a tool is still a fool".&lt;/p&gt;
&lt;p&gt;Many companies will already have started on their journey to "zero trust"
without having it named as such. Layered security, security in depth, and other
statements already contribute to the zero trust approach. If you want to
approach zero trust, it is wise to consider where you are at already, and what
main principles you want to address next. You can call it "zero trust" or your
"zero trust strategy" to get attention, but beware of external influences that
might want to inject complexity because you called it "zero trust". The benefit
is not in attaining a zero trust compliant architecture, but in ensuring the
company has a good security posture, including the flexibility to adjust as the
environment evolves.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1445380710706073613"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="zero-trust"></category><category term="security"></category><category term="enterprise"></category><category term="network-security"></category></entry><entry><title>Scale is a cloud threat</title><link href="https://blog.siphos.be/2021/09/scale-is-a-cloud-threat/" rel="alternate"></link><published>2021-09-28T17:00:00+02:00</published><updated>2021-09-28T17:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-28:/2021/09/scale-is-a-cloud-threat/</id><summary type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Perimeter controls and isolation domains&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before tackling the scale of a cloud service, let's consider an on premise
service. Services that run on premise for a company are often built up
specifically for that company, and have no relationship with other customers of
the same service. Taking the NoSQL example, companies can perfectly run NoSQL
database services on premise that have no internet presence. Moreover, these
services are also often not directly exposed to the internet.&lt;/p&gt;
&lt;p&gt;Running services within your own premises reduces the likelihood that attackers
exploit vulnerabilities of that service. Attackers that are not particularly
eyeballing your company might not know you have that service on premise. Even if
they do know, having proper protections in place should prevent direct access to
those services.&lt;/p&gt;
&lt;p&gt;&lt;img alt="On premise services" src="https://blog.siphos.be/images/202109/cloud-scale-on-premise.png"&gt;&lt;/p&gt;
&lt;p&gt;Some situations do require services to expose themselves to the internet. This
exposure increases the &lt;em&gt;attack surface&lt;/em&gt; for the service significantly.
However, these services are still part of a rather isolated deployment that I
call an &lt;strong&gt;isolation domain&lt;/strong&gt;: a logical aggregation of services that share
one or more integrations and interactions, broadening the scope of
potential vulnerabilities and misconfigurations.&lt;/p&gt;
&lt;p&gt;Separate isolation domains imply that vulnerabilities or misconfigurations
that rely on information from the domain cannot spread. This is not the same as
separate deployments or environments, as those often do share certain
integrations. For instance, all NoSQL databases within a company might use that
company's identity provider for federated authentication. But NoSQL databases
exposed to the internet from two completely different companies are often in
separate isolation domains.&lt;/p&gt;
&lt;p&gt;The Cosmos DB vulnerability exploited the fact that all Cosmos DB deployments
are part of the same isolation domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perimeter and isolation domain challenges for public cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Public cloud platform services, like Cosmos DB, are often lacking these two
attributes: they have different perimeter protections in place, and share the
same isolation domain.&lt;/p&gt;
&lt;p&gt;I do not want to imply that public cloud providers do not provide perimeter
protections against their services. They most definitely do, but the scope of
the perimeter is different from what a company would apply. Whereas a company
gains some measurable security by hiding services or ensuring those services are
not reachable from unauthorized contexts, public cloud platform services need to
be easily accessible for the public cloud to become successful. Security
paradigms like &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;Zero
Trust&lt;/a&gt; are needed to
raise the security posture of these services. Companies that are building
solutions within the public cloud will find that this requires a different
mindset, and that these environments are not comparable with the traditional
on-premise designs.&lt;/p&gt;
&lt;p&gt;For the Cosmos DB vulnerability, the FAQ mentions that instances that are not
internet facing are still somewhat impacted (as the credentials could have been
leaked) but accessing the database (by using the credentials) will not be
possible without additional vulnerabilities or misconfigurations being
addressed. This is comparable to an administrator password leakage for your
properly isolated on-premise database: while your database might not be
immediately accessible by attackers, you're still going to change the password
as soon as possible to prevent it from being used in later attacks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Public cloud platform services" src="https://blog.siphos.be/images/202109/cloud-scale-public-cloud.png"&gt;&lt;/p&gt;
&lt;p&gt;The isolation domain is a bigger hurdle to take though, as this is almost always
by design. Platform services always share interactions or integrations across
all the customers of the public cloud. Even though you have your own logical
deployment (or even ask for your own physical deployment), the main interface to
access your service is shared. The service you use to authenticate users or
systems is shared (even when it will eventually use federated authentication to
your own identity provider, the initial service is still the same).&lt;/p&gt;
&lt;p&gt;This shared isolation domain makes each public cloud service a fantastic target
for attackers (and luckily also security researchers). Exploits might not just
reveal data or insights from one customer, but from thousands of customers all
over the world. And the bigger the cloud provider, the bigger the impact.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shared control planes also imply sharing the isolation domain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This problem of using a shared isolation domain is not restricted to public
cloud platform services only. Even on premise deployments that use a public
control plane are taking part in the same isolation domain as all other
customers of the same service.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Control plane also implies sharing isolation domains" src="https://blog.siphos.be/images/202109/cloud-scale-control-plane.png"&gt;&lt;/p&gt;
&lt;p&gt;Suppose you set up a big data platform on premise, but use your vendor's SaaS
service as a control plane to manage this big data platform. This SaaS service
is also used by the other customers of that vendor, so your deployment is part
of the same isolation domain.&lt;/p&gt;
&lt;p&gt;While such setups have benefits (such as using the same control plane for
multiple deployments across different environments and even hosting setups, and
not having to manage and maintain the control services yourself) they do
increase the risk exposure in a not dissimilar fashion from the pure public
cloud services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to tackle these concerns&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Knowing about these increased risks (reachability/exposure, and the shared
isolation domain) is the foremost important part that this article wants to
address. Once these risks are considered, companies can start taking
precautions. I've mentioned the zero trust model as a way to address the
reachability/exposure risk. To address the shared isolation domain, reducing the
impact of a successful exploit can be done through proper architecture and
design that uses the "it is not if, but when" principle for cyberattacks.&lt;/p&gt;
&lt;p&gt;For instance, the data within the databases can use application-level encryption
(meaning the encryption is not done by or through the database, but by the
front-end application that interacts with the database) to reduce the impact of
data leakage through such vulnerabilities. Proper data governance processes
should also be in place to remove any data that is no longer needed on that
database. Active security validations on log data should exist to detect
deviating access patterns, and access controls should be in place to prevent
unauthorized access even from succesfully authenticated users or systems.&lt;/p&gt;
&lt;p&gt;In the Cosmos DB case, the vulnerability was possible through a selectable
feature: deployments that do not have the Jupyter Notebook feature active would
not leak the credentials. Hence, proper configuration management of services and
disabling features that are not going to be used is paramount for cloud
services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If architects are sufficiently aware of the added risks of public cloud
services, they can properly balance these risks against the benefits of the
public cloud, and make appropriate adjustments to the architecture and design of
the solutions. The main challenge here is to make sure this awareness is raised,
and that this awareness is not only reaching the architects, but also the
engineers and other stakeholders. If not, architects risk that they will be seen
as "innovation inhibitors" if they would recommend changes and improvements to
tackle these risks.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1442867880639401989"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="cloud"></category><category term="vulnerability"></category></entry><entry><title>Naming conventions</title><link href="https://blog.siphos.be/2021/09/naming-conventions/" rel="alternate"></link><published>2021-09-15T19:00:00+02:00</published><updated>2021-09-15T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-15:/2021/09/naming-conventions/</id><summary type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Naming conventions imply standardization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you settle on a naming convention, you're effectively putting some
standardization in place which you expect everybody to follow, and which should
also cover 100% of the cases. So, when assessing a possible naming convention,
first identify what standards you need to enforce and are future proof.&lt;/p&gt;
&lt;p&gt;Say you are addressing database object naming conventions. Are you able to
enforce this at all times? You might want to start tables with &lt;code&gt;tbl_&lt;/code&gt; and views
with &lt;code&gt;vw_&lt;/code&gt;, but when you are dealing with ISV software, they generally do not
allow such freedom on 'their' database definitions. Your DBAs thus will learn
to deal with setups that are more flexible anyway.&lt;/p&gt;
&lt;p&gt;Using a naming convention for internal development is of course still a
possible path to pursue. But in that case, you will need to look at the
requirements from the development teams (and related stakeholders).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standardization does not imply naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The inverse isn't true: even though you might have certain standards in place,
it doesn't mean that the object names need to reflect the standards. If your
company standardizes on two operating systems (like Red Hat Enterprise Linux
and Microsoft Windows), it doesn't mean that server names have to include an
identifier that maps to Linux or Windows.&lt;/p&gt;
&lt;p&gt;I personally often fall into this trap - I see standards, so I want to see them
fixed in the naming convention because that allows better control over
following the standards. But naming conventions aren't about control, they are
about exposing identifiable information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure it for readability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trying to add too much information in a naming convention makes it
more complex for users to deal with. You might be able to read and understand
the naming convention immediately upon seeing it, but are all the other
stakeholders equally invested in understanding the naming conventions? &lt;/p&gt;
&lt;p&gt;Say that you have a hostname that looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppchypkc05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While I can tell you that this name comes from the following convention, it
might be overdoing things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s&lt;/strong&gt; to identify it is a server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is a physical server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is hosted in a production environment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c&lt;/strong&gt; to identify it is a cattle-alike managed server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hypk&lt;/strong&gt; to identify the ownership (in this case, hypervisor usage, KVM)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c05&lt;/strong&gt; to identify it is the fifth cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;m01&lt;/strong&gt; to identify it is the first master node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reg1&lt;/strong&gt; to identify the first region (location)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if you still want to include this information, using separators might make
this more obvious. For instance, for the given name, I would suggest splitting
this as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppc-hypk-c05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first two parts are then global naming convention requirements, with the
first set being about the type of system whereas the second is about ownership,
and the third is then a naming convention specific to that owner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assets that follow a certain naming convention provide information about that
asset that a reader can immediately assume, without having to do additional
lookups. The intention here is that you want to define important information
that many stakeholders will need immediately to support their work (and thus
their efficiency). Insights that are useful for a select set of stakeholders
might not be suitable for a naming convention (or at least not a global one).&lt;/p&gt;
&lt;p&gt;You should consider every stakeholder that comes in contact with the name of
the asset, and how that stakeholder would obtain the information they need. If
you have a central, easily accessible configuration management system, it might
be possible to have many structured insights exposed through that interface,
but is that useful when you are dealing with lists of assets?&lt;/p&gt;
&lt;p&gt;Suppose you do not include the host class for hostnames, with the host class
being what class of system the host is (server, workstation, router, firewall,
appliance, ...). Does your SOC team need this insight every time they are going
through events? Does your helpdesk need that information? What about the
resource managers?&lt;/p&gt;
&lt;p&gt;If all these stakeholders do need that information over and over again, it
might be sensible to include it in the naming convention. If, however, only a
few stakeholders need that information, you might want to expose that easily
through different means. For instance, resource managers might be able to easily
join that information with the asset management system information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information NOT to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes, you want to have some information about objects easily available,
but not for everybody. It might be information that can be abused for nefarious
purposes. In that case, you want this information to be shielded and only
offered to authenticated and authorized users. For instance, if you use separate
accounts for administering systems, you might not want to add information about
what type of admin account it is, as account enumeration might reveal too much
immediately and provide attackers with better insights.&lt;/p&gt;
&lt;p&gt;So, rather than having &lt;code&gt;ken_adadmin&lt;/code&gt; for Ken's Active Directory administration
account, stick to a nonsensible account identification like &lt;code&gt;ua1503&lt;/code&gt; (user
account 1503). Stakeholders that need information about accounts, in this case,
can still notice it is a user account rather than a system or machine account
and will need to query the central repositories for more information (such as
AD to get information about the user - and don't forget to add sensitive users
to, for instance, the &lt;code&gt;Protected Users&lt;/code&gt; group in AD).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use layered naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With "global naming convention" I am suggesting the ability to add naming
conventions for specific purposes, but leave that open in general. A server
name could, for instance, require an indication of the environment (production or
not) and the fact that it is a server (and not a workstation), but leave a part
of the name open for the administrators. The administrators can then add their
local naming convention to it.&lt;/p&gt;
&lt;p&gt;An active directory group, for instance, might have a standard global naming
convention (usually the start of the group name) and leave the second part
open, whereas specific teams can then use that part to add in their local naming
convention. Groups that are used for NAS access might then use a naming
convention to identify which NAS share and which privileges are assigned,
whereas a group that is used for remote access support can use VPN naming
conventions.&lt;/p&gt;
&lt;p&gt;The University of Wisconsin has their &lt;a href="https://kb.wisc.edu/iam/page.php?id=30600"&gt;Campus Active Directory - Naming
Convention&lt;/a&gt; published online, and
the workstation and server object part is a good example of this: while the
objects in AD have to follow a global naming convention (because Active
Directory is often core to an organization) it leaves some room for local
department policies to assign their own requirements:
&lt;code&gt;&amp;lt;department&amp;gt;&amp;lt;objectfunction&amp;gt;-&amp;lt;suffix&amp;gt;&lt;/code&gt; only has the first two fields
standardized globally, with the &lt;code&gt;&amp;lt;suffix&amp;gt;&lt;/code&gt; field left open (but within certain
length constraints).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consider the full name for your naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do want to add information in a naming convention, do not consider
this purely on a single object type, but at the full name. A hostname by itself
is just a hostname, but when you consider the fully qualified hostname (thus
including domain names) you know that certain information points can be put in
the domain name rather than the hostname. The people over at &lt;a href="https://www.serverdensity.com/"&gt;Server
Density&lt;/a&gt; have a post titled "&lt;a href="https://blog.serverdensity.com/server-naming-conventions-and-best-practices/"&gt;Server Naming
Conventions and Best
Practices&lt;/a&gt;"
where they describe that the data center location (for the server) is a
subdomain.&lt;/p&gt;
&lt;p&gt;Another example is for databases, where you not only have a table, but also the
database in which the table is located. Hence, ownership of that table can
easily be considered on the database level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learn from mistakes or missing conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you approach naming conventions, you will make mistakes. But before making
mistakes yourself, try looking out for public failures that might have been due
to (bad or missing) naming conventions. Now, most public root cause analysis
reports do not go in-depth on the matter completely, but they do provide some
insights we might want to learn from.&lt;/p&gt;
&lt;p&gt;For instance, the incident that AWS had on February 28th, 2017, has a &lt;a href="https://aws.amazon.com/message/41926/"&gt;Summary of
the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1)
Region&lt;/a&gt;. While there is no immediate
indication about the naming conventions used (mainly that a wrong command input
impacted more servers than it should), we could ask ourselves if the functional
purpose of the servers was included in the name (or, if not in the name, if it
was added in other labeling information that the playbook should use). The
analysis does reveal that AWS moved on to implement partitions (which they call
cells), and the cell name will likely become part of the naming convention (or
other identifiers).&lt;/p&gt;
&lt;p&gt;Also internally, it is important to go over the major incidents and their
root causes, and see if the naming conventions of the company are appropriate
or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Still need examples?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While most commercial companies will not expose their own naming conventions
(as there is no value for them to receive, and it exposes information that
malicious users might abuse), many governmental agencies and educational
institutions do have this information publicly available, given their
organization public nature. Hence, searching for "naming convention" on &lt;code&gt;*.gov&lt;/code&gt;
and &lt;code&gt;*.edu&lt;/code&gt; already reveals many examples.&lt;/p&gt;
&lt;p&gt;Personally, I am still a stickler for naming conventions, but I am slowly
accepting that some information might be better exposed elsewhere.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1438175688444596227"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="naming"></category></entry><entry><title>Location view of infrastructure</title><link href="https://blog.siphos.be/2021/09/location-view-of-infrastructure/" rel="alternate"></link><published>2021-09-07T18:00:00+02:00</published><updated>2021-09-07T18:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-07:/2021/09/location-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;


&lt;p&gt;&lt;img alt="Location view representation" src="https://blog.siphos.be/images/202109/location-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through the layers from bottom to top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Easiest to represent: geographic location and facilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The geographic location is the least IT-specific view out there, as it
represents where everything is in the world. These views are popular not only to
scope projects better (like data center locations) but also to support getting
important infrastructural metrics.&lt;/p&gt;
&lt;p&gt;WAN latency, for instance, is limited by the distance (you can't outsmart
physics), and by knowing the path between two points, you can calculate the
throughput and latency (such as through the &lt;a href="https://wintelguy.com/wanlat.html"&gt;Wintelguy WAN Latency
Estimator&lt;/a&gt;). When designing redundant network
connections between separate locations, you might depend on multiple line
providers. To ensure there are no chokepoints where both providers have their
lines go through the same location, you can ask for the fiber path details to
validate this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A10's DDoS Threat Intelligence
map" src="https://blog.siphos.be/images/202109/a10-ddos-threat-intelligence.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://www.a10networks.com/"&gt;A10 Networks&lt;/a&gt;, through their &lt;a href="https://www.a10networks.com/products/network-security-services/threat-intelligence-service/"&gt;DDoS
Threat Intelligence
Service&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Using geographic locations also facilitates understanding by other stakeholders,
even if it has a less technological impact on the case at hand. For instance,
while undergoing active DDoS attacks, a geographic representation of where they
come from helps to get more understanding from management, even though on
network-level you're more interested in the Autonomous System (AS) networks that
are involved. Those are very large groups of networks that cover the main
global-wide routing of data.&lt;/p&gt;
&lt;p&gt;If we drill down from a geographic location, the next view is the
facility-related one. Here, the view focuses on building design and
infrastructure (such as HVAC and power distribution in data rooms or data
centers), as well as the location of individual devices (floor plans, rack
spaces). Facility views help not just with initial network designs where you
want to onboard a new headquarter location, but also with capacity management
within data centers (identifying hotspots), dealing with wireless networks and
their impact on the surroundings, cable management for networks, ensuring the
resilience of infrastructure services and more.&lt;/p&gt;
&lt;p&gt;A decent facility view is very helpful when dealing with operational technology
environments (IoT), and can be dynamically generated. A while ago, I was at a
conference where they showed people's movement based on the data received from
their smartphones. They used the view to see which areas were too crowded (it
was pre-COVID times), as well as to see if there are sudden movements that might
indicate problems or threats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Foundations for networks: connectivity, underlay, and virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next three layers in the location view focus on the foundations for a
company's network. They are strongly IT oriented with the main stakeholders
being the telco- and infrastructure related teams and roles. Unlike the higher
level viewpoints, the foundations require more thought in their design as errors
are harder to correct.&lt;/p&gt;
&lt;p&gt;The connectivity focuses on the cabling and other connections made between
devices. This includes backplane-related connectivity, something that is
relevant when using enclosures or pre-engineered systems. Connectivity and
cabling seem rudimentary, but are critical for the proper functioning of a
network. Remember the science report about possible faster-than-light
neutrino's? Well, a &lt;a href="http://blogs.nature.com/news/2012/02/faster-than-light-neutrino-measurement-has-two-possible-errors.html"&gt;faulty connection was partially to
blame&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The network underlay is the network view that network engineers have on their
network. For small environments, the network view from the engineering point of
view might be the same as the view from the application side, but for larger
environments, I often see a distinction between the two. And when that occurs,
the underlay view is less of a concern for application engineers and business
stakeholders (unless of course there are major issues with the underlay design),
but that does not make it less important. People are often not aware of how our
electricity net works, but if it fails, we're all affected. Similarly, if the
network underlay is badly designed, the higher networks will see troubles too.&lt;/p&gt;
&lt;p&gt;The network virtualization stack is a technology component that supports
building virtual networks on top of the underlay environment. So while the
underlay is like the foundation on top of which all networks are hosted, the
virtualization makes this possible. In that sense, it is similar to the
hypervisor level on the component view (and perhaps is less of a location view
than a component one, although network virtualization technologies do require a
common understanding of the full network to function properly).&lt;/p&gt;
&lt;p&gt;Companies use different virtualization technologies and concepts. A simple
network virtualization technology is the VLAN (Virtual LAN), which presents
itself as a single broadcast domain to the participating systems, even though
these systems might not be physically connected to the same switch or switch
environment. It is even possible to stretch VLANs across wide areas.&lt;/p&gt;
&lt;p&gt;But virtualization can go further. Technologies such as Cisco ACI or VMware NSX
don't just focus on the LAN level, but also virtualize the network on the
addressing and routing part. And with Network Functions Virtualization (NFV), we
also include firewalls and traffic control. However, do not consider NFV to just
be the next phase beyond Software Defined Networks (SDN), as NFV and SDN are
different beasts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;View from application and system side: topology and protocols&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest levels in the location view focus on the network as it is seen by
the business applications and systems that a company hosts.&lt;/p&gt;
&lt;p&gt;The network topology is the view on segregation, segments/subnets, and the
network functions that take part in the overall environment (such as DNS/DHCP/IP
address management, firewall functionality, proxies, and other gateways). This
is the view that is probably going to change the most, as it constantly evolves
based on business demand and IT evolutions. Topology views are not just
one-of-a-kind: depending on the scope you want to address, multiple views might
be needed to convey the message you want to tell.&lt;/p&gt;
&lt;p&gt;One type of view within the topology is the &lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning
view&lt;/a&gt; which I've
covered before, and which is very expressive towards the other stakeholders: it
covers the entire company's environment while abstracting enough of the details
so that it remains understandable.&lt;/p&gt;
&lt;p&gt;If we were to zoom in further on the network topology, you get into the specific
interactions that are made between systems, which are standardized in protocols.
But while the network (and application) protocols are often very standardized,
they are also very challenging to understand.&lt;/p&gt;
&lt;p&gt;The main challenge is that there are so many protocols out there, with so many
options and implementation choices, that you need to be an expert to
troubleshoot issues if they arise. Web applications aren't just disclosed over
the HTTP protocol: you have channel encryption (TLS), might be using WebSockets,
or even the QUIC protocol. And if you think you understand HTTP, do you
understand HTTP/2?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The past few posts (with a few historical ones) make up what I consider being 
the infrastructure domain, and how to structurally approach changes within. Of
course, these are not the only views out there, and based on the project ahead,
different viewpoints might come up. But for a holistic view of the
infrastructure domain, I think these five cover it well.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1435271642507264000"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="location"></category><category term="network"></category><category term="virtualization"></category><category term="protocol"></category></entry><entry><title>Process view of infrastructure</title><link href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/" rel="alternate"></link><published>2021-09-01T11:20:00+02:00</published><updated>2021-09-01T11:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-01:/2021/09/process-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Six process groups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of process frameworks out there. I've covered many of these
in a previous article (&lt;a href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/"&gt;What is the infrastructure domain?&lt;/a&gt;), with &lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL&lt;/a&gt;
and &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT&lt;/a&gt; being my main resources.&lt;/p&gt;
&lt;p&gt;Companies often select a mature framework to align their IT on. After all, why
invent everything over and over again if a popular framework with all its
resources exists. But just like how companies like to use commercially available
resources, they also like to adjust and nudge it left and right to fit
the organization better. And that's fine.&lt;/p&gt;
&lt;p&gt;I'm going to give a spin to it and combine processes with non-functionals, as
infrastructure is often about non-functionals. That doesn't mean that frameworks
like ITIL or CObIT are not good, but explaining them easily is sometimes a
challenge, and I'm not versed enough in the intricacies of these frameworks
to use them as a starting point, rather as a reference.&lt;/p&gt;
&lt;p&gt;The six groups that I feel cover the infrastructure domain sufficiently are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt;, which is about the company and the organization
  used within the company.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Consumers &amp;amp; Suppliers&lt;/em&gt;, which is about the interaction (and supporting needs)
  with the consumers of our services, as well as with the providers.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Research &amp;amp; Development&lt;/em&gt;, which is about preparing updates on the services
  and architecture&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Risk &amp;amp; Security&lt;/em&gt;, which enables risk reduction strategies and facilitates
  secure integrations of services&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Custodianship&lt;/em&gt;, which is about facilitating maintenance and support
  improvements that are more environment-wide&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Engineering &amp;amp; Operations&lt;/em&gt;, which focuses on the operational control and
  support of the services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In some ugly visualization, these groups (and the processes or non-functionals
that are within) can be represented as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Six process groups for infrastructure" src="https://blog.siphos.be/images/202109/six-process-groups.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through each of the groups in a bit more detail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Governance and Organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first group is titled &lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt; and covers the company
specifics. It has five processes or non-functionals in it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strategy and Innovation&lt;/li&gt;
&lt;li&gt;Enterprise Architecture&lt;/li&gt;
&lt;li&gt;Organizational Efficiency&lt;/li&gt;
&lt;li&gt;Separation of Concerns&lt;/li&gt;
&lt;li&gt;Formalization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;strategy&lt;/em&gt; of a company is an important starting point for larger changes,
as architects need to make sure that the changes they want to guide, coach, or
support are aligned with the company strategy. Most companies will have a
hierarchy of strategies, with the main strategy being translated to specific
strategies or strategic objectives, which are then further formalized down.&lt;/p&gt;
&lt;p&gt;Alongside supporting the company strategy in general, domain architects might
need to plan the strategy for their domain as well (as I had to do for the
infrastructure domain). This, again, is a translation of the overall strategy,
showing how infrastructure will support the strategic objectives.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;innovation&lt;/em&gt; part, the infrastructure domain needs to make clear how to
support innovative ideas and suggestions in the organization. Often,
the infrastructure and operations field is considered to be protective, and
might be perceived as obstructing innovative ideas. That's not a correct view -
while the operations field often has a more conservative view of the
infrastructure to make sure the production environment is available and secure,
it also has a viewpoint on how to deal with innovation. For instance, the
delivery of sandbox environments in which innovations can play a role, or
prototype environments that have limited, secured integration possibilities
toward the rest of the environment.&lt;/p&gt;
&lt;p&gt;Innovation is not only limited to technological innovation. Innovative ideas on
governance and organization (such as when agile development practices were
being formulated) are also important to track, as they influence many
other processes and non-functional attributes.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;enterprise architecture&lt;/em&gt; part covers abstracting and guiding the
organization, the business, the product development, etc. in a coherent and
well-documented manner. It lays the foundations for an effective and efficient,
collaborative, large organization. Domain architects ensure that their domain
architecture is related to the enterprise architecture (and contributes to the
enterprise architecture directly).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Organizational efficiency&lt;/em&gt; focuses more on how the organization functions.
Does the organization use DevOps teams, for instance, or is it more traditional
in the development versus operations part? Does the organization have vertical
technology-oriented teams, or more horizontal, solution-driven teams, or a
mixture? Knowing how the company organizes its IT is important to properly
structure and present changes. Domain architects also provide input towards
reorganizations, as they can easily define how such shifts in responsibilities
will affect the organization and its efficiency.&lt;/p&gt;
&lt;p&gt;With the &lt;em&gt;segregation of duties&lt;/em&gt;, I focus on which roles can be shared and which
ones can't. Knowing which segregation applies in the organization (such as
the different security officer focus, the segregation between risk officers and
audit officers, the exclusivity between domain administrators (in Active
Directory terms) and regular server administrators) is an important driver
for architecting. Architects can suggest introducing additional segregations or
suggest methods for removing the need to segregate these duties (as often those
are inspired by historical events and assessed on older capabilities). I've
mentioned DevOps before, and those who supported a DevOps transition will know
about the historical segregation between development and operations.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;formalization&lt;/em&gt; is about how to have formal evidence of decisions.
While this is often just part of a company's 'governance', I focus on it
specifically, as it is always balancing efficiency and effectiveness. When
formal evidence is expected by the organization, it is wise to keep track of why
this is. Often, formalization can be optimized further without reducing the
benefits or impacting the requirements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consumers and Suppliers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whereas the first group focused on the company itself, the group on consumers
and suppliers focuses on the users of the infrastructure services that are
offered (which often, but not always, are internal customers) and the suppliers
for the various services the organization consumes.&lt;/p&gt;
&lt;p&gt;It covers the following four processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost &amp;amp; Licensing&lt;/li&gt;
&lt;li&gt;Portfolio&lt;/li&gt;
&lt;li&gt;Agreements &amp;amp; Support&lt;/li&gt;
&lt;li&gt;Incidents &amp;amp; Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;cost and licensing&lt;/em&gt; part is often a large time-consumer. I include
chargeback and showback here as well, although I must be very clear that actual
costs and chargeback-reported costs are not the same. For the services
infrastructure offers internally, knowing the costs (showback) and charging the
costs through to the internal customers (chargeback) are hard processes to
tackle, requiring intensive thoughts on what is and isn't allowed, how the
company looks at the services, etc.&lt;/p&gt;
&lt;p&gt;While looking at the suppliers, an important part is to understand and optimize
the licensing. Each product and service that you consume costs money one way or
another. I've had the "pleasure" of being an Oracle license manager (as in,
responsible within my company for tracking, reporting, and optimizing the costs
associated with all Oracle products we consumed) for a while, and being part of
the Microsoft license management team (with responsibility for data center
oriented products). Knowing the license requirements, the terms and conditions,
the contractual obligations (and deviations that a company negotiated), etc. is
very useful.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;portfolio&lt;/em&gt; is about knowing what you consume (from vendors) and offer
internally, and how you intend to track and evolve it. For infrastructure
services, for instance, you want to make sure you have a decent catalog (which
is part of the portfolio) that your internal customers can consume. Designing
the catalog is an important first step in assessing and deriving the domain
architecture if it isn't available yet.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;agreements and support&lt;/em&gt; I look not just at the contractual agreements
related to service consumption (as in, the terms and conditions related to the
licensing), but also towards agreements on areas such as support (Can we call
the vendor any time of the day? How much time does the vendor contractually
have before they 'pick up the phone'? Is the service agreement conforming to
market expectations?). The same is true for the service agreements offered
internally - something that is best aligned with the portfolio.&lt;/p&gt;
&lt;p&gt;Deriving decent service level agreements (SLA), and ensuring they can be
tracked and asserted, can be important if the vendor isn't all that
trustworthy, as well as to show your internal customers that you care about
reaching and keeping the SLAs.&lt;/p&gt;
&lt;p&gt;Support is also about how to reach and interact with the support organization.
From an infrastructure point of view, that isn't always as easy as it sounds.
Some vendors require specific applications to interact with their support
organization, and your company might not allow those applications. Or, certain
metrics need to be sent out to a cloud service, but that cloud service isn't
easily identified as being secure and compliant enough with the regulations you
have to cover.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;incidents and problems&lt;/em&gt; item covers the standard incident and problem
resolution processes, and how these are handled in the organization. It is
about standardizing what incidents are, how to react to them, how to derive
problems from the observations, prioritizing work related to incidents and
problems, and more. A decent incident and problem tracking solution is a must,
but the solution itself is just part of the setup. A good tool does not give
you an efficient and effective organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research &amp;amp; Development&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next group is about evolving the service offerings. I consider the following
four processes and non-functionals as part of this group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Design&lt;/li&gt;
&lt;li&gt;Product Lifecycle&lt;/li&gt;
&lt;li&gt;Quality Control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With &lt;em&gt;architecture&lt;/em&gt;, I focus on the solution architecture, and how the solutions
interact with the domain architecture. The domain architecture is generally
part of the enterprise architecture, whereas the solution architecture is
something that regularly needs updates based on the changes that are being
planned. Most of this architecture is done by the system architects with
the support or coaching of the domain architect.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;design&lt;/em&gt; is the next phase of a development cycle and is more detailed
than the solution architecture. Designs are often handled by the
engineering teams themselves, with the support of the system architects. For
domain architects, knowing where the designs are and to read them/understand
them is vital for a good collaboration between architects and engineering teams.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;product life-cycle&lt;/em&gt; focuses on the entire life-cycle of a product, starting
with innovative ideas and research, prototypes, towards supporting the
development of the product, and even after deployment towards end-of-life
support/tracking, or in case of bought products the end-of-sale,
end-of-premier-support, extended support, custom support, and whatnot.&lt;/p&gt;
&lt;p&gt;Balancing the product life-cycles against each other is a common occurrence for
architects and product owners, as it is always a puzzle about when to make which
changes, release what versions, etc. If you don't track the life-cycle of a
product continuously, you might face situations where you need to
purchase older products because your reinvestment wasn't planned yet, but
capacity limits require an increase anyway.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;quality control&lt;/em&gt; is about ensuring the quality of the products is 
according to expectations. This includes support for different environments
(pre-production), specific quality testing (which I'll discuss later as well),
supporting QA teams (if you have those), and the processes for reporting
defects, etc. It also includes quality assurance on products purchased from
third parties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Risk &amp;amp; Security&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A major part of my work is to assess the risk exposure and ensuring a secure
and reliable infrastructure. Hence, it shouldn't come as a surprise that it is
an entire group by itself.&lt;/p&gt;
&lt;p&gt;While security is a large domain (with lots of focus on processes and
assurance), the following processes and non-functionals are strongly
represented in the infrastructure domain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crypto&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorization&lt;/li&gt;
&lt;li&gt;Privacy&lt;/li&gt;
&lt;li&gt;Access Control&lt;/li&gt;
&lt;li&gt;Audit &amp;amp; Compliance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;em&gt;crypto&lt;/em&gt;, a major challenge is not only to ensure cryptographic services or
protocols are used where it makes sense (or better said, not used where it
makes sense) but also to understand the intricate details of the cryptographic
services, knowing what service is used for which purpose, etc. I could make an
entire article purely to discuss the sense and nonsense of transparent
encryption on file systems or databases...&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;authentication&lt;/em&gt; processes (and the closely related identity processes) are
to support assurance on the identity of a user, process, system, device, or
any other type of subject. Knowing how the authentication is handled, which
authentication protocols are used, the landscape in case of federated
authentication, etc. can take up several days to know. Authentication is no
longer based on user IDs and passwords. We have OpenID Connect, SAML, Kerberos,
TACACS, RADIUS, NTLM, and more, which all have their quirks. And those are just
the protocols to handle authentication: they don't talk about user management,
the processes you will need to support in case of account abuse, etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Authorization&lt;/em&gt;, while often combined with the authentication phase, is about
knowing what a (freshly authenticated) identity may do (authorized). Here, we
have the challenges of coarse-grained authorization versus fine-grained
authorizations, dynamic authorizations, transient authorizations, or
authorizations that are inherited from others. Often, architects will need to
design the authorization granularity and approach based on the organizational
and security requirements.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;privacy&lt;/em&gt; controls are about ensuring confidential or strictly confidential
data (if those are the terms used by the organization) are properly protected.
Data can be anonymized, pseudonymized, redacted, tokenized, encrypted, and/or
de-identified. Architects should know which control is possible where, which
services can be used, what the impact is of the controls, as well as what the
organizational data requirements are.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;access control&lt;/em&gt; part is closely related to the authorization part. In
effect, it should be what enforces the authorizations. Access control is a wide
domain with many, many products and services working with (or against
apparently) each other. Especially in more modern architectures where zero
trust plays a role, you'll notice that access control is a challenging beast,
with dynamic and contextual controls becoming primary services rather than the
standard, relatively static (role-based) access controls.&lt;/p&gt;
&lt;p&gt;The last part is the &lt;em&gt;audit and compliance&lt;/em&gt;, where audit focuses on obtaining
traceability of all events (what has happened where, when, by whom), whereas
compliance looks at assuring current state and processes are according to
expectations. Compliance can be about assuring adherence to the organizational
processes and standards, but also that a system's configuration is accurate and
still in effect. So, yes, it is more than what a "compliance" department would
focus on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custodianship&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In custodianship, I group processes and non-functionals that often play a more
active role after having a successful deployment, or after a project is
finished. While these do not imply that they are to be implemented by different
teams (that's the organizational efficiency which has to decide on this), I
notice that they are challenging to keep up properly for a large organization.&lt;/p&gt;
&lt;p&gt;In it, I cover the following processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Governance&lt;/li&gt;
&lt;li&gt;Rationalization&lt;/li&gt;
&lt;li&gt;Reporting &amp;amp; Insights&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Data governance&lt;/em&gt; is about defining and tracking data, data flows, data
definitions, as well as their purpose. You need proper data governance to know
which privacy measures to apply, as one of its measures is the retention of the
data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Rationalization&lt;/em&gt; is the effort to rationalize existing infrastructure usage
and services. While major rationalization exercises are company-wide
initiatives, there are many benefits to achieve with small, incremental
rationalization exercises. Most of the time, rationalization exercises are
about cost reduction, but that doesn't always need to be the case. Of course,
eventually, everything is about finances nowadays.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;reporting and insights&lt;/em&gt;, I consider the means to report on various areas
(such as capacity, cost, performance, and SLA breaches), as well as gain
insights from the data at hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Engineering &amp;amp; Operations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final group covers the disciplines that I see on the engineering and
operations side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Orchestration&lt;/li&gt;
&lt;li&gt;Testing&lt;/li&gt;
&lt;li&gt;Change Management&lt;/li&gt;
&lt;li&gt;Operational Control&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;configuration&lt;/em&gt; part is to ensure that the systems and services are
properly configured and that the life-cycle of the configuration items is
guaranteed as well. &lt;/p&gt;
&lt;p&gt;With &lt;em&gt;orchestration&lt;/em&gt;, the focus is on ensuring larger environments are optimally
used and the appropriate abstractions are in place. Kubernetes is a good
example of an orchestration that requires close attention and support. But
others exist as well, such as those in the Hadoop ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt; focuses on the various testing strategies that enable us to trust the
final product and services and help in ensuring no regressions are
creeping in. Testing on the infrastructure side is about load and performance
testing, smoke testing, regression testing, destructive testing, etc.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;change management&lt;/em&gt; process is about properly staging the changes,
communicating the changes, following up on the changes, etc. It is not just
about preparing a deployment and then hitting a button: you want to validate
that the change is successful, track the performance to ensure nothing is acting
strangely, etc.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;operational control&lt;/em&gt;, I consider the systems that drive the operational
systems autonomously. Self-driving and self-healing are the two non-functionals
I embed under this. Many cluster management systems are part of this, and
designing for self-driving and self-healing infrastructure comes up more and
more with modern systems.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;monitoring&lt;/em&gt; covers tracking the telemetry of the systems, the logs
that are generated, and derive the right insights from it. I initially wanted
to call it "Observation" as that seems to be the term that comes up more and
more (monitoring too much resembles watching telemetry for thresholds, whereas
observation goes much beyond that) but monitoring seems to resound most amongst
users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A huge list that covers most of the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This list of processes and non-functional attributes covers most, if not all,
requirements that are related to the infrastructure domain. The component
view that I mentioned in a previous post, for instance, is part of the
architecture and design processes in the Research &amp;amp; Development group.&lt;/p&gt;
&lt;p&gt;However, because they are still processes and non-functionals, they can often
seem to be less tangible. Sure, you have to manage the costs, but how do you do
that? What processes do you have in place to manage cost insights? How do you
deliver these insights? What tooling is used to support the organization? What
tooling is mandatory to use (like license management tools from larger vendors)?
Detailing this and making the right choices is part of being an architect.&lt;/p&gt;
&lt;p&gt;In the next post, I will look at the location view. Unlike the process view,
which is often shared with other IT domains, the location view is something
that is often more exclusive for the infrastructure domain.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1432996739846397957"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="process"></category></entry><entry><title>Component view of infrastructure</title><link href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/" rel="alternate"></link><published>2021-08-27T21:10:00+02:00</published><updated>2021-08-27T21:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-08-27:/2021/08/component-view-of-infrastructure/</id><summary type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Infrastructure domain in viewpoints&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While searching for a way to describe the infrastructure domain,
I tend to align with certain viewpoints as well, as it allows architects
to decompose a complex situation into more manageable parts. So the question
is no longer "how do I show what the infrastructure domain is", but rather
"what different viewpoints do I need to cover the scope of (and
explanation on) the infrastructure domain".&lt;/p&gt;
&lt;p&gt;I currently settle on five views:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;component view&lt;/em&gt;, which covers the vertical stack of an IT infrastructure
  component.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;location view&lt;/em&gt;, which is the horizontal stack for IT infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;process view&lt;/em&gt;, which covers the general enterprise requirements for IT
  infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;service view&lt;/em&gt;, which provides insights into what functional offerings are
  provided (and for which I posted a current view a short while ago, titled "&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An
  IT services overview&lt;/a&gt;")&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;zoning view&lt;/em&gt;, which represents the IT environment landscape. A few years
  ago, I covered this as well in "&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;Structuring infrastructural
  deployments&lt;/a&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these views are related to each other, but represent insights that are
particularly useful for certain discussions or representations. Some viewpoints
are even details for another. For instance, the &lt;em&gt;zoning view&lt;/em&gt; is a view that
provides more detail on a particular layer in the &lt;em&gt;location view&lt;/em&gt;. A simple
relationship between the above five views is the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between the five infrastructure views" src="https://blog.siphos.be/images/202108/five-infra-views.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, this isn't a proper meta-model, just a representation. It starts
with what the infrastructure domain has to accomplish (process view),
which defines the services the domain has to support. These services
comprise several components, and these are deployed in various
zones across the organization. The zone overview is part of the more
elaborate location views.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Components are a good introduction to infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While a good coverage of the infrastructure domain would start with the
process view, I think it is not always the easiest. Not all stakeholders
are fully acquainted with processes and what they entail, and I feel it
might be easier to start with a more tangible view, i.e. a component
view.&lt;/p&gt;
&lt;p&gt;For instance, when explaining what IT infrastructure is to an outsider
(say, a family member that isn't active in the IT world), I often start with
a component view (often using a cellphone as a starting example), then going
about the massive amount of components that need to be managed, hence the need
for proper processes. After elaborating a bit on the various processes involved,
we can then go to a service overview, to then move on to the hosting of all
those services in a structured and reliable environment (zoning), with the
various challenges related to locations.&lt;/p&gt;
&lt;p&gt;So, what is the component view that I reuse a lot? It is basically
the vertical stack that most hosting-related services use to explain where
their product is situated:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layered view on a component level" src="https://blog.siphos.be/images/202108/component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;If you start with a cellphone view, then you can easily describe the hardware,
operating system, application, and data layers in the view. You can mention that
the hardware is an expensive one-time investment that the user hopes to use for
a few years (so you can explain &lt;em&gt;capital expenditures (CapEx)&lt;/em&gt; and &lt;em&gt;operational
expenditures (OpEx)&lt;/em&gt;. The latter can be a cloud service that the
user synchronizes its data to, like Apple iCloud or Google Drive).&lt;/p&gt;
&lt;p&gt;The distinction between operating system and application, and its impact on
the users, can also be explained easily: operating system upgrades are
heavier, and users often want to choose when this occurs, as operating system
upgrades are not always fully backward compatible. Or, the user's hardware isn't
supported on the next operating system (e.g. upgrading Apple iOS 12 to iOS 13,
or Android 10 to Android 11). Applications, on the other hand, are often
automatically updated and are less intrusive. However, because there are
many applications, managing the application landscape can be more daunting than
the operating system one.&lt;/p&gt;
&lt;p&gt;Then we can move on to the scaling challenges that an organization has to
face, which will gradually build up more insights into the component layers. For
instance, if a company is developing and maintaining a mobile application, it
wants to test its new releases on different operating system
versions.  But it would not be sensible to have each developer walk around with
six phones because they need to test the application on iOS 12, iOS 13, iOS 14,
Android 9, Android 10, and Android 11. Instead, testing could be done on
emulators (which can be considered hypervisors, albeit often not that exhaustive
in features).&lt;/p&gt;
&lt;p&gt;This introduces concepts of optimizing resources for cost, but also the
benefits of having these services available 'at distance' (remote access
to the emulation environments) as well as first steps in virtualization.
You can state that this emulation is something the user can do on their
laptops, but that in enterprise environments this is done with either
cloud services or on the enterprise servers, as that facilitates collaboration
with team members, and simplifies managing these assets when the teams get
larger or smaller. And these servers, well, they too are virtualized for
resource optimization.&lt;/p&gt;
&lt;p&gt;We can also discuss the data layer, and the challenge that a regular user
has when their phone is near its limits (e.g. storage is full), the options the
user has (add SD card if the phone supports it, or use cloud storage services),
and compare that with larger enterprises where data hosting is often either
centralized or abstracted, so that systems are not bound to the limits of their
device's storage capacity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views enable scalability and cost insights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The layered view on components, of course, is a meta-view rather than an actual
one: it shows how a stack can be built up, but the actual benefit is when
you look at the component view of a solution.&lt;/p&gt;
&lt;p&gt;For instance, if we were to assess a Kubernetes cluster, it could be represented
as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kubernetes component view" src="https://blog.siphos.be/images/202108/k8s-component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Going bottom-up on this view, we can identify (and thus elaborate on) the
various layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the hardware level, we see four physical servers (named sppc01 to sppc04).
  These servers are of a particular brand and have 32 Gb of memory each (which
  isn't a lot, the cluster is rather small).&lt;/li&gt;
&lt;li&gt;KVM is used as the hypervisor. The hypervisor combines the four physical
  servers in a single cluster.&lt;/li&gt;
&lt;li&gt;KVM then provides eight virtual systems (named svpc01 to svpc08) from the
  cluster. The first three are used for the Kubernetes control plane, the others
  are the worker nodes. Note that it is recommended to host the nodes of the
  control plane on different physical machines so that a failure on one physical
  machine doesn't jeopardize the cluster availability. This can be configured on
  the hypervisor, but that is outside the scope of this article.&lt;/li&gt;
&lt;li&gt;The physical servers use a hardened Gentoo Linux operating system using the
  musl C library, whereas the virtual servers use a regular Gentoo Linux
  installation as their operating system.&lt;/li&gt;
&lt;li&gt;The orchestration layer is Kubernetes itself, using the CRI-O container
  runtime as middleware.&lt;/li&gt;
&lt;li&gt;The applications depicted are those of the Kubernetes ecosystem, with the main
  control plane applications and worker node applications listed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we were to host an application inside the Kubernetes cluster, it would
be deployed on the worker nodes. The logical design of a Kubernetes cluster
is not something to be represented in a component view (that's more for
the location view, as there we will talk about the topology of services).&lt;/p&gt;
&lt;p&gt;With such component views, we can have some insights into the costs. Of course,
this is just a simple Kubernetes cluster, and built with pure open-source
software, so the costs are going to be on the hardware side (and the resources
they consume). In larger enterprises, however, the hypervisor is often a
commercially backed one like Hyper-V (Microsoft) and vSphere (VMware), which
have their specific licensing terms (which could be the number of machines or
even CPUs).  Also, enterprises often use a commercially backed Kubernetes, like
Rancher or OpenShift (Red Hat, part of IBM), which often have per-node licensing
terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views are just the beginning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I use a component view to explain what infrastructure is about,
it is merely the beginning. It provides a rudimentary layered view, which most
people can easily relate to. Content-wise, it is reasonably understandable (or
easy enough to explain) for people that aren't IT savvy, and is something that
you can easily find a lot of material for online.&lt;/p&gt;
&lt;p&gt;If we delve into the processes of (or related to) infrastructure, it becomes
more challenging to keep the readers/listeners with you. Processes can (will)
often be very abstract, and going into the details of each process is a lengthy
endeavor. I'll cover that in a later post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback? Comments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few days ago I've dropped Disqus as comment engine from my blog site, mainly
for concerns about my visitor's security, as well as the advertisements that it
embedded. I want my blog to be simple and straightforward, so I decided to not
have any other third-party services with it for now.&lt;/p&gt;
&lt;p&gt;So, if you have feedback or comments, don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1431332634370711552"&gt;discussion on
Twitter&lt;/a&gt;&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="component"></category><category term="viewpoint"></category></entry><entry><title>Disaster recovery in the public cloud</title><link href="https://blog.siphos.be/2021/07/disaster-recovery-in-the-public-cloud/" rel="alternate"></link><published>2021-07-30T20:00:00+02:00</published><updated>2021-07-30T20:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-30:/2021/07/disaster-recovery-in-the-public-cloud/</id><summary type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is a disaster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters are major incidents that have wide-ranging consequences to the
regular operations of the business. What entails a disaster can be different
between organizations, although they are commonly tied to the size of the
infrastructure and the organizational and infrastructural maturity. I'll get
back to the size dependency later when covering public cloud.&lt;/p&gt;
&lt;p&gt;A small organization that only has a few systems can declare a
disaster when all those systems are unreachable because their network
provider's line is interrupted. A larger organization probably has
redundancy of the network in place to mitigate that threat. And even
without the redundancy, organizations might just not depend that much
on those services.&lt;/p&gt;
&lt;p&gt;The larger the environment becomes though, the more the business depends
on the well-being of the services. And while I can only hope that high
availability, resiliency and appropriate redundancy are taken into account
as well, there are always threats that could jeopardize the availability
of services.&lt;/p&gt;
&lt;p&gt;When the problem at hand is specific to one or a manageable set of services,
then taking appropriate action to remediate that threat is generally not a
disaster. It can be a severe incident, but in general it is taken up
by the organization as an incident with a sufficiently small yet
efficient and well organized coordination: the teams involved are 
low in numbers, and the coordination can be done accurately.&lt;/p&gt;
&lt;p&gt;However, when the problem is significant or has a very wide scope, then
depending on the standard incident coordination will be insufficient. You
need to coordinate across too many teams, make sure communication is done
correctly, business is continuously involved/consulted, and most of all - 
you want to make sure that the organization doesn't independently try
to resolve issues when they don't have a full view on the situation
themselves.&lt;/p&gt;
&lt;p&gt;The latter is a major reason in my opinion why a DRP is so important
to have (the plan/procedure, not an actual disaster). If there is no
proper, well-aligned plan of action, teams will try to get in touch
with other teams, polluting communication and only getting incomplete
information. They might take action that other teams should know about
(but won't) or are heavily impacted by (e.g. because they are at that
time trying to do activities themselves). It can make the situation
much worse.&lt;/p&gt;
&lt;p&gt;Because we have to make a distinction between incident management
and disaster management, an organization has to formally declare
a problem as a disaster, and communicate that singular fact ("we
are now in disaster mode") so that all teams know how to respond: 
according to the Disaster Recovery Plan (DRP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disasters are not just 'force majeure'&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters aren't extraordinary events or circumstances beyond the
control of the organization. Depending on the business needs, you
might very well take precautionary actions against situations you've
never encountered before and will never encounter. We've recently had
a disastrous weather in Belgium (and other countries in Western Europe)
with floods happening in large areas. But that doesn't mean that
for an organization a flood event will trigger a disaster declaration
within a company (the disastrous weather was a disaster from a
human side, with several dozen deaths and millions of damage, so it
feels somewhat incorrect to consider the threat from a theoretical
standpoint here).&lt;/p&gt;
&lt;p&gt;If you're located in a flood-sensitive environment, you can still take
precautionary actions and assess what to do in case of a flood event. 
Depending on the actions taken, a flood event (threat) will not manifest
into availability concerns, data and infrastructure destruction, people
unavailability, etc. It is only when the threat itself turns into an
unforeseen (or non-remediated) situation that we speak of a disaster.&lt;/p&gt;
&lt;p&gt;This is why disasters depend on organizations, and how risk averse
the organization (and business) is. Some businesses might not want to
take precautionary actions against situations that in the past only
occur once every 100 years, especially if the investment they would
have to do is too significant compared to the losses they might have.&lt;/p&gt;
&lt;p&gt;Common disaster threats (sometimes also called catastrophic events)
that I'm used to evaluate from an infrastructure point of view, with a
company that has four strategic data centers, multiple headquarter
locations and a high risk averse setting (considering the financial
market it plays in) are cyberattacks, local but significant infrastructure
disruptions (data center failures or destruction), people-oriented
threats (targetting key personnel), critical provider outages,
disgruntled employees, and so forth. Searching for risk matrices
online can give you some interesting pointers, such as the European
Commission's &lt;a href="https://ec.europa.eu/echo/sites/default/files/swd_2017_176_overview_of_risks_2.pdf"&gt;Overview of Natural and Man-made Disaster Risks the
European Union may
face&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Public cloud related events&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In case of public cloud, the catastrophic events that might occur are
different, and it would be wrong to just consider the same events and
with the same action plan. A prime example, and the one I really want
people to focus on, is regional outages.&lt;/p&gt;
&lt;p&gt;If your current company considers region-wide failures (for
instance because you have two data centers but within the same
region) more from a reactive point of view rather than preventive
(e.g. the DRP in case of region-wide failures is to approach
the reconstruction within the region whenever possible, rather
than fail over to a different region), it might feel the same about
public cloud region failures.&lt;/p&gt;
&lt;p&gt;That would be wrong though. Whereas it is likely that a region-wide
failure for a company is not going to happen in its lifetime, a public
cloud provider is so much more massive in size, that the likelihood
of region-wide failures is higher. If you do a quick search for
region-wide failures in AWS or Azure, you'll find plenty of examples.
And while the failures themselves might be considered 'incidents' from
the public cloud provider point of view, they might be disasters for
the companies/customers that rely on them.&lt;/p&gt;
&lt;p&gt;For me, tackling disaster recovery when dealing with public cloud strongly
focuses on region failures and (coordinated) recovery from region failures.
Beyond region failures, I also strongly recommend to look into the dependencies
that the public cloud consumption has with services outside of the public cloud.
Some of these dependencies might also play a role in certain catastrophic
events. Say that you depend on Azure AD for your authentication and
authorization, and Microsoft is suddenly facing not just a world-wide
Azure AD outage, but also has to explain to you that they cannot restore its
data.&lt;/p&gt;
&lt;p&gt;Preparing for disasters is about preparing for multiple possible catastrophic
events, and in case of public cloud, you're required to think at massive scales.
And that includes designing for region-wide failures as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Impact of public cloud disasters to the organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Generally, if your organization has a decent maturity in dealing with disaster
recovery planning, they will be using Service Level Agreements with the
business to help decide what to do in case of disasters. Important
non-functionals that are included here are RTO (Recovery Time Objective), RPO
(Recovery Point Objective), and MTD (Maximum Tolerable Downtime). There are
others possibly listed in the SLA as well, but let me focus on these three.
If you want to learn more about contigency planning in general, I recommend
to go through the &lt;a href="https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf"&gt;NIST Special Publication 800-34 Rev.1, "Contingency Planning
Guide for Federal Informatino
Systems"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the RTO, we represent the time it is allowed to take to recover a service
to the point that it functions again. This might include with reduced capacity
or performance degradation. The RTO can be expressed in hours, days, weeks
or other arbitrary value. It is a disaster-oriented value, not availability!
As long as no disaster is declared, the RTO is not considered.&lt;/p&gt;
&lt;p&gt;The RPO identifies how much data loss is acceptable by the business in case of
a disaster. Again, this is disaster-oriented: the business can very well take
extra-ordinary steps to ensure full transactional consistency outside of 
disaster situations, yet allow for a "previous day" RPO in case of a disaster.&lt;/p&gt;
&lt;p&gt;The MTD is most often declared not on a single service, but at business service
level, and explains how long unavailability of that service is tolerated before
it seriously impacts the survivability of the business. It is related to the
RTO, as most services will have an RTO that is more strict/lower value than the
overall MTD, whereas the MTD is near non-negotiable.&lt;/p&gt;
&lt;p&gt;Now, what does public cloud disasters have to do with this? Well, in theory
nothing, as this model and process of capturing business requirements is quite
sensible and maps out perfectly as well. However (and here's the culprit),
an organization that sets up new services on a frequent basis might get
accustomed to certain values, and these values might not be as easy to approach
in a public cloud environment. Furthermore, the organization might not be
accustomed to different disaster scenario's for the SLA: having different sets
of RTO/RPO depending on the category of disaster.&lt;/p&gt;
&lt;p&gt;Let's get back to the region-wide disasters. A company might have decided not
to have region-wide proactive measures in place, and fixate their SLA
non-functionals on local disasters: a data center failure is considered a threat
that still requires proactive measures, whereas regional failures are treated
differently.  The organization decides to only have one SLA set defined, and
includes RTO and RPO values based on the current, local threat matrix. They
might decide that a majority of applications or services has a RPO of "last
transaction", meaning the data has to be fully restored at the latest situation
in case of a disaster.&lt;/p&gt;
&lt;p&gt;This generally implies synchronous replication as an infrastructural
solution. If the organization is used to having this method available (for
instance through SAN replication, cluster technologies, database recovery,
etc.) then they won't sweat at all if the next dozen services all require
the same RPO.&lt;/p&gt;
&lt;p&gt;But now comes the public cloud, and a strong point of attention is region-wide
failures. Doing synchronous replication across regions is not a proper tactic
(as it implies significant performance degradation) and especially not sensible
to do at the same scale as with local replication (e.g. between availability
zones in the same region). Now you have to tell your business that this RPO
value is not easily attainable in the public cloud. The public cloud, which
solves all the wonders in the world. The public cloud, which has more maturity
on operations than your own company. Yet you can't deliver the same SLA?&lt;/p&gt;
&lt;p&gt;Apples and pears. The disasters are different, so your offering might be
different. Of course, you should explain that your 'on premise' disaster
scenarios do not include region-wide failures, and that if you include
the same scenarios for 'on premise' that that RPO value would not be
attainable on premise either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The public cloud provides many capabilities, and has to deal with a
significantly larger environment than companies are used to. This also means
that disasters that are considered 'extremely unlikely' are now 'likely' (given
the massive scale of the public cloud), and that the threats you have to
consider while dealing with disaster recovery have to be re-visited for public
cloud enabled scenarios.&lt;/p&gt;
&lt;p&gt;My recommendation is to tackle the disaster-oriented non-functional requirements
by categorizing the disasters and having different requirements based on the
disaster at hand. Mature your cloud endeavours so that regional outages
are not a problem anymore (moving them away from the 'disaster' board), and 
properly map all dependencies you have through the public cloud exercises so
that you can build up a good view on what possible threats exist that would
require a well-coordinated approach to tackle the event.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="DRP"></category></entry><entry><title>What is the infrastructure domain?</title><link href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/" rel="alternate"></link><published>2021-07-19T15:20:00+02:00</published><updated>2021-07-19T15:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-19:/2021/07/what-is-the-infrastructure-domain/</id><summary type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;


&lt;p&gt;Of course, I do not want to approach this solely from my own viewpoint. There
are plenty of reference architectures and frameworks out there that could assist
in this. However, I still have the feeling that they are either too complex to
use for non-architect stakeholders, too simple to expose the domain properly, or
just don't fit the situation that I am currently faced with. And that's OK,
many of these frameworks are intended for architects, and from those frameworks
I can borrow insights left and right to use for a simple visualization, a
landscaping of some sort.&lt;/p&gt;
&lt;p&gt;So, let's first look at the frameworks and references out there. Some remarks
though that might be important to understand the post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When I use "the infrastructure domain", I reflect on how I see it. Within the
  company that I work for, there is some guidance on what the scope is of the
  infratructure domain, and that of course strongly influences how I look at
  "the infrastructure domain". But keep in mind that this is still somewhat
  organization- or company oriented. YMMV.&lt;/li&gt;
&lt;li&gt;While I am lucky enough to have received the time and opportunity to learn
  about enterprise architecture and architecture frameworks (and even got a
  masters degree for it), I also learned that I know nothing. Enterprises are
  complex, enterprise architecture is not a single framework or approach, and
  the enterprise architecture landscape is continuously evolving. So it is very
  well possible that I am missing something (and I will gladly learn from
  feedback on this).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Open Group Architecture Framework (TOGAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you would ask for one common framework out there for enterprise architecture,
then &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/"&gt;TOGAF&lt;/a&gt; is
probably it. It is a very exhaustive framework that focuses on various aspects
of enterprise architecture: the architecture development methodology, techniques
to use, the content of an architecture view (like metamodel descriptions), and the
capabilities that a mature organization should pursue.&lt;/p&gt;
&lt;p&gt;A core part of TOGAF is the &lt;em&gt;Architecture Development Method&lt;/em&gt; cycle, which has
several phases, including phases that are close to the infrastructure domain:
"Technology Architecture (D)" as well as areas of "Opportunities and Solutions
(E)" and "Information Systems Architectures (C)". Infrastructure is more than
'just' technology, but the core of it does fit within the technology part.&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF Cycle" src="https://blog.siphos.be/images/202107/togaf-adm-cycle.png"&gt;
&lt;em&gt;The ADM cycle, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap04.html"&gt;The Open Group, TOGAF 9.2, Chapter 4&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With TOGAF, you can support a full enterprise architecture view from the
strategy and vision up to the smallest change and its governance. However, the
key word here is &lt;em&gt;support&lt;/em&gt;, as TOGAF will not really give you much food for
simply representing the scope of infrastructure.&lt;/p&gt;
&lt;p&gt;I'm not saying it isn't a good framework, on the contrary. Especially with
&lt;a href="http://www.opengroup.org/archimate-forum"&gt;ArchiMate&lt;/a&gt; as modeling language (also
from The Open Group), using TOGAF and its meta model is a good way to facilitate
a mature architecture practice and enterprise-wide view within the
organization. But just like how application architecture and design requires a
bit more muscle than &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap30.html"&gt;TOGAF's
metamodel&lt;/a&gt;
supports, the same is true for infrastructure.&lt;/p&gt;
&lt;p&gt;There are also plenty of other enterprise frameworks out there that can easily
be mapped to TOGAF. Most of these focus mainly on the layering (business,
information, application, technology), processes (requirement management and the
like) and viewpoints (how to visualize certain insights) and, if you're fluent
in TOGAF, you can easily understand these other frameworks as well. I will not
be going through those in detail, but I also do not want to insinuate that they
are not valid anymore if you compare them with TOGAF: TOGAF is very extensive
and has a huge market adoption, but sometimes an extensive and exhaustive
framework isn't what a company needs...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TOGAF Technical Reference Model / Integrated Information Infrastructure
Reference Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As TOGAF is extremely extensive, it has parts that can be used to reference or
visualize infrastructure a bit better. In TOGAF 9.1, we had the &lt;em&gt;TOGAF Technical
Reference Model (TRM)&lt;/em&gt; and &lt;em&gt;TOGAF Integrated Information Infrastructure
Reference Model (III-RM)&lt;/em&gt; where you might feel that this is closer to what I
am seeking (for instance,
&lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;III-RM&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF III-RM" src="https://blog.siphos.be/images/202107/togaf-iii-rm.png"&gt;
&lt;em&gt;Focus of the III-RM, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;The Open Group, TOGAF 9.1, Chapter 44&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While it does become a bit more tangible, TOGAF does not expand much on this
reference model. Instead, it is more meant as a starting point for organizations
to develop their own reference models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Information Technology Infrastructure Library (ITIL)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL 4&lt;/a&gt; is another very
common framework, this time owned by AXELOS Limited. The focus of ITIL is on
process support, with many processes (sorry,
'&lt;a href="https://wiki.en.it-processmaps.com/index.php/ITIL_4"&gt;practices&lt;/a&gt;' as they are
called now) being very much close to what I consider to be part of the
infrastructure domain. The practices give a good overview of 'what things to
think about' when dealing with the infrastructure domain. Now, ITIL is not
exclusive to the infrastructure domain, and the company that I work for
currently considers many of these practices as processes that need to be tackled
across all domains.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ITIL Practices" src="https://blog.siphos.be/images/202107/itil-practices.jpg"&gt;
&lt;em&gt;ITIL 4 Practices, taken from &lt;a href="https://valueinsights.ch/-the-itil-4-practices-overview/"&gt;Value Insights, The ITIL 4 Practices Overview&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Still, I do want to see some of the ITIL practices reflected in the generic
infrastructure view as they are often influencing the infrastructure domain and
the projects within. The ITIL practices make it possible to explain that it
isn't just about downloading and installing software or quickly buying an
appliance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference Model of Open Distributed Processing (RM-ODP)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.rm-odp.net/"&gt;RM-ODP standard&lt;/a&gt; has a strong focus on distributed
processing (hence the name), which is a big part of what the infrastructure
domain is about. If we ignore the workplace environment for a bit, and focus on
hosting of applications, the majority of today's hosting initiatives are on
distributed platforms.&lt;/p&gt;
&lt;p&gt;&lt;img alt="RM-ODP Five Viewpoints" src="https://blog.siphos.be/images/202107/rm-odp.png"&gt;
&lt;em&gt;Five viewpoints of RM-ODP, taken from &lt;a href="https://sparxsystems.com/products/3rdparty/odp/index.html"&gt;MDG Technology for ODP - UML for ODP, SparxSystems&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within RM-ODP guidance is given on how to handle requirement management, how to
document the processing semantics, how to identify the components, integrations
and limitations of the systems, and how to select the most appropriate
technology. The intention of RM-ODP is to be as precise and formal as possible,
and leave no room for interpretation. To accomplish that, RM-ODP uses an object
model approach.&lt;/p&gt;
&lt;p&gt;Unlike the more business and information system oriented frameworks, RM-ODP has
a strong focus on infrastructure. Its viewpoints include engineering,
computational and technology for instance. The challenge that rises here
however is that it sticks to the more engineering oriented abstractions, which
make perfect sense for people and stakeholders involved in the infrastructure
domain, but is often Chinese for others.&lt;/p&gt;
&lt;p&gt;Personally, I consider RM-ODP to be a domain-specific standard strongly
associated with the infrastructure domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Department of Defense Architecture Framework (DoDAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dodcio.defense.gov/library/dod-architecture-framework/"&gt;DoDAF&lt;/a&gt;
 is an architecture framework that has a strong focus on the definition
and visualization of the different viewpoints. It is less tangible than RM-ODP,
instead focusing on view definitions: what and how should something be
presented to the stakeholders. The intention of DoDAF is that organizations
develop and use an enterprise architecture that supports and uses the
viewpoints that DoDAF prescribes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="DoDAF viewpoints" src="https://blog.siphos.be/images/202107/dodaf-viewpoints.png"&gt;
&lt;em&gt;DoDAF viewpoints, taken from &lt;a href="https://www.visual-paradigm.com/guide/enterprise-architecture/what-is-dodaf-framework/"&gt;"What is DoDAF Framework", Visual Paradigm&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unlike broad scale architecture frameworks that look at an enterprise in its
entirety, my impression is that DoDAF is more towards product architecture.
That makes DoDAF more interesting for solution architectures, which often
require to be more detailed and thus hit closer to home when looking at the
infrastructure domain. However, it is not something I can 'borrow' from to
easily explain what infrastructure is about.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nato.int/cps/en/natohq/topics_157575.htm"&gt;NATO's Architecture Framework (NAF)&lt;/a&gt;
seems to play witin the same realm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sherwood Applied Business Security Architecture (SABSA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SABSA framework and methodology has a strong security focus, but covers the
requirements from full enterprise view up to the solutions themselves. One of
the benefits of SABSA is inherent to this security-orientation: you really need
to know and understand how things work before you can show that they are
secure. Hence, SABSA is a quite complete framework and methodology.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SABSA Metamodel" src="https://blog.siphos.be/images/202107/sabsa-metamodel.png"&gt;
&lt;em&gt;SABSA metamodel, taken from &lt;a href="https://sabsa.org/the-chief-architects-blog-a-brief-history-of-sabsa-21-years-old-this-year/"&gt;"A Brief History of SABSA: Part 1", sabsa.org&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important focus area in SABSA is the integration between services, which is
something I am frequently confronted with at work. Yet unlike the more solution
driven frameworks, SABSA retains its business-oriented top-down approach, which
places it alongside the TOGAF one in my opinion. Moreover, we can apply TOGAF's
development method while using SABSA to receive more direct requirements and
design focus.&lt;/p&gt;
&lt;p&gt;Its risk and enabler orientation offers a help to not only explain how things
are set up, but also why. Especially in the sector I am currently active in
(financial sector) having a risk-based, security-conscious approach is a good
fit. The supporting list of attributes, metrics, security services, etc. allow
for defining more complete architectures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Control Objectives for IT (CObIT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a similar area as ITIL, the &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT
framework&lt;/a&gt; focuses less on a complete
enterprise architecture framework and more on processes, process maturity, and
alignment of the processes within the organization. I am personally a fan of
CObIT as it is a more tangible framework, with more clear deliverables and
requirements, compared to others. But like with most frameworks, it has
received numerous updates to fit in continuously growing environments and
cultures which makes it heavy to use.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CObIT Core Model" src="https://blog.siphos.be/images/202107/cobit-core-model.jpg"&gt;
&lt;em&gt;The CObIT 2019 Core Model, taken from "&lt;a href="https://www.isaca.org/resources/news-and-trends/industry-news/2020/using-cobit-2019-to-plan-and-execute-an-organization-transformation-strategy"&gt;Using CObIT 2019 to plan and execute
an organization transformation strategy,
ISACA.org&lt;/a&gt;"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The framework is less about the content of infrastructure and technology, and
more about how to assess, develop, build, maintain, operate and control
whatever service is taken up. However, there are references to infrastructure
(especially when dealing with non-functionals) or controls that are actively
situated in infrastructure (such as backup/restore, disaster recovery, etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IT for IT (IT4IT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open Group has a similar framework like CObIT, called
&lt;a href="https://www.opengroup.org/it4it"&gt;IT4IT&lt;/a&gt;. It does have a reference architecture
view that attempts to group certain deliverables/services together to create a
holistic view on what IT should offer. But unlike the larger enterprise
frameworks it focuses strongly on service delivery and its dependencies.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IT4IT Reference Architecture" src="https://blog.siphos.be/images/202107/it4it-reference-architecture.png"&gt;
&lt;em&gt;IT4IT Reference Architecture, taken from &lt;a href="https://www.opengroup.org/it4it-forum"&gt;The Open Group IT4IT
Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within the IT4IT reference architecture, a grouping is attempted that maps on a
value stream, starting from a strategy over the deployment up to the detection
and correction facilities. This value stream orientation is common across
frameworks, but often feels like the value is always to "add more", "deliver
more". In my opinion, rationalization exercises, decommissioning and
custodianship is too much hidden. Sure, it is part of the change management
processes and operational maintenance, but those are extremely valuable areas
that are not expressively visible in these frameworks. Compare that to the
attention that risk and security receives: while security consciousness should
be included at all phases of the value stream, security is always explicitly
mentioned.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendor-specific visualizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Several vendors have their own visualization methodology that represents more
specific requirements from the domain(s) in which those vendors are active.
These are generally domain-specific visualizations, even with a vendor-specific
view. Such methodologies are nice to use when dealing with specific viewpoints,
but I do not believe these should be considered "architecture" frameworks. They
don't deal with requirement management, strategy alignment, and often lack
functional and non-functional insights. Still, they are a must to know in the
infrastructure areas.&lt;/p&gt;
&lt;p&gt;If you are active in Amazon AWS for instance, then you've undoubtedly come
across drawings like the one visible in "&lt;a href="https://aws.amazon.com/blogs/architecture/wordpress-best-practices-on-aws/"&gt;Wordpress: Best Practices on
AWS&lt;/a&gt;".
These drawings provide a deployment viewpoint that lists the main interactions
between AWS services.&lt;/p&gt;
&lt;p&gt;When you are more network oriented, then you've been immersed in Cisco's network
diagrams, like the one visible in "&lt;a href="https://www.cisco.com/c/en/us/td/docs/solutions/Verticals/PCI_Retail/roc.html"&gt;Verizon Business Assessment for: Cisco PCI
Solution for
Retail&lt;/a&gt;".
These network diagrams again focus on the deployment viewpoint of the network
devices and their main interactions.&lt;/p&gt;
&lt;p&gt;There are probably plenty more of these specific visualizations, but the two
mentioned above are most visible to me currently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of frameworks out there to learn from, and some of these can be
used to find ways of explaining what the infrastructure domain is about.
However, they are often all very complete and require an architectural mindset
to start from, which is not obvious when trying to convey something to outside
or indirect stakeholders.&lt;/p&gt;
&lt;p&gt;Few frameworks have a reference that is directly consumable by non-architect
stakeholders. The most tangible ones seem to be related to the IT processes, but
those still require an IT mindset to interpret.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="pattern"></category></entry><entry><title>Organizing service documentation</title><link href="https://blog.siphos.be/2021/07/organizing-service-documentation/" rel="alternate"></link><published>2021-07-08T09:20:00+02:00</published><updated>2021-07-08T09:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-08:/2021/07/organizing-service-documentation/</id><summary type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Why I tend to document some of it myself&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within the company I currently work for, not all architecture and designs are
handled by a central repository, but that doesn't mean there is no architecture
and design available. They are more commonly handled through separate documents,
online project sites and the like. If we had a common way of documenting
everything in the same tool using the same processes and the same taxonomy, it
wouldn't make sense to document things myself... unless even then I would find
that I am missing some information.&lt;/p&gt;
&lt;p&gt;It all started when I tried to keep track of past decisions for a service or
solution. Decisions on architecture boards, on risk forums, on department
steering committees and what not. &lt;em&gt;Historical insights&lt;/em&gt; I call it, and it often
provides a good sense of why a solution or service came up, what the challenges
were, which principles were used, etc.&lt;/p&gt;
&lt;p&gt;Once I started tracking the historical decisions and topics, I quickly moved on
to a common structure: an entry page with the most common information about the
service or solution, and then subpages for the following categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Administration (processes, authorizations, procedures)&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorizationn&lt;/li&gt;
&lt;li&gt;Auditing and logging&lt;/li&gt;
&lt;li&gt;Configuration management&lt;/li&gt;
&lt;li&gt;Cost management&lt;/li&gt;
&lt;li&gt;Cryptography and privacy&lt;/li&gt;
&lt;li&gt;Data management (data handling, definitions, governance, lineage,
  backup/restore, ...)&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;Design and development (incl. naming convention)&lt;/li&gt;
&lt;li&gt;Figures&lt;/li&gt;
&lt;li&gt;High availability and disaster recovery&lt;/li&gt;
&lt;li&gt;History&lt;/li&gt;
&lt;li&gt;Operations (actor groups, source systems &amp;amp; interactions/external interfacing)&lt;/li&gt;
&lt;li&gt;Organization (management, organisational structure within the company, etc.)&lt;/li&gt;
&lt;li&gt;Performance management&lt;/li&gt;
&lt;li&gt;Patterns&lt;/li&gt;
&lt;li&gt;Processes&lt;/li&gt;
&lt;li&gt;Quality assurance &amp;amp; reporting&lt;/li&gt;
&lt;li&gt;Roadmap and restrictions (incl. lifecycle)&lt;/li&gt;
&lt;li&gt;Risks and technical debt&lt;/li&gt;
&lt;li&gt;Runtime information&lt;/li&gt;
&lt;li&gt;Terminology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, I won't go in depth about all the different categories listed. Perhaps some
areas warrant closer scrutiny in later posts, but for now the names of the
categories should be sufficiently self-explanatory.&lt;/p&gt;
&lt;p&gt;If there is an internal service (website or similar) that covers the
details properly, then I will of course not duplicate that information. Instead,
I will add a link to that resource and perhaps just document how to interpret
the information on that resource.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The entry page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The entry page of a service or solution always looks the same. It starts off
with a few attributes associated with the service:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The taxonomy used within the company&lt;/li&gt;
&lt;li&gt;The main point of contact(s)&lt;/li&gt;
&lt;li&gt;The backlog where the responsible team tracks its progress and evolution&lt;/li&gt;
&lt;li&gt;A link to the main documentation resources&lt;/li&gt;
&lt;li&gt;The internal working name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;taxonomy&lt;/em&gt; is something I strongly appreciate in my current company. It is
a uniform identifier associated with the product, service or solution, and is
used for several of the operational processes the company has. This taxonomy
comes up in things like chargeback, service level agreements, responsibility
overviews, data classifications, enterprise architectural overviews, etc.&lt;/p&gt;
&lt;p&gt;For instance, a managed macbook (asset) might have a taxonomy identifier of
&lt;code&gt;mmac&lt;/code&gt;, or we have a service for exchanging internal company data identified as
&lt;code&gt;cd70&lt;/code&gt; (it doesn't need to have an identifier that "reads" properly). Of course,
people don't just run around shouting the identifiers, but when we go through
the information available at the company, this identifier is often the primary
key so to speak to find the information.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;main points of contacts&lt;/em&gt;, I usually just document the person that is
my go-to person to get some quick insights. The full list of all contacts (like
product owner, product manager, system architect, business analyst, release
manager, etc.) is managed in a central tool (and uses the taxonomy to quickly
find the right person), so I just have the few names that I quickly need listed
here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;backlog&lt;/em&gt; is something I recently added to support any questions on "when
will we have feature XYZ". In the past, I had to contact the people to get this
information, but that often became cumbersome, especially when the team uses a
proper tool for tracking the work on the service.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;main documentation&lt;/em&gt; is often the most important part. It is a link to the
documentation that the team provides for end users, architects or other roles.
Some teams still have their information on a NAS, others in a document library
on SharePoint, others use a wiki, and there are teams that use a structured
document management system. Working for a big company has its consequences...&lt;/p&gt;
&lt;p&gt;Finally, the &lt;em&gt;internal working name&lt;/em&gt; is the name that a service or solution
receives the most. For infrastructure services, this is often the name of the
product from the time the product entered the organization. While the vendor has
switched the name of the product numerous times since, the old name sticks. For
instance, while I will document IBM's cloud offering as "IBMCloud" (its current
name) I will list its working name as "Bluemix" because that's how the company
internally often refers to it.&lt;/p&gt;
&lt;p&gt;After the basic attributes, I have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a few paragraphs for &lt;em&gt;description&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;a diagram or architecture view to give a &lt;em&gt;high level design&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;most important questions&lt;/em&gt; surrounding the service or solution&lt;/li&gt;
&lt;li&gt;some &lt;em&gt;tips and titbits&lt;/em&gt; for myself&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The high level design is often a view that I maintain myself, which uses the
&lt;a href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/"&gt;abstraction&lt;/a&gt; that I
mentioned earlier in my blog. It is not covering everything, but to a sufficient
level that I can quickly understand the context of the service or solution and
how it is generally implemented.&lt;/p&gt;
&lt;p&gt;The most important questions are mostly a refresher for questions that pop up
during discussions. For instance, for an API gateway, common questions might be
"What are the security controls that it enforces" or "Does the API GW perform
schema validation on JSON structures?".&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The history of a service&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below the entry page, the various categories come up. As I mentioned, it all
started with the historical insights on the service. By having a chronological
overview of all decisions and related material per service, I can quickly find
the most important information in many cases.&lt;/p&gt;
&lt;p&gt;Want to know the latest architecture for a service? Let's look in the history
when the last architectural review was, and at which decision body or board it
came up. Once found, I just need to go to the meeting minutes or case details to
find it.&lt;/p&gt;
&lt;p&gt;Want to know why the decision was taken to allow a non-standard integration?
Take a look at the history where this decision was taken, and consult its
meeting minutes.&lt;/p&gt;
&lt;p&gt;Need to ask for a continuance for something but you're just new in the team and
don't know why or how it was approved in the past? No worries, I'll share with
you the history of the service, where you can find the information, and coach
you a bit through our organization.&lt;/p&gt;
&lt;p&gt;Having the history for services and solutions available has been a massive
efficiency gain for myself and my immediate stakeholders. Of course, I would
have loved if the organization tracked this themselves, but as long as they
don't (especially since organization changes more often than technology) I will
put time and effort to track it myself (at least for the services and solutions
that I am interested in).&lt;/p&gt;
&lt;p&gt;The historical information I track myself is not a copy/paste of the meeting
minutes of those entries. I try to use a single paragraph explaination, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ARB  2020/12/05     &amp;quot;Switch of PostgreSQL authentication provider to PAM+sssd&amp;quot;
    Approval to switch the authentication provider of the PostgreSQL
    database assets from the internal authentication to a PAM-supported
    method, and use the RHEL sssd solution as a facilitator. Link with
    Active Directory.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ARB&lt;/code&gt; is the name of the decision body (here it could be the &lt;em&gt;Architecture
Review Board&lt;/em&gt;) and tells me where I can find more details if needed. I don't
really bother with trying to add actual links, because that takes time and often
the links become invalid after we switch from one solution to another.&lt;/p&gt;
&lt;p&gt;Since then, I also started adding information related to the service that isn't
just decision body oriented:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Incident    2021/06/08  &amp;quot;Fastly major outage&amp;quot;
    A major outage occurred on Fastly, a widely used cloud edge provider,
    between 09:47 UTC and 12:35 UTC. This impacted service ABC and DEF.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Incidents can be internal or external, and if they are internal I'll document
the incident and root cause analysis numbers associated with the incident as
well.&lt;/p&gt;
&lt;p&gt;It also doesn't need to be about problems. It can be announcements from the
vendor as well, as long as the announcement is or can be impactful for my work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How complete is this overview&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My overview is far, far, far from complete. It is also not my intention to make
it a complete overview, but instead use it as a quick reference when needed.
Services that are commonly discussed (for instance because they have wide
implications on other domain's architectures) are documented more in depth than
services that are barely influential to my meetings and projects. And that
doesn't mean that the services themselves are not important.&lt;/p&gt;
&lt;p&gt;Furthermore, the only areas that I truly want to have up-to-date, is the entry
page and the history. For all the other information I always hope to be able to
link to existing documentation that is kept up-to-date by the responsible teams.&lt;/p&gt;
&lt;p&gt;But in case the information isn't available, using the same structure for noting
down what insights that I gather helps out tremendously.&lt;/p&gt;
&lt;p&gt;I also don't want my overview to become a critical asset. It is protected from
prying eyes (as there is occasionally confidential information inside) and I am
coaching the organization to take up a lot of the information gathering and
documentation in a structured way. For instance, for managing EOL information,
we are publishing this in a standard way for all internal stakeholders to see
(and report on). The roadmap and strategy for the services within the domain are
now being standardized within the backlog tool as well, so that everybody can
clearly document when they expect to work on something, when certain investments
are needed, etc.&lt;/p&gt;
&lt;p&gt;In the past, architects often had to scramble all that information together
(hence one of my categories on &lt;em&gt;Roadmap&lt;/em&gt;) whereas we can now use the backlog
tools of the teams themselves to report on it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which tool to use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Personally, I use a wiki-alike service for this, so that I can search through
the pages, move and merge information, use tagging and labels and what not. I
also think that, unless the company already has a central tool for this, a well
maintained wiki with good practices and agreements on how to use it would do
wonders.&lt;/p&gt;
&lt;p&gt;I've been playing around in my spare time with several wiki technologies.
&lt;a href="https://www.dokuwiki.org/dokuwiki"&gt;Dokuwiki&lt;/a&gt; is still one of my favorites due
to its simplicity, whereas &lt;a href="https://www.mediawiki.org/wiki/MediaWiki"&gt;MediaWiki&lt;/a&gt;
is one of my go-to's for when the organization really wants to pursue a scalable
and flexible organization-wide wiki. However, considering that I try to
structure the information in a hierarchical way, I am planning to play around
with &lt;a href="https://www.bookstackapp.com/"&gt;BookStack&lt;/a&gt; a bit more.&lt;/p&gt;
&lt;p&gt;But while having a good tool is important, it isn't the critical part of
documenting information. Good documentation, in my opinion, comes from a good
structure and a coherent way of working. If you do it yourself, then of course
it is coherent, but it takes time and effort to maintain it. If you collaborate
on it, you have to make sure everybody follows the same practices and agreements
- so don't make them too complex.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="documentation"></category><category term="structure"></category><category term="wiki"></category></entry><entry><title>Not sure if TOSCA will grow further</title><link href="https://blog.siphos.be/2021/06/not-sure-if-TOSCA-will-grow-further/" rel="alternate"></link><published>2021-06-30T14:30:00+02:00</published><updated>2021-06-30T14:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-30:/2021/06/not-sure-if-TOSCA-will-grow-further/</id><summary type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;


&lt;p&gt;But while I do see some adoption of TOSCA, I get the feeling that it is
struggling with its position against the various infrastructure-as-code
(IaC) frameworks that are out there. While many of these frameworks do
not inherently support the abstraction that TOSCA has, it is not all that
hard to apply similar principles and use those frameworks to facilitate
multi-cloud deployments.&lt;/p&gt;
&lt;p&gt;Before considering the infrastructural value of TOSCA further, let's
see what TOSCA is about first.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplifying and abstracting cloud deployments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TOSCA offers a model where you can declare how an application should be
hosted in the cloud, or in a cloud-native environment (like a Kubernetes
cluster). For instance, you might want to describe a certain document
management system, which has a web application front-end deployed on 
a farm of web application servers with a load balancer in front of it,
a backend processing system, a database system and a document storage
system. With TOSCA, you can define these structural elements with their
resource requirements.&lt;/p&gt;
&lt;p&gt;For instance, for the database system, we could declare that it has to
be a PostgreSQL database system with a certain administration password,
and within the database system we define two databases with their
own user roles:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;topology_template:
  ...
  node_templates:
    db_server:
      type: tosca.nodes.Compute
      ...
    postgresql:
      type: tosca.nodes.DBMS.PostgreSQL
      properties:
        root_password: &amp;quot;...&amp;quot;
      requirements:
        host: db_server
    db_filemeta:
      type: tosca.nodes.Database.PostgreSQL
      properties:
        name: db_filemeta
        user: fmusr
        password: &amp;quot;...&amp;quot;
      artifacts:
        db_content:
          file: files/file_server_metadata.txt
          type: tosca.artifacts.File
      requirements:
        - host: postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The parameters can, and should be further parameterized. TOSCA supports
declaring inputs that are provided upon deployment so you can safely
publicize the TOSCA structure without putting passwords in there
for instance. Furthermore, TOSCA allows you to add scripts to execute
when a resource is created, which is a common requirement for database
systems.&lt;/p&gt;
&lt;p&gt;But that's not all. Within TOSCA, you then further define the relationship
between the different systems (nodes), including connectivity requirements.
Connections can then be further aligned with virtual networks to model
the network design of the application.&lt;/p&gt;
&lt;p&gt;One of the major benefits of TOSCA is that it also provides abstraction on
the requirements. While the above example explicitly pushes for a PostgreSQL
database hosted on a specific compute server, we could also declare that we
need a database management system, or for the network part we need firewall
capabilities. The TOSCA interpreter, when mapping the model to the target
cloud environment, can then either suggest or pick the technology service
itself. The TOSCA model can then have different actions depending on the
selected technology. For the database for instance, it would have different
deployment scripts.&lt;/p&gt;
&lt;p&gt;The last major benefit that I would like to point out are the workflow
and policy capabilities of TOSCA. You can declare how for instance a 
backup process should look like, or how to cleanly stop and start the
application. You can even model how a rolling upgrade of the application
or database could be handled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is not just theoretical&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Standards can often be purely theoretical, with one or a few reference
implementations out there. That is not the case with TOSCA. While reading
up on TOSCA, it became clear that it has a strong focus on Network
Functions Virtualization (NFV), a term used to denote the shift
from appliance-driven networking capabilities towards an environment
that has a multitude of solutions running in virtual environments, and
where the infrastructure adopts to this virtualized situation with
(for network) virtual routers, firewalls, etc. Another standards body,
namely the European Telecommunications Standards Institute (ETSI), seems
to be the driving force behind the NFV architecture.&lt;/p&gt;
&lt;p&gt;TOSCA has a simple profile for NFV, which aligns with ETSI's NFV and 
ensures that TOSCA parsers that support this profile can easily be used
to set up and manage solutions in virtualized environments (and thus
also cloud environments). The amount of online information about TOSCA
with respect to the NFV side is large, although still in my opinion
strongly vendor-oriented (products that support TOSCA) and standards-oriented
(talks about how well it fits). On TOSCA's &lt;a href="https://www.oasis-open.org/tosca-implementation-stories/"&gt;implementation stories&lt;/a&gt;
page, two of the three non-vendor stories are within the telco industry.&lt;/p&gt;
&lt;p&gt;There are a few vendors that heavily promote
TOSCA: &lt;a href="https://cloudify.co/"&gt;Cloudify&lt;/a&gt; and &lt;a href="https://designer.otc-service.com"&gt;Ubicity&lt;/a&gt;
offer multi-cloud orchestrators that are TOSCA-based. Many vendors, 
including the incumbent network technology vendors like Cisco and Nokia,
embrace TOSCA NFV. But most information from practical TOSCA usage out
there is in open source solutions. The list of &lt;a href="https://github.com/oasis-open/tosca-community-contributions/wiki/Known-TOSCA-Implementations"&gt;known TOSCA implementations&lt;/a&gt;
mentions plenty of open source products. One of the solutions that I
am considering of playing around with is &lt;a href="https://turandot.puccini.cloud/"&gt;turandot&lt;/a&gt;,
which uses TOSCA to compose and orchestrate Kubernetes workloads.&lt;/p&gt;
&lt;p&gt;As an infrastructure architect, TOSCA could be a nice way of putting
initial designs into practice: after designing solutions in a language
like ArchiMate, which is in general not 'executable', the next step could
be to move the deployment specifications into TOSCA and have the next
phases of the project use and enhance the TOSCA definition. But that
easily brings me to what I consider to be shortcomings of the current
situation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inhibitors for growth potential&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are a number of issues I have with the current state of TOSCA.&lt;/p&gt;
&lt;p&gt;TOSCA's ecosystem &lt;em&gt;seems&lt;/em&gt; to be lacking sufficient visualization support.
I did come across &lt;a href="https://projects.eclipse.org/projects/soa.winery"&gt;Winery&lt;/a&gt;
but that seems to be it. I would really like to see a solution that reads
TOSCA and generates an architectural overview. For instance, for the
example I started this blog with, something like the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visualization of a deployment" src="https://blog.siphos.be/images/202106/tosca-archimate.png"&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, my impression is that TOSCA is strongly and mostly Infrastructure
as a Service (IaaS) oriented. The company I currently work for strongly
focuses on platform services, managed cloud services rather than the
more traditional infrastructure services where we would still have to do
the blunt of the management ourselves. Can TOSCA still play a role
in solutions that are fully using platform services? Perhaps the answer
here is "no", as those managed services are often very cloud-vendor specific,
but that isn't always the case, and can often also be tackled using the
abstraction and implementation specifics that TOSCA supports.&lt;/p&gt;
&lt;p&gt;I also have to rely too much on impressions. While the TOSCA ecosystem
has plenty of open source solutions out there, I find it hard to get
tangible examples: TOSCA definitions of larger-scale definitions that
not only show an initial setup, but are actively maintained to show
maintenance evolution of the solution. If TOSCA is so great for vendors
to have a cloud-independent approach, why do I find it hard to find
vendors that expose TOSCA files? If the adoption of TOSCA stops
at the standards bodies and too few vendors, then it is not likely
to grow much further.&lt;/p&gt;
&lt;p&gt;TOSCA orchestration engines often are in direct competition with
general IaC orchestration like Terraform. Cloudify has a post that
&lt;a href="https://cloudify.co/blog/terraform-vs-cloudify/"&gt;compares Terraform with their solution&lt;/a&gt;
but doesn't look into how Terraform is generally used in CI/CD
processes that join Terraform with the other services that create a
decent end-to-end orchestration for cloud deployments. For Kubernetes,
it competes with Helm and the likes - not fully, as TOSCA has other 
benefits of course, but if you compare how quickly Helm is taking
the lead in Kubernetes you can understand the struggle that TOSCA in
my opinion has.&lt;/p&gt;
&lt;p&gt;Another inhibitor is TOSCA's name. If you search for information on
TOSCA, you need to exclude &lt;a href="https://www.tricentis.com/resources/tricentis-tosca-overview/"&gt;Tricentis'&lt;/a&gt;
continuous testing platform, the &lt;a href="https://en.wikipedia.org/wiki/Tosca"&gt;1900's Opera&lt;/a&gt;,
and several other projects, films, and other entities that use the same
name. You'll need to explicitly mention OASIS and/or cloud as well if
you want to find decent articles about TOSCA, knowing well that there
can be pages out there that are missed because of it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I appreciate the value TOSCA brings, I feel that it might not grow
to its fullest potential. I hope to be wrong of course, and I would
like to see big vendors publish their reference architecture TOSCA material
so that large-scale solutions are shown to be manageable using TOSCA and
that solution architects do not need to reinvent the wheel over and
over again, as well as link architecture with the more operations
side of things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To learn more about TOSCA, there are a few resources that I would recommend
here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=tosca"&gt;OASIS TOSCA Technical Committee&lt;/a&gt;
  has a number of resources linked. The &lt;a href="https://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.3/os/TOSCA-Simple-Profile-YAML-v1.3-os.pdf"&gt;TOSCA Simple Profile in YAML Version 1.3&lt;/a&gt;
  PDF is a good read which gradually explains the structure of a TOSCA YAML
  file with plenty of examples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.etsi.org/technologies/nfv"&gt;Network Functions Virtualisation (NFV)&lt;/a&gt;
  is the ETSI site on NFV. Given the focus on NFV that I find around when
  looking at TOSCA (and is even referenced on this page) understanding what
  NFV is about clarifies a bit more how valuable TOSCA is/can be in this
  environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NHYRESmE6uA"&gt;OCB: AMA on TOSCA the Topology and Orchestration Specification for Cloud Applications - Tal Liron&lt;/a&gt;
  is an hour-long briefing that covers TOSCA not only in theory but also applies
  it in practice, and covers some of the new features that are coming up.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="TOSCA"></category><category term="OASIS"></category><category term="topology"></category><category term="orchestration"></category><category term="infrastructure"></category><category term="IaC"></category><category term="NFV"></category></entry><entry><title>Integrating or customizing SaaS within your own cloud environment</title><link href="https://blog.siphos.be/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/" rel="alternate"></link><published>2021-06-23T15:10:00+02:00</published><updated>2021-06-23T15:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-23:/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/</id><summary type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Integrations versus customizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of ways to integrate solutions in a larger ecosystem. Most
integrations focus on data integration (file transfers mostly) and service
integration (using APIs or Enterprise Service Bus solutions), although many
other creative methods are used to facilitate the integration of a SaaS
within the organization’s portfolio.&lt;/p&gt;
&lt;p&gt;This creativity, in my opinion, often transforms into customization of a SaaS
solution rather than an integration approach. SaaS services are being extended
with new, customized functionality, but in a way that we’re no longer thinking
about integrating this customization with the SaaS, but injecting the SaaS
with closely tied services. And SaaS providers are often happy to support
this, as it binds the customer to their solution.&lt;/p&gt;
&lt;p&gt;Now, customizations are not integrations, and integrations are not
customizations. If you customize a SaaS offering, then you still need an
integration between the custom development and the SaaS offering. Sometimes
this integration is as simple as uploading the customization into the SaaS
and the SaaS does the rest. Or the customization is a completely separate
application service, which integrates over managed APIs with the SaaS. Or you
use an intermediate solution that bridges the two solutions.&lt;/p&gt;
&lt;p&gt;While many integrations are possible, I feel that there are a few integration
approaches that are in most cases just wrong. One of these is linking the
SaaS within your own private solution (network or cloud).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don’t just extend a SaaS environment into your own&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I don’t believe that it is wise to just extend a SaaS environment into your
own, even when the SaaS provider enables this. Services like VPC peering,
which can be used to link your VPC with the SaaS provider’s VPC, are easy
ways to do so, but applying it without adjusting the architecture for it
makes your long-term maintainability and security more challenging. How
do you ensure that the SaaS does not abuse this link? How do you ensure
that you don’t accidentally leak information to the SaaS? How do you ensure
high availability and resiliency is retained?&lt;/p&gt;
&lt;p&gt;In traditional architectures, we would have these provider’s locations
considered as a separate network, and introduce the appropriate controls
(like those offered by application firewalls, reverse proxies, etc.) in
between the business application and the third party. This would often be
facilitated by the network operations teams, and governed through more
centralized environments.&lt;/p&gt;
&lt;p&gt;In cloud environments, architectures can be completely different, and
individual applications might pick integration architectures that are more
fruitful for them, without considering the larger environment. If the DevOps
teams that manage the solution architecture are mature, then they too will
consider the various non-functionals that play a role with such integrations.
But if the experience is lacking, setting up direct extensions towards the
SaaS might seem to be a quick and valid solution.&lt;/p&gt;
&lt;p&gt;Some areas that I would focus on when such integrations are requested, are
(in no particular order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impact of the integration to the deployment architecture of the solution,
  considering the availability zones and region concepts used within the
  solution&lt;/li&gt;
&lt;li&gt;Authentication of the integration at various levels (not just
  authentication based on the identity being used)&lt;/li&gt;
&lt;li&gt;Isolation of the integration, ensuring that no other parties are impacted
  by the integration&lt;/li&gt;
&lt;li&gt;Filtering capabilities on network and application level&lt;/li&gt;
&lt;li&gt;Logging and metrics to get visibility on the integration, its usage, the
  volumes sent over, etc.&lt;/li&gt;
&lt;li&gt;Resilience in case of temporary failures (be it through buffering
  mechanisms, or asynchronous integrations)&lt;/li&gt;
&lt;li&gt;Registration of the integration in the enterprise architecture, so that
  assessments, vendor relationship, and other processes are aware of the
  integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When properly tackled, then services like AWS PrivateLink of course do
have a role to play. But it isn’t to just link one solution with another
and be done with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granting SaaS providers limited access to your resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another approach that I see happening is to grant a SaaS provider
administrative access to your own resources. Just like with extending SaaS
environments, I feel that this is not something to apply by default and has
to be carefully assessed. For some SaaS solutions, this is part of their
selling proposal, and is something you know up front. But not all SaaS
solutions are equally obvious in this requirement.&lt;/p&gt;
&lt;p&gt;Some organizations might not have their cloud architecture, account structure
and the like designed to enable SaaS providers to get administrative access
to (some) resources. If the current architectures focus on highly integrated
accounts and solutions, then granting a SaaS administrative access might
jeopardize the security and stability of your overall architecture.&lt;/p&gt;
&lt;p&gt;Furthermore, granting a third party access to your resources also has
implications on accountability. If a SaaS has access to storage within your
account, it could accidentally manipulate data it shouldn’t have access to,
upload another customer’s data on your storage (or vice-versa), or due to a
cyber incident upload toxic data to your account. The provider might also
inadvertently access the data in an economically unfriendly way.&lt;/p&gt;
&lt;p&gt;Using such access patterns should be carefully designed. While you can
often not implement specific IT or security measures on the solution design,
it might be possible to use separate accounts for instance, focusing on
the integration between your ‘core’ solutions and this intermediate one
to ensure a secure and resilient setup, while optimizing cost management
for this intermediate account. You can even consider putting such accounts
under a different tree in the organization structure and apply restrictive
policies such as through AWS’ Service Control Policies (SCP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Creating solutions that link with third parties requires thought and
design. Cloud providers make it a lot easier to change and apply connections
and integrations, but that does not make the architectural work that
precedes it less obvious - on the contrary.&lt;/p&gt;
&lt;p&gt;Customizations with SaaS providers still need to be carefully assessed
and integrated, with attention on the non-functionals such as resilience,
availability, security and the like.&lt;/p&gt;
&lt;p&gt;If the SaaS provider needs access to your own resources, carefully assess
how fine-grained this can be implemented and how the accountability is
assigned. See if an intermediate account can be used where both you and
the SaaS provider have administrative access to, while keeping the rest
of the organization’s data and solutions elsewhere.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="SaaS"></category><category term="integration"></category><category term="customization"></category></entry><entry><title>An IT services overview</title><link href="https://blog.siphos.be/2021/06/an-it-services-overview/" rel="alternate"></link><published>2021-06-14T17:30:00+02:00</published><updated>2021-06-14T17:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-14:/2021/06/an-it-services-overview/</id><summary type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;A single picture doesn’t say it all&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To start off with my overview, I had a need to structure the hundreds of
technology services that I want to keep an eye on in a way that I can 
quickly find it back, as well as present to other stakeholders what 
infrastructure services are about. This structure, while not perfect, 
currently looks like in the figure below. Occasionally, I move one or
more service groups left or right, but the main intention is just to
have a structure available.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overview of the IT services" src="https://blog.siphos.be/images/202106/it_service_overview.png"&gt;&lt;/p&gt;
&lt;p&gt;Figures like these often come about in mapping exercises, or capability models.
A capability model that I recently skimmed through is the
&lt;a href="https://www.if4it.com/SYNTHESIZED/MODELS/ENTERPRISE/enterprise_capability_model.html"&gt;IF4IT Enterprise Capability Model&lt;/a&gt;.
I stumbled upon this model after searching for some reference architectures
on approaching IT services, including a paper titled
&lt;a href="https://www.researchgate.net/publication/238620971_IT_Services_Reference_Catalog"&gt;IT Services Reference Catalog&lt;/a&gt;
by Nelson Gama, Maria do Mar Rosa, and Miguel Mira da Silva.&lt;/p&gt;
&lt;p&gt;Capability models, or service overviews like the one I presented, do not fit
each and every organization well. When comparing the view I maintain with
others (or the different capability and service references out there), I
notice two main distinctions: grouping, and granularity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Certain capabilities might be grouped one way in one reference, and use a
  totally different grouping in another. A database system might be part of
  a “Databases” group in one, a “Data Management” group in another, or even
  “Information Management” in a third. Often, this grouping also reveals the
  granularity that the author wants to pursue.&lt;br&gt;
  Grouping allows for keeping things organized and easier to explain, but has
  no structural importance. Of course, a well-chosen grouping also allows you
  to tie principles and other architectural concerts to the groups themselves,
  and not services in particular. But that still falls under the explainability
  part.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The granularity is more about how specific a grouping is. In the example
  above, “Information Management” is the most coarse-grained grouping, whereas
  “Databases” might be a very specific one. Granularity can convey more insights
  in the importance of services, although it can also be due to historical
  reasons, or because an overview started from one specific service and expanded
  later. In that case, it is very likely that the specific service and its
  dependencies are more elaborately documented.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the figure I maintain, the grouping is often based both on the extensiveness 
of a group (if a group contains far too many services, I might want to see if I
can split up the group) as well as historical and organizational choices. For
instance, if the organization has a clear split between network oriented
teams and server oriented teams, then the overview will most likely convey
the same message, as we want to have the overview interpretable by many
stakeholders - and those stakeholders are often aware of the organizational
distinctions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services versus solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I try to keep track of the evolutions of &lt;em&gt;services&lt;/em&gt; and &lt;em&gt;solutions&lt;/em&gt; within this
overview. Now, the definition of a “service” versus “solution” does warrant
a bit more explanation, as it can have multiple meanings. I even use “service”
for different purposes depending on the context.&lt;/p&gt;
&lt;p&gt;For domain architecture, I consider an “&lt;em&gt;infrastructure service&lt;/em&gt;” as a product
that realizes or supports an IT capability. It is strongly product oriented
(even when it is internally developed, or a cloud service, or an appliance)
and makes a distinction between products that are often very closely related.
For instance, Oracle DB is an infrastructure service, as is the Oracle
Enterprise Manager. The Oracle DB is a product that realizes a “relational
database” capability, whereas OEM is a “central infrastructure management”
capability.&lt;/p&gt;
&lt;p&gt;The reason I create distinct notes for these is because they have different
life cycles, might have different organizational responsible teams, different
setups, etc. Hence, components (parts of products) I generally do not consider
as separate, although there are definitely examples where it makes sense to
consider certain components separate from the products in which they are
provided.&lt;/p&gt;
&lt;p&gt;The several hundred infrastructure services that the company is rich in are
all documented under this overview.&lt;/p&gt;
&lt;p&gt;Alongside these infrastructure services I also maintain a solution overview.
The grouping is exactly the same as the infrastructure services, but the
intention of solutions is more from a full internal offering point of view.&lt;/p&gt;
&lt;p&gt;Within &lt;em&gt;solution architectures&lt;/em&gt;, I tend to focus on the choices that the
company made and the design that follows it. Many solutions are considered
‘platforms’ on top of which internal development, third party hosting or
other capabilities are provided. Within the solution, I describe how the
various infrastructure services interact and work together to make the
solution reality.&lt;/p&gt;
&lt;p&gt;A good example here is the mainframe platform. Whereas the mainframe itself
is definitely an infrastructure service, how we internally organize the
workload and the runtimes (such as the LPARs), how it integrates with the
security services, database services, enterprise job scheduling, etc. is
documented in the solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Not all my domain though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not all services and solutions that I track are part of ‘my’ domain though.
For instance, at my company, we make a distinction between the
infrastructure-that-is-mostly-hosting, and
infrastructure-that-is-mostly-workplace. My focus is on the ‘mostly hosting’
orientation, whereas I have a colleague domain architect responsible for
the ‘mostly workplace’ side of things.&lt;/p&gt;
&lt;p&gt;But that’s just about accountability. Not knowing how the other solutions
and services function, how they are set up, etc. would make my job harder,
so tracking their progress and architecture is effort that pays off.&lt;/p&gt;
&lt;p&gt;In a later post I’ll explain what I document about services and solutions
and how I do it when I have some time to spend.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="overview"></category><category term="service"></category><category term="landscape"></category><category term="catalog"></category><category term="capability"></category></entry><entry><title>The three additional layers in the OSI model</title><link href="https://blog.siphos.be/2021/06/the-three-additional-layers-in-the-OSI-model/" rel="alternate"></link><published>2021-06-09T11:10:00+02:00</published><updated>2021-06-09T11:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-09:/2021/06/the-three-additional-layers-in-the-OSI-model/</id><summary type="html">&lt;p&gt;At my workplace, I jokingly refer to the three extra layers on top of the
OSI network model as a way to describe the difficulties of discussions or
cases. These three additional layers are Financial Layer, Politics Layer
and Religion Layer, and the idea is that the higher up you go, the more
challenging discussions will be.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At my workplace, I jokingly refer to the three extra layers on top of the
OSI network model as a way to describe the difficulties of discussions or
cases. These three additional layers are Financial Layer, Politics Layer
and Religion Layer, and the idea is that the higher up you go, the more
challenging discussions will be.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Recap on the OSI model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open Systems Interconnection (OSI) model is a conceptual model for
network-oriented communications, which has 7 layers - each with their own
specific peculiarities and service offerings.&lt;/p&gt;
&lt;p&gt;The seven layers are, from top to bottom:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Application Layer which provide the high-level interfaces to the
   application&lt;/li&gt;
&lt;li&gt;The Presentation Layer which maps data as seen / used by an application
   to the network &lt;/li&gt;
&lt;li&gt;The Session Layer which enables continuous / conversational communication
   between nodes&lt;/li&gt;
&lt;li&gt;The Transport Layer which looks at reliable transmissions between systems&lt;/li&gt;
&lt;li&gt;The Network Layer which handles functions like addressing and routing&lt;/li&gt;
&lt;li&gt;The Data Link Layer which looks at the interchange of frames (collection of
   bits and the interpretation of it) between two systems that are connected by
   a physical medium&lt;/li&gt;
&lt;li&gt;The Physical Layer which looks at how bits are sent over the physical medium&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Many network protocols can be mapped to one or more of these layers. The WiFi
standards focus on the physical and data link layers, while the IP standard
is network layer oriented. The TCP standard is strongly session and transport
layers oriented. The HTTP standard has a strong focus on the application and
presentation layer.&lt;/p&gt;
&lt;p&gt;While the OSI model isn’t 100% applied in standards and protocols, it is
conceptually a very common way of looking at communications and network
stacks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OSI layers as complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I jokingly refer to the OSI model for complexity and difficulties of
discussions, it is through the assumption that the challenges rise the higher
up in the stack you go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The increasing complexity of discussions" src="https://blog.siphos.be/images/202106/OSI_extended.png"&gt;&lt;/p&gt;
&lt;p&gt;Discussions related to the physical aspects are often the easiest: they are about
tangible objects, people understand or at least properly observe the physical
aspects. There is little misinterpretation of the information presented to the
discussion.&lt;/p&gt;
&lt;p&gt;Then, we start rising up the stack. Discussions that are close to the physical
aspects are still easy to moderate, but the higher up we go the more chance we
have of misinterpretation of the information at hand. People make more
assumptions to understand the discussion, which might lead to misunderstandings.&lt;/p&gt;
&lt;p&gt;Higher layers are also because we want to abstract away the complexity of
reality. Discussions are held on topics that have wider consequences, and thus we
abstract this complexity in models so that the discussions can move forward
quickly enough. If not all people understand the abstraction (and its
consequences), the discussions might quickly move to details and specifics that
do not provide much value to the discussion, but are considered as necessary by
some of the attendees.&lt;/p&gt;
&lt;p&gt;Even worse, the higher up we go, the more flexibility organizations expect. On
the lowest level we often have few and highly standardized choices, whereas on
higher levels we have different standards due to business units with other
requirements. The complexity rises tremendously, and we are often challenged by
stakeholders with different agendas.&lt;/p&gt;
&lt;p&gt;The OSI model’s highest layer is the application layer, which is the layer that
covers discussions on (business) applications, which is often the level of
discussions systems architects often have to deal with. A business unit wants
solution X, but that solution isn’t compatible with the standards and enablers
that the organization already has in place. Another business unit also wants
solution X, but in a different way. And while we have solution Y in place as
well that covers 80% of the functionality… Well, you know where this is going.&lt;/p&gt;
&lt;p&gt;Well, there are situations even worse, which make discussions and the attempts
to find a consensus even harder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Financial Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first one (layer 8 so to speak) is the Financial Layer. Here the
discussions are on the economic side of things, and if you think those are
the easier discussions out there (because financials are mathematically
sound, right?) I urge you to dive deeper into this subject, because
there is plenty of work out there to simplify these discussions.&lt;/p&gt;
&lt;p&gt;The only commonality I see with financial or economical discussions
is that they use a currency as a unit together with a time indication.
But that’s about it. Even the value of (or interpretation on) the
currency isn’t set in stone. &lt;/p&gt;
&lt;p&gt;Companies for instance often use internal charging for service usages.
This charging isn’t a cost that the company spends, it is mostly just
to attribute things to one business unit or the other. You might say it
is monopoly (the game) money. But that’s not always the case, as internal
charging in larger companies might also effectively reflect themselves in
the books / ledgers.&lt;/p&gt;
&lt;p&gt;And when you are charged - regardless if it is monopoly money or not - you
tend to really try to get the best deal… even though we are talking about
chargeback and thus not actual “profits and loss” of the company. If you
force your charging to be lower, you’re forcing other’s charging to get
higher. And they don’t like that, but rather than discussing this amongst
themselves, the discussions are often pointed towards the owner of the
service and its chargeback method.&lt;/p&gt;
&lt;p&gt;Or when project leaders make a business case (to show if a project is
beneficial from the financial side for the company) using chargeback
information. Chargeback doesn’t always (and in my opinion, mostly never)
reflect the true costs of a service, so using chargeback-currency for a
profit &amp;amp; loss currency is a big no-no. Yet this is oh so common still.&lt;/p&gt;
&lt;p&gt;Even when we have the right mindset and focus on actual costs… Well,
they are hard to obtain, because actual costs are complex beasts. You’ll
find plenty of resources online about the Total Cost of Ownership (TCO),
but none of them are truly the right resource to never look at others.
TCOs are hard, you need to consider things you don’t even knew you have
to consider.&lt;/p&gt;
&lt;p&gt;Or when you want to know just how something incurs on license costs.
Well, good luck in understanding how the vendor measures it (looking
at you, Oracle and Microsoft), or what deals your company has made with
the vendors under the umbrella of “enterprise agreements” which make all
the online resources you find useless. I had the role of license manager
for all our Oracle products for a few years, and was involved with the
Microsoft license management within my company, and it took me a while
to streamline it and communicate it properly within the organization
so all stakeholders were sufficiently aware of what it meant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Politics Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But financial discussions are peanuts compared with discussions that
reflect internal politics. I’m explicitly not calling this layer
strategy, although most of the discussions related to a companies’
strategy (or the strategy of the business units) are situated here
as well. It is also not governmental politics here, but the internal
domains where internal politics are reflected.&lt;/p&gt;
&lt;p&gt;These are the discussions where you, as an architect or engineer,
come in with a presentation that covers all the ends, handles the
financial side correctly and with agreements from other architects
or engineers that the figures are sound, the solution alternatives
correctly evaluated, and the preferred solution a good balance of
all the requirements… only to find that the temperature in the room
says otherwise.&lt;/p&gt;
&lt;p&gt;Discussions on the politics layer cover everything except what is
truly at the heart of what the discussion is about. They are about
hidden agendas that you will not be presented with at the meeting.
Many senior profiles (or, perhaps better, expert profiles) in an
organization tend to know what these hidden agendas are about, and
know how to ‘play’ and use these politics further. While their
seniority of course also focuses on the efficiency side (for instance,
a senior project leader can tackle larger projects, structure projects
better, are fluent in reporting, etc.) it is truly their organizational
experience (and sometimes ‘reading people’ skills) that makes them
quickly grow in their expertise.&lt;/p&gt;
&lt;p&gt;The challenge is to obtain the hidden agendas and work on disentangling
the complexity of it. Hidden agendas are often based on wrong assumptions,
past emotional stress, or missing information. While you often cannot
try to attack these agendas head-first, understanding their nature can
often help in presenting cases in a way that facilitates the discussions.
You can disarm the emotional stress, or start with tackling the assumptions
that might float around.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Religion Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sadly, even seasoned expert profiles however come across discussions that
are not just about internal politics or people’s hidden agendas. Sometimes
discussions just take on a religious nature. This is when people are adamant
that something is true, regardless of the facts or other material that you
bring in.&lt;/p&gt;
&lt;p&gt;Religious discussion can be found all over the place, not just on management
level. They can be about tooling, operating systems, designs, hosting choices,
etc. You all know there are people who can discuss Linux versus Windows for
ages and ages. Discussions on cloud versus on premise also often are religious
in nature. And the less time the organization allows for finding facts and
figures, the more these kinds of discussions pop up, and nobody wins on these
in the long term.&lt;/p&gt;
&lt;p&gt;Knowing when a discussion is internal politics (and thus can still be tackled)
versus religion is hard. But once I know a discussion is no longer going to be
settled with facts, figures, and emotional/psychological approaches, then I
will be likely to try and evade those meetings. I’ll try to facilitate a
management decision (if it isn’t religious on management level either) or
just deal with the onslaught.&lt;/p&gt;
&lt;p&gt;Challenges that are to be settled on the religion layer are no longer about
finding the optimal solution, but finding a solution or decision that is
“not in the wrong direction”. And while this is a much lower bar, it is hard
enough to reach it.&lt;/p&gt;</content><category term="Misc"></category><category term="OSI"></category><category term="meeting"></category><category term="humor"></category></entry><entry><title>Virtualization vs abstraction</title><link href="https://blog.siphos.be/2021/06/virtualization-vs-abstraction/" rel="alternate"></link><published>2021-06-03T10:10:00+02:00</published><updated>2021-06-03T10:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-03:/2021/06/virtualization-vs-abstraction/</id><summary type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Abstraction: common and simplified interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The term "abstraction" has slightly different connotations based on the
context in which the term is used. Generally speaking, an abstraction
provides a less detailed view on an object and shows the intrinsic qualities
upon which one looks at that object. Let's say we have a PostgreSQL database
and a MariaDB database. An abstract view on it could find that it has a lot
of commonalities, such as tabular representation of data, a network-facing
interface through which their database clients can interact with the
database, etc.&lt;/p&gt;
&lt;p&gt;We then further generalize this abstraction to come to the generalized
"relational database management system" concept. Furthermore, rather than
focusing on the database-specific languages of the PostgreSQL database and
the MariaDB database (i.e. the commands that database clients send to the
database), we abstract the details that are not shared across the two, and
create a more common set of commands that both databases support.&lt;/p&gt;
&lt;p&gt;Once you standardize on this common set of commands, you get more freedom in
exchanging one database technology for the other. This is exactly what
happened several dozen years ago, and resulted in the SQL standard
(ISO/IEC 9075). This standard is a language that, if all your relational
database technologies support it, allows you - as an organization - to work
with a multitude of database technologies while still having a more efficient
and standardized way of dealing with it.&lt;/p&gt;
&lt;p&gt;Now, the SQL language standard is one example. IT is filled with many other
examples, some more formally defined as standards than others. Let's look at
a more recent example, within the area of application containerization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRI and OCI are abstraction implementations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the Docker project, now supported through the Docker company, started
with enabling application containerization in a more efficient way, it leaned
upon the capabilities that the Linux kernel offered on hiding information and
isolating resources (namespaces and control groups) and expanded on it to
make it user friendly. It was an immediate hit, and has since then resulted
in a very competitive market.&lt;/p&gt;
&lt;p&gt;With Docker, applications could be put in more isolated environments and run
in parallel on the same system, without these applications seeing the other
ones. Each application has its own, private view on the system. With these
containers, the most important service that is still shared is the kernel,
with the kernel offering only those services to the containers that it can
keep isolated.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Container runtime abstraction" src="https://blog.siphos.be/images/202106/container-runtimes.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/"&gt;https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, while Docker can be easily attributed to bringing this to the wider
public, other initiatives quickly followed suit. Multiple container
technologies were coming to life, and started to bid for a place in the
containerization market. To be able to compete here, many of these attempted
to use the same interfaces (be it system calls, commands or other) as Docker
used, so the users can more easily switch. But while trying to copy and
implement the same interfaces is a possible venue, it is still strongly
controlled by the evolution that Docker is taking.&lt;/p&gt;
&lt;p&gt;Since then, larger projects like Kubernetes have started introducing an
abstraction between the container runtime (which implements the actual
containerization) and the container definitions and management (which uses
the containerization). Within Kubernetes for instance, this is through the
Common Runtime Interface (CRI), and the Open Container Interface (OCI) is
used to link the container runtime management with the underlying container
technologies.&lt;/p&gt;
&lt;p&gt;Introducing such an abstraction is a common way to establish a bit more
foothold in the market. Rather than trying to copy the market leader
verbatim, you create an intermediate layer, with immediate implementation
for the market leader as well, but with the promise that anyone that uses
the intermediate layer will be less tied to a single vendor or project: it
abstracts that vendor or project specifics away and shows mainly the
intrinsic qualities needed.&lt;/p&gt;
&lt;p&gt;If that abstraction is successful, other implementations for this abstraction
layer can easily come in and replace the previous technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstraction is not virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The introduction of abstraction layers, abstract technologies or abstract
languages should not be misunderstood for virtualization. Abstraction does
not hide or differently represent the resources beneath. It does not represent
itself as something else, but merely leaves out details that might make
interactions with the different technologies more complex.&lt;/p&gt;
&lt;p&gt;Virtualization on the other hand takes a different view. Rather than removing
the specific details, it represents a resource as something that it isn't
(or isn't completely). Hypervisors like KVM create a virtual hardware view,
and translates whatever calls towards the virtual hardware into calls to the
actual hardware - sometimes to the same type of hardware, but often towards
the CPU or resources that simulate the virtualized hardware further.&lt;/p&gt;
&lt;p&gt;Abstraction is a bit like classification, and defining how to work with a
resource through the agreed upon interfaces for that class. If you plug in
a USB device like a USB stick or USB keyboard or mouse, operating systems
will be able to interact with it regardless of its vendor and product,
because it uses the abstraction offered by the device classes: the USB mass
storage device class for the USB stick, or the USB human interface device
class for the keyboard and mouse. It abstracts away the complexity of
dealing with multiple implementations, but the devices themselves still
need to be classified as such.&lt;/p&gt;
&lt;p&gt;On hypervisors, you can create a virtual USB stick which in reality is just
a file on the hypervisor host or on a network file share. The hypervisor
virtualizes the view towards this file as if it is a USB stick, but in reality
there is no USB involved at all. Again, this doesn't have to be the case,
the hypervisor might as well enable virtualization of the USB device and
still eventually interact with an actual USB device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLANs are virtualized networks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another example of a virtualization is network virtualization through the
use of VLANs. In a virtual local area network (VLAN), all systems that
interact with this VLAN will see each other on the network as if they are
part of the same broadcast domain. Well, they are part of the same broadcast
domain. But if you look at the physical network implementation, this does not
imply that all these systems are attached to the same switch, and that no
routers are put in between to facilitate the communication.&lt;/p&gt;
&lt;p&gt;In larger enterprises, the use of VLANs is ubiquitous. Network virtualization
enables the telco teams and departments to optimize the actual physical
network without continuously impacting the configurations and deployments of
the services that use the network. Teams can create hundreds or thousands of
such VLANs while keeping the actual hardware investments under control, and
even be able to change and manage the network without impacting services.&lt;/p&gt;
&lt;p&gt;This benefit is strongly tied to virtualization, as we see the same in
hardware virtualization for server and workstation resources. By offering
virtualized systems, the underlying hardware can be changed, replaced or
switched without impact on the actual software that is running within the
virtualized environment. Well, mostly without impact, because not all
virtualization technologies or implementations are creating a full
virtualized view - sometimes shortcuts are created to improve efficiency
and performance. But in general, it works Just Fine (tm).&lt;/p&gt;
&lt;p&gt;Resource optimization and consolidation is easily accomplished when using
virtualization. You need far fewer switches in a virtualized network, and
you need far fewer servers for a virtualized server farm. But, it does come
at a cost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Virtualization introduces different complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you introduce a virtualization layer, be it for network, storage,
hardware or application runtimes, you introduce a layer that needs to be
actively managed. Abstraction is often much less resource intensive, as it
is a way to simplify the view on the actual resources while still being 100%
aligned with those underlying resources. Virtualization means that you need
to manage the virtualized resources, and keep track of how these resources
map to the actual underlying resources.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vSphere services" src="https://blog.siphos.be/images/202106/vsphere.png"&gt;
&lt;em&gt;Source: &lt;a href="https://virtualgyaan.com/vmkernel-components-and-functionality/"&gt;https://virtualgyaan.com/vmkernel-components-and-functionality/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's look at virtualized hardware for servers. On top of it, you have to
run and maintain the hypervisor, which represents the virtual hardware to
the operating systems. Within those (virtually running) operating systems,
you have a virtual view on resources: CPU, memory, etc. The sum of all
(virtual) CPUs is often not the same as the sum of all (actual) CPUs
(depending on configuration of course), and in larger environments the
virtual operating systems might not even be running on the same hardware
as they did a few hours ago, even though the system has not been restarted.&lt;/p&gt;
&lt;p&gt;Doing performance analysis implies looking at the resources within (virtual)
as well as mapped on the actual resources, which might not be of the same
type. A virtual GPU representation might be mapped to an actual GPU (and if
you want performance, I hope it is) but doesn't have to be. I've done
investigations on a virtual Trusted Platform Module (TPM) within a virtual
system running on a server that didn't have a TPM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assessing which standardization to approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I'm confronted with an increase in IT complexity, I will often be
looking at a certain degree of standardization to facilitate this in the
organization. But what type of standardization to approach depends strongly
on the situation.&lt;/p&gt;
&lt;p&gt;Standardization by rationalization is often triggered by cost optimization
or knowledge optimization. An organization that has ten different relational
database technologies in use could benefit of a rationalization in the number
of technologies to support. However, unless there is also sufficient
abstraction put in place, this rationalization can be intensive. Another
rationalization example could be on public cloud, where an organization
chooses to only focus on a single or two cloud providers but not more.&lt;/p&gt;
&lt;p&gt;While rationalization is easy to understand and explain, it does have adverse
consequences: you miss the benefits of whatever you're rationalized away,
and unless another type of standardization is put in place, it will be hard
to switch later on if the rationalization was ill-advised or picked the
wrong targets to rationalize towards.&lt;/p&gt;
&lt;p&gt;Standardization by abstraction focuses more on simplification. You are
introducing technologies that might have better interoperability through
this abstraction, but this can only be successful if the abstraction is
comprehensive enough to still use the underlying resources in an optimal
manner.&lt;/p&gt;
&lt;p&gt;My own observation on abstraction is that it is not commonly accepted by
engineers and developers at face value. It requires much more communication
and explanation than rationalization, which is often easy to put under "cost
pressure". Abstraction focuses on efficiency in a different way, and thus
requires different communication. At the company I currently work for,
we've introduced the Open Service Broker (OSB) API as an abstraction for
service instantiation and service catalogs, and after even more than a
year, including management support, it is still a common endeavor to
explain and motivate why we chose this.&lt;/p&gt;
&lt;p&gt;Virtualization creates a highly efficient environment and supports resource
optimizations that aren't possible in other ways. Its benefits are much easier
to explain to the organization (and to management), but has a downside that
is often neglected: it introduces complexity. Hence, virtualization should
only be pursued if you can manage the complexity, and that it isn't much
worse than the complexity you want to remove. Virtualization requires
organizational support which is more platform-oriented (and thus might be
further away from the immediate 'business value' IT often has to explain),
in effect creating a new type of technology within the ever increasing
catalog of IT services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software-defined infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While virtualization has been going around in IT for quite some time (before
I was born), a new kid on the block is becoming very popular: software-defined
infrastructure. The likes of Software Defined Network (SDN), Compute (SDC) and
Storage (SDS) are already common or becoming common. Other implementations,
like the Software Defined Perimeter, are getting jabbed by vendors as well.&lt;/p&gt;
&lt;p&gt;Now, SDI is not its own type of standardization. It is a way of managing
resources through code, and thus is a way of abstracting the infrastructure.
But unlike using a technology-agnostic abstraction, it pulls you into a
vendor-defined abstraction. That has its pros and cons, and as an architect
it is important to consider how to approach infrastructure-as-code, as SDI
implementations are not the only way to accomplish this.&lt;/p&gt;
&lt;p&gt;Furthermore, SDI does not imply virtualization. Certainly, if a technology
is virtualized, then SDI will also easily interact with it, and help you
define and manage the virtualized infrastructure as well as its underlay
infrastructure. But virtualization isn't a prerequisite for SDI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you're confronted with chaos and complexity, don't immediately start
removing technologies under the purview of "rationalization". Consider your
options on abstraction and virtualization, but be aware of the pros and cons
of each.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="virtualization"></category><category term="abstraction"></category></entry><entry><title>SELinux System Administration 3rd Edition</title><link href="https://blog.siphos.be/2021/01/selinux-system-administration-3rd-edition/" rel="alternate"></link><published>2021-01-06T20:00:00+01:00</published><updated>2021-01-06T20:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-01-06:/2021/01/selinux-system-administration-3rd-edition/</id><summary type="html">&lt;p&gt;As I mentioned previously, recently my latest installment of "SELinux System
Administration" has been released by Packt Publishing. This is already the
third edition of the book, after the first (2013) and second (2016) editions
have gotten reasonable success given the technical and often hard nature of
full SELinux administration.&lt;/p&gt;
&lt;p&gt;Like with the previous editions, this book remains true to the public of
system administrators, rather than SELinux policy developers. Of course,
SELinux policy development is not ignored in the book.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As I mentioned previously, recently my latest installment of "SELinux System
Administration" has been released by Packt Publishing. This is already the
third edition of the book, after the first (2013) and second (2016) editions
have gotten reasonable success given the technical and often hard nature of
full SELinux administration.&lt;/p&gt;
&lt;p&gt;Like with the previous editions, this book remains true to the public of
system administrators, rather than SELinux policy developers. Of course,
SELinux policy development is not ignored in the book.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What has changed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First and foremost, it of course updates the content of the previous edition
to be up to date with the latest evolutions within SELinux. There are no earth
shattering changes, so the second edition isn't suddenly deprecated. The examples
are brought up to date with a recent distribution setup, for which I used
Gentoo Linux and CentOS.&lt;/p&gt;
&lt;p&gt;The latter is, given the recent announcement of CentOS stopping support for
CentOS version 8 in general, a bit confrontational, although it doesn't
really matter that much for the scope of the book. I hope that &lt;a href="https://rockylinux.org/"&gt;Rocky
Linux&lt;/a&gt; will get the focus and support it deserves.&lt;/p&gt;
&lt;p&gt;Anyway, I digress. A significant part of the updates on the existing content
is on SELinux-enabled applications, applications that act as a so-called object
manager themselves. While quite a few were already covered in the past, these
applications continue to enhance their SELinux support, and in the third edition
a few of these receive a full dedicated chapter.&lt;/p&gt;
&lt;p&gt;There are also a small set of SELinux behavioral changes, like SELinux' NNP
support, as well as SELinux userspace changes like specific extended attributes
for restorecon.&lt;/p&gt;
&lt;p&gt;Most of the book though isn't about changes, but about new content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What has been added&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As administrators face SELinux-aware applications more and more, the book
goes into much more detail on how to tune SELinux with those SELinux-aware
applications. If we look at the book's structure, you'll find that it has
roughly three parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using SELinux, which covers the fundamentals of using SELinux and
   understanding what SELinux is.&lt;/li&gt;
&lt;li&gt;SELinux-aware platforms, which dives into the SELinux-aware application
   suites that administrators might come in contact with&lt;/li&gt;
&lt;li&gt;Policy management, which focuses on managing, analyzing and even
   developing SELinux policies.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By including additional content on SEPostgreSQL, libvirt, container
platforms like Kubernetes, and even Xen Security Modules (which is not
SELinux itself, but strongly influenced and aligned to it to the level
that it even uses the SELinux userspace utilities) the book is showing how
wide SELinux is being used.&lt;/p&gt;
&lt;p&gt;Even on policy development, the book now includes more support than before.
While another book of mine, SELinux Cookbook, is more applicable to policy
development, I did not want to keep administrators out of the loop on how
to develop SELinux policies at all. Especially not since there are more
tools available nowadays that support policy creation, like udica.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux CIL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the changes I also introduced in the book is to include SELinux
Common Intermediate Language (CIL) information and support. When we need
to add in a small SELinux policy change, the book will suggest CIL based
changes as well.&lt;/p&gt;
&lt;p&gt;SELinux CIL is not commonly used in large-scale policy development. Or at
least, not directly. The most significant policy development out there,
the &lt;a href="https://github.com/SELinuxProject/refpolicy/wiki"&gt;SELinux Reference Policy&lt;/a&gt;,
does not use CIL directly itself, and the level of support you find for
the current development approach is very much the default way of working. So
I do not ignore this more traditional approach.&lt;/p&gt;
&lt;p&gt;The reason I did include more CIL focus is because CIL has a few advantages
up its sleeve that is harder to get with the traditional language. Nothing
major perhaps, but enough that I feel it should be more actively promoted
anyway. And this book is hopefully a nice start to it.&lt;/p&gt;
&lt;p&gt;I hope the book is a good read for administrators or even architects that
would like to know more about the technology.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="packt"></category><category term="book"></category></entry><entry><title>Abstracting infrastructure complexity</title><link href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/" rel="alternate"></link><published>2020-12-25T23:00:00+01:00</published><updated>2020-12-25T23:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-12-25:/2020/12/abstracting-infrastructure-complexity/</id><summary type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Complexity is a challenging beast&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If someone were to attempt drawing out how the IT infrastructure of a larger
IT environment looks like in reality, it would soon become very, very large and
challenging to explain. Perhaps not chaotic, but definitely complicated.&lt;/p&gt;
&lt;p&gt;One of the challenges is the amount of "something" that is out there. That can
be the amount of devices you have, the amount of servers in the network, the
amount of flows going through firewalls or gateways, the amount of processes
running on a server, the amount of workstations and end user devices in use,
the amount of containers running in the container platform, the amount of cloud
platform instances that are active... &lt;/p&gt;
&lt;p&gt;The "something" can even be less tangible than the previous examples such as
the amount of projects that are being worked on in parallel or the amount of
changes that are being prepared. However, that complexity is not one I'll deal
with in this post.&lt;/p&gt;
&lt;p&gt;Another challenge is the virtualized nature of IT infrastructure, which has
a huge benefit for the organization and simplifies infrastructure services
for its own consumers, but does make it more, well, complicated to deal with.&lt;/p&gt;
&lt;p&gt;Virtual networks (vlans), virtual systems (hypervisors), virtual firewalls,
virtual applications (with support for streaming desktop applications to the
end user device without having the applications installed on that device),
virtual storage environments, etc. are all wonderful technologies which allow
for much more optimized resource usage, but does introduce a higher complexity
of the infastructure at large.&lt;/p&gt;
&lt;p&gt;To make sense of such larger structures, we start making abstractions of what
we see, structuring it in a way that we can more easily explain, assess or analyze
the environment and support changes properly. These abstract views do reflect
reality, but only to a certain extend. Not every question that can be asked can
be answered satisfactory with the same abstract view, but when it can, it is very
effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstracting service complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my day-to-day job, I'm responsible for the infrastructure of a reasonably
large environment. With "responsible" I don't want to imply that I'm the one
and only party involved of course - responsibilities are across a range of
people and roles. I am accountable for the long-term strategy on
infrastructure and the high-level infrastructure architecture and its offerings,
but how that plays out is a collaborative aspect.&lt;/p&gt;
&lt;p&gt;Because of this role, I do want to keep a close eye on all the services that
we offer from infrastructure side of things. And hence, I am often confronted
with the complexity mentioned earlier. To resolve this, I try to look at all
infastructure services in an abstract way, and document it in the same way so
that services are more easily explained.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An Archimate based view on the abstractions listed" src="https://blog.siphos.be/images/202012/abstracting-infrastructure-complexity-kvm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 - A possible visualization of the abstraction model, here in Archimate&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The abstraction I apply is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with &lt;em&gt;components&lt;/em&gt;, building blocks that are used and which refer
  to a single product or technology out there. A specific Java product can
  be considered such a component, because by itself it hardly has any value.&lt;/li&gt;
&lt;li&gt;Components are put together to create a &lt;em&gt;solution&lt;/em&gt;. This is something that
  is intended to provide value to the organization at large, and is the level
  at which something is documented, has an organizational entity responsible
  for it, etc. Solutions are not yet instantiated though. An example of a
  solution could be a Kafka-based pub/sub solution, or an OpenLDAP-based
  directory solution.&lt;/li&gt;
&lt;li&gt;Solutions are used to create &lt;em&gt;services&lt;/em&gt;. A service is something that has
  an SLA attached to it. In most cases, the same solution is used to create
  multiple services. We can think of the Kafka-based pub/sub solution that
  has three services in the organization: a regular non-production one,
  a regular production one, and a highly available production service.&lt;/li&gt;
&lt;li&gt;Services are supported through one or more &lt;em&gt;clusters&lt;/em&gt;. These are a
  way for teams to organize resources in support of a service. Some services
  might be supported by multiple clusters, for instance spread across
  different data centers. An OpenLDAP-based service might be supported by
  a single OpenLDAP cluster with native synchronization support spread across
  two data centers, or by two OpenLDAP clusters with a different
  synchronization mechanism between the two clusters.&lt;/li&gt;
&lt;li&gt;Clusters exist out of one or more &lt;em&gt;instances&lt;/em&gt;. These are the actual deployed
  technology processes that enable the cluster. In an OpenLDAP cluster, you
  could have two master processes (&lt;code&gt;slapd&lt;/code&gt; processes) running, which are the
  instances within the cluster.&lt;/li&gt;
&lt;li&gt;On top of the clusters, we enable &lt;em&gt;containers&lt;/em&gt; (I call those containers, but
  they don't have anything to do with container technology like Docker containers).
  The containers are what the consumers are actually interested in. That could
  be an organization unit in an LDAP structure, a database within an RDBMS, 
  a set of topics within a Kafka cluster, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are the basic abstractions I apply for most of the technologies, allowing
me to easily make a good view on the environment. Let's look at a few examples
here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: Virtualization of Wintel systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a large, virtualized environment, you generally have a specific hypervisor
software being used: be it RHV (Red Hat Virtualization) based upon
KVM, Microsoft HyperV, VMWare vSphere or something else - the technology used
is generally well known. That's one of the components being used, but that is
far from the only component.&lt;/p&gt;
&lt;p&gt;To better manage the virtualized environment the administration teams might
use an orchestration engine like Ansible, Puppet or Saltstack. They might also
have a component in use for automatically managing certificates and what not.&lt;/p&gt;
&lt;p&gt;All these components are needed to build a full virtualization solution. For
me, as an architect, knowing which components are used is useful for things
like lifecycle management (which components are EOL, which components can be
easily replaced with a different one versus components that are more lock-in
oriented, etc.) or inventory management (which component is deployed where,
which version is used), which supports things like vulnerability management
(if we can map components to their Common Platform Enumeration (CPE) then
we can easily see which vulnerabilities are reported through the Common
Vulnerabilities and Exposure (CVE) reports).&lt;/p&gt;
&lt;p&gt;The interaction between all these components creates a sensible solution,
which is the virtualization solution. At this level, I'm mostly interested
in the solution roadmap, the responsibilities and documentation associated
with it, the costs, maturity of the offering within the organization, etc.
It is also on the solution level that most architectural designs are made,
and the best practices (and malpractices) are documented.&lt;/p&gt;
&lt;p&gt;The virtualization solution itself is then instantiated within the
organization to create one or more services. These could be different
services based on the environment (a lab/sandbox virtualization service
with low to no SLA, a non-production one with standard SLA, a non-production
one with specific disaster recovery requirements, a production one with
standard SLA (and standard disaster recovery requirements), a high-performance
production one, etc.&lt;/p&gt;
&lt;p&gt;These services are mostly important for other architects, project leads
or other stakeholders that are going to make active use of the virtualization
services - the different services (which one could document as "service
plans") make it more obvious on what the offering is, and what differentiation
is supported.&lt;/p&gt;
&lt;p&gt;Let's consider a production, standard SLA virtualization service. The
system administrators of the virtualization environment might enable this
service across multiple clusters. This could be for several reasons: this
could be due to limits (maximum number of hosts per cluster), or because
of particular resource requirements (different CPU architecture requirements
- yes even with virtualization this is still a thing), or to make
things manageable for the administrators in general.&lt;/p&gt;
&lt;p&gt;While knowing which cluster an application is on is, in general, not
that important, it can be very important when there are problems, or when
limits are being reached. As an architect, I'm definitely interested in
knowing why multiple clusters are made (what is the reasoning behind it) as
it gives a good view on what the administrators are generally dealing with.&lt;/p&gt;
&lt;p&gt;Within a cluster (to support the virtualization) you'll find multiple hosts.
Often, a cluster is sized to be able to deal with one or two host fall-outs
so that the virtual machines (which are hosted on the cluster - these are
the "containers" that I spoke of) can be migrated to another host with only
a short downtime as a consequence (if their main host crashed) or no downtime
at all (if it is scheduled maintenance of the host). These hosts are the
instances of the cluster.&lt;/p&gt;
&lt;p&gt;By using this abstraction, I can "map" the virtualization environment in
a way that I have a good enough view, without proclaiming to be anything
more than an informed architect, on this setup to support my own work,
and to be able to advice management on major investment requirements,
challenges, strategic evolutions and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More than just documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the above method is used for documenting the environment in which
I work (and which works well for the size of the environment I have to deal
with), it can be used for simplifying management of the technologies as
well. This level of abstraction can easily be used in environments that
push self-servicing forward.&lt;/p&gt;
&lt;p&gt;Let's take the &lt;a href="https://www.openservicebrokerapi.org/"&gt;Open Service Broker API&lt;/a&gt;
as an example. This is an API that defines how to expose (infrastructure)
services to consumers that can easily create (provision) and destroy 
(deprovision) their own services. Brokers that support the API will
then automatically handle the service management. This model can easily
be put in to support the previous abstraction.&lt;/p&gt;
&lt;p&gt;Take the virtualization environment again. If we want to enable self-servicing
on a virtualized environment, we can think of an offering where internal customers
can create new virtual machines (provision) either based on a company-vetted
template, or through an image (like with virtual appliances). The team that
manages the virtualization environment has a number of services, which they
describe in the service plans exposed by the API. An internal customer, when
privisioning a virtual machine, is thus creating a "container" for the right
service (based on their selected service plan) and on the right cluster
(based upon the parameters that the internal customer passes along with the
creation of its machine).&lt;/p&gt;
&lt;p&gt;We can do the same with databases: a certain database solution (say PostgreSQL)
has its own offerings (exposed through service plans linked to the service), and
internal customers can create their own database ("container") on the right
cluster through this API.&lt;/p&gt;
&lt;p&gt;I personally have a few scripts that I use at home myself to quickly set
up a certain technology, using the above abstraction level as the foundation.
Rather than having to try and remember how to set up a multi-master OpenLDAP
service, or a replicated Kafka setup, I have scripts that create this based
upon this abstraction: the script parameters always use the service, cluster,
instance and container terminology and underlyingly map this to the
technology-specific approach.&lt;/p&gt;
&lt;p&gt;It is my intention to also promote this abstraction usage within my
work environment, as I believe it allows us to more easily explain what
all the infrastructure is used for, but also to more easily get new employees
known to our environment. But even if that isn't reached, the abstraction is
a huge help for me to assess and understand the multitude of technologies
that are out there, be it our mainframe setup, the SAN offerings, the
network switching setup, the databases, messaging services, cloud
landing zones, firewall setups, container platforms and more.&lt;/p&gt;</content><category term="Architecture"></category><category term="infrastructure"></category><category term="archimate"></category></entry><entry><title>Working on infra strategy</title><link href="https://blog.siphos.be/2020/10/working-on-infra-strategy/" rel="alternate"></link><published>2020-10-04T13:20:00+02:00</published><updated>2020-10-04T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-10-04:/2020/10/working-on-infra-strategy/</id><summary type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Transitioning to domain architect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I have been an infrastructure architect for quite some time already, my
focus then was always on specific areas within infrastructure (databases,
scheduling, big data), or on general infrastructure projects or challenges
(infrastructure zoning concept, region-wide disaster recovery analysis and
design). As one of my ex-colleagues and mentors put it, as infrastructure
architects you are allowed to piss on each other's area: you can (and perhaps
should) challenge the vision and projects of others, of course in a
professional way.&lt;/p&gt;
&lt;p&gt;I heeded the advice of this person, and was able to get a much better grip on
all our infrastructure services, their designs and challenges. I mentioned
earlier on that my day job became more intensive: it was not just the direct
responsibilities that I had that became more challenging, my principle to learn
and keep track of all infrastructure evolutions were a large part of it as
well. This pays off, as feedback and advice within the architecture review
boards is more to the point, more tied to the situation.&lt;/p&gt;
&lt;p&gt;Furthermore, as an architect, I still try to get my hands dirty on everything
bouncing around. When I was focusing on big data, I learned Spark and
pySpark, I revisited my Python knowledge, and used this for specific cases
(like using Python to create reports rather than Excel) to make sure I get a
general feel of what engineers and developers have to work with. When my focus
was on databases, I tried to get acquainted with DBA tasks. When we were
launching our container initiative, I set up and used Kubernetes myself (back
then this was also to see if SELinux is working properly with Kubernetes and
during the installation).&lt;/p&gt;
&lt;p&gt;While this does not make me anything near what our engineers and experts are
doing, I feel it gives me enough knowledge to be able to talk and discuss
topics with these colleagues without being that "ivory tower" architect,
and better understand (to a certain level) what they are going through when
new initiatives or solutions are thrown at them.&lt;/p&gt;
&lt;p&gt;End of 2019, the company decided that a reorganization was due, not only on
department and directorate level, but also on the IT and Enterprise
Architecture level. One of the areas that improved was to make sure the
infrastructure in general was also covered and supported by the EA team.
Part of that move, two of my infrastructure architect colleagues and
myself joined the EA team. One colleague is appointed to tackle a strategic
theme, another is now domain architect for workplace/workforce,
and I got the task of covering the infrastructure domain. Well, it is called
infrastructure, but focus on the infrastructure related to hosting of
applications and services: cloud hosting, data center, network, compute,
private cloud, container platform, mainframe, integration services,
middleware, etc. Another large part of what I consider "infrastructure" is
part of the workplace domain, which my colleague is pushing forward.&lt;/p&gt;
&lt;p&gt;While I was still handing over my previous workload, coaching the new colleague
that got thrown in to make sure both him and the teams involved are not left
with a gap, the various domain enterprise architects got a first task: draft
up the strategy for the domain… and don't wait too long ;-)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tackling a domain strategy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, I've drafted infrastructural strategies quite a few times already,
although those have most focus on the technology side. The domain view
goes beyond just the technological means: to be able to have a well-founded
strategy, I also have to tackle the resources and human side of things, the
ability of the organization to deal with (yet another?) transformation, the
processes involved, etc.&lt;/p&gt;
&lt;p&gt;Unlike the more specific area focus I had in the past, where the number of
direct stakeholders is limited, the infrastructure domain I now support has
many more direct stakeholders involved. If I count the product managers, system
architects, product owners (yes we are trying the Scaled Agile approach in our
organization) and the managerial roles within the domain, I have 148 people to
involve, spread across 7 Agile Release Trains with different directorate
steering. The consumers of the infrastructure services (which are more part of
business delivery services rather than on IT level) are even much larger than
that, and are the most important ones (but also more difficult) to get in touch
with.&lt;/p&gt;
&lt;p&gt;Rather than just asking what the main evolutions are in the several areas of
the domains, I approached this more according to practices I read in books like
"Good Strategy, Bad Strategy" by Richard Rumelt. I started with interviews of
all the stakeholders to get to learn what their challenges and problems are.
I wanted our strategy to tackle the issues at hand, not focus on technological
choices. Based on these interviews, I grouped the issues and challenges to see
what are the primary causes of these issues.&lt;/p&gt;
&lt;p&gt;Then I devised what action domains I need to focus on in the strategy. An
action domain was an area that more clearly describes the challenges ahead:
while I had close to 200 challenges observed from the interviews, I can assign
the huge majority of them to one of two action domains: if we tackle these
domains then we are helping the organization in most of their challenges.
After validating that these action domains are indeed covering the needs of
the organization, I started working on the principles how to tackle these
issues.&lt;/p&gt;
&lt;p&gt;Within the principles I want to steer the evolution within the infrastructure
domain, without already focusing on the tangible projects to accomplish that.
The principles should map to both larger projects (which I wanted to describe
in the strategy as well) as well as smaller or more continuity-related
projects. I eventually settled with four principles:
  - one principle covering how to transform the environment,
  - one principle covering what we offer (and thus also what we won't be
    offering anymore),
  - one principle which extends our scope with a major area that our internal
    customers are demanding, and
  - one principle describing how we will design our services&lt;/p&gt;
&lt;p&gt;Four principles are easy enough to remember for all involved, and if they are
described well, they are steering enough for the organization to take up in
their solutions. But with principles alone the strategy is not tangible enough
for everyone, and many choices to be made are not codified within those
principles. The next step was to draw out the vision for  infrastructure, based
upon current knowledge and the principles above, and show the major areas of
work that lays ahead, as well as give guidance on what these areas should
evolve to.&lt;/p&gt;
&lt;p&gt;I settled for eight vision statements, each worked out further with high level
guidance, as well as impact information: how will this impact the organization?
Do we need specific knowledge or other profiles that we miss? Is this a vision
that instills a cultural change (which often implies a slower adoption and the
need for more support)? What are the financial consequences? What will happen
if we do not pursue this vision?&lt;/p&gt;
&lt;p&gt;Within each vision, I collaborated with the various system architects and other
stakeholders to draft out epics, changes that support the vision and are ready
to be taken up in the Scaled Agile approach of the organization. The epics that
would be due soon were fully expanded, with a lean business case (attempt) and
phasing. Epics that are scheduled later (the strategy is a 5-year plan) are
mainly paraphrased as expanding those right now provides little value.&lt;/p&gt;
&lt;p&gt;While the epics themselves are not fully described in the strategy (the visions
give the rough approach), drafting these out is a way to verify if the vision
statements are feasible and correct, and is a way to check if the organization
understands and supports the vision.&lt;/p&gt;
&lt;p&gt;From the moment I got the request to the final draft of the strategy note,
around 2 months have passed. The first draft was slideware and showed the
intentions towards management (who wanted feedback within a few weeks after
the request), after which the strategy was codified in a large document, and
brought for approval on the appropriate boards.&lt;/p&gt;
&lt;p&gt;That was only the first hurdle though. Next was to communicate this strategy
further…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication and involvement are key&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The strategic document was almost finalized when COVID-19 struck. The company
moved to working at home, and the way of working changed a lot. This also
impacted how to approach the communication of the strategy and trying to get
involvement of people. Rather than physically explaining the strategy, watching
the body language of the people to see if they understand and support it or
not, I was facing digital meetings where we did not yet have video.
Furthermore, the organization was moving towards a more distributed approach
with smaller teams (higher agility) with fewer means of bringing out
information to larger groups.&lt;/p&gt;
&lt;p&gt;I selected a few larger meetings (such as those where all product managers and
system architects are present) to present and discuss the strategy, but also
started making webinars on this so that interested people could get informed
about it. I even decided to have two webinars: a very short one (3 minutes)
which focuses on the principles alone (and quickly summarizes the vision
statements), and an average one (20-ish minutes) which covers the principles
and vision statements.&lt;/p&gt;
&lt;p&gt;I also made recordings of the full explanations (e.g. those to management
team), which take 1 hour, but did not move those towards a webinar (due to
time pressure). Of course, I also published the strategy document itself for
everyone, as well as the slides that accompany it.&lt;/p&gt;
&lt;p&gt;One of the next steps is to translate the strategy further towards the
specific agile release trains, drafting up specific roadmaps, etc. This will
also allow me to communicate and explain the strategy further. Right now, this
is where we are at - and while I am happy with the strategy content, I do feel
that the communication part received too little attention from myself, and is
something I need to continue to put focus on.&lt;/p&gt;
&lt;p&gt;If a strategy is not absorbed by the organization, it fails as a strategy. And
if you do not have sufficient collaboration on the strategy after it was
'finalized' (not just communication but collaboration) then the organization
cannot absorb it. I also understand that the infrastructure strategy isn't
the only one guiding the organization: each domain has a strategy, and
while the domain architects do try to get the strategies aligned (or at least
not contradictory to each other), it is still not a single, company-wide
strategy.&lt;/p&gt;
&lt;p&gt;Right now, colleagues are working on consolidating the various strategies on
architectural level, while the agile organization is using the strategies to
formulate their specific solution visions (and for a handful of solutions I'm
also directly involved).&lt;/p&gt;
&lt;p&gt;We'll see how it pans out.&lt;/p&gt;
&lt;p&gt;So, do you think this is a sensible approach I took? How did you tackle
communication and collaboration of such initiatives during COVID-19 measures? &lt;/p&gt;</content><category term="Architecture"></category></entry><entry><title>cvechecker 3.9 released</title><link href="https://blog.siphos.be/2018/09/cvechecker-3.9-released/" rel="alternate"></link><published>2018-09-09T13:20:00+02:00</published><updated>2018-09-09T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-09-09:/2018/09/cvechecker-3.9-released/</id><content type="html">&lt;p&gt;Thanks to updates from Vignesh Jayaraman, Anton Hillebrand and Rolf Eike Beer,
a new release of &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; is
now made available.&lt;/p&gt;
&lt;p&gt;This new release (v3.9) is a bugfix release.&lt;/p&gt;
</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Automating compliance checks</title><link href="https://blog.siphos.be/2018/03/automating-compliance-checks/" rel="alternate"></link><published>2018-03-03T13:20:00+01:00</published><updated>2018-03-03T13:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-03-03:/2018/03/automating-compliance-checks/</id><summary type="html">&lt;p&gt;With the configuration baseline for a technical service being described fully (see the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first&lt;/a&gt;, &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second&lt;/a&gt; and &lt;a href="https://blog.siphos.be/2018/01/documenting-a-rule/"&gt;third&lt;/a&gt; post in this series), it is time to consider the validation of the settings in an automated manner. The preferred method for this is to use &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, which is nowadays managed by the &lt;a href="https://oval.cisecurity.org/"&gt;Center for Internet Security&lt;/a&gt;, abbreviated as CISecurity. Previously, OVAL was maintained and managed by Mitre under NIST supervision, and Google searches will often still point to the old sites. However, documentation is now maintained on CISecurity's &lt;a href="https://github.com/OVALProject/Language/tree/5.11.2/docs"&gt;github repositories&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But I digress...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the configuration baseline for a technical service being described fully (see the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first&lt;/a&gt;, &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second&lt;/a&gt; and &lt;a href="https://blog.siphos.be/2018/01/documenting-a-rule/"&gt;third&lt;/a&gt; post in this series), it is time to consider the validation of the settings in an automated manner. The preferred method for this is to use &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, which is nowadays managed by the &lt;a href="https://oval.cisecurity.org/"&gt;Center for Internet Security&lt;/a&gt;, abbreviated as CISecurity. Previously, OVAL was maintained and managed by Mitre under NIST supervision, and Google searches will often still point to the old sites. However, documentation is now maintained on CISecurity's &lt;a href="https://github.com/OVALProject/Language/tree/5.11.2/docs"&gt;github repositories&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But I digress...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Read-only compliance validation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the main ideas with OVAL is to have a language (XML-based) that represents state information (what something should be) which can be verified in a read-only fashion. Even more, from an operational perspective, it is very important that compliance checks &lt;em&gt;do not alter&lt;/em&gt; anything, but only report.&lt;/p&gt;
&lt;p&gt;Within its design, OVAL engineering has considered how to properly manage huge sets of assessment rules, and how to document this in an unambiguous manner. In the previous blog posts, ambiguity was resolved through writing style, and not much through actual, enforced definitions.&lt;/p&gt;
&lt;p&gt;OVAL enforces this. You can't write a generic or ambiguous rule in OVAL. It is very specific, but that also means that it is daunting to implement the first few times. I've written many OVAL sets, and I still struggle with it (although that's because I don't do it enough in a short time-frame, and need to reread my own documentation regularly).&lt;/p&gt;
&lt;p&gt;The capability to perform read-only validation with OVAL leads to a number of possible use cases. In the &lt;a href="http://oval.mitre.org/language/version5.10/OVAL_Language_Specification_09-14-2011.pdf"&gt;5.10 specification&lt;/a&gt; a number of use cases are provided. Basically, it boils down to vulnerability discovery (is a system vulnerable or not), patch management (is the system patched accordingly or not), configuration management (are the settings according to the rules or not), inventory management (detect what is installed on the system or what the systems' assets are), malware and threat indicator (detect if a system has been compromised or particular malware is active), policy enforcement (verify if a client system adheres to particular rules before it is granted access to a network), change tracking (regularly validating the state of a system and keeping track of changes), and security information management (centralizing results of an entire organization or environment and doing standard analytics on it).&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm focusing on configuration management.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OVAL structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although the OVAL standard (just like the XCCDF standard actually) entails a number of major components, I'm going to focus on the OVAL definitions. Be aware though that the results of an OVAL scan are also standardized format, as are results of XCCDF scans for instance.&lt;/p&gt;
&lt;p&gt;OVAL definitions have 4 to 5 blocks in them:
- the &lt;strong&gt;definition&lt;/strong&gt; itself, which describes what is being validated and how. It refers to one or more tests that are to be executed or validated for the definition result to be calculated
- the &lt;strong&gt;test&lt;/strong&gt; or tests, which are referred to by the definition. In each test, there is at least a reference to an object (what is being tested) and optionally to a state (what should the object look like)
- the &lt;strong&gt;object&lt;/strong&gt;, which is a unique representation of a resource or resources on the system (a file, a process, a mount point, a kernel parameter, etc.). Object definitions can refer to multiple resources, depending on the definition.
- the &lt;strong&gt;state&lt;/strong&gt;, which is a sort-of value mapping or validation that needs to be applied to an object to see if it is configured correctly
- the &lt;strong&gt;variable&lt;/strong&gt;, an optional definition which is what it sounds like, a variable that substitutes an abstract definition with an actual definition,  allowing to write more reusable tests.&lt;/p&gt;
&lt;p&gt;Let's get an example going, but without the XML structure, so in human language. We want to define that the Kerberos definition on a Linux system should allow forwardable tickets by default. This is accomplished by ensuring that, inside the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file (which is an INI-style configuration file), the value of the &lt;code&gt;forwardable&lt;/code&gt; key inside the &lt;code&gt;[libdefaults]&lt;/code&gt; section is set to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In OVAL, the definition itself will document the above in human readable text, assign it a unique ID (like &lt;code&gt;oval:com.example.oval:def:1&lt;/code&gt;) and mark it as being a definition for configuration validation (&lt;code&gt;compliance&lt;/code&gt;). Then, it defines the criteria that need to be checked in order to properly validate if the rule is applicable or not. These criteria include validation if the OVAL statement is actually being run on a Linux system (as it makes no sense to run it against a Cisco router) which is Kerberos enabled, and then the criteria of the file check itself. Each criteria links to a test.&lt;/p&gt;
&lt;p&gt;The test of the file itself links to an object and a state. There are a number of ways how we can check for this specific case. One is that the object is the &lt;code&gt;forwardable&lt;/code&gt; key in the &lt;code&gt;[libdefaults]&lt;/code&gt; section of the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file, and the state is the value &lt;code&gt;true&lt;/code&gt;. In this case, the state will point to those two entries (through their unique IDs) and define that the object must exist, and all matches must have a matching state. The "all matches" here is not that important, because there will generally only be one such definition in the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Note however that a different approach to the test can be declared as well. We could state that the object is the &lt;code&gt;[libdefaults]&lt;/code&gt; section inside the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; file, and the state is the value &lt;code&gt;true&lt;/code&gt; for the &lt;code&gt;forwardable&lt;/code&gt; key. In this case, the test declares that multiple objects must exist, and (at least) one must match the state.&lt;/p&gt;
&lt;p&gt;As you can see, the OVAL language tries to map definitions to unambiguous definitions. So, how does this look like in OVAL XML?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The OVAL XML structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://blog.siphos.be/static/2018/oval.xml"&gt;full example&lt;/a&gt; contains a few more entries than those we declare next, in order to be complete. The most important definitions though are documented below.&lt;/p&gt;
&lt;p&gt;Let's start with the definition. As stated, it will refer to tests that need to match for the definition to be valid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;definitions&amp;gt;
  &amp;lt;definition id=&amp;quot;oval:com.example.oval:def:1&amp;quot; version=&amp;quot;1&amp;quot; class=&amp;quot;compliance&amp;quot;&amp;gt;
    &amp;lt;metadata&amp;gt;
      &amp;lt;title&amp;gt;libdefaults.forwardable in /etc/krb5.conf must be set to true&amp;lt;/title&amp;gt;
      &amp;lt;affected family=&amp;quot;unix&amp;quot;&amp;gt;
        &amp;lt;platform&amp;gt;Red Hat Enterprise Linux 7&amp;lt;/platform&amp;gt;
      &amp;lt;/affected&amp;gt;
      &amp;lt;description&amp;gt;
        By default, tickets obtained from the Kerberos environment must be forwardable.
      &amp;lt;/description&amp;gt;
    &amp;lt;/metadata&amp;gt;
    &amp;lt;criteria operator=&amp;quot;AND&amp;quot;&amp;gt;
      &amp;lt;criterion test_ref=&amp;quot;oval:com.example.oval:tst:1&amp;quot; comment=&amp;quot;Red Hat Enterprise Linux is installed&amp;quot;/&amp;gt;
      &amp;lt;criterion test_ref=&amp;quot;oval:com.example.oval:tst:2&amp;quot; comment=&amp;quot;/etc/krb5.conf&amp;#39;s libdefaults.forwardable is set to true&amp;quot;/&amp;gt;
    &amp;lt;/criteria&amp;gt;
  &amp;lt;/definition&amp;gt;
&amp;lt;/definitions&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first thing to keep in mind is the (weird) identification structure. Just like with XCCDF, it is not sufficient to have your own id convention. You need to start an id with &lt;code&gt;oval:&lt;/code&gt; followed by the reverse domain definition (here &lt;code&gt;com.example.oval&lt;/code&gt;), followed by the type (&lt;code&gt;def&lt;/code&gt; for definition) and a sequence number.&lt;/p&gt;
&lt;p&gt;Also, take a look at the criteria. Here, two tests need to be compliant (hence the &lt;code&gt;AND&lt;/code&gt; operator). However, more complex operations can be done as well. It is even allowed to nest multiple criteria, and refer to previous definitions, like so (taken from the &lt;a href="https://raw.githubusercontent.com/GovReady/ubuntu-scap/master/ssg-rhel6-oval.xml"&gt;ssg-rhel6-oval.xml file&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;criteria comment=&amp;quot;package hal removed or service haldaemon is not configured to start&amp;quot; operator=&amp;quot;OR&amp;quot;&amp;gt;
  &amp;lt;extend_definition comment=&amp;quot;hal removed&amp;quot; definition_ref=&amp;quot;oval:ssg:def:211&amp;quot;/&amp;gt;
  &amp;lt;criteria operator=&amp;quot;AND&amp;quot; comment=&amp;quot;service haldaemon is not configured to start&amp;quot;&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 0&amp;quot; test_ref=&amp;quot;oval:ssg:tst:212&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 1&amp;quot; test_ref=&amp;quot;oval:ssg:tst:213&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 2&amp;quot; test_ref=&amp;quot;oval:ssg:tst:214&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 3&amp;quot; test_ref=&amp;quot;oval:ssg:tst:215&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 4&amp;quot; test_ref=&amp;quot;oval:ssg:tst:216&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 5&amp;quot; test_ref=&amp;quot;oval:ssg:tst:217&amp;quot;/&amp;gt;
    &amp;lt;criterion comment=&amp;quot;haldaemon runlevel 6&amp;quot; test_ref=&amp;quot;oval:ssg:tst:218&amp;quot;/&amp;gt;
  &amp;lt;/criteria&amp;gt;
&amp;lt;/criteria&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, let's look at the tests.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;tests&amp;gt;
  &amp;lt;unix:file_test id=&amp;quot;oval:com.example.oval:tst:1&amp;quot; version=&amp;quot;1&amp;quot; check_existence=&amp;quot;all_exist&amp;quot; check=&amp;quot;all&amp;quot; comment=&amp;quot;/etc/redhat-release exists&amp;quot;&amp;gt;
    &amp;lt;unix:object object_ref=&amp;quot;oval:com.example.oval:obj:1&amp;quot; /&amp;gt;
  &amp;lt;/unix:file_test&amp;gt;
  &amp;lt;ind:textfilecontent54_test id=&amp;quot;oval:com.example.oval:tst:2&amp;quot; check=&amp;quot;all&amp;quot; check_existence=&amp;quot;all_exist&amp;quot; version=&amp;quot;1&amp;quot; comment=&amp;quot;The value of forwardable in /etc/krb5.conf&amp;quot;&amp;gt;
    &amp;lt;ind:object object_ref=&amp;quot;oval:com.example.oval:obj:2&amp;quot; /&amp;gt;
    &amp;lt;ind:state state_ref=&amp;quot;oval:com.example.oval:ste:2&amp;quot; /&amp;gt;
  &amp;lt;/ind:textfilecontent54_test&amp;gt;
&amp;lt;/tests&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There are two tests defined here. The first test just checks if &lt;code&gt;/etc/redhat-release&lt;/code&gt; exists. If not, then the test will fail and the definition itself will result to false (as in, not compliant). This isn't actually a proper definition, because you want the test to not run when it is on a different platform, but for the sake of example and simplicity, let's keep it as is.&lt;/p&gt;
&lt;p&gt;The second test will check for the value of the &lt;code&gt;forwardable&lt;/code&gt; key in &lt;code&gt;/etc/krb5.conf&lt;/code&gt;. For it, it refers to an object and a state. The test states that all objects must exist (&lt;code&gt;check_existence="all_exist"&lt;/code&gt;) and that all objects must match the state (&lt;code&gt;check="all"&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The object definition looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;objects&amp;gt;
  &amp;lt;unix:file_object id=&amp;quot;oval:com.example.oval:obj:1&amp;quot; comment=&amp;quot;The /etc/redhat-release file&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;unix:filepath&amp;gt;/etc/redhat-release&amp;lt;/unix:filepath&amp;gt;
  &amp;lt;/unix:file_object&amp;gt;
  &amp;lt;ind:textfilecontent54_object id=&amp;quot;oval:com.example.oval:obj:2&amp;quot; comment=&amp;quot;The forwardable key&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;ind:filepath&amp;gt;/etc/krb5.conf&amp;lt;/ind:filepath&amp;gt;
    &amp;lt;ind:pattern operation=&amp;quot;pattern match&amp;quot;&amp;gt;^\s*forwardable\s*=\s*((true|false))\w*&amp;lt;/ind:pattern&amp;gt;
    &amp;lt;ind:instance datatype=&amp;quot;int&amp;quot; operation=&amp;quot;equals&amp;quot;&amp;gt;1&amp;lt;/ind:instance&amp;gt;
  &amp;lt;/ind:textfilecontent54_object&amp;gt;
&amp;lt;/objects&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first object is a simple file reference. The second is a text file content object. More specifically, it matches the line inside &lt;code&gt;/etc/krb5.conf&lt;/code&gt; which has &lt;code&gt;forwardable = true&lt;/code&gt; or &lt;code&gt;forwardable = false&lt;/code&gt; in it. An expression is made on it, so that we can refer to the subexpression as part of the test.&lt;/p&gt;
&lt;p&gt;This test looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;states&amp;gt;
  &amp;lt;ind:textfilecontent54_state id=&amp;quot;oval:com.example.oval:ste:2&amp;quot; version=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;ind:subexpression datatype=&amp;quot;string&amp;quot;&amp;gt;true&amp;lt;/ind:subexpression&amp;gt;
  &amp;lt;/ind:textfilecontent54_state&amp;gt;
&amp;lt;/states&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This test refers to a subexpression, and wants it to be &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing the checks with Open-SCAP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open-SCAP tool is able to test OVAL statements directly. For instance, with the above definition in a file called &lt;code&gt;oval.xml&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap oval eval --results oval-results.xml oval.xml
Definition oval:com.example.oval:def:1: true
Evaluation done.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output of the command shows that the definition was evaluated successfully. If you want more information, open up the &lt;code&gt;oval-results.xml&lt;/code&gt; file which contains all the details about the test. This results file is also very useful while developing OVAL as it shows the entire result of objects, tests and so forth.&lt;/p&gt;
&lt;p&gt;For instance, the &lt;code&gt;/etc/redhat-release&lt;/code&gt; file was only checked to see if it exists, but the results file shows what other parameters can be verified with it as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;unix-sys:file_item id=&amp;quot;1233781&amp;quot; status=&amp;quot;exists&amp;quot;&amp;gt;
  &amp;lt;unix-sys:filepath&amp;gt;/etc/redhat-release&amp;lt;/unix-sys:filepath&amp;gt;
  &amp;lt;unix-sys:path&amp;gt;/etc&amp;lt;/unix-sys:path&amp;gt;
  &amp;lt;unix-sys:filename&amp;gt;redhat-release&amp;lt;/unix-sys:filename&amp;gt;
  &amp;lt;unix-sys:type&amp;gt;regular&amp;lt;/unix-sys:type&amp;gt;
  &amp;lt;unix-sys:group_id datatype=&amp;quot;int&amp;quot;&amp;gt;0&amp;lt;/unix-sys:group_id&amp;gt;
  &amp;lt;unix-sys:user_id datatype=&amp;quot;int&amp;quot;&amp;gt;0&amp;lt;/unix-sys:user_id&amp;gt;
  &amp;lt;unix-sys:a_time datatype=&amp;quot;int&amp;quot;&amp;gt;1515186666&amp;lt;/unix-sys:a_time&amp;gt;
  &amp;lt;unix-sys:c_time datatype=&amp;quot;int&amp;quot;&amp;gt;1514927465&amp;lt;/unix-sys:c_time&amp;gt;
  &amp;lt;unix-sys:m_time datatype=&amp;quot;int&amp;quot;&amp;gt;1498674992&amp;lt;/unix-sys:m_time&amp;gt;
  &amp;lt;unix-sys:size datatype=&amp;quot;int&amp;quot;&amp;gt;52&amp;lt;/unix-sys:size&amp;gt;
  &amp;lt;unix-sys:suid datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:suid&amp;gt;
  &amp;lt;unix-sys:sgid datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:sgid&amp;gt;
  &amp;lt;unix-sys:sticky datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:sticky&amp;gt;
  &amp;lt;unix-sys:uread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:uread&amp;gt;
  &amp;lt;unix-sys:uwrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:uwrite&amp;gt;
  &amp;lt;unix-sys:uexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:uexec&amp;gt;
  &amp;lt;unix-sys:gread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:gread&amp;gt;
  &amp;lt;unix-sys:gwrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:gwrite&amp;gt;
  &amp;lt;unix-sys:gexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:gexec&amp;gt;
  &amp;lt;unix-sys:oread datatype=&amp;quot;boolean&amp;quot;&amp;gt;true&amp;lt;/unix-sys:oread&amp;gt;
  &amp;lt;unix-sys:owrite datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:owrite&amp;gt;
  &amp;lt;unix-sys:oexec datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:oexec&amp;gt;
  &amp;lt;unix-sys:has_extended_acl datatype=&amp;quot;boolean&amp;quot;&amp;gt;false&amp;lt;/unix-sys:has_extended_acl&amp;gt;
&amp;lt;/unix-sys:file_item&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, this is just on OVAL level. The final step is to link it in the XCCDF file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Referring to OVAL in XCCDF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The XCCDF Rule entry allows for a &lt;code&gt;check&lt;/code&gt; element, which refers to an automated check for compliance.&lt;/p&gt;
&lt;p&gt;For instance, the above rule could be referred to like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;Rule id=&amp;quot;xccdf_com.example_rule_krb5-forwardable-true&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Enable forwardable tickets on RHEL systems&amp;lt;/title&amp;gt;
  ...
  &amp;lt;check system=&amp;quot;http://oval.mitre.org/XMLSchema/oval-definitions-5&amp;quot;&amp;gt;
    &amp;lt;check-content-ref href=&amp;quot;oval.xml&amp;quot; name=&amp;quot;oval:com.example.oval:def:1&amp;quot; /&amp;gt;
  &amp;lt;/check&amp;gt;
&amp;lt;/Rule&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this set in the Rule, Open-SCAP can validate it while checking the configuration baseline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf eval --oval-results --results xccdf-results.xml xccdf.xml
...
Title   Enable forwardable kerberos tickets in krb5.conf libdefaults
Rule    xccdf_com.example_rule_krb5-forwardable-tickets
Ident   RHEL7-01007
Result  pass
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A huge advantage here is that, alongside the detailed results of the run, there is also better human readable output as it shows the title of the Rule being checked.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The detailed capabilities of OVAL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example I've used two examples: a file validation (against &lt;code&gt;/etc/redhat-release&lt;/code&gt;) and a file content one (against &lt;code&gt;/etc/krb5.conf&lt;/code&gt;). However, OVAL has many more checks and support for it, and also has constraints that you need to be aware of.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://github.com/OVALProject/Language/tree/master/docs"&gt;OVAL Project&lt;/a&gt; github account, the Language repository keeps track of the current documentation. By browsing through it, you'll notice that the OVAL capabilities are structured based on the target technology that you can check. Right now, this is AIX, Android, Apple iOS, Cisco ASA, Cisco CatOS, VMWare ESX, FreeBSD, HP-UX, Cisco iOS and iOS-XE, Juniper JunOS, Linux, MacOS, NETCONF, Cisco PIX, Microsoft SharePoint, Unix (generic), Microsoft Windows, and independent.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/independent-definitions-schema.md"&gt;independent&lt;/a&gt; one contains tests and support for resources that are often reusable toward different platforms (as long as your OVAL and XCCDF supporting tools can run it on those platforms). A few notable supporting tests are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filehash58_test&lt;/code&gt; which can check for a number of common hashes (such as SHA-512 and MD5). This is useful when you want to make sure that a particular (binary or otherwise) file is available on the system. In enterprises, this could be useful for license files, or specific library files.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;textfilecontent54_test&lt;/code&gt; which can check the content of a file, with support for regular expressions.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xmlfilecontent_test&lt;/code&gt; which is a specialized test toward XML files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keep in mind though that, as we have seen above, INI files specifically have no specialization available. It would be nice if CISecurity would develop support for common textual data formats, such as CSV (although that one is easily interpretable with the existing ones), JSON, YAML and INI.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/unix-definitions-schema.md"&gt;unix&lt;/a&gt; one contains tests specific to Unix and Unix-like operating systems (so yes, it is also useful for Linux), and together with the &lt;a href="https://github.com/OVALProject/Language/blob/master/docs/linux-definitions-schema.md"&gt;linux&lt;/a&gt; one a wide range of configurations can be checked. This includes support for generic extended attributes (&lt;code&gt;fileextendedattribute_test&lt;/code&gt;) as well as SELinux specific rules (&lt;code&gt;selinuxboolean_test&lt;/code&gt; and &lt;code&gt;selinuxsecuritycontext_test&lt;/code&gt;), network interface settings (&lt;code&gt;interface_test&lt;/code&gt;), runtime processes (&lt;code&gt;process58_test&lt;/code&gt;), kernel parameters (&lt;code&gt;sysctl_test&lt;/code&gt;), installed software tests (such as &lt;code&gt;rpminfo_test&lt;/code&gt; for RHEL and other RPM enabled operating systems) and more.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="oval"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Documenting a rule</title><link href="https://blog.siphos.be/2018/01/documenting-a-rule/" rel="alternate"></link><published>2018-01-24T20:40:00+01:00</published><updated>2018-01-24T20:40:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-24:/2018/01/documenting-a-rule/</id><summary type="html">&lt;p&gt;In the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first post&lt;/a&gt; I talked about why configuration documentation is important. In the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second post&lt;/a&gt; I looked into a good structure for configuration documentation of a technological service, and ended with an XCCDF template in which this documentation can be structured.&lt;/p&gt;
&lt;p&gt;The next step is to document the rules themselves, i.e. the actual content of a configuration baseline.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In the &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;first post&lt;/a&gt; I talked about why configuration documentation is important. In the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;second post&lt;/a&gt; I looked into a good structure for configuration documentation of a technological service, and ended with an XCCDF template in which this documentation can be structured.&lt;/p&gt;
&lt;p&gt;The next step is to document the rules themselves, i.e. the actual content of a configuration baseline.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Fine-grained rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While from a high-level point of view, configuration items could be documented in a coarse-grained manner, a proper configuration baseline documents rules very fine-grained. Let's first consider a bad example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All application code files are root-owned, with read-write privileges for owner and group, and executable where it makes sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While such a rule could be interpreted correctly, it also leaves room for misinterpretation and ambiguity. Furthermore, it is not explicit. What are application code files? Where are they stored? What about group ownership? The executable permission, when does that make sense? Does the rule also imply that there is no privilege for world-wide access, or does it just ignore that?&lt;/p&gt;
&lt;p&gt;A better example (or set of examples) would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; is recursively user-owned by root&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; is recursively group-owned by root&lt;/li&gt;
&lt;li&gt;No files under &lt;code&gt;/opt/postgresql&lt;/code&gt; are executable except when specified further&lt;/li&gt;
&lt;li&gt;All files in &lt;code&gt;/opt/postgresql/bin&lt;/code&gt; are executable&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql&lt;/code&gt; has &lt;code&gt;system_u:object_r:usr_t:s0&lt;/code&gt; as SELinux context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/opt/postgresql/bin/postgres&lt;/code&gt; has &lt;code&gt;system_u:object_r:postgresql_exec_t:s0&lt;/code&gt; as SELinux context&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And even that list is still not complete, but you get the gist. The focus here is to have fine-grained rules which are explicit and not ambiguous.&lt;/p&gt;
&lt;p&gt;Of course, the above configuration rule is still a "simple" permission set. Configuration baselines go further than that of course. They can act on file content ("no PAM configuration files can refer to pam_rootok.so except for runuser and su"), run-time processes ("The processes with /usr/sbin/sshd as command and with -D as option must run within the sshd_t SELinux domain"), database query results, etc.&lt;/p&gt;
&lt;p&gt;This granularity is especially useful later on when you want to automate compliance checks, because the more fine-grained a description is, the easier it is to develop and maintain checks on it. But before we look into remediation, let's document the rule a bit further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metadata on the rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's consider the following configuration rule:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;/opt/postgresql/bin/postgres&lt;/code&gt; has &lt;code&gt;system_u:object_r:postgresql_exec_t:s0&lt;/code&gt; as SELinux context&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the configuration baseline, we don't just want to state that this is the rule, and be finished. We need to describe the rule in more detail, as was described in the &lt;a href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/"&gt;previous post&lt;/a&gt;. More specifically, we definitely want to
- know the rule's severity is, or how "bad" it would be if we detect a deviation from the rule
- have an indication if the rule is security-sensitive or more oriented to manageability
- a more elaborate description of the rule than just the title
- an indication why this rule is in place (what does it solve, fix or simplify)
- information on how to remediate if a deviation is found
- know if the rule is applicable to our environment or not&lt;/p&gt;
&lt;p&gt;The severity in the &lt;em&gt;Security Content Automation Protocol (SCAP)&lt;/em&gt; standard, which defines the XCCDF standard as well as OVAL and a few others like CVSS, uses the following possible values for severity: unknown, info, low, medium, high.&lt;/p&gt;
&lt;p&gt;To indicate if a rule is security-oriented or not, XCCDF's role attribute is best used. With the role attribute, you state if a rule is to be included in the final scoring (a weighted value given to the compliance of a system) or not. If it is, then it is security sensitive.&lt;/p&gt;
&lt;p&gt;The indication of a rule applicability in the environment might seem strange. If you document the configuration baseline, shouldn't it include only those settings you want? Well, yes and no. Personally, I like to include recommendations that we &lt;em&gt;do not follow&lt;/em&gt; in the baseline as well.&lt;/p&gt;
&lt;p&gt;Suppose for instance that an audit comes along and says you need to enable data encryption on the database. Let's put aside that an auditor should focus mainly/solely on the risks, and let the solutions be managed by the team (but be involved in accepting solutions of course), the team might do an assessment and find that data encryption on the database level (i.e. the database files are encrypted so non-DBA users with operating system interactive rights cannot read the data) is actually not going to remediate any risk, yet introduce more complexity.&lt;/p&gt;
&lt;p&gt;In that situation, and assuming that the auditor agrees with a different control, you might want to add a rule to the configuration baseline about this. Either you document the wanted state (database files do not need to be encrypted), or you document the suggestion (database files should be encrypted) but explicitly state that you do not require or implement it, and document the reasoning for it. The rule is then augmented with references to the audit recommendation for historical reasons and to facilitate future discussions.&lt;/p&gt;
&lt;p&gt;And yes, I know the rule "database files should be encrypted" is still ambiguous. The actual rule should be more specific to the technology).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documenting a rule in XCCDF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In XCCDF, a rule is defined through the &lt;code&gt;Rule&lt;/code&gt; XML entity, and is placed within a &lt;code&gt;Group&lt;/code&gt;. The Group entities are used to structure the document, while the &lt;code&gt;Rule&lt;/code&gt; entities document specific configuration directives.&lt;/p&gt;
&lt;p&gt;The postgres related rule of above could be written as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;Rule id=&amp;quot;xccdf_com.example_rule_pgsql-selinux-context&amp;quot;
      role=&amp;quot;full&amp;quot;
      selected=&amp;quot;1&amp;quot;
      weight=&amp;quot;5.1&amp;quot;
      severity=&amp;quot;high&amp;quot;
      cluster-id=&amp;quot;network&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;
    /opt/postgresql/bin/postgres has system_u:object_r:postgresql_exec_t:s0 as SELinux context
  &amp;lt;/title&amp;gt;
  &amp;lt;description&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The postgres binary is the main binary of the PostgreSQL database daemon. Once started, it launches the necessary workers. To ensure that PostgreSQL runs in the proper SELinux domain (postgresql_t) its binary must be labeled with postgresql_exec_t.
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The current state of the label can be obtained using stat, or even more simple, the -Z option to ls:
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:pre&amp;gt;~$ ls -Z /opt/postgresql/bin/postgres
-rwxr-xr-x. root root system_u:object_r:postgresql_exec_t:s0 /opt/postgresql/bin/postgres
    &amp;lt;/xhtml:pre&amp;gt;
  &amp;lt;/description&amp;gt;
  &amp;lt;rationale&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      The domain in which a process runs defines the SELinux controls that are active on the process. Services such as PostgreSQL have an established policy set that controls what a database service can and cannot do on the system.
    &amp;lt;/xhtml:p&amp;gt;
    &amp;lt;xhtml:p&amp;gt;
      If the PostgreSQL daemon does not run in the postgresql_t domain, then SELinux might either block regular activities of the database (service availability impact), block behavior that impacts its effectiveness (integrity issue) or allow behavior that shouldn&amp;#39;t be allowed. The latter can have significant consequences once a vulnerability is exploited.
    &amp;lt;/xhtml:p&amp;gt;
  &amp;lt;/rationale&amp;gt;
  &amp;lt;fixtext&amp;gt;
    Restore the context of the file using restorecon or chcon.
  &amp;lt;/fixtext&amp;gt;
  &amp;lt;fix strategy=&amp;quot;restrict&amp;quot; system=&amp;quot;urn:xccdf:fix:script:sh&amp;quot;&amp;gt;restorecon /opt/postgresql/bin/postgres
  &amp;lt;/fix&amp;gt;
  &amp;lt;ident system=&amp;quot;http://example.com/configbaseline&amp;quot;&amp;gt;pgsql-01032&amp;lt;/ident&amp;gt;
&amp;lt;/Rule&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although this is lots of XML, it is easy to see what each element declares. The &lt;a href="https://csrc.nist.gov/CSRC/media/Publications/nistir/7275/rev-4/final/documents/nistir-7275r4_updated-march-2012_clean.pdf"&gt;NIST IR 7275 document&lt;/a&gt; is a very good resource to continuously consult in order to find the right elements and their interpretation.&lt;/p&gt;
&lt;p&gt;There is one element added that is "specific" to the content of this blog post series and not the XCCDF standard, namely the identification. As mentioned in an earlier post, organizations might have their own taxonomy for technical service identification, and requirements on how to number or identify rules. In the above example, the rule is identified as &lt;code&gt;pgsql-01032&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There is another attribute in use above that might need more clarification: the weight of the rule.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abusing CVSS for configuration weight scoring&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example, a weight is given to the rule scoring (weight of 5.1). This number is obtained through a &lt;a href="https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:N/AC:H/PR:N/UI:N/S:C/C:L/I:L/A:N/E:U/RL:O/CR:H/IR:H/AR:H/MAV:N/MAC:H/MPR:N/MUI:N/MS:U/MC:L/MI:N/MA:L"&gt;CVSS calculator&lt;/a&gt;, which is generally used to identify the risk of a security issue or vulnerability. CVSS stands for &lt;em&gt;Common Vulnerability Scoring System&lt;/em&gt; and is a popular way to weight security risks (which are then associated with vulnerability reports, &lt;em&gt;Common Vulnerabilities and Exposures (CVE)&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Misconfigurations can also be slightly interpreted as a security risk, although it requires some mental bridges. Rather than scoring the rule, you score the risk that it mitigates, and consider the worst thing that could happen if that rule is not implemented correctly. Now, worst-case thinking is subjective, so there will always be discussion on the weight of a rule. It is therefore important to have a consensus in the team (if the configuration baseline is team-owned) if this weight is actively used. Of course, an organization might choose to ignore the weight, or use a different scoring mechanism.&lt;/p&gt;
&lt;p&gt;In the above situation, I scored what would happen if a vulnerability in PostgreSQL was successfully exploited, and SELinux couldn't mitigate the risk as the label of the file was wrong. The result of a wrong label &lt;em&gt;could be&lt;/em&gt; that the PostgreSQL service runs in a higher privileged domain, or even in an unconfined domain (no SELinux restrictions active), so there is a heightened risk of confidentiality loss (beyond the database) and even integrity risk.&lt;/p&gt;
&lt;p&gt;However, the confidentiality risk is scored as low, and integrity even in between (base risk is low, but due to other constraints put in place integrity impact is reduced further) because PostgreSQL runs as a non-administrative user on the system, and perhaps because the organization uses dedicated systems for database hosting (so other services are not easily impacted).&lt;/p&gt;
&lt;p&gt;As mentioned, this is somewhat abusing the CVSS methodology, but is imo much more effective than trying to figure out your own scoring methodology. With CVSS, you start with scoring the risk regardless of context (CVSS Base), then adjust based on recent state or knowledge (CVSS Temporal), and finally adjust further with knowledge of the other settings or mitigating controls in place (CVSS Environmental).&lt;/p&gt;
&lt;p&gt;Personally, I prefer to only use the CVSS Base scoring for configuration baselines, because the other two are highly depending on time (which is, for documentation, challenging) and the other controls (which is more of a concern for service technical documentation). So in my preferred situation, the rule would be scored as 5.4 rather than 5.1. But that's just me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isn't this CCE?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People who use SCAP a bit more might already be thinking if I'm not reinventing the wheel here. After all, SCAP also has a standard called &lt;em&gt;Common Configuration Enumeration (CCE)&lt;/em&gt; which seems to be exactly what I'm doing here: enumerating the configuration of a technical service. And indeed, if you look at the &lt;a href="https://nvd.nist.gov/config/cce/index"&gt;CCE list&lt;/a&gt; you'll find a number of Excel sheets (sigh) that define common configurations.&lt;/p&gt;
&lt;p&gt;For instance, for Red Hat Enterprise Linux v5, there is an enumeration identified as CCE-4361-2, which states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;File permissions for /etc/pki/tls/ldap should be set correctly&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The CCE description then goes on stating that this is a permission setting (CCE Parameter), which can be rectified with &lt;code&gt;chmod&lt;/code&gt; (CCE Technical Mechanism), and refers to a source for the setting.&lt;/p&gt;
&lt;p&gt;However, CCE has a number of downsides.&lt;/p&gt;
&lt;p&gt;First of all, it isn't being maintained anymore. And although XCCDF itself is also a quite old standard, it is still being looked into (a draft new version is being prepared) and is actively used as a standard. Red Hat is investing time and resources into secure configurations and compliancy aligned with SCAP, and other vendors publish SCAP-specific resources as well. CCE however would be a list, and thus requires continuous management. That RHELv5 is the most recent RHEL CCE list is a bad thing.&lt;/p&gt;
&lt;p&gt;Second, CCE's structure is for me insufficient to use in configuration baselines. XCCDF has a much more mature and elaborate set of settings for this. What CCE does is actually what I use in the above example as the organization-specific identifier.&lt;/p&gt;
&lt;p&gt;Finally, there aren't many tools that actively use CCE, unlike CVSS, XCCDF, OVAL, CVSS and other standards under the SCAP umbrella, which are all still actively used and developed upon by tools such as Open-SCAP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Profiling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before finishing this post, I want to talk about profiling.&lt;/p&gt;
&lt;p&gt;Within an XCCDF benchmark, several profiles can be defined. In the XCCDF template I defined a single profile that covers all rules, but this can be fine-tuned to the needs of the organization. In XCCDF profiles, you can select individual rules (which ones are active for a profile and which ones aren't) and even fine-tune values for rules. This is called tailoring in XCCDF.&lt;/p&gt;
&lt;p&gt;A first use case for profiles is to group different rules based on the selected setup. In case of Nginx for instance, one can consider Nginx being used as either a reverse proxy, a static website hosting or a dynamic web application hosting. In all three cases, some rules will be the same, but several rules will be different. Within XCCDF, you can document all rules, and then use profiles to group the rules related to a particular service use.&lt;/p&gt;
&lt;p&gt;XCCDF allows for profile inheritance. This means that you can define a base Profile (all the rules that need to be applied, regardless of the service use) and then extend the profiles with individual rule selections.&lt;/p&gt;
&lt;p&gt;With profiles, you can also fine-tune values. For instance, you could have a password policy in place that states that passwords on internal machines have to be at least 10 characters long, but on DMZ systems they need to be at least 15 characters long. Instead of defining two rules, the rule could refer to a particular variable (Value in XCCDF) which is then selected based on the Profile. The value for a password length is then by default 10, but the Profile for DMZ systems selects the other value (15).&lt;/p&gt;
&lt;p&gt;Now, value-based tailoring is imo already a more advanced use of XCCDF, and is best looked into when you also start using OVAL or other automated checks. The tailoring information is then passed on to the automated compliance check so that the right value is validated.&lt;/p&gt;
&lt;p&gt;Value-based tailoring also makes rules either more complex to write, or ambiguous to interpret without full profile awareness. Considering the password length requirement, the rule could become:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The /etc/pam.d/password-auth file must refer to pam_passwdqc.so for the password service with a minimal password length of 10 (default) or 15 (DMZ) for the N4 password category&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At least the rule is specific. Another approach would be to document it as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The /etc/pam.d/password-auth file must refer to pam_passwdqc.so with the proper organizational password controls&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The documentation of the rule might document the proper controls further, but the rule is much less specific. Later checks might report that a system fails this check, referring to the title, which is insufficient for engineers or administrators to resolve.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generating the guide&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To close off this post, let's finish with how to generate the guide based on an XCCDF document. Personally, I use two approaches for this.&lt;/p&gt;
&lt;p&gt;The first one is to rely on Open-SCAP. With Open-SCAP, you can generate guides easily:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ oscap xccdf generate guide xccdf.xml &amp;gt; ConfigBaseline.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The second one, which I use more often, is a custom XSL style sheet, which also introduces the knowledge and interpretations of what this blog post series brings up (including the organizational identification). The end result is similar (the same content) but uses a structure/organization that is more in line with expectations.&lt;/p&gt;
&lt;p&gt;For instance, in my company, the information security officers want to have a tabular overview of all the rules in a configuration baseline. So the XSL style sheet generates such a tabular overview, and uses in-documenting linking to the more elaborate descriptions of all the rules.&lt;/p&gt;
&lt;p&gt;An &lt;a href="https://blog.siphos.be/static/2018/xccdf.xsl"&gt;older version&lt;/a&gt; is online for those interested. It uses JavaScript as well (in case you are security sensitive you might want to look into it) to allow collapsing rule documentation for faster online viewing.&lt;/p&gt;
&lt;p&gt;The custom XSL has an additional advantage, namely that there is no dependency on Open-SCAP to generate the guides (even though it is perfectly possible to copy the XSL and continue). I can successfully generate the guide using &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=21714"&gt;Microsoft's msxml&lt;/a&gt; utility, using xsltproc, etc depending on the platform I'm on.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Structuring a configuration baseline</title><link href="https://blog.siphos.be/2018/01/structuring-a-configuration-baseline/" rel="alternate"></link><published>2018-01-17T09:10:00+01:00</published><updated>2018-01-17T09:10:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-17:/2018/01/structuring-a-configuration-baseline/</id><summary type="html">&lt;p&gt;A good configuration baseline has a readable structure that allows all stakeholders to quickly see if the baseline is complete, as well as find a particular setting regardless of the technology. In this blog post, I'll cover a possible structure of the baseline which attempts to be sufficiently complete and technology agnostic.&lt;/p&gt;
&lt;p&gt;If you haven't read the blog post on &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;documenting configuration changes&lt;/a&gt;, it might be a good idea to do so as it declares the scope of configuration baselines and why I think XCCDF is a good match for this.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A good configuration baseline has a readable structure that allows all stakeholders to quickly see if the baseline is complete, as well as find a particular setting regardless of the technology. In this blog post, I'll cover a possible structure of the baseline which attempts to be sufficiently complete and technology agnostic.&lt;/p&gt;
&lt;p&gt;If you haven't read the blog post on &lt;a href="https://blog.siphos.be/2018/01/documenting-configuration-changes/"&gt;documenting configuration changes&lt;/a&gt;, it might be a good idea to do so as it declares the scope of configuration baselines and why I think XCCDF is a good match for this.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Chaptered documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As mentioned previously, a configuration baseline describes the configuration of a particular technological service (rather than a business service which is an integrated set of technologies and applications). To document and maintain the configuration state of the technology, I suggest the following eight chapters (to begin with):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Operating system and services&lt;/li&gt;
&lt;li&gt;Software deployment and file system&lt;/li&gt;
&lt;li&gt;Technical service settings&lt;/li&gt;
&lt;li&gt;Authentication, authorization, access control and auditing&lt;/li&gt;
&lt;li&gt;Service specific settings&lt;/li&gt;
&lt;li&gt;Cryptographic services&lt;/li&gt;
&lt;li&gt;Data and information handling&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Within each chapter, sections can be declared depending on how the technology works. For instance, for database technologies one can have a distinction between system-wide settings, instance-specific settings and even database-specific settings. Or, if the organization has specific standards on user definitions, a chapter on "User settings" can be used. The above is just a suggestion in an attempt to cover most aspects of a configuration baseline.&lt;/p&gt;
&lt;p&gt;With the sections of the chapter, rules are then defined which specify the actual configuration setting (or valid range) applicable to the technology. But the rule goes further than just a single-line configuration setting description.&lt;/p&gt;
&lt;p&gt;Each rule should have a &lt;em&gt;unique identifier&lt;/em&gt; so that other documents can reliably link to the rules in the document. Although XCCDF has a convention for this, I feel that the XCCDF way here is more useful for technical referencing while the organization is better off with a more human addressable approach. So while a rule in XCCDF has the identifier &lt;code&gt;xccdf_com.example.postgresql_rule_selinux-enforcing&lt;/code&gt; the human addressable identifier would be &lt;code&gt;postgresql_selinux-enforcing&lt;/code&gt; or even &lt;code&gt;postgresql-00001&lt;/code&gt;. In the company that I work for, we already have a taxonomy for services and a decision to use numerical identifiers on the configuration baseline rules.&lt;/p&gt;
&lt;p&gt;Each rule should be properly described, documenting what the rule is for. In case of a ranged value, it should also document how this range can be properly applied. For instance, if the number of worker threads is based on the number of cores available in the system, document the formula.&lt;/p&gt;
&lt;p&gt;Each rule should also document the risk that it wants to mitigate (be it a security risk, or a manageability aspect of the service, or a performance related tuning parameter). This aspect of the baseline is important whenever an implementation wants an exception to the rule (not follow it) or a deviation (different value). Personally, to make sure that the baseline is manageable, I don't expect engineers to immediately fill in the risk in great detail, but rather holistically. The actual risk determination is then only done when an implementation wants an exception or deviation, and then includes a list of potential mitigating actions to take. This way, a 300+ rule document does not require all 300+ rules to have a risk determination, especially if only a dozen or so rules have exceptions or deviations in the organization.&lt;/p&gt;
&lt;p&gt;Each rule should have sources linked to it. These sources help the reader understand what the rule is based on, such as a publicly available secure configuration baseline, an audit recommendation, a specific incident, etc. If the rule is also controversial, it might benefit from links to meeting minutes.&lt;/p&gt;
&lt;p&gt;Each rule might have consequences listed as well. These are known changes or behavior aspects that follow the implementation of the rule. For instance, a rule might state that TLS mutual authentication is mandatory, and the consequence is that all interacting clients must have a properly defined certificate (so proper PKI requirements) as well as client registration in the application.&lt;/p&gt;
&lt;p&gt;Finally, and importantly as well, each rule identifies the scope at which exceptions or deviations can be granted. For smaller groups and organizations, this might not matter that much, but for larger organizations, some configuration baseline rules can be "approved" by a small team or by the application owner, while others need formal advise of a security officer and approval on a decision body.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding a balanced approval hierarchy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The exception management for configuration baselines should not be underestimated. It is not viable to have all settings handled by top management decision bodies, but some configuration changes might result in such a huge impact that a formal decision needs to be taken somewhere, with proper accountability assigned (yes, this is the architect in me speaking).&lt;/p&gt;
&lt;p&gt;Rather than attempting to create a match for all rules, I again like to keep the decision here in the middle, just like I do with the risk determination. The maintainer of the configuration baseline can leave the "scope" of a rule open, and have an intermediate decision body as the main decision body. Whenever an exception or deviation is asked, the risk determination is made and filled in, and with this documented rule now complete a waiver is asked on the decision body. Together with the waiver request, the maintainer also asks this decision body if the rule in the future also needs to be granted on that decision body or elsewhere.&lt;/p&gt;
&lt;p&gt;The scope is most likely tied to the impact of the rule towards other services. A performance specific rule that only affects the application hosted on the technology can be easily scoped as being application-only. This means that the application or service owner can decide to deviate from the baseline. A waiver for a rule that influences system behavior might need to be granted by the system administrator (or team) as well as application or service owners that use this system. Following this logic, I generally use the following scope terminology:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tbd (to be determined), meaning that there is no assessment done yet&lt;/li&gt;
&lt;li&gt;application, meaning that the impact is only on a single application and thus can be taken by the application owner&lt;/li&gt;
&lt;li&gt;instance, meaning that the impact is on an instance and thus might be broader than a single application, but is otherwise contained to the technology. Waivers are granted by the responsible system administrator and application owner(s)&lt;/li&gt;
&lt;li&gt;system, meaning that the impact is on the entire system and thus goes beyond the technology. Waivers are granted by the responsible system administrator, application owner(s) and with advise from a security officer&lt;/li&gt;
&lt;li&gt;network, meaning that the impact can spread to other systems or influence behavior of other systems, but remains technical in nature. Waivers are granted by an infrastructure architecture board with advise from a security officer&lt;/li&gt;
&lt;li&gt;organization, meaning that the impact goes beyond technical influence but also impacts business processes. Waivers are granted by an architecture board with advise from a security officer and senior service owner, and might even be redirected to a higher management board.&lt;/li&gt;
&lt;li&gt;group, meaning that the impact influences multiple businesses. Waivers are granted by a specific management board&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each scope also has a "-pending" value, so "network-pending". This means that the owner of the configuration baseline suggests that this is the scope on which waivers can be established, but still needs to receive formal validation.&lt;/p&gt;
&lt;p&gt;The main decision body is then a particular infrastructure architecture board, which will redirect requests to other decision bodies if the scope goes beyond what that architecture board handles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Architectural settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first chapter in a baseline is perhaps the more controversial one, as it is not a technical setting and hard to validate. However, in my experience, tying architectural constraints in a configuration baseline is much more efficient than having a separate track for a number of reasons.&lt;/p&gt;
&lt;p&gt;For one, I strongly believe that architecture deviations are like configuration deviations. They should be documented similarly, and follow the same path as configuration baseline deviations. The scope off architectural rules are also all over the place, from application-level impact up to organization-wide.&lt;/p&gt;
&lt;p&gt;Furthermore, architectural positioning of services should not be solely an (infrastructure) architecture concern, but supported by the other stakeholders as well, and especially the responsible for the technology service.&lt;/p&gt;
&lt;p&gt;For instance, a rule could be that no databases should be positioned within an organizations &lt;em&gt;DeMilitarized Zone (DMZ)&lt;/em&gt;, which is a network design that shields off internally positioned services from the outside world. Although this is not a configuration setting, it makes sense to place it in the configuration baseline of the database technology. There are several ways to validate automatically if this rule is followed, depending for instance the organization IP plan.&lt;/p&gt;
&lt;p&gt;Another rule could be that web applications that host browser-based applications should only be linked through a reverse proxy, or that a load balancer must be put in front of an application server, etc. This might result in additional rules in the chapter that covers access control as well (such as having a particular IP filter in place), but these rules are the consequence of the architectural positioning of the service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operating system and services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The second chapter covers settings specific to the operating system on which the technology is deployed. Such settings can be system-wide settings like Linux' sysctl parameters, services which need to be enabled or disabled when the technology is deployed, and deviations from the configuration baseline of the operating system.&lt;/p&gt;
&lt;p&gt;An example of the latter depends of course on the configuration baseline of the operating system (assuming this is a baseline for a technology deployed on top of an operating system, it could very well be a different platform). Suppose for instance that the baseline has the &lt;code&gt;squashfs&lt;/code&gt; kernel module disabled, but the technology itself requires squashfs, then a waiver is needed. This is the level where this is documented.&lt;/p&gt;
&lt;p&gt;Another setting could be an extension of the SSH configuration (the term "services" in the chapter title here focuses on system services, such as OpenSSH), or the implementation of additional audit rules on OS-level (although auditing can also be covered in a different section).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software deployment and file system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The third chapter focuses on the installation of the technology itself, and the file system requirements related to the technology service.&lt;/p&gt;
&lt;p&gt;Rules here look into file ownership and permissions, mount settings, and file system declarations. Some baselines might even define rules about integrity of certain files (the &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt; supports checksum-based validations) although I think this is better tackled through a specific integrity process. Still, if such an integrity process does not exist and automated validation of baselines is implemented, then integrity validation of critical files could be in scope.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technical service settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the fourth chapter, settings are declared regarding the service without being service-specific. A service-specific setting is one that requires functional knowledge of the service, whereas technical service settings can be interpreted without functionally understanding the technology at hand.&lt;/p&gt;
&lt;p&gt;Let's take PostgreSQL as an example. A service-specific setting would be the maximum number of non-frozen transaction IDs before a VACUUM operation is triggered (the &lt;code&gt;autovacuum_freeze_max_age&lt;/code&gt; parameter). If you are not working with PostgreSQL much, then this makes as much sense as &lt;a href="https://en.wikipedia.org/wiki/Prisencolinensinainciusol"&gt;Prisencolinensinainciusol&lt;/a&gt;. It sounds like English, but that's about as far as you get.&lt;/p&gt;
&lt;p&gt;A technical service setting on PostgreSQL that is likely more understandable is the runtime account under which the database runs (you don't want it to run as root), or the TCP port on which it listens. Although both are technical in nature, they're much more understandable for others and, perhaps the most important reason of all, often more reusable in deployments across technologies.&lt;/p&gt;
&lt;p&gt;This reusability is key for larger organizations as they will have numerous technologies to support, and the technical service settings offer a good baseline for initial secure setup. They focus on the runtime account of the service, the privileges of the runtime account (be it capability-based on Linux or account rights on Windows), the interfaces on which the service is reachable, the protocol or protocols it supports, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authentication, authorization, access control and auditing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next chapter focuses on the &lt;em&gt;Authentication, Authorization and Accounting (AAA)&lt;/em&gt; services, but slightly worded differently (AAA is commonly used in networking related setups, I just borrow it and extend it). If the configuration baseline is extensive, then it might make sense to have separate sections for each of these security concepts.&lt;/p&gt;
&lt;p&gt;Some technologies have a strong focus on user management as well. In that case, it might make sense to first describe the various types of users that the technology supports (like regular users, machine users, internal service users, shared users, etc.) and then, per user type, document how these security services act on it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service specific settings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next chapter covers settings that are very specific to the service. These are often the settings that are found in the best practices documentation, secure deployment instructions of the vendor, performance tuning parameters, etc.&lt;/p&gt;
&lt;p&gt;I tend to look first at the base configuration and administration guides for technologies, and see what the main structure is that those documents follow. Often, this can be borrowed for the configuration baseline. Next, consider performance related tuning, as that is often service specific and not related to the other chapters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cryptographic services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this chapter, the focus is on the cryptographic services and configuration.&lt;/p&gt;
&lt;p&gt;The most well-known example here is related to any TLS configuration and tuning. Whereas the location of the private key (used for TLS services) is generally mentioned in the third chapter (or at least the secure storage of the private key), this section will focus on using this properly. It looks at selecting proper TLS version, making a decent and manageable set of ciphers to support, enabling &lt;em&gt;Online Certificate Status Protocol (OCSP)&lt;/em&gt; on web servers, etc.&lt;/p&gt;
&lt;p&gt;But services often use cryptographic related algorithms in various other places as well. Databases can provide transparent data file encryption to ensure that offline access to the database files does not result in data leakage for instance. Or they implement column-level encryption.&lt;/p&gt;
&lt;p&gt;Application servers might support crypto related routines to the applications they host, and the configuration baseline can then identify which crypto modules are supported and which ones aren't.&lt;/p&gt;
&lt;p&gt;Services might be using cryptographic hashes which are configurable, or could be storing user passwords in a database using configurable settings. OpenLDAP for instance supports multiple hashing methods (and also supports storing in plain-text if you want this), so it makes sense to select a hashing method that is hard to brute-force (slow to compute for instance) and is salted (to make certain types of attacks more challenging).&lt;/p&gt;
&lt;p&gt;If the service makes use of stored credentials or keytabs, document how they are protected here as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data and information handling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Information handling covers both the regular data management activities (like backup/restore, data retention, archival, etc.) as well as sensitive information handling (to comply with privacy rules).&lt;/p&gt;
&lt;p&gt;The regular data management related settings look into both the end user data handling (as far as this is infrastructurally related - this isn't meant to become a secure development guide) as well as service-internal data handling. When the technology is meant to handle data (like a database or LDAP) then certain related settings could be both in the service specific settings chapter or in this one. Personally, I tend to prefer that technology-specific and non-reusable settings are in the former, while the data and information handling chapter covers the integration and technology-agnostic data handling.&lt;/p&gt;
&lt;p&gt;If the service handles sensitive information, it is very likely that additional constraints or requirements were put in place beyond the "traditional" cryptographic requirements. Although such requirements are often implemented on the application level (like tagging the data properly and then, based on the tags, handle specific fine-grained access controls, archival and data retention), more and more technologies provide out-of-the-box (or at least reusable) methods that can be configured.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An XCCDF template&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To support the above structure, I've made an &lt;a href="https://blog.siphos.be/static/2018/xccdf-template.xml"&gt;XCCDF template&lt;/a&gt; that might be a good start for documenting the configuration baseline of a technology. It also structures the chapters a bit more with various sections, but those are definitely not mandatory to use as it strongly depends on the technology being documented, the maturity of the organization, etc.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>Documenting configuration changes</title><link href="https://blog.siphos.be/2018/01/documenting-configuration-changes/" rel="alternate"></link><published>2018-01-07T21:20:00+01:00</published><updated>2018-01-07T21:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2018-01-07:/2018/01/documenting-configuration-changes/</id><summary type="html">&lt;p&gt;IT teams are continuously under pressure to set up and maintain infrastructure services quickly, efficiently and securely. As an infrastructure architect, my main concerns are related to the manageability of these services and the secure setup. And within those realms, a properly documented configuration setup is in my opinion very crucial.&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm going to look into using the &lt;em&gt;Extensible Configuration Checklist Description Format (XCCDF)&lt;/em&gt; as the way to document these. This first post is an introduction to XCCDF functionally, and what I position it for.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT teams are continuously under pressure to set up and maintain infrastructure services quickly, efficiently and securely. As an infrastructure architect, my main concerns are related to the manageability of these services and the secure setup. And within those realms, a properly documented configuration setup is in my opinion very crucial.&lt;/p&gt;
&lt;p&gt;In this blog post series, I'm going to look into using the &lt;em&gt;Extensible Configuration Checklist Description Format (XCCDF)&lt;/em&gt; as the way to document these. This first post is an introduction to XCCDF functionally, and what I position it for.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Documentation is a good thing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the ongoing struggle for time and resources, documenting configurations and architectures is often not top-of-mind. However, the lack of this information also leads to various problems: incidents due to misconfiguration, slow recovery timings due to incomprehensible setups, and not to forget: meetings. Yes, meetings, which are continuously discussing service aspects that influence one or more parameters, without any good traceability of past decisions.&lt;/p&gt;
&lt;p&gt;Some technologies allow to keep track of some metadata regarding to configurations. In configuration management tools like &lt;a href="https://puppet.com"&gt;Puppet&lt;/a&gt; or &lt;a href="https://saltstack.com"&gt;Saltstack&lt;/a&gt; engineers define the target state of their infrastructure, and the configuration management tool enforces this state on the service. Engineers can add in historical information as comments into these systems, and use version control on the files to have traceability of the settings.&lt;/p&gt;
&lt;p&gt;However, although in-line comments are very important, even for configuration sets, it is not a full documentation approach. In larger environments, where you are regularly audited for quality and security, or where multiple roles and stakeholders need to understand the settings and configuration of services, pointing to the code is not going to cut it.&lt;/p&gt;
&lt;p&gt;Configuration items need to be documented not solely with the documentation rule itself, but with the motivation related to it, and additional fields of interest depending on how the organization deals with it. This documentation can then be referred to from the configuration management infrastructure (so engineers and technical stakeholders can trace back settings) but also vice-versa: the documentation can refer to the configuration management implementation (so other stakeholders can deduce how the settings are implemented or even enforced).&lt;/p&gt;
&lt;p&gt;With a proper configuration document at hand, especially if it is supported through the configuration management tool(s) in the organization (regardless if it is one or multiple), it is much easier to have the necessary interviews with auditors, project leaders, functional and technical analysts, architects or even remote support teams.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Two-part documentation hierarchy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing to decide upon is at which level a team will document the settings. Is a single document possible for all infrastructure services? Most likely not. I believe that settings should be documented on the technology level (as it is specific to a particular technology) and on the 'business service' level (as it is specific to a particular implementation).&lt;/p&gt;
&lt;p&gt;On the technology level, we're talking about configuration documentation for "PostgreSQL", "Apache Knox" or "Nginx". At this level, the baseline is defined for a technology. The resulting document is then the &lt;em&gt;configuration baseline&lt;/em&gt; for that component.&lt;/p&gt;
&lt;p&gt;On the business service level, we're talking about configuration documentation for a particular service that is a combination of multiple implementations. For instance, a company intranet portal service is operationally implemented through a reverse proxy (HAProxy), an intelligent load balancer (Seesaw), next-gen firewall (pfSense), web server (Nginx), application server (Node.js), database (PostgreSQL), and operating systems (Linux). And more technologies come into play when we consider software deployment, monitoring, backup/restore, software-defined network, storage provisioning, archival solutions, license management services, etc.&lt;/p&gt;
&lt;p&gt;Hence, a configuration document should be available on this service level ("company intranet portal") which defines the usage profile of a service (more about that later) and the specific parameters related to this service, but only when they either deviate from the configuration baseline, or take a particular value within a range defined in the configuration baseline. This document is the &lt;em&gt;service technical configuration&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, as an example, on the Nginx configuration baseline, a rule might state that the maximum file size per upload is 12M (through the &lt;code&gt;client_max_body_size&lt;/code&gt; parameter). If the service has no problem with this rule, then it does not need to be documented on the service technical configuration. However, if this needs to be adapted (say that for the company portal the maximum file size is 64M) then it is documented.&lt;/p&gt;
&lt;p&gt;Another example is a ranged setting, where the baseline identifies a set of valid values and the service technical configuration makes a particular selection. For instance, the Nginx configuration baseline might mention that there must be between 5 and 50 worker processes (through the &lt;code&gt;worker_processes&lt;/code&gt; parameter). In the service technical configuration the particular value is then selected.&lt;/p&gt;
&lt;p&gt;From an architecture and security point of view, the first example is a deviation which must consider the risks and consequences that are applicable to the rule. These are (or should be) documented in the configuration baseline, including where this deviation can be approved (assuming the organization has a decision body for such things). The second example is not a deviation and, as such, is free to be chosen by the implementation team. The configuration baseline will generally inform the implementation teams about how to pick a proper value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service usage profiles&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've talked about a &lt;em&gt;service usage profile&lt;/em&gt; earlier up, but didn't expand on it yet. So, what are these service usage profiles?&lt;/p&gt;
&lt;p&gt;Well, most technologies can be implemented for a number of targets and functional purposes. A database could be implemented as a dedicated service (one set of databases on a dedicated set of instances for a single business service) or a shared service (multiple databases, possibly on multiple instances for several business services). It can be tuned for online transactional purposes (OLTP) or online analytical processing (OLAP), often through data warehouse designs.&lt;/p&gt;
&lt;p&gt;A service usage profile is part of the configuration baseline, with settings specific to that particular usage. So for a database the engineering team responsible for the database technology setup might devise that the following usage profiles are applicable to their component:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dedicated OLTP&lt;/li&gt;
&lt;li&gt;Shared OLTP&lt;/li&gt;
&lt;li&gt;Dedicated DWH&lt;/li&gt;
&lt;li&gt;Shared DWH&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each usage profile has a number of configuration settings (of which many, if not most, are shared across other usage profiles) and a range of valid values (fine-tuning for a service). The service technical configuration for a particular business service then selects the particular usage profile. For instance, the company intranet portal might use a Dedicated OLTP usage profile for its PostgreSQL database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How XCCDF supports this structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Until now, I've only spoken about the values related to configuration documentation, and a high-level introduction to the hierarchy on the configurations. But how does the &lt;a href="https://scap.nist.gov/specifications/xccdf/"&gt;Extensible Configuration Checklist Description&lt;/a&gt; position itself in this?&lt;/p&gt;
&lt;p&gt;A number of reasons why XCCDF is a valid choice for configuration documentation are given next.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XCCDF allows technical writers to write the documentation in (basic) HTML while still linking the documentation to specific rules. Rather than having to use a tabular expression on all the valid configuration sets (like using a large spreadsheet table for all rules) and trying to force some documentation in it (Excel is not a text editor), XCCDF uses a hierarchical approach to structure documentation in logical sections (which it calls &lt;em&gt;Groups&lt;/em&gt;) and then refers to the rules applicable to that section (using the &lt;em&gt;Rule&lt;/em&gt; identifier).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF has out-of-the-box support for service profiles (through &lt;em&gt;Profile&lt;/em&gt; declarations). Fine-tuning and selecting profiles is called &lt;em&gt;tailoring&lt;/em&gt; in XCCDF. This also includes support for ranged values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is meant to (but does not have to) refer to the (automated or interview-based) validation of the rules as well. Automated validation of settings means that an engine can read the XCCDF document (and the referred statements) and check if an implementation adheres to the baseline. The standard for this is called &lt;em&gt;Open Vulnerability and Assessment Language (OVAL)&lt;/em&gt;, and a popular free software engine for this is &lt;a href="https://www.open-scap.org"&gt;OpenSCAP&lt;/a&gt;. The standard for interview-based validation is &lt;em&gt;Open Checklist Interactive Language (OCIL)&lt;/em&gt;. I have not played around with OCIL and supporting tooling, so comments on this are always welcome.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is an XML-based format, so its "source code" can easily be versioned in common version control systems like Git. This allows organizations to not only track changes on the documentation, but also have an active development lifecycle management on the configuration documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDFs schema implies a set of metadata to be defined during various declarations. It includes support for the &lt;a href="http://www.dublincore.org/specifications/"&gt;Dublin core metadata&lt;/a&gt; terms for content, references to link other resources structurally, and most importantly has a wide set of supporting entities for rules (which is the level on which configuration items are documented). This includes the rationale (why is the rule defined as is), fix text (human readable), fix (machine readable), rule role (is it security-sensitive and as such must be taken up in a security assessment report or not), severity (how bad is it if this rule is not followed), and many more. This both forces the user to consider the consequences of the rule, as well as guide the writer into properly structured documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XCCDF also suggests a number of classes for the documentation to standardize certain information types. This includes warnings, critical text, examples, and instructions. Such semantic declarations allow for a more uniform set of documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, a few constraints exist that you need to be aware of when approaching XCCDF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XCCDF is an XML-based document format, and although NIST offers the necessary XML Schema definitions, writing proper XML has always been a challenge for many people. Also, no decent GUI or WYSIWYG tool that manages XCCDF files exists in my opinion. Yes, we have the &lt;a href="https://www.open-scap.org/tools/scap-workbench/"&gt;SCAP Workbench&lt;/a&gt; and the &lt;a href="https://www.g2-inc.com/scap.html"&gt;eSCAPe editor&lt;/a&gt;, but I feel that they are not as effective as they should be. As a result, the team or teams that write the baselines should either be XML-savvy, or you need to provide supporting infrastructure and services for it. However, YMMV.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the organization is not interested in compliance checks themselves (i.e. automated validation of adherence to the configuration baseline and service technical configuration) then XCCDF will entail too much overhead versus just having a template or approach (such as documenting items in a wiki). However, with some support (and perhaps automation) writing and maintaining XCCDF based configuration baselines becomes much easier.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past I've &lt;a href="http://blog.siphos.be/tag/xccdf/"&gt;blogged about XCCDF&lt;/a&gt; already, but that was with a previous blog technology and the migration wasn't as successful as I originally thought. XML snippets were all removed, and I'm too lazy to go back to my backups from 2013 and individually correct blogs.&lt;/p&gt;
&lt;p&gt;A good resource on XCCDF is the &lt;a href="https://csrc.nist.gov/CSRC/media/Publications/nistir/7275/rev-4/final/documents/nistir-7275r4_updated-march-2012_clean.pdf"&gt;NIST IR-7275 publication (PDF)&lt;/a&gt; which covers the XCCDF standard in much detail.&lt;/p&gt;
&lt;p&gt;The Center for Internet Security (CISecurity) maintains more than a hundred &lt;a href="https://www.cisecurity.org/cis-benchmarks/"&gt;CIS Benchmarks&lt;/a&gt;, all available for free as PDFs, and are often based on XCCDF (available to subscribed members).&lt;/p&gt;
&lt;p&gt;In the next blog post, I'll talk about the in-document structure of a good configuration baseline.&lt;/p&gt;</content><category term="Security"></category><category term="xccdf"></category><category term="scap"></category><category term="baseline"></category></entry><entry><title>SELinux and extended permissions</title><link href="https://blog.siphos.be/2017/11/selinux-and-extended-permissions/" rel="alternate"></link><published>2017-11-20T17:00:00+01:00</published><updated>2017-11-20T17:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-11-20:/2017/11/selinux-and-extended-permissions/</id><summary type="html">&lt;p&gt;One of the features present in the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;August release&lt;/a&gt; of the SELinux user space is its support for ioctl xperm rules in modular policies. In the past, this was only possible in monolithic ones (and CIL). Through this, allow rules can be extended to not only cover source (domain) and target (resource) identifiers, but also a specific number on which it applies. And ioctl's are the first (and currently only) permission on which this is implemented.&lt;/p&gt;
&lt;p&gt;Note that ioctl-level permission controls isn't a new feature by itself, but the fact that it can be used in modular policies is.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;One of the features present in the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;August release&lt;/a&gt; of the SELinux user space is its support for ioctl xperm rules in modular policies. In the past, this was only possible in monolithic ones (and CIL). Through this, allow rules can be extended to not only cover source (domain) and target (resource) identifiers, but also a specific number on which it applies. And ioctl's are the first (and currently only) permission on which this is implemented.&lt;/p&gt;
&lt;p&gt;Note that ioctl-level permission controls isn't a new feature by itself, but the fact that it can be used in modular policies is.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is ioctl?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many interactions on a Linux system are done through system calls. From a security perspective, most system calls can be properly categorized based on who is executing the call and what the target of the call is. For instance, the unlink() system call has the following prototype:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;int unlink(const char *pathname);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Considering that a process (source) is executing unlink (system call) against a target (path) is sufficient for most security implementations. Either the source has the permission to unlink that file or directory, or it hasn't. SELinux maps this to the unlink permission within the file or directory classes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : { file dir }  unlink;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, &lt;code&gt;ioctl()&lt;/code&gt; is somewhat different. It is a system call that allows device-specific operations which cannot be expressed by regular system calls. Devices can have multiple functions/capabilities, and with &lt;code&gt;ioctl()&lt;/code&gt; these capabilities can be interrogated or updated. It has the following interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;int ioctl(int fd, unsigned long request, ...);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The file descriptor is the target device on which an operation is launched. The second argument is the request, which is an integer whose value identifiers what kind of operation the &lt;code&gt;ioctl()&lt;/code&gt; call is trying to execute. So unlike regular system calls, where the operation itself is the system call, &lt;code&gt;ioctl()&lt;/code&gt; actually has a parameter that identifies this.&lt;/p&gt;
&lt;p&gt;A list of possible parameter values on a socket for instance is available in the Linux kernel source code, under &lt;a href="https://elixir.free-electrons.com/linux/latest/source/include/uapi/linux/sockios.h"&gt;include/uapi/linnux/sockios.h&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SELinux allowxperm&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For SELinux, having the purpose of the call as part of a parameter means that a regular mapping isn't sufficient. Allowing &lt;code&gt;ioctl()&lt;/code&gt; commands for a domain against a resource is expressed as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This of course does not allow policy developers to differentiate between harmless or informative calls (like SIOCGIFHWADDR to obtain the hardware address associated with a network device) and impactful calls (like SIOCADDRT to add a routing table entry).&lt;/p&gt;
&lt;p&gt;To allow for a fine-grained policy approach, the SELinux developers introduced an extended allow permission, which is capable of differentiating based on an integer value.&lt;/p&gt;
&lt;p&gt;For instance, to allow a domain to get a hardware address (SIOCGIFHWADDR, which is 0x8927) from a TCP socket:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : tcp_socket ioctl 0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This additional parameter can also be ranged:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl 0x8910-0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And of course, it can also be used to complement (i.e. allow all ioctl parameters except a certain value):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allowxperm &amp;lt;domain&amp;gt; &amp;lt;resource&amp;gt; : &amp;lt;class&amp;gt; ioctl ~0x8927;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Small or negligible performance hit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;According to a &lt;a href="http://kernsec.org/files/lss2015/vanderstoep.pdf"&gt;presentation given by Jeff Vander Stoep&lt;/a&gt; on the Linux Security Summit in 2015, the performance impact of this addition in SELinux is well under control, which helped in the introduction of this capability in the Android SELinux implementation.&lt;/p&gt;
&lt;p&gt;As a result, interested readers can find examples of allowxperm invocations in the SELinux policy in Android, such as in the &lt;a href="https://android.googlesource.com/platform/system/sepolicy/+/master/private/app.te"&gt;app.te&lt;/a&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# only allow unprivileged socket ioctl commands
allowxperm { appdomain -bluetooth } self:{ rawip_socket tcp_socket udp_socket } ioctl { unpriv_sock_ioctls unpriv_tty_ioctls };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with that, we again show how fine-grained the SELinux access controls can be.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="ioctl"></category></entry><entry><title>SELinux Userspace 2.7</title><link href="https://blog.siphos.be/2017/09/selinux-userspace-2.7/" rel="alternate"></link><published>2017-09-26T14:50:00+02:00</published><updated>2017-09-26T14:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-09-26:/2017/09/selinux-userspace-2.7/</id><summary type="html">&lt;p&gt;A few days ago, &lt;a href="http://blog.perfinion.com/"&gt;Jason "perfinion" Zaman&lt;/a&gt; stabilized the 2.7 SELinux userspace on
Gentoo. This release has quite a &lt;a href="https://raw.githubusercontent.com/wiki/SELinuxProject/selinux/files/releases/20170804/RELEASE-20170804.txt"&gt;few new features&lt;/a&gt;, which I'll cover in later
posts, but for distribution packagers the main change is that the userspace
now has many more components to package. The project has split up the
policycoreutils package in separate packages so that deployments can be made
more specific.&lt;/p&gt;
&lt;p&gt;Let's take a look at all the various userspace packages again, learn what their
purpose is, so that you can decide if they're needed or not on a system. Also,
when I cover the contents of a package, be aware that it is based on the deployment
on my system, which might or might not be a complete installation (as with Gentoo,
different USE flags can trigger different package deployments).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago, &lt;a href="http://blog.perfinion.com/"&gt;Jason "perfinion" Zaman&lt;/a&gt; stabilized the 2.7 SELinux userspace on
Gentoo. This release has quite a &lt;a href="https://raw.githubusercontent.com/wiki/SELinuxProject/selinux/files/releases/20170804/RELEASE-20170804.txt"&gt;few new features&lt;/a&gt;, which I'll cover in later
posts, but for distribution packagers the main change is that the userspace
now has many more components to package. The project has split up the
policycoreutils package in separate packages so that deployments can be made
more specific.&lt;/p&gt;
&lt;p&gt;Let's take a look at all the various userspace packages again, learn what their
purpose is, so that you can decide if they're needed or not on a system. Also,
when I cover the contents of a package, be aware that it is based on the deployment
on my system, which might or might not be a complete installation (as with Gentoo,
different USE flags can trigger different package deployments).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;libsepol - manipulating SELinux binary policies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first package, known in Gentoo as &lt;code&gt;sys-libs/libsepol&lt;/code&gt;, is the library that
enables manipulating the SELinux binary policies. This is a core library, and is
the first SELinux userspace package that is installed on a system.&lt;/p&gt;
&lt;p&gt;It contains one command, &lt;code&gt;chkcon&lt;/code&gt;, which allows users to validate if a specific
security context exists within a binary policy file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ chkcon policy.29 user_u:user_r:mozilla_t:s0
user_u:user_r:mozilla_t:s0 is valid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The package does contain two manpages of old commands which are no longer available
(or I'm blind, either way, they're not installed and not found in the SELinux userspace
repository either) such as genpolusers and genpolbools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;libselinux - the main SELinux handling library&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The libselinux library, known in Gentoo as &lt;code&gt;sys-libs/libselinux&lt;/code&gt;, is the main SELinux
library. Almost all applications that are SELinux-aware (meaning they not only know SELinux
is a thing, but are actively modifying their behavior with SELinux-specific code) will
link to libselinux.&lt;/p&gt;
&lt;p&gt;Because it is so core, the package also provides the necessary bindings for different
scripting languages besides the standard shared objects approach, namely Python (as
many SELinux related tooling is written in Python) and Ruby.&lt;/p&gt;
&lt;p&gt;Next to the bindings and libraries, libselinux also offers quite a few executables
to query and manipulate SELinux settings on the system, which are shortly described
on the &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Tools"&gt;SELinux userspace wiki&lt;/a&gt; but repeated here for convenience. Most of these
are meant for debugging purposes, as they are simple wrappers toward the libselinux
provided functions, but some of them are often used by administrations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;avcstat&lt;/code&gt; gives statistics about the in-kernel access vector cache, such as number
  of lookups, hits and misses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_create&lt;/code&gt; queries the kernel security server for a transition decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_av&lt;/code&gt; queries the kernel security server for an access vector decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_relabel&lt;/code&gt; queries the kernel security server for a relabel decision&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_member&lt;/code&gt; queries the kernel security server for a labeling decision on a
  polyinstantiated object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getconlist&lt;/code&gt; uses the &lt;code&gt;security\_compute\_user()&lt;/code&gt; function, and orders the resulting
  list based on the &lt;code&gt;default\_contexts&lt;/code&gt; file and per-user context files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getdefaultcon&lt;/code&gt; is like &lt;code&gt;getconlist&lt;/code&gt; but only returns the first context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compute_user&lt;/code&gt; queries the kernel security server fo a set of reachable user contexts
  from a source context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getfilecon&lt;/code&gt; gets the context of a file by path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getpidcon&lt;/code&gt; gets the context of a process by PID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getseuser&lt;/code&gt; queries the &lt;code&gt;seuser&lt;/code&gt; file for the resulting SELinux user and contxt for a
  particular linux login and login context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getsebool&lt;/code&gt; gets the current state of a SELinux boolean in the SELinux security server&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchpathcon&lt;/code&gt; queries the active filecontext file for how a particular path should
  be labeled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;policyvers&lt;/code&gt; queries the kernel security server for the maximum policy version supported&lt;/li&gt;
&lt;li&gt;&lt;code&gt;getenforce&lt;/code&gt; gets the enforcing state of the kernel access vector cache&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sefcontext_compile&lt;/code&gt; generates binary filecontext files, optimized for fast querying&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_lookup&lt;/code&gt; looks up what the target default context is for various classes
  (supporting the X related SELinux types, database types, etc.)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_digest&lt;/code&gt; calculates the SHA1 digest of spec files, and returns a list
  of the specfiles used to calculate the digest. This is used by Android.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_partial_match&lt;/code&gt; determines if a direct or partial match is possible
  on a file path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selabel_lookup_best_match&lt;/code&gt; obtains the best matching SELinux security context
  for file-based operations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinux_check_securetty_context&lt;/code&gt; checks whether a SELinux tty security context
  is defined as a securetty context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinux_check_access&lt;/code&gt; checks if the source context has the access permission
  for the specified class on the target context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxexeccon&lt;/code&gt; reports the SELinux context for an executable&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxenabled&lt;/code&gt; returns if SELinux is enabled or not&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setfilecon&lt;/code&gt; sets the context of a path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setenforce&lt;/code&gt; sets the enforcing state of the kernel access vector cache&lt;/li&gt;
&lt;li&gt;&lt;code&gt;togglesebool&lt;/code&gt; toggles a SELinux boolean, but only runtime (so it does not
  persist across reboots)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;checkpolicy - policy compiler&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The checkpolicy package, known in Gentoo as &lt;code&gt;sys-apps/checkpolicy&lt;/code&gt;, provides two
main applications, &lt;code&gt;checkpolicy&lt;/code&gt; and &lt;code&gt;checkmodule&lt;/code&gt;. Both applications are compilers
(unlike what the name implies) which build a binary SELinux policy. The main difference
between these two is that one builds a policy binary, whereas the other one builds a 
SELinux module binary.&lt;/p&gt;
&lt;p&gt;Developers don't often call these applications themselves, but use the build scripts.
For instance, the &lt;code&gt;semodule_package&lt;/code&gt; binary would be used to combine the binary policy
with additional files such as file contexts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;libsemanage - facilitating use of SELinux overall&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The libsemanage library, known in Gentoo as &lt;code&gt;sys-libs/libsemanage&lt;/code&gt;, contains SELinux
supporting functions that are needed for any regular SELinux use. Whereas libselinux
would be used everywhere, even for embedded systems, libsemanage is generally not for
embedded systems but is very important for Linux systems in overall.&lt;/p&gt;
&lt;p&gt;Most SELinux management applications that administrators come in contact with will be
linked with the libsemanage library. As can be expected, the &lt;code&gt;semanage&lt;/code&gt; application
as offered by the &lt;code&gt;selinux-python&lt;/code&gt; package is one of them.&lt;/p&gt;
&lt;p&gt;The only application that is provided by libsemanage is the &lt;code&gt;semanage_migrate_store&lt;/code&gt;,
used to migrate the policy store from the &lt;code&gt;/etc/selinux&lt;/code&gt; to the &lt;code&gt;/var/lib/selinux&lt;/code&gt;
location. This was done with the introduction of the 2.4 userspace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-python - Python-based command-line management utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-python package, known in Gentoo as &lt;code&gt;sys-apps/selinux-python&lt;/code&gt;, is one of
the split packages that originally where part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. It
contains the majority of management utilities that administrators use for handling
SELinux on their systems.&lt;/p&gt;
&lt;p&gt;The most known application here is &lt;code&gt;semanage&lt;/code&gt;, but it contains quite a few others
as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sepolgen&lt;/code&gt; generates an initial SELinux policy module template, and is short for
  the &lt;code&gt;sepolicy generate&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;&lt;code&gt;audit2why&lt;/code&gt; translates SELinux audit messages into a description of why the access
  was denied. It is short for the &lt;code&gt;audit2allow -w&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;audit2allow&lt;/code&gt; generates SELinux policy allow/dontaudit rules from logs of denied
  operations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolgen-ifgen&lt;/code&gt; generates an overview of available interfaces. This overview is used
  by &lt;code&gt;audit2allow&lt;/code&gt; to guess the right interface to use when allowing or dontauditing certain
  operations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolicy&lt;/code&gt; is the SELinux policy inspection tool, allowing to query various aspects of
  a SELinux configuration (namely booleans, communication flows, interfaces, network information
  and transition information). It also provides the ability to generate skeleton policies (as
  described with &lt;code&gt;sepolgen&lt;/code&gt;) and manual pages.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chcat&lt;/code&gt; changes a file's SELinux security category&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sepolgen-ifgen-attr-helper&lt;/code&gt; generates an overview of attributes and attribute mappings.
  This overview is used by &lt;code&gt;audit2allow&lt;/code&gt; to guess the right attribute to use when allowing
  or dontauditing certain operations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semanage&lt;/code&gt; is a SELinux policy management tool, allowing a multitude of operations
  against the SELinux policy and the configuration. This includes definition import/export,
  login mappings, user definitions, ports and interface management, module handling, 
  file contexts, booleans and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;semodule-utils - Developing SELinux modules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The semodule-utils package, known in Gentoo as &lt;code&gt;sys-apps/semodule-utils&lt;/code&gt;, is another split package
that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. In it, SELinux policy module
development utilities are provided. The package is not needed for basic operations such
as loading and unloading modules though.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;semodule_expand&lt;/code&gt; expands a SELinux base module package into a kernel binary policy file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_deps&lt;/code&gt; shows the dependencies between SELinux policy packages&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_link&lt;/code&gt; links SELinux policy module packages together into a single SELinux policy
  module&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_unpackage&lt;/code&gt; extracts a SELinux module into the binary policy and its associated
  files (such as file context definitions)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule_package&lt;/code&gt; combines a modular binary policy file with its associated files (such
  as file context definitions) into a module package&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;mcstrans - Translate context info in human readable names&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The mcstrans package, known in Gentoo as &lt;code&gt;sys-apps/mcstrans&lt;/code&gt;, is another split package
that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. In it, the MCS translation
daemon is hosted. This daemon translates the SELinux-specific context ranges, like 
&lt;code&gt;s0-s0:c0.c1024&lt;/code&gt; to a human-readable set, like &lt;code&gt;SystemLow-SystemHigh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is a purely cosmetic approach (as SELinux internally always uses the sensitivity
and category numbers) but helps when dealing with a large number of separate categories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;restorecond - Automatically resetting file contexts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The restorecond package, known in Gentoo as &lt;code&gt;sys-apps/restorecond&lt;/code&gt;, is another split
package that originally was part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. It contains the
&lt;code&gt;restorecond&lt;/code&gt; daemon, which watches over files and directories and forces the right
SELinux label on it.&lt;/p&gt;
&lt;p&gt;This daemon was originally intended to resolve a missing feature in SELinux (having
more fine-grained rules for label naming) but with the named file transition support, the
need for this daemon has diminished a lot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;secilc - SELinux common intermediate language compiler&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The secilc package, known in Gentoo as &lt;code&gt;sys-apps/secilc&lt;/code&gt;, is the CIL compiler which
builds kernel binary policies based on the passed on CIL code. Although the majority
of policy development still uses the more traditional SELinux language (and supporting
macro's from the reference policy), developers can already use CIL code for policy generation.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;secilc&lt;/code&gt;, a final policy file can be generated through the CIL code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-dbus - SELinux DBus server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-dbus package (not packaged in Gentoo at this moment) provides a SELinux DBus
service which systems can use to query and interact with SELinux management utilities
on the system. If installed, the &lt;code&gt;org.selinux&lt;/code&gt; domain is used for various supported
operations (such as listing SELinux modules, through &lt;code&gt;org.selinux.semodule_list&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-gui - Graphical SELinux settings manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-gui package (not packaged in Gentoo at this moment) provides the
&lt;code&gt;system-config-selinux&lt;/code&gt; application which offers basic SELinux management support
in a graphical application. It supports boolean handling, file labeling, user mapping,
SELinux user management, network port definitions and module handling. As such, it can
be seen as the graphical helper utility for the &lt;code&gt;semanage&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;selinux-sandbox - Sandbox utility utilizing SELinux sandbox domains&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The selinux-sandbox package (not packaged in Gentoo at this moment) is a set of scripts
to facilitate the creation of SELinux sandboxes. With these utilities, which not only
use SELinux sandbox domains like &lt;code&gt;sandbox_t&lt;/code&gt; but also Linux namespaces, end users can
launch applications in a restricted environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;policycoreutils - Core SELinux management utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The policycoreutils package, known in Gentoo as &lt;code&gt;sys-apps/policycoreutils&lt;/code&gt;, contains 
basic SELinux tooling which is necessary to handle SELinux in a regular environment.
Supported utilities are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;newrole&lt;/code&gt; to switch a user session from one role to another&lt;/li&gt;
&lt;li&gt;&lt;code&gt;secon&lt;/code&gt; to query the SELinux context of a file, program or user input&lt;/li&gt;
&lt;li&gt;&lt;code&gt;genhomedircon&lt;/code&gt; to regenerate home directory context files, necessary when new users are
  defined on the system&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setfiles&lt;/code&gt; to set SELinux file security contexts on resources&lt;/li&gt;
&lt;li&gt;&lt;code&gt;semodule&lt;/code&gt; to list, load and unload SELinux modules&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_init&lt;/code&gt; to launch an init script in the right domain&lt;/li&gt;
&lt;li&gt;&lt;code&gt;open_init_pty&lt;/code&gt; to run a program under a pseudo terminal with the right context set&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sestatus&lt;/code&gt; to query current policy status&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setsebool&lt;/code&gt; to set and, if wanted, persist a SELinux boolean value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selinuxconfig&lt;/code&gt; to display the current active configuration paths&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restorecon&lt;/code&gt; to set SELinux file security contexts on resources&lt;/li&gt;
&lt;li&gt;&lt;code&gt;load_policy&lt;/code&gt; to load the SELinux policy, generally called from initramfs systems if the
  init system is not SELinux-aware&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restorecon_xattr&lt;/code&gt; manages the &lt;code&gt;security.restorecon_last&lt;/code&gt; extended attribute which is set
  by &lt;code&gt;setfiles&lt;/code&gt; or &lt;code&gt;restorecon&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gentoo also adds in two additional scripts:
* &lt;code&gt;rlpkg&lt;/code&gt; to reset file contexts on files provided by a Gentoo package
* &lt;code&gt;selocal&lt;/code&gt; to easily handle small SELinux rule additions to the active policy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There are even more&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attentive readers will notice that the &lt;code&gt;setools&lt;/code&gt; package is not discussed here. This package
is not provided by the SELinux userspace project, but is an important package for SELinux
policy developers as it contains the &lt;code&gt;sesearch&lt;/code&gt; command - an often used command to query
the active policy.&lt;/p&gt;
&lt;p&gt;The above list is thus a picture of the SELinux userspace utilities, which is becoming
quite a big application set now that some functionality is split off from the &lt;code&gt;policycoreutils&lt;/code&gt;
package.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Authenticating with U2F</title><link href="https://blog.siphos.be/2017/09/authenticating-with-u2f/" rel="alternate"></link><published>2017-09-11T18:25:00+02:00</published><updated>2017-09-11T18:25:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-09-11:/2017/09/authenticating-with-u2f/</id><summary type="html">&lt;p&gt;In order to further secure access to my workstation, after the &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switch to Gentoo
sources&lt;/a&gt;, I now enabled two-factor authentication through my Yubico U2F
USB device. Well, at least for local access - remote access through SSH requires
both userid/password as well as the correct SSH key, by &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining authentication
methods in OpenSSH&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enabling U2F on (Gentoo) Linux is fairly easy. The various guides online which talk
about the &lt;code&gt;pam_u2f&lt;/code&gt; setup are indeed correct that it is fairly simple. For completeness
sake, I've documented what I know on the Gentoo Wiki, as the &lt;a href="https://wiki.gentoo.org/wiki/Pam_u2f"&gt;pam_u2f article&lt;/a&gt;.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In order to further secure access to my workstation, after the &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switch to Gentoo
sources&lt;/a&gt;, I now enabled two-factor authentication through my Yubico U2F
USB device. Well, at least for local access - remote access through SSH requires
both userid/password as well as the correct SSH key, by &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining authentication
methods in OpenSSH&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enabling U2F on (Gentoo) Linux is fairly easy. The various guides online which talk
about the &lt;code&gt;pam_u2f&lt;/code&gt; setup are indeed correct that it is fairly simple. For completeness
sake, I've documented what I know on the Gentoo Wiki, as the &lt;a href="https://wiki.gentoo.org/wiki/Pam_u2f"&gt;pam_u2f article&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The setup, basically&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The setup of U2F is done in a number of steps:
1. Validate that the kernel is ready for the USB device
2. Install the PAM module and supporting tools
3. Generate the necessary data elements for each user (keys and such)
4. Configure PAM to require authentication through the U2F key&lt;/p&gt;
&lt;p&gt;For the kernel, the configuration item needed is the raw HID device support.
Now, in current kernels, two settings are available that both talk about
raw HID device support: &lt;code&gt;CONFIG_HIDRAW&lt;/code&gt; is the general raw HID device support,
while &lt;code&gt;CONFIG_USB_HIDDEV&lt;/code&gt; is the USB-specific raw HID device support.&lt;/p&gt;
&lt;p&gt;It is very well possible that only a single one is needed, but both where active
on my kernel configuration already, and Internet sources are not clear which one is
needed, so let's assume for now both are.&lt;/p&gt;
&lt;p&gt;Next, the PAM module needs to be installed. On Gentoo, this is a matter of installing
the &lt;code&gt;pam\_u2f&lt;/code&gt; package, as the necessary dependencies will be pulled in automatically:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge pam_u2f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, for each user, a registration has to be made. This registration is needed for the
U2F components to be able to correctly authenticate the use of a U2F key for a particular
user. This is done with &lt;code&gt;pamu2fcfg&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ pamu2fcfg -u&amp;lt;username&amp;gt; &amp;gt; ~/.config/Yubico/u2f_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The U2F USB key must be plugged in when the command is executed, as a succesful keypress (on the
U2F device) is needed to complete the operation.&lt;/p&gt;
&lt;p&gt;Finally, enable the use of the &lt;code&gt;pam\_u2f&lt;/code&gt; module in PAM. On my system, this is done
through the &lt;code&gt;/etc/pam.d/system-local-login&lt;/code&gt; PAM configuration file used by all
local logon services.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;auth     required     pam_u2f.so
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Consider the problems you might face&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When fiddling with PAM, it is important to keep in mind what could fail. During the setup, it
is recommended to have an open administrative session on the system so that you can validate if
the PAM configuration works, without locking yourself out of the system.&lt;/p&gt;
&lt;p&gt;But other issues need to be considered as well.&lt;/p&gt;
&lt;p&gt;My Yubico U2F USB key might have a high MTBF (Mean Time Between Failures) value, but once it fails,
it would lock me out of my workstation (and even remote services and servers that use it). For
that reason, I own a second one, safely stored, but is a valid key nonetheless for my workstation
and remote systems/services. Given the low cost of a simple U2F key, it is a simple solution for
this threat.&lt;/p&gt;
&lt;p&gt;Another issue that could come up is a malfunction in the PAM module itself. For me, this is handled
by having remote SSH access done without this PAM module (although other PAM modules are still involved,
so a generic PAM failure itself wouldn't resolve this). Of course, worst case, the system needs to be
rebooted in single user mode.&lt;/p&gt;
&lt;p&gt;One issue that I faced was the SELinux policy. Some applications that provide logon services don't have
the proper rights to handle U2F, and because PAM just works in the address space (and thus SELinux
domain) of the application, the necessary privileges need to be added to these services. My initial
investigation revealed the following necessary policy rules (refpolicy-style);&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;udev_search_pids(...)
udev_read_db(...)
dev_rw_generic_usb_dev(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first two rules are needed because the operation to trigger the USB key uses the udev tables
to find out where the key is located/attached, before it interacts with it. This interaction is then
controlled through the first rule.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simple yet effective&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Enabling the U2F authentication on the system is very simple, and gives a higher confidence that
malicious activities through regular accounts will have it somewhat more challenging to switch to
a more privileged session (one control is the SELinux policy of course, but for those domains that
are allowed to switch then the PAM-based authentication is another control), as even evesdropping on
my password (or extracting it from memory) won't suffice to perform a successful authentication.&lt;/p&gt;
&lt;p&gt;If you want to use a different two-factor authentication, check out the use of the &lt;a href="https://wiki.gentoo.org/wiki/Google_Authenticator"&gt;Google
authenticator&lt;/a&gt;, another nice article on the Gentoo wiki. It is also possible to use Yubico keys
for remote authentication, but that uses the OTP (One Time Password) functionality which isn't active
on the Yubico keys that I own.&lt;/p&gt;</content><category term="Security"></category><category term="gentoo"></category><category term="security"></category><category term="yubico"></category><category term="u2f"></category><category term="pam"></category></entry><entry><title>Using nVidia with SELinux</title><link href="https://blog.siphos.be/2017/08/using-nvidia-with-selinux/" rel="alternate"></link><published>2017-08-23T19:04:00+02:00</published><updated>2017-08-23T19:04:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-08-23:/2017/08/using-nvidia-with-selinux/</id><summary type="html">&lt;p&gt;Yesterday I've &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switched to the gentoo-sources kernel package&lt;/a&gt; on Gentoo Linux.
And with that, I also attempted (succesfully) to use the propriatary nvidia drivers
so that I can enjoy both a smoother 3D experience while playing minecraft, as well
as use the CUDA support so I don't need to use cloud-based services for small
exercises.&lt;/p&gt;
&lt;p&gt;The move to nvidia was quite simple, as the &lt;a href="https://wiki.gentoo.org/wiki/NVidia/nvidia-drivers"&gt;nvidia-drivers wiki article&lt;/a&gt; on
the Gentoo wiki was quite easy to follow.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Yesterday I've &lt;a href="http://blog.siphos.be/2017/08/switch-to-gentoo-sources/"&gt;switched to the gentoo-sources kernel package&lt;/a&gt; on Gentoo Linux.
And with that, I also attempted (succesfully) to use the propriatary nvidia drivers
so that I can enjoy both a smoother 3D experience while playing minecraft, as well
as use the CUDA support so I don't need to use cloud-based services for small
exercises.&lt;/p&gt;
&lt;p&gt;The move to nvidia was quite simple, as the &lt;a href="https://wiki.gentoo.org/wiki/NVidia/nvidia-drivers"&gt;nvidia-drivers wiki article&lt;/a&gt; on
the Gentoo wiki was quite easy to follow.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Signing the modules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One difference I found with the article (which I've promply changed) is that
the signing command, necessary to sign the Linux kernel modules so that they
can be loaded (as unsigned or wrongly signed modules are not allowed on the
system), was different.&lt;/p&gt;
&lt;p&gt;It used to be as follows (example for a single module, it had to be repeated
for each affected kernel module):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# perl /usr/src/linux/scripts/sign-file sha512 \
      /usr/src/linux/signing_key.priv \
      /usr/src/linux/signing_key.x509 \
      /lib/modules/4.12.5-gentoo/video/nvidia-uvm.ko
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, from version 4.3.3 onward (as also explained by this excellent
&lt;a href="https://wiki.gentoo.org/wiki/Signed_kernel_module_support"&gt;Signed kernel module support article&lt;/a&gt; on the Gentoo wiki) this command
no longer uses a Perl script, but is an ELF binary. Also, the location
of the default signing key is moved into a &lt;code&gt;certs/&lt;/code&gt; subdirectory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling nvidia device files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the nvidia modules are loaded, additional device files are enabled.
One is the &lt;code&gt;nvidia0&lt;/code&gt; character device file, while the other is the
&lt;code&gt;nvidiactl&lt;/code&gt; character device file. And although I can imagine that the
&lt;code&gt;nvidiactl&lt;/code&gt; one is a control-related device file, I don't exactly know
for sure.&lt;/p&gt;
&lt;p&gt;However, attempts to use 3D applications showed (through SELinux denials)
that access to these device files is needed. Without that, applications just
crashed, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;org.lwjgl.LWJGLException: X Error - disp: 0x7fd164907b00 serial: 150 error: BadValue (integer parameter out of range for operation) request_code: 153 minor_code: 24
        at org.lwjgl.opengl.LinuxDisplay.globalErrorHandler(LinuxDisplay.java:320)
        at org.lwjgl.opengl.LinuxContextImplementation.nCreate(Native Method)
        at org.lwjgl.opengl.LinuxContextImplementation.create(LinuxContextImplementation.java:51)
        at org.lwjgl.opengl.ContextGL.&amp;lt;init&amp;gt;(ContextGL.java:132)
        at org.lwjgl.opengl.Display.create(Display.java:850)
        at org.lwjgl.opengl.Display.create(Display.java:757)
        at org.lwjgl.opengl.Display.create(Display.java:739)
        at bib.at(SourceFile:635)
        at bib.aq(SourceFile:458)
        at bib.a(SourceFile:404)
        at net.minecraft.client.main.Main.main(SourceFile:123)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Not really useful to debug for me, but the SELinux denials were a bit more obvious,
showing requests for read and write to the &lt;code&gt;nvidiactl&lt;/code&gt; character device.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;code&gt;matchpathcon&lt;/code&gt; I found out that the device files had to have the
&lt;code&gt;xserver_misc_device_t&lt;/code&gt; type (which they didn't have to begin with, as the device
files were added after the automated &lt;code&gt;restorecon&lt;/code&gt; was done on the &lt;code&gt;/dev&lt;/code&gt; location).&lt;/p&gt;
&lt;p&gt;So, adding the following command to my local init script fixed the context setting
at boot up:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;restorecon /dev/nvidiactl /dev/nvidia0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Also, the domains that needed to use nVidia had to receive the following
addition SELinux-policy-wise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dev_rw_xserver_misc(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Perhaps this can be made more fine-grained (as there are several other device
files marked as &lt;code&gt;xserver_misc_device_t&lt;/code&gt;) but for now this should suffice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optimus usage with X server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The other challenge I had was that my workstation uses an integrated Intel
device, and offloads calculations and rendering to nVidia. The detection by
X server did not work automatically though, and it took some fiddling to get
it to work.&lt;/p&gt;
&lt;p&gt;In the end, I had to add in an &lt;code&gt;nvidia.conf&lt;/code&gt; file inside &lt;code&gt;/etc/X11/xorg.conf.d&lt;/code&gt;
with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Section &amp;quot;ServerLayout&amp;quot;
        Identifier      &amp;quot;layout&amp;quot;
        Screen  0       &amp;quot;nvidia&amp;quot;
        Inactive        &amp;quot;intel&amp;quot;
EndSection

Section &amp;quot;Device&amp;quot;
        Identifier      &amp;quot;nvidia&amp;quot;
        Driver          &amp;quot;nvidia&amp;quot;
        BusID           &amp;quot;PCI:1:0:0&amp;quot;
EndSection

Section &amp;quot;Screen&amp;quot;
        Identifier      &amp;quot;nvidia&amp;quot;
        Device          &amp;quot;nvidia&amp;quot;
        Option          &amp;quot;AllowEmptyInitialConfiguration&amp;quot;
EndSection

Section &amp;quot;Device&amp;quot;
        Identifier      &amp;quot;intel&amp;quot;
        Driver          &amp;quot;modesetting&amp;quot;
EndSection

Section &amp;quot;Screen&amp;quot;
        Identifier      &amp;quot;intel&amp;quot;
        Device          &amp;quot;intel&amp;quot;
EndSection
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with a single &lt;code&gt;xrandr&lt;/code&gt; command I re-enabled split screen support (as by
default it now showed the same output on both screens):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ xrandr --output eDP-1-1 --left-of HDMI-1-2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I also had to set the output source to the nVidia device, by adding the following
lines to my &lt;code&gt;~/.xinitrc&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;xrandr --setprovideroutputsource modesetting NVIDIA-0
xrandr --auto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And with that, another thing was crossed off from my TODO list. Which has become
quite large after my holidays (went to Kos, Greece) as I had many books and articles
on my ebook reader with me, which inspired a lot.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="nvidia"></category></entry><entry><title>Switch to Gentoo sources</title><link href="https://blog.siphos.be/2017/08/switch-to-gentoo-sources/" rel="alternate"></link><published>2017-08-22T19:04:00+02:00</published><updated>2017-08-22T19:04:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-08-22:/2017/08/switch-to-gentoo-sources/</id><summary type="html">&lt;p&gt;You've might already read it on the Gentoo news site, the &lt;a href="https://www.gentoo.org/news/2017/08/19/hardened-sources-removal.html"&gt;Hardened Linux kernel sources
are removed from the tree&lt;/a&gt; due to the &lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; change where the grsecurity
Linux kernel patches are no longer provided for free. The decision was made due to
supportability and maintainability reasons.&lt;/p&gt;
&lt;p&gt;That doesn't mean that users who want to stick with the grsecurity related hardening
features are left alone. &lt;a href="https://blogs.gentoo.org/ago/2017/08/21/sys-kernel-grsecurity-sources-available/#utm_source=feed&amp;amp;utm_medium=feed&amp;amp;utm_campaign=feed"&gt;Agostino Sarubbo has started providing sys-kernel/grsecurity-sources&lt;/a&gt;
for the users who want to stick with it, as it is based on &lt;a href="https://github.com/minipli/linux-unofficial_grsec"&gt;minipli's unofficial patchset&lt;/a&gt;.
I seriously hope that the patchset will continue to be maintained and, who knows, even evolve further.&lt;/p&gt;
&lt;p&gt;Personally though, I'm switching to the Gentoo sources, and stick with SELinux as one of the
protection measures. And with that, I might even start using my NVidia graphics card a bit more, 
as that one hasn't been touched in several years (I have an Optimus-capable setup with both an
Intel integrated graphics card and an NVidia one, but all attempts to use nouveau for the one game
I like to play - minecraft - didn't work out that well).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;You've might already read it on the Gentoo news site, the &lt;a href="https://www.gentoo.org/news/2017/08/19/hardened-sources-removal.html"&gt;Hardened Linux kernel sources
are removed from the tree&lt;/a&gt; due to the &lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; change where the grsecurity
Linux kernel patches are no longer provided for free. The decision was made due to
supportability and maintainability reasons.&lt;/p&gt;
&lt;p&gt;That doesn't mean that users who want to stick with the grsecurity related hardening
features are left alone. &lt;a href="https://blogs.gentoo.org/ago/2017/08/21/sys-kernel-grsecurity-sources-available/#utm_source=feed&amp;amp;utm_medium=feed&amp;amp;utm_campaign=feed"&gt;Agostino Sarubbo has started providing sys-kernel/grsecurity-sources&lt;/a&gt;
for the users who want to stick with it, as it is based on &lt;a href="https://github.com/minipli/linux-unofficial_grsec"&gt;minipli's unofficial patchset&lt;/a&gt;.
I seriously hope that the patchset will continue to be maintained and, who knows, even evolve further.&lt;/p&gt;
&lt;p&gt;Personally though, I'm switching to the Gentoo sources, and stick with SELinux as one of the
protection measures. And with that, I might even start using my NVidia graphics card a bit more, 
as that one hasn't been touched in several years (I have an Optimus-capable setup with both an
Intel integrated graphics card and an NVidia one, but all attempts to use nouveau for the one game
I like to play - minecraft - didn't work out that well).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;How secure is Gentoo sources with SELinux?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is hard to just say that one kernel tree or another is safe(r) or not. Security is not something
one can get with a simple check-list. It is a matter of properly configuring services and systems,
patching it when needed, limiting expoosure and what not.&lt;/p&gt;
&lt;p&gt;A huge advantage of grsecurity was that it had very insightful and advanced protection measures
(many of them focusing on memory-related attacks), and prevented unwanted behavior from applications
(and users) in a very fine-grained manner. With SELinux, I can still prevent some unwanted behavior,
but it is important to know that SELinux and grsecurity's kernel hardening features are orthogonal
to each other. It is only the grsecurity RBAC model that is somewhat in competition with SELinux.&lt;/p&gt;
&lt;p&gt;SELinux is able to define and manage behavior between types. However, within a single type, many
actions are not governed at all. SELinux can manage which types (domains) are able to invoke which
system calls, but once a call is allowed, SELinux doesn't do any additional controls anymore.&lt;/p&gt;
&lt;p&gt;Loosing protection controls from grsecurity, as a security activist, is not something I like. But
on the other hand, I need to consider the wide SELinux using audience in Gentoo, who is most likely
going to switch to the gentoo sources as well (at least the majority of them).&lt;/p&gt;
&lt;p&gt;Gentoo sources is not insecure by itself, as are many other kernel sources. A huge advantage is that
the gentoo sources are well maintained, so any kernel vulnerability that gets reported and fixed will
receive the proper fix in the Gentoo sources quickly as well (and if you think it can go even faster,
consider &lt;a href="https://wiki.gentoo.org/wiki/Project:Security/Padawan_Process"&gt;becoming a Gentoo security padawan&lt;/a&gt;. And with SELinux enabled, some additional security
controls can be implemented (the efficacy of it depends on the quality of the policy).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://kernsec.org/wiki/index.php/Kernel_Self_Protection_Project"&gt;Kernel Self Protection Project&lt;/a&gt; also aims to improve the Linux kernel security, and immediately
through upstreamed and accepted patches. This means that the protection measures, once in the kernel,
should remain inside (awkward regressions notwithstanding). I truly hope that the KSPP moves forward.
In the mean time, read up on the &lt;a href="https://www.kernel.org/doc/html/latest/security/self-protection.html"&gt;Kernel Self-Protection&lt;/a&gt; document to learn more about how to harden
the Linux kernel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So that's it, just one less security control?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For now, there is no immediate substitute. But that doesn't mean that there is nothing one can do
to increase the secure state of a Linux desktop, workstation or even IoT device. Although remotely
executable exploits do pop up and exist, many vulnerabilities in the Linux kernel are mainly exploitable
through a local access pattern.&lt;/p&gt;
&lt;p&gt;That means that vulnerabilities often can only be exploited through a local invocation (or through chaining
by using other vulnerabilities - often in completely different applications or services - in order to
execute the local malware). Hence, hardening of the entire system is extremely important.&lt;/p&gt;
&lt;p&gt;Previously, I had an account with multiple SELinux roles assigned to it. Depending on what I wanted to
do, I transitioned to the right role (either through the &lt;code&gt;newrole&lt;/code&gt; command, or through &lt;code&gt;sudo&lt;/code&gt; which
has integrated SELinux support). With the switch to the gentoo sources, I decided to make it a bit
harder for malware on my system to work: i start using separate Linux accounts depending on the purpose
(which I call persona).&lt;/p&gt;
&lt;p&gt;Developing SELinux policies is now done on a separate account, managing remote systems through another
account (although my servers use multi-factor authentication so there was already some additional safeguard
in place there), handling my side-work with another account, playing games with another account, etc.&lt;/p&gt;
&lt;p&gt;It isn't that I don't trust SELinux for this (as each domain is well isolated and controlled). But SELinux
cannot prevent vulnerabilities within applications if the action/result of a succesfully exploited
vulnerability does not change the expected behavior of the application versus the other resources
on the system (and even there, the fine-grained approach of policies might not even be sufficiently
fine-grained, as SELinux uses labels, and many resources have the same label assigned).&lt;/p&gt;
&lt;p&gt;Suppose some malware is able to capture me giving in my password, or is trying to phish for it. By
using separate accounts (with separate passphrazes of course) the impact is reduced a bit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other things on the plate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The change to different accounts was one thing I wanted to establish before switching to a new kernel
tree. There are other aspects that I want to investigate in the near future as well though.&lt;/p&gt;
&lt;p&gt;First of all, I'm probably going to enable &lt;a href="https://github.com/Yubico/pam-u2f"&gt;U2F authentication&lt;/a&gt; on my workstation as well for
all interactive accounts. It has been on my list for quite some time, and quickly going through the
publicly available fora doesn't reveal any major challenges to do so. Build the PAM module, update
the PAM service configurations and you're done. Hopefully. ;-)&lt;/p&gt;
&lt;p&gt;Next, I'm going to play around a bit with &lt;a href="https://wiki.gentoo.org/wiki/AddressSanitizer"&gt;AddressSanitizer&lt;/a&gt;. ASAN was incompatible with grsecurity,
but now that that's out of the way, there's no reason not to investigate it further. I am not going
to enable it for the kernel though (as some KSPP implemented measures are incompatible with ASAN as well),
and probably not for my complete workstation yet (even though it is sufficiently powerful to handle the major
performance impact).&lt;/p&gt;
&lt;p&gt;I'm going to put some more focus on &lt;a href="https://wiki.gentoo.org/wiki/Integrity_Measurement_Architecture"&gt;Integrity Measurement Architecture support&lt;/a&gt;, although my main protection
measure with IMA - the TPM or Trusted Platform Module - has been fried (don't ask) so I can't use it anymore.
Perhaps I'm going to buy a very lightweight/small system with a TPM on it to continue development. We'll see.&lt;/p&gt;
&lt;p&gt;My current knowledge of &lt;a href="https://en.wikipedia.org/wiki/Seccomp"&gt;seccomp&lt;/a&gt; is fairly theoretical (with a few hands-on tutorials, but that's it). It
has been on my TODO list for some time to look in more depth to it. Perhaps this is the right time.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="hardened"></category><category term="grsecurity"></category><category term="selinux"></category></entry><entry><title>Project prioritization</title><link href="https://blog.siphos.be/2017/07/project-prioritization/" rel="alternate"></link><published>2017-07-18T20:40:00+02:00</published><updated>2017-07-18T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-07-18:/2017/07/project-prioritization/</id><summary type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Structure your workload, the SAFe approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first exercise you want to implement is to structure the workload, ideas or
projects. Some changes are small, others are large. Some are disruptive, others
are evolutionary. Trying to prioritize all different types of ideas and changes
in the same way is not feasible.&lt;/p&gt;
&lt;p&gt;Structuring workload is a common approach. Changes are grouped in projects,
projects grouped in programs, programs grouped in strategic tracks. Lately,
with the rise in Agile projects, a similar layering approach is suggested in
the form of SAFe.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="http://www.scaledagileframework.com/"&gt;Scaled Agile Framework&lt;/a&gt; a structure is suggested that uses, as a
top-level approach, value streams. These are strategically aligned steps that
an organization wants to use to build solutions that provide a continuous flow
of value to a customer (which can be internal or external). For instance, for a
financial service organization, a value stream could focus on 'Risk Management
and Analytics'.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SAFe full framework" src="https://blog.siphos.be/images/201707/safe-full.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;SAFe full framework overview, picture courtesy of www.scaledagileframework.com&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;The value streams are supported through solution trains, which implement
particular solutions. This could be a final product for a customer (fitting in
a particular value stream) or a set of systems which enable capabilities for a
value stream. It is at this level, imo, that the benefits exercises from IT
portfolio management and benefits realization management research plays its
role (more about that later). For instance, a solution train could focus on an
'Advanced Analytics Platform'.&lt;/p&gt;
&lt;p&gt;Within a solution train, agile release trains provide continuous delivery for
the various components or services needed within one or more solutions. Here,
the necessary solutions are continuously delivered in support of the solution
trains. At this level, focus is given on the culture within the organization
(think DevOps), and the relatively short-lived delivery delivery periods. This
is the level where I see 'projects' come into play.&lt;/p&gt;
&lt;p&gt;Finally, you have the individual teams working on deliverables supporting a
particular project.&lt;/p&gt;
&lt;p&gt;SAFe is just one of the many methods for organization and development/delivery
management. It is a good blueprint to look into, although I fear that larger
organizations will find it challenging to dedicate resources in a manageable
way. For instance, how to deal with specific expertise across solutions which
you can't dedicate to a single solution at a time? What if your organization
only has two telco experts to support dozens of projects? Keep that in mind,
I'll come back to that later...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get non-content information about the value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the structuring of the workload, you need to obtain information about
the solutions that you want to implement (keeping with the SAFe terminology).
And bear in mind that seemingly dull things such as ensuring your firewalls are
up to date are also deliverables within a larger ecosystem. Now, with
information about the solutions, I don't mean the content-wise information, but
instead focus on other areas.&lt;/p&gt;
&lt;p&gt;Way back, in 1952, Harry Markowitz introduced &lt;a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory"&gt;Modern portfolio theory&lt;/a&gt; as a
mathematical framework for assembling a portfolio of assets such that the
expected return is maximized for a given level of risk (quoted from Wikipedia).
This was later used in an IT portfolio approach by McFarlan in his &lt;a href="https://hbr.org/1981/09/portfolio-approach-to-information-systems"&gt;Portfolio
Approach to Information Systems&lt;/a&gt; article, published in September 1981.&lt;/p&gt;
&lt;p&gt;There it was already introduced that risk and return shouldn't be looked at
from an individual project viewpoint, but how it contributes to the overall
risk and return. A balance, if you wish. His article attempts to categorize
projects based on risk profiles on various areas. Personally, I see the
suggested categorization more as a way of supporting workload assessments (how
many mandays of work will this be), but I digress.&lt;/p&gt;
&lt;p&gt;Since then, other publications came up which tried to document frameworks and
methodologies that facilitate project portfolio prioritization and management.
The focus often boils down to value or benefits realization. In &lt;a href="https://books.google.be/books/about/The_Information_Paradox.html?id=mk60QgAACAAJ&amp;amp;redir_esc=y&amp;amp;hl=en"&gt;The
Information Paradox&lt;/a&gt; John Thorp comes up with a benefits realization
approach, which enables organizations to better define and track benefits
realization - although it again boils down on larger transformation exercises
rather than the lower-level backlogs. The realm of &lt;a href="https://en.wikipedia.org/wiki/IT_portfolio_management"&gt;IT portfolio management&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Benefits_realisation_management"&gt;Benefits realization management&lt;/a&gt; gives interesting pointers as to
the lecture part of prioritizing projects.&lt;/p&gt;
&lt;p&gt;Still, although one can hardly state the resources are incorrect, a common
question is how to make this tangible. Personally, I tend to view the above on
the value stream level and solution train level.  Here, we have a strong
alignment with benefits and value for customers, and we can leverage the ideas
of past research.&lt;/p&gt;
&lt;p&gt;The information needed at this level often boils down to strategic insights and
business benefits, coarse-grained resource assessments, with an important focus
on quality of the resources. For instance, a solution delivery might take up
500 days of work (rough estimation) but will also require significant back-end
development resources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Handling value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we implement this on the highest level in the structure, it should be
conceivable that the overview of the value streams (a dozen or so) and
solutions (a handful per value stream) is manageable, and something that at an
executive level is feasible to work with. These are the larger efforts for
structuring and making strategic alignment. Formal methods for prioritization
are generally not implemented or described.&lt;/p&gt;
&lt;p&gt;In my company, there are exercises that are aligning with SAFe, but it isn't
company-wide. Still, there is a structure in place that (within IT) one could
map to value streams (with some twisting ;-) and, within value streams, there
are structures in place that one could map to the solution train exercises.&lt;/p&gt;
&lt;p&gt;We could assume that the enterprise knows about its resources (people, budget
...) and makes a high-level suggestion on how to distribute the resources in
the mid-term (such as the next 6 months to a year). This distribution is
challenged and worked out with the value stream owners. See also "lean
budgeting" in the SAFe approach for one way of dealing with this.&lt;/p&gt;
&lt;p&gt;There is no prioritization of value streams. The enterprise has already made
its decision on what it finds to be the important values and benefits and
decided those in value streams.&lt;/p&gt;
&lt;p&gt;Within a value stream, the owner works together with the customers (internal or
external) to position and bring out solutions. My experience here is that
prioritization is generally based on timings and expectations from the
customer. In case of resource contention, the most challenging decision to make
here is to put a solution down (meaning, not to pursue the delivery of a
solution), and such decisions are hardly taken.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prioritizing the projects and changes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the lower echelons of the project portfolio structure, we have the projects
and changes. Let's say that the levels here are projects (agile release trains)
and changes (team-level). Here, I tend to look at prioritization on project
level, and this is the level that has a more formal approach for
prioritization.&lt;/p&gt;
&lt;p&gt;Why? Because unlike the higher levels, where the prioritization is generally
quality-oriented on a manageable amount of streams and solutions, we have a
large quantity of projects and ideas. Hence, prioritization is more
quantity-oriented in which formal methods are more efficient to handle.&lt;/p&gt;
&lt;p&gt;The method that is used in my company uses scoring criteria on a per-project
level. This is not innovative per se, as past research has also revealed that
project categorization and mapping is a powerful approach for handling project
portfolio's. Just look for "categorizing priority projects it portfolio" in
Google and you'll find ample resources. Kendal's &lt;a href="https://www.amazon.com/Advanced-Project-Portfolio-Management-PMO/dp/1932159029"&gt;Advanced Project Portfolio
Management and the PMO&lt;/a&gt; (book) has several example project scoring
criteria's. But allow me to explain our approach.&lt;/p&gt;
&lt;p&gt;It basically is like so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each project selects three value drivers (list decided up front)&lt;/li&gt;
&lt;li&gt;For the value drivers, the projects check if they contribute to it slightly (low), moderately (medium) or fully (high)&lt;/li&gt;
&lt;li&gt;The value drivers have weights, as do the values. Sum the resulting products to get a priority score&lt;/li&gt;
&lt;li&gt;Have the priority score validated by a scoring team&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get to the details of it.&lt;/p&gt;
&lt;p&gt;For the IT projects within the infrastructure area (which is what I'm active
in), we have around 5 scoring criteria (value drivers) that are value-stream
agnostic, and then 3 to 5 scoring criteria that are value-stream specific. Each
scoring criteria has three potential values: low (2), medium (4) and high (9).
The numbers are the weights that are given to the value.&lt;/p&gt;
&lt;p&gt;A scoring criteria also has a weight. For instance, we have a scoring criteria
on efficiency (read: business case) which has a weight of 15, so a score of
medium within that criteria gives a total value of 60 (4 times 15). The
potential values here are based on the "return on investment" value, with low
being a return less than 2 years, medium within a year, and high within a few
months (don't hold me on the actual values, but you get the idea).&lt;/p&gt;
&lt;p&gt;The sum of all values gives a priority score. Now, hold your horses, because
we're not done yet. There is a scoring rule that says a project can only be
scored by at most 3 scoring criteria. Hence, project owners need to see what
scoring areas their project is mostly visible in, and use those scoring
criteria. This rule supports the notion that people don't bring around ideas
that will fix world hunger and make a cure for cancer, but specific, well
scoped ideas (the former are generally huge projects, while the latter requires
much less resources).&lt;/p&gt;
&lt;p&gt;OK, so you have a score - is that your priority? No. As a project always falls
within a particular value stream, we have a "scoring team" for each value
stream which does a number of things. First, it checks if your project really
belongs in the right value stream (but that's generally implied) and has a
deliverable that fits the solution or target within that stream. Projects that
don't give any value or aren't asked by customers are eliminated.&lt;/p&gt;
&lt;p&gt;Next, the team validates if the scoring that was used is correct: did you
select the right values (low, medium or high) matching the methodology for said
criteria? If not, then the score is adjusted.&lt;/p&gt;
&lt;p&gt;Finally, the team validates if the resulting score is perceived to be OK or
not. Sometimes, ideas just don't map correctly on scoring criteria, and even
though a project has a huge strategic importance or deliverable it might score
low. In those cases, the scoring team can adjust the score manually. However,
this is more of a fail-safe (due to the methodology) rather than the norm.
About one in 20 projects gets its score adjusted. If too many adjustments come
up, the scoring team will suggest a change in methodology to rectify the
situation.&lt;/p&gt;
&lt;p&gt;With the score obtained and validated by the scoring team, the project is given
a "go" to move to the project governance. It is the portfolio manager that then
uses the scores to see when a project can start.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Providing levers to management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, these scoring criteria are not established from a random number generator.
An initial suggestion was made on the scoring criteria, and their associated
weights, to the higher levels within the organization (read: the people in
charge of the prioritization and challenging of value streams and solutions).&lt;/p&gt;
&lt;p&gt;The same people are those that approve the weights on the scoring criteria. If
management (as this is often the level at which this is decided) feels that
business case is, overall, more important than risk reduction, then they will
be able to put a higher value in the business case scoring than in the risk
reduction.&lt;/p&gt;
&lt;p&gt;The only constraint is that the total value of all scoring criteria must be
fixed. So an increase on one scoring criteria implies a reduction on at least
one other scoring criteria. Also, changing the weights (or even the scoring
criteria themselves) cannot be done frequently. There is some inertia in
project prioritization: not the implementation (because that is a matter of
following through) but the support it will get in the organization itself.&lt;/p&gt;
&lt;p&gt;Management can then use external benchmarks and other sources to gauge the
level that an organization is at, and then - if needed - adjust the scoring
weights to fit their needs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resource allocation in teams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Portfolio managers use the scores assigned to the projects to drive their
decisions as to when (and which) projects to launch. The trivial approach is to
always pick the projects with the highest scores. But that's not all.&lt;/p&gt;
&lt;p&gt;Projects can have dependencies on other projects. If these dependencies are
"hard" and non-negotiable, then the upstream project (the one being dependent
on) inherits the priority of the downstream project (the one depending on the
first) if the downstream project has a higher priority. Soft dependencies
however need to validate if they can (or have to) wait, or can implement
workarounds if needed.&lt;/p&gt;
&lt;p&gt;Projects also have specific resource requirements. A project might have a high
priority, but if it requires expertise (say DBA knowledge) which is unavailable
(because those resources are already assigned to other ongoing projects) then
the project will need to wait (once resources are fully allocated and the
projects are started, then they need to finish - another reason why projects
have a narrow scope and an established timeframe).&lt;/p&gt;
&lt;p&gt;For engineers, operators, developers and other roles, this approach allows them
to see which workload is more important versus others. When their scope is
always within a single value stream, then the mentioned method is sufficient.
But what if a resource has two projects, each of a different value stream? As
each value stream has its own scoring criteria it can use (and weight), one
value stream could systematically have higher scores than others...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mixing and matching multiple value streams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To allow projects to be somewhat comparable in priority values, an additional
rule has been made in the scoring methodology: value streams must have a
comparable amount of scoring criteria (value drivers), and the total value of
all criteria must be fixed (as was already mentioned before). So if there are
four scoring criteria and the total value is fixed at 20, then one value stream
can have its criteria at (5,3,8,4) while another has it at (5,5,5,5).&lt;/p&gt;
&lt;p&gt;This is still not fully adequate, as one value stream could use a single
criteria with the maximum amount (20,0,0,0). However, we elected not to put in
an additional constraint, and have management work things out if the situation
ever comes out. Luckily, even managers are just human and they tend to follow
the notion of well-balanced value drivers.&lt;/p&gt;
&lt;p&gt;The result is that two projects will have priority values that are currently
sufficiently comparable to allow cross-value-stream experts to be exchangeable
without monopolizing these important resources to a single value stream
portfolio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Current state&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The scoring methodology has been around for a few years already. Initially, it
had fixed scoring criteria used by three value streams (out of seven, the other
ones did not use the same methodology), but this year we switched to support
both value stream agnostic criteria (like in the past) as well as value stream
specific ones.&lt;/p&gt;
&lt;p&gt;The methodology is furthest progressed in one value stream (with focus of around
1000 projects) and is being taken up by two others (they are still looking at
what their stream-specific criteria are before switching).&lt;/p&gt;</content><category term="Architecture"></category><category term="pmo"></category><category term="strategy"></category><category term="SAFe"></category><category term="prioritization"></category><category term="project"></category></entry><entry><title>Structuring infrastructural deployments</title><link href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/" rel="alternate"></link><published>2017-06-07T20:40:00+02:00</published><updated>2017-06-07T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-06-07:/2017/06/structuring-infrastructural-deployments/</id><summary type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;As always, start with the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From an infrastructure architect point of view, creating structure is one way
of dealing with the onslaught in complexity that is prevalent within the wider
organizational architecture. By creating a framework in which infrastructural
services can be positioned, architects and other stakeholders (such as
information security officers, process managers, service delivery owners, project
and team leaders ...) can support the wide organization in its endeavor of
becoming or remaining competitive.&lt;/p&gt;
&lt;p&gt;Structure can be provided through various viewpoints. As such, while creating
such framework, the initial intention is not to start drawing borders or
creating a complex graph. Instead, look at attributes that one would assign
to an infrastructural service, and treat those as labels. Create a nice
portfolio of attributes which will help guide the development of such framework.&lt;/p&gt;
&lt;p&gt;The following list gives some ideas in labels or attributes that one can use.
But be creative, and use experienced people in devising the "true" list of
attributes that fits the needs of your organization. Be sure to describe them
properly and unambiguously - the list here is just an example, as are the
descriptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;tenant&lt;/strong&gt; identifies the organizational aggregation of business units which are
  sufficiently similar in areas such as policies (same policies in use),
  governance (decision bodies or approval structure), charging, etc. It
  could be a hierarchical aspect (such as organization) as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;location&lt;/strong&gt; provides insight in the physical (if applicable) location of the
  service. This could be an actual building name, but can also be structured
  depending on the size of the environment. If it is structured, make sure to
  devise a structure up front. Consider things such as regions, countries,
  cities, data centers, etc. A special case location value could be the
  jurisdiction, if that is something that concerns the organization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;service type&lt;/strong&gt; tells you what kind of service an asset is. It can be a
  workstation, a server/host, server/guest, network device, virtual or
  physical appliance, sensor, tablet, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trust level&lt;/strong&gt; provides information on how controlled and trusted the service
  is. Consider the differences between unmanaged (no patching, no users doing
  any maintenance), open (one or more admins, but no active controlled
  maintenance), controlled (basic maintenance and monitoring, but with still
  administrative access by others), managed (actively maintained, no privileged
  access without strict control), hardened (actively maintained, additional
  security measures taken) and kiosk (actively maintained, additional security
  measures taken and limited, well-known interfacing).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compliance set&lt;/strong&gt; identifies specific compliance-related attributes, such as the
  PCI-DSS compliancy level that a system has to adhere to.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consumer group&lt;/strong&gt; informs about the main consumer group, active on the service.
  This could be an identification of the relationship that consumer group has
  with the organization (anonymous, customer, provider, partner, employee, ...)
  or the actual name of the consumer group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;architectural purpose&lt;/strong&gt; gives insight in the purpose of the service in
  infrastructural terms. Is it a client system, a gateway, a mid-tier system,
  a processing system, a data management system, a batch server, a reporting
  system, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt; could be interpreted as to the company purpose of the system. Is it for
  commercial purposes (such as customer-facing software), corporate functions
  (company management), development, infrastructure/operations ...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;production status&lt;/strong&gt; provides information about the production state of a
  service. Is it a production service, or a pre-production (final testing before
  going to production), staging (aggregation of multiple changes) or development
  environment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the final set of labels, the next step is to aggregate results to create
a high-level view of the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating a layered structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chances are high that you'll end up with several attributes, and many of these
will have multiple possible values. What we don't want is to end in an
N-dimensional infrastructure architecture overview. Sure, it sounds sexy to do
so, but you want to show the infrastructure architecture to several stakeholders
in your organization. And projecting an N-dimensional structure on a
2-dimensional slide is not only challenging - you'll possibly create a projection
which leaves out important details or make it hard to interpret.&lt;/p&gt;
&lt;p&gt;Instead, we looked at a &lt;em&gt;layered approach&lt;/em&gt;, with each layer handling one or two
requirements. The top layer represents the requirement which the organization
seems to see as the most defining attribute. It is the attribute where, if its
value changes, most of its architecture changes (and thus the impact of a service
relocation is the largest).&lt;/p&gt;
&lt;p&gt;Suppose for instance that the domain attribute is seen as the most defining one:
the organization has strict rules about placing corporate services and commercial
services in separate environments, or the security officers want to see the
commercial services, which are well exposed to many end users, be in a separate
environment from corporate services. Or perhaps the company offers commercial
services for multiple tenants, and as such wants several separate "commercial
services" environments while having a single corporate service domain.&lt;/p&gt;
&lt;p&gt;In this case, part of the infrastructure architecture overview on the top level
could look like so (hypothetical example):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Top level view" src="https://blog.siphos.be/images/201706/07-1-toplevelview.png"&gt;&lt;/p&gt;
&lt;p&gt;This also shows that, next to the corporate and commercial interests of the
organization, a strong development support focus is prevalent as well. This
of course depends on the type of organization or company and how significant
in-house development is, but in this example it is seen as a major decisive
factor for service positioning.&lt;/p&gt;
&lt;p&gt;These top-level blocks (depicted as locations, for those of you using Archimate)
are what we call "&lt;strong&gt;zones&lt;/strong&gt;". These are not networks, but clearly bounded areas in
which multiple services are positioned, and for which particular handling rules
exist. These rules are generally written down in policies and standards - more
about that later.&lt;/p&gt;
&lt;p&gt;Inside each of these zones, a substructure is made available as well, based on
another attribute. For instance, let's assume that this is the architectural
purpose. This could be because the company has a requirement on segregating
workstations and other client-oriented zones from the application hosting related
ones. Security-wise, the company might have a principle where mid-tier services
(API and presentation layer exposures) are separate from processing services,
and where data is located in a separate zone to ensure specific data access or
more optimal infrastructure services.&lt;/p&gt;
&lt;p&gt;This zoning result could then be depicted as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Detailed top-level view" src="https://blog.siphos.be/images/201706/07-1-detailedtoplevel.png"&gt;&lt;/p&gt;
&lt;p&gt;From this viewpoint, we can also deduce that this company provides separate
workstation services: corporate workstation services (most likely managed
workstations with focus on application disclosure, end user computing, etc.)
and development workstations (most likely controlled workstations but with more
open privileged access for the developer).&lt;/p&gt;
&lt;p&gt;By making this separation explicit, the organization makes it clear that the
development workstations will have a different position, and even a different
access profile toward other services within the company.&lt;/p&gt;
&lt;p&gt;We're not done yet. For instance, on the mid-tier level, we could look at the
consumer group of the services:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mid-tier explained" src="https://blog.siphos.be/images/201706/07-1-midtier.png"&gt;&lt;/p&gt;
&lt;p&gt;This separation can be established due to security reasons (isolating services
that are exposed to anonymous users from customer services or even partner
services), but one can also envision this to be from a management point of
view (availability requirements can differ, capacity management is more
uncertain for anonymous-facing services than authenticated, etc.)&lt;/p&gt;
&lt;p&gt;Going one layer down, we use a production status attribute as the defining
requirement:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anonymous user detail" src="https://blog.siphos.be/images/201706/07-1-anonymousdetail.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point, our company decided that the defined layers are sufficiently
established and make for a good overview. We used different defining properties
than the ones displayed above (again, find a good balance that fits the company
or organization that you're focusing on), but found that the ones we used were
mostly involved in existing policies and principles, while the other ones are
not that decisive for infrastructure architectural purposes. &lt;/p&gt;
&lt;p&gt;For instance, the tenant might not be selected as a deciding attribute, because
there will be larger tenants and smaller tenants (which could make the resulting
zone set very convoluted) or because some commercial services are offered toward
multiple tenants and the organizations' strategy would be to move toward
multi-tenant services rather than multiple deployments.&lt;/p&gt;
&lt;p&gt;Now, in the zoning structure there is still another layer, which from an
infrastructure architecture point is less about rules and guidelines and more
about manageability from an organizational point of view. For instance, in the
above example, a SAP deployment for HR purposes (which is obviously a corporate
service) might have its Enterprise Portal service in the &lt;code&gt;Corporate Services&lt;/code&gt; &amp;gt; 
&lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone. However, another service such as
an on-premise SharePoint deployment for group collaboration might be in &lt;code&gt;Corporate
Services&lt;/code&gt; &amp;gt; &lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone as well. Yet both
services are supported through different teams.&lt;/p&gt;
&lt;p&gt;This "final" layer thus enables grouping of services based on the supporting
team (again, this is an example), which is organizationally aligned with the
business units of the company, and potentially further isolation of services
based on other attributes which are not defining for all services. For instance,
the company might have a policy that services with a certain business impact
assessment score must be in isolated segments with no other deployments within
the same segment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about management services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, the above picture is missing some of the (in my opinion) most important
services: infrastructure support and management services. These services do not
shine in functional offerings (which many non-IT people generally look at) but
are needed for non-functional requirements: manageability, cost control,
security (if security can be defined as a non-functional - let's not discuss
that right now).&lt;/p&gt;
&lt;p&gt;Let's first consider &lt;em&gt;interfaces&lt;/em&gt; - gateways and other services which are
positioned between zones or the "outside world". In the past, we would speak of
a demilitarized zone (DMZ). In more recent publications, one can find this as
an interface zone, or a set of Zone Interface Points (ZIPs) for accessing and
interacting with the services within a zone.&lt;/p&gt;
&lt;p&gt;In many cases, several of these interface points and gateways are used in the
organization to support a number of non-functional requirements. They can be
used for intelligent load balancing, providing virtual patching capabilities,
validating content against malware before passing it on to the actual services,
etc.&lt;/p&gt;
&lt;p&gt;Depending on the top level zone, different gateways might be needed (i.e.
different requirements). Interfaces for commercial services will have a strong
focus on security and manageability. Those for the corporate services might be
more integration-oriented, and have different data leakage requirements than
those for commercial services.&lt;/p&gt;
&lt;p&gt;Also, inside such an interface zone, one can imagine a substructure to take
place as well: egress interfaces (for communication that is exiting the zone),
ingress interfaces (for communication that is entering the zone) and internal
interfaces (used for routing between the subzones within the zone).&lt;/p&gt;
&lt;p&gt;Yet, there will also be requirements which are company-wide. Hence, one could
envision a structure where there is a company-wide interface zone (with
mandatory requirements regardless of the zone that they support) as well as a
zone-specific interface zone (with the mandatory requirements specific to that
zone).&lt;/p&gt;
&lt;p&gt;Before I show a picture of this, let's consider &lt;em&gt;management services&lt;/em&gt;. Unlike
interfaces, these services are more oriented toward the operational management
of the infrastructure. Software deployment, configuration management, identity
&amp;amp; access management services, etc. Are services one can put under management
services.&lt;/p&gt;
&lt;p&gt;And like with interfaces, one can envision the need for both company-wide
management services, as well as zone-specific management services.&lt;/p&gt;
&lt;p&gt;This information brings us to a final picture, one that assists the
organization in providing a more manageable view on its deployment landscape.
It does not show the 3rd layer (i.e. production versus non-production
deployments) and only displays the second layer through specialization
information, which I've quickly made a few examples for (you don't want to make
such decisions in a few hours, like I did for this post).&lt;/p&gt;
&lt;p&gt;&lt;img alt="General overview" src="https://blog.siphos.be/images/201706/07-1-firstgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;If the organization took an alternative approach for structuring (different
requirements and grouping) the resulting diagram could look quite different:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alternative general overview" src="https://blog.siphos.be/images/201706/07-1-secondgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flows, flows and more flows&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the high-level picture ready, it is not a bad idea to look at how flows
are handled in such an architecture. As the interface layer is available on
both company-wide level as well as the next, flows will cross multiple zones.&lt;/p&gt;
&lt;p&gt;Consider the case of a corporate workstation connecting to a reporting server
(like a Cognos or PowerBI or whatever fancy tool is used), and this reporting
server is pulling data from a database system. Now, this database system is
positioned in the &lt;code&gt;Commercial&lt;/code&gt; zone, while the reporting server is in the
&lt;code&gt;Corporate&lt;/code&gt; zone. The flows could then look like so:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Flow example" src="https://blog.siphos.be/images/201706/07-1-flow.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;Note for the Archimate people: I'm sorry that I'm abusing the flow relation
here. I didn't want to create abstract services in the locations and then use
the "serves" or "used by" relation and then explaining readers that the arrows
are then inverse from what they imagine.&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;In this picture, the corporate workstation does not connect to the reporting
server directly. It goes through the internal interface layer for the corporate
zone. This internal interface layer can offer services such as reverse proxies
or intelligent load balancers. The idea here is that, if the organization
wants, it can introduce additional controls or supporting services in this
internal interface layer without impacting the system deployments themselves
much.&lt;/p&gt;
&lt;p&gt;But the true flow challenge is in the next one, where a processing system
connects to a data layer. Here, the processing server will first connect to the
egress interface for corporate, then through the company-wide internal
interface, toward the ingress interface of the commercial and then to the data
layer.&lt;/p&gt;
&lt;p&gt;Now, why three different interfaces, and what would be inside it?&lt;/p&gt;
&lt;p&gt;On the corporate level, the egress interface could be focusing on privacy
controls or data leakage controls. On the company-wide internal interface more
functional routing capabilities could be provided, while on the commercial
level the ingress could be a database activity monitoring (DAM) system such as
a database firewall to provide enhanced auditing and access controls.&lt;/p&gt;
&lt;p&gt;Does that mean that all flows need to have at least three gateways? No, this is
a functional picture. If the organization agrees, then one or more of these
interface levels can have a simple pass-through setup. It is well possible that
database connections only connect directly to a DAM service and that such flows
are allowed to immediately go through other interfaces.&lt;/p&gt;
&lt;p&gt;The importance thus is not to make flows more difficult to provide, but to
provide several areas where the organization can introduce controls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making policies and standards more visible&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the effects of having a better structure of the company-wide deployments
(i.e. a good zoning solution) is that one can start making policies more clear,
and potentially even simple to implement with supporting tools (such as
software defined network solutions).&lt;/p&gt;
&lt;p&gt;For instance, a company might want to protect its production data and establish
that it cannot be used for non-production use, but that there are no
restrictions for the other data environments. Another rule could be that
web-based access toward the mid-tier is only allowed through an interface.&lt;/p&gt;
&lt;p&gt;These are simple statements which, if a company has a good IP plan, are easy to
implement - one doesn't need zoning, although it helps. But it goes further
than access controls.&lt;/p&gt;
&lt;p&gt;For instance, the company might require corporate workstations to be under
heavy data leakage prevention and protection measures, while developer
workstations are more open (but don't have any production data access). This
not only reveals an access control, but also implies particular minimal
requirements (for the &lt;code&gt;Corporate&lt;/code&gt; &amp;gt; &lt;code&gt;Workstation&lt;/code&gt; zone) and services (for the
&lt;code&gt;Corporate&lt;/code&gt; interfaces).&lt;/p&gt;
&lt;p&gt;This zoning structure does not necessarily make any statements about the
location (assuming it isn't picked as one of the requirements in the
beginning). One can easily extend this to include cloud-based services or
services offered by third parties.&lt;/p&gt;
&lt;p&gt;Finally, it also supports making policies and standards more realistic. I often
see policies that make bold statements such as "all software deployments must
be done through the company software distribution tool", but the policies don't
consider development environments (production status) or unmanaged, open or
controlled deployments (trust level). When challenged, the policy owner might
shrug away the comment with "it's obvious that this policy does not apply to
our sandbox environment" or so.&lt;/p&gt;
&lt;p&gt;With a proper zoning structure, policies can establish the rules for the right
set of zones, and actually pin-point which zones are affected by a statement.
This is also important if a company has many, many policies. With a good zoning
structure, the policies can be assigned with meta-data so that affected roles
(such as project leaders, architects, solution engineers, etc.) can easily get
an overview of the policies that influence a given zone.&lt;/p&gt;
&lt;p&gt;For instance, if I want to position a new management service, I am less
concerned about workstation-specific policies. And if the management service is
specific for the development environment (such as a new version control system)
many corporate or commercially oriented policies don't apply either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above approach for structuring an organization is documented here in a
high-level manner. It takes many assumptions or hypothetical decisions which
are to be tailored toward the company itself. In my company, a different zoning
structure is selected, taking into account that it is a financial service
provider with entities in multiple countries, handling several thousand of
systems and with an ongoing effort to include cloud providers within its
infrastructure architecture.&lt;/p&gt;
&lt;p&gt;Yet the approach itself is followed in an equal fashion. We looked at
requirements, created a layered structure, and finished the zoning schema. Once
the schema was established, the requirements for all the zones were written out
further, and a mapping of existing deployments (as-is) toward the new zoning
picture is on-going. For those thinking that it is just slideware right now -
it isn't. Some of the structures that come out of the zoning exercise are
already prevalent in the organization, and new environments (due to mergers and
acquisitions) are directed to this new situation.&lt;/p&gt;
&lt;p&gt;Still, we know we have a large exercise ahead before it is finished, but I
believe that it will benefit us greatly, not only from a security point of
view, but also clarity and manageability of the environment.&lt;/p&gt;</content><category term="Architecture"></category><category term="segmentation"></category><category term="zoning"></category><category term="deployments"></category><category term="landscape"></category></entry><entry><title>Matching MD5 SSH fingerprint</title><link href="https://blog.siphos.be/2017/05/matching-md5-ssh-fingerprint/" rel="alternate"></link><published>2017-05-18T18:20:00+02:00</published><updated>2017-05-18T18:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-05-18:/2017/05/matching-md5-ssh-fingerprint/</id><summary type="html">&lt;p&gt;Today I was attempting to update a local repository, when SSH complained
about a changed fingerprint, something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:p4ZGs+YjsBAw26tn2a+HPkga1dPWWAWX+NEm4Cv4I9s.
Please contact your system administrator.
Add correct host key in /home/user/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/user/.ssh/known_hosts:9
ECDSA host key for 192.168.56.101 has changed and you have requested strict checking.
Host key verification failed.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</summary><content type="html">&lt;p&gt;Today I was attempting to update a local repository, when SSH complained
about a changed fingerprint, something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:p4ZGs+YjsBAw26tn2a+HPkga1dPWWAWX+NEm4Cv4I9s.
Please contact your system administrator.
Add correct host key in /home/user/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/user/.ssh/known_hosts:9
ECDSA host key for 192.168.56.101 has changed and you have requested strict checking.
Host key verification failed.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;I checked if the host was changed recently, or the alias through
which I connected switched host, or the SSH key changed. But that
wasn't the case. Or at least, it wasn't the case recently, and I
distinctly remember connecting to the same host two weeks ago.&lt;/p&gt;
&lt;p&gt;Now, what happened I don't know yet, but I do know I didn't want
to connect until I reviewed the received SSH key fingerprint. I
obtained the fingerprint from the administration (who graceously
documented it on the wiki)...&lt;/p&gt;
&lt;p&gt;... only to realize that the documented fingerprint are MD5
hashes (and in hexadecimal result) whereas the key shown by the
SSH command shows it in base64 SHA256 by default.&lt;/p&gt;
&lt;p&gt;Luckily, a quick search revealed this &lt;a href="https://superuser.com/questions/929566/sha256-ssh-fingerprint-given-by-the-client-but-only-md5-fingerprint-known-for-se"&gt;superuser&lt;/a&gt;
post which told me to connect to the host using the
&lt;code&gt;FingerprintHash md5&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -o FingerprintHash=md5 192.168.56.11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is SSH displaying the MD5 hashed fingerprint which I
can now validate against the documented one. Once I validated that
the key is the correct one, I accepted the change and continued
with my endeavour.&lt;/p&gt;
&lt;p&gt;I later discovered (or, more precisely, have strong assumptions)
that I had an old elliptic curve key registered in my &lt;code&gt;known_hosts&lt;/code&gt;
file, which was not used for the communication for quite some time.
I recently re-enabled elliptic curve support in OpenSSH (with Gentoo's
USE="-bindist") which triggered the validation of the old key.&lt;/p&gt;</content><category term="Security"></category><category term="openssh"></category><category term="fingerprint"></category><category term="md5"></category></entry><entry><title>Switched to Lineage OS</title><link href="https://blog.siphos.be/2017/04/switched-to-lineage-os/" rel="alternate"></link><published>2017-04-09T16:40:00+02:00</published><updated>2017-04-09T16:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-04-09:/2017/04/switched-to-lineage-os/</id><summary type="html">&lt;p&gt;I have been a long time user of &lt;a href="https://en.wikipedia.org/wiki/CyanogenMod"&gt;Cyanogenmod&lt;/a&gt;, 
which discontinued its services end of 2016. Due to lack of (continuous) time, I was not
able to switch over toward a different ROM. Also, I wasn't sure if
&lt;a href="https://www.lineageos.org/"&gt;LineageOS&lt;/a&gt; would remain the best choice for me or not. I wanted
to review other ROMs for my Samsung Galaxy SIII (the i9300 model) phone.&lt;/p&gt;
&lt;p&gt;Today, I made my choice and installed LineageOS.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I have been a long time user of &lt;a href="https://en.wikipedia.org/wiki/CyanogenMod"&gt;Cyanogenmod&lt;/a&gt;, 
which discontinued its services end of 2016. Due to lack of (continuous) time, I was not
able to switch over toward a different ROM. Also, I wasn't sure if
&lt;a href="https://www.lineageos.org/"&gt;LineageOS&lt;/a&gt; would remain the best choice for me or not. I wanted
to review other ROMs for my Samsung Galaxy SIII (the i9300 model) phone.&lt;/p&gt;
&lt;p&gt;Today, I made my choice and installed LineageOS.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The requirements list&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When looking for new ROMs to use, I had a number of requirements, some must-have, others
should-have or would-have (using the &lt;a href="https://en.wikipedia.org/wiki/MoSCoW_method"&gt;MoSCoW method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First of all, I want the ROM to be installable through ClockworkMod 6.4.0.something. This
is a mandatory requirement, because I don't want to venture out in installing a different
recovery (like &lt;a href="https://twrp.me/"&gt;TWRP&lt;/a&gt;). Not that much that I'm scared from it, but it might
require me to install stuff like Heimdal and update my SELinux policies on my system to allow
it to run, and has the additional risk that things still fail.&lt;/p&gt;
&lt;p&gt;I tried updating the recovery ROM in the past (a year or so ago) using the mobile application
approaches themselves (which require root access, that my phone had at the time) but it continuously
said that it failed and that I had to revert to the more traditional way of flashing the
recovery.&lt;/p&gt;
&lt;p&gt;Given that I know I need to upgrade within a day (and have other things planned today) I didn't
want to loose too much time in upgrading the recovery first.&lt;/p&gt;
&lt;p&gt;Second, the ROM had to allow OTA updates. With CyanogenMod, the OTA didn't fully work on
my phone (it downloaded and verified the images correctly, but couldn't install it
automatically - I had to reboot in recovery manually and install the ZIP), but it worked
sufficiently for me to easily update the phone on a weekly basis. I wanted to keep this luxury,
and who knows, move towards an end-to-end working OTA.&lt;/p&gt;
&lt;p&gt;Furthermore, the ROM had to support Android 7.1. I want the latest Android to see how long
this (nowadays aged) phone can handle things. Once the phone cannot get the latest Android
anymore, I'll probably move towards a new phone. But as long as I don't have to, I'll put
my money in other endeavours ;-)&lt;/p&gt;
&lt;p&gt;Finally, the ROM must be in active development. One of the reasons I want the latest Android
is also because I want to keep receiving the necessary security fixes. If a ROM doesn't
actively follow the security patches and code, then it might become (too) vulnerable for
comfort.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ROMs, ROMs everywhere (?)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, I visited the &lt;a href="https://forum.xda-developers.com/galaxy-s3/development"&gt;Galaxy S3 discussion&lt;/a&gt;
on the XDA-Developers site. This often contains enough material to find ROMs which have 
a somewhat active development base.&lt;/p&gt;
&lt;p&gt;I was still positively surprised by the activity on this quite old phone (the i9300 was
first released in May, 2012, making this phone almost 5 years old).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://forum.xda-developers.com/galaxy-s3/development/vanir-aosp-t3568393"&gt;Vanir&lt;/a&gt;
mod seemed to imply that TWRP was required, but past articles on Vanir showed that CWM
should also work. However, from the discussion I gathered that it is based on LineageOS.
Not that that's bad, but it makes LineageOS the "preferred" ROM first (default installed
software list, larger upstream community, etc.)&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://forum.resurrectionremix.com/"&gt;Ressurrection Remix&lt;/a&gt;
shows a very active discussion with good feedback from the developer(s). It is based on
a number of other resources (including CyanogenMod), so seems to borrow and implement
various other features. Although I got the slight impression that it would be a bit more
filled with applications I might not want, I kept it on the short-list.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forum.xda-developers.com/galaxy-s3/development/slimrom-t3580824"&gt;SLIMROM&lt;/a&gt; is
based on AOSP (the Android Open Source Project). It doesn't seem to support OTA though,
and its release history is currently still premature. However, I will keep an eye on this
one for future reference.&lt;/p&gt;
&lt;p&gt;After a while, I started looking for ROMs based on AOSP, as the majority of ROMs shown
are based on LineageOS (abbreviated to LOS). Apparently, for the Samsung S3, LineageOS
seems to be one of the most popular sources (and ROMs).&lt;/p&gt;
&lt;p&gt;So I put my attention to LineageOS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It &lt;a href="https://lineageosrom.org/install-lineageos-cwm/"&gt;supports CWM installations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It offers OTA update support&lt;/li&gt;
&lt;li&gt;It closely tracks upstream&lt;/li&gt;
&lt;li&gt;It is in active development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, why not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using LineageOS without root&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While deciding to use LineageOS or go through with additional ROM seeking, I stumbled
upon the installation instructions that showed that the ROM can be installed without
automatically enabling rooted Android access. I'm not sure if this was the case with
Cyanogenmod (I've been running with a rooted Cyanogenmod for too long to remember) but
it opened a possiblity for me...&lt;/p&gt;
&lt;p&gt;Personally, I don't mind having a rooted phone, as long as it is the user who decides
which applications can get root access and which can't. For me, the two applications
that used root access was an open source ad blocker called &lt;a href="https://adaway.org/"&gt;AdAway&lt;/a&gt;
and the Android shell (for troubleshooting purposes, such as killing the media server
if it locked my camera).&lt;/p&gt;
&lt;p&gt;But some applications seem to think that a rooted phone automatically means that the
phone is open access and full of malware. It is hard to find any trustworthy, academical
research on the actual secure state of rooted versus non-rooted devices. I believe 
that proper application vetting (don't install applications that aren't popular
and long-existing, check the application vendors, etc.) and keeping your phone up-to-date
is much more important than not rooting.&lt;/p&gt;
&lt;p&gt;And although these applications happily function on old, unpatched Android 4.x devices
they refuse to function on my (rooted) Android 7.1 phone. So, the ability to install
LineageOS without root (rooting actually requires flashing an additional package) is
a nice thing as I can start with a non-rooted device first, and switch back to a rooted
device if I need it later.&lt;/p&gt;
&lt;p&gt;With that, I decided to flash my phone with the latest LineageOS nightly for my phone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Switching password manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I tend to use such ROM switches (or, in case of CyanogenMod, major version upgrades)
as a time to revisit the mobile application list, and reduce it to what I really used
the last few months.&lt;/p&gt;
&lt;p&gt;One of the changes I did on my mobile application list is switch the password application.
I used to use &lt;a href="https://play.google.com/store/apps/details?id=ebeletskiy.gmail.com.passwords"&gt;Remember Passwords&lt;/a&gt;
but it hasn't seen updates for quite some time, and the backup import failed last
time I migrated to a higher CyanogenMod version (possibly Android version related).
Because I don't want to synchronize the passwords or see the application have any Internet
oriented activity, I now use &lt;a href="https://play.google.com/store/apps/details?id=keepass2android.keepass2android_nonet"&gt;Keepass2Android Offline&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is for passwords which I don't auto-generate using &lt;a href="https://chriszarate.github.io/supergenpass/"&gt;SuperGenPass&lt;/a&gt;,
my favorite password manager. I don't use the bookmarklet approach myself, but download
and run it separately when generating passwords - or use a &lt;a href="https://play.google.com/store/apps/details?id=info.staticfree.SuperGenPass&amp;amp;hl=en"&gt;SuperGenPass mobile application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First impressions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is too soon to say if it is fully functional or not. Most standard functionality
works OK (phone, SMS, camera) but it is only after a few days that specific issues
can come up.&lt;/p&gt;
&lt;p&gt;Only the first boot was very slow (probably because it was optimizing the application
list in the background), the second boot was well below half a minute. I didn't count it,
but it's fast enough for me.&lt;/p&gt;</content><category term="Misc"></category><category term="cyanogenmod"></category><category term="lineageos"></category><category term="mobile"></category><category term="android"></category></entry><entry><title>cvechecker 3.8 released</title><link href="https://blog.siphos.be/2017/03/cvechecker-3.8-released/" rel="alternate"></link><published>2017-03-27T19:00:00+02:00</published><updated>2017-03-27T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-27:/2017/03/cvechecker-3.8-released/</id><summary type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;


&lt;p&gt;It is recommended to remove any historical files from &lt;code&gt;/var/lib/cvechecker/cache&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rm /var/lib/cvechecker/cache/nvdcve-2.0-2017.*
~# rm /var/lib/cvechecker/cache/nvdcve-2.0-modified.*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will make sure that the next run of &lt;code&gt;pullcves pull&lt;/code&gt; will re-download those files, and attempt to load
the resulting CVEs back in the database.&lt;/p&gt;
&lt;p&gt;Sorry for this issue :-(&lt;/p&gt;</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Handling certificates in Gentoo Linux</title><link href="https://blog.siphos.be/2017/03/handling-certificates-in-gentoo-linux/" rel="alternate"></link><published>2017-03-06T22:20:00+01:00</published><updated>2017-03-06T22:20:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-06:/2017/03/handling-certificates-in-gentoo-linux/</id><summary type="html">&lt;p&gt;I recently created a new article on the Gentoo Wiki titled &lt;a href="https://wiki.gentoo.org/wiki/Certificates"&gt;Certificates&lt;/a&gt;
which talks about how to handle certificate stores on Gentoo Linux. The write-up
of the article (which might still change name later, because it does not handle
&lt;em&gt;everything&lt;/em&gt; about certificates, mostly how to handle certificate stores) was
inspired by the observation that I had to adjust the certificate stores of both
Chromium and Firefox separately, even though they both use NSS.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I recently created a new article on the Gentoo Wiki titled &lt;a href="https://wiki.gentoo.org/wiki/Certificates"&gt;Certificates&lt;/a&gt;
which talks about how to handle certificate stores on Gentoo Linux. The write-up
of the article (which might still change name later, because it does not handle
&lt;em&gt;everything&lt;/em&gt; about certificates, mostly how to handle certificate stores) was
inspired by the observation that I had to adjust the certificate stores of both
Chromium and Firefox separately, even though they both use NSS.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Certificates?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, when a secure communication is established from a browser to a site (or any
other interaction that uses SSL/TLS, but let's stay with the browser example for now)
part of the exchange is to ensure that the target site is actually the site it claims
to be. Don't want someone else to trick you into giving your e-mail credentials do you?&lt;/p&gt;
&lt;p&gt;To establish this, the certificate presented by the remote site is validated (alongside
other &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake"&gt;handshake steps&lt;/a&gt;).
A certificate contains a public key, as well as information about what the certificate can
be used for, and who (or what) the certificate represents. In case of a site, the identification
is (or should be) tied to the fully qualified domain name.&lt;/p&gt;
&lt;p&gt;Of course, everyone could create a certificate for accounts.google.com and try to trick you
into leaving your credentials. So, part of the validation of a certificate is to verify that
it is signed by a third party that you trust to only sign certificates that are trustworthy.
And to validate this signature, you hence need the certificate of this third party as well.&lt;/p&gt;
&lt;p&gt;So, what about this certificate? Well, turns out, this one is also often signed by
another certificate, and so on, until you reach the "top" of the certificate tree. This top
certificate is called the "root certificate". And because we still have to establish that this
certificate is trustworthy, we need another way to accomplish this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enter certificate stores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The root certificates of these trusted third parties (well, let us call them "Certificate Authorities"
from now onward, because they &lt;a href="https://en.wikipedia.org/wiki/DigiNotar"&gt;sometimes will lose your trust&lt;/a&gt;)
need to be reachable by the browser. The location where they are stored in is (often) called
the truststore (a naming that I came across when dealing with Java and which stuck).&lt;/p&gt;
&lt;p&gt;So, what I wanted to accomplish was to remove a particular CA certificate from the certificate
store. I assumed that, because Chromium and Firefox both use NSS as the library to support their
cryptographic uses, they would also both use the store location at &lt;code&gt;~/.pki/nssdb&lt;/code&gt;. That was wrong.&lt;/p&gt;
&lt;p&gt;Another assumption I had was that NSS also uses the &lt;code&gt;/etc/pki/nssdb&lt;/code&gt; location as a system-wide one.
Wrong again (not that NSS doesn't allow this, but it seems that it is very much up to, and often
ignored by, the NSS-implementing applications).&lt;/p&gt;
&lt;p&gt;Oh, and I also assumed that there wouldn't be a hard-coded list in the application. Yup. Wrong again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How NSS tracks root CA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Basically, NSS has a hard-coded root CA list inside the &lt;code&gt;libnssckbi.so&lt;/code&gt; file. On Gentoo, this
file is provided by the &lt;code&gt;dev-libs/nss&lt;/code&gt; package. Because it is hard-coded, it seemed like there
was little I could do to remove it, yet still through the user interfaces offered by Firefox and
Chromium I was able to remove the trust bits from the certificate.&lt;/p&gt;
&lt;p&gt;Turns out that Firefox (inside &lt;code&gt;~/.mozilla/firefox/*.default&lt;/code&gt;) and Chromium (inside &lt;code&gt;~/.pki/nssdb&lt;/code&gt;)
store the (modified) trust bits for those locations, so that the hardcoded list does not need to
be altered if all I want to do was revoke the trust on a specific CA. And it isn't that this hard-coded
list is a bad list: Mozilla has a &lt;a href="https://www.mozilla.org/en-US/about/governance/policies/security-group/certs/"&gt;CA Certificate Program&lt;/a&gt;
which controls the CAs that are accepted inside this store.&lt;/p&gt;
&lt;p&gt;Still, I find it sad that the system-wide location (at &lt;code&gt;/etc/pki/nssdb&lt;/code&gt;) is not by default used as
well (or I have something wrong on my system that makes it so). On a multi-user system, administrators
who want to have some control over the certificate stores might need to either use login scripts to
manipulate the user certificate stores, or adapt the user files directly currently.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="certificates"></category><category term="nss"></category></entry><entry><title>cvechecker 3.7 released</title><link href="https://blog.siphos.be/2017/03/cvechecker-3.7-released/" rel="alternate"></link><published>2017-03-02T10:00:00+01:00</published><updated>2017-03-02T10:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-03-02:/2017/03/cvechecker-3.7-released/</id><summary type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What has been changed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, what has changed? With this release (now at version 3.7) two bugs have been
fixed, one having a wrong URL in the CVE download and the other about the CVE
sequence numbers.&lt;/p&gt;
&lt;p&gt;The first bug was an annoying one, which I should have fixed a long time ago.
Well, it was fixed in the repository, but I didn't make a new release for it. 
When downloading the &lt;code&gt;nvdcve-2.0-Modified.xml&lt;/code&gt; file, the &lt;code&gt;pullcves&lt;/code&gt; command used
the lowercase filename, which doesn't exist.&lt;/p&gt;
&lt;p&gt;The second bug is about parsing the CVE sequence. On &lt;a href="https://cve.mitre.org/cve/identifiers/syntaxchange.html"&gt;January 2014&lt;/a&gt;
the syntax changed to allow for sequence identifiers longer than 4 digits. The
cvechecker tool however did a hard validation on the length of the identifier,
and cut off longer fields.&lt;/p&gt;
&lt;p&gt;That means that some CVE reports failed to parse in cvechecker, and thus cvechecker
didn't "know" about these vulnerabilities. This has been fixed in this release,
although I am not fully satisfied...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What still needs to be done&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The codebase for cvechecker is from 2010, and is actually based on a prototype
that I wrote which I decided not to rewrite into proper code. As a result, the
code is not up to par.&lt;/p&gt;
&lt;p&gt;I'm going to gradually improve and clean up the code in the next few [insert
timeperiod here]. I don't know if there will be feature improvements in the
next few releases (not that there aren't many feature enhancements needed) but
I hope that, once the code is improved, new functionality can be added more
easily.&lt;/p&gt;
&lt;p&gt;But that's for another time. Right now, enjoy the new release.&lt;/p&gt;</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>I missed FOSDEM</title><link href="https://blog.siphos.be/2017/02/i-missed-fosdem/" rel="alternate"></link><published>2017-02-07T17:06:00+01:00</published><updated>2017-02-07T17:06:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-02-07:/2017/02/i-missed-fosdem/</id><content type="html">&lt;p&gt;I sadly had to miss out on the FOSDEM event. The entire weekend was filled with
me being apathetic, feverish and overall zombie-like. Yes, sickness can be cruel.
It wasn't until today that I had the energy back to fire up my laptop.&lt;/p&gt;
&lt;p&gt;Sorry for the crew that I promised to meet at FOSDEM. I'll make it up, somehow.&lt;/p&gt;
</content><category term="Misc"></category><category term="gentoo"></category><category term="fosdem"></category></entry><entry><title>SELinux System Administration, 2nd Edition</title><link href="https://blog.siphos.be/2016/12/selinux-system-administration-2nd-edition/" rel="alternate"></link><published>2016-12-22T19:26:00+01:00</published><updated>2016-12-22T19:26:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-12-22:/2016/12/selinux-system-administration-2nd-edition/</id><content type="html">&lt;p&gt;While still working on a few other projects, one of the time consumers of the
past half year (haven't you noticed? my blog was quite silent) has come to an
end: the &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
book is now available. With almost double the amount of pages and a serious
update of the content, the book can now be bought either through Packt Publishing
itself, or the various online bookstores such as &lt;a href="https://www.amazon.com/SELinux-System-Administration-Sven-Vermeulen-ebook/dp/B01LWM02WI"&gt;Amazon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the holidays now approaching, I hope to be able to execute a few tasks
within the Gentoo community (and of the Gentoo Foundation) and get back on track.
Luckily, my absence was not jeopardizing the state of &lt;a href="https://wiki.gentoo.org/wiki/SELinux"&gt;SELinux&lt;/a&gt;
in Gentoo thanks to the efforts of Jason Zaman.&lt;/p&gt;
</content><category term="SELinux"></category><category term="selinux"></category><category term="gentoo"></category><category term="rhel"></category><category term="redhat"></category><category term="packt"></category><category term="book"></category><category term="publishing"></category></entry><entry><title>GnuPG: private key suddenly missing?</title><link href="https://blog.siphos.be/2016/10/gnupg-private-key-suddenly-missing/" rel="alternate"></link><published>2016-10-12T18:56:00+02:00</published><updated>2016-10-12T18:56:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-10-12:/2016/10/gnupg-private-key-suddenly-missing/</id><summary type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can't find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can&amp;#39;t find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;GnuPG upgrade to 2.1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The update I did also upgraded GnuPG to the 2.1 series. This version has &lt;a href="https://www.gnupg.org/faq/whats-new-in-2.1.html"&gt;quite
a few updates&lt;/a&gt;, one of which is
a change towards a new private key storage approach. I thought that it might have
done a wrong conversion, or that the key which was used was of a particular method
or strength that suddenly wasn't supported anymore (PGP-2 is mentioned in the
article).&lt;/p&gt;
&lt;p&gt;But the key is a relatively standard RSA4096 one. Yet still, when I listed my
private keys, I did not see this key. I even tried to re-import the &lt;code&gt;secring.gpg&lt;/code&gt;
file, but it only found private keys that it already saw previously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I'm blind - the key never disappeared&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Luckily, when I tried to sign something with the key, &lt;code&gt;gpg-agent&lt;/code&gt; still asked me
for the passphraze that I had used for a while on that key. So it isn't gone. What
happened?&lt;/p&gt;
&lt;p&gt;Well, the key id is not my private key id, but the key id of one of the subkeys.
Previously, &lt;code&gt;gpg-agent&lt;/code&gt; sought and found the private key associated with the subkey,
but now it no longer does. I don't know if this is a bug in the past that I accidentally
used, or if this is a bug in the new version. I might investigate that a bit more,
but right now I'm happy that I found it.&lt;/p&gt;
&lt;p&gt;All I had to do was use the right key id in keychain, and things worked again.&lt;/p&gt;
&lt;p&gt;Good, now I can continue debugging networking issues with an azure-hosted system...&lt;/p&gt;</content><category term="Free-Software"></category><category term="gnupg"></category></entry><entry><title>We do not ship SELinux sandbox</title><link href="https://blog.siphos.be/2016/09/we-do-not-ship-selinux-sandbox/" rel="alternate"></link><published>2016-09-27T20:47:00+02:00</published><updated>2016-09-27T20:47:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-09-27:/2016/09/we-do-not-ship-selinux-sandbox/</id><summary type="html">&lt;p&gt;A few days ago a vulnerability was reported in the SELinux sandbox user space
utility. The utility is part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. Luckily, Gentoo's
&lt;code&gt;sys-apps/policycoreutils&lt;/code&gt; package is not vulnerable - and not because we were
clairvoyant about this issue, but because we don't ship this utility.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago a vulnerability was reported in the SELinux sandbox user space
utility. The utility is part of the &lt;code&gt;policycoreutils&lt;/code&gt; package. Luckily, Gentoo's
&lt;code&gt;sys-apps/policycoreutils&lt;/code&gt; package is not vulnerable - and not because we were
clairvoyant about this issue, but because we don't ship this utility.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is the SELinux sandbox?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux sandbox utility, aptly named &lt;code&gt;sandbox&lt;/code&gt;, is a simple C application which
executes its arguments, but only after ensuring that the task it launches is
going to run in the &lt;code&gt;sandbox_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;This domain is specifically crafted to allow applications most standard privileges
needed for interacting with the user (so that the user can of course still use the
application) but removes many permissions that might be abused to either obtain 
information from the system, or use to try and exploit vulnerabilities to gain
more privileges. It also hides a number of resources on the system through
namespaces.&lt;/p&gt;
&lt;p&gt;It was &lt;a href="http://danwalsh.livejournal.com/28545.html"&gt;developed in 2009&lt;/a&gt; for Fedora
and Red Hat. Given the necessary SELinux policy support though, it was usable on
other distributions as well, and thus became part of the SELinux user space itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is the vulnerability about?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux sandbox utility used an execution approach that did not shield off
the users' terminal access sufficiently. In the &lt;a href="http://www.openwall.com/lists/oss-security/2016/09/25/1"&gt;POC post&lt;/a&gt;
we notice that characters could be sent to the terminal through the &lt;code&gt;ioctl()&lt;/code&gt;
function (which executes the ioctl system call used for input/output operations
against devices) which are eventually executed when the application finishes.&lt;/p&gt;
&lt;p&gt;That's bad of course. Hence the CVE-2016-7545 registration, and of course also
a possible &lt;a href="https://github.com/SELinuxProject/selinux/commit/acca96a135a4d2a028ba9b636886af99c0915379"&gt;fix has been committed upstream&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why isn't Gentoo vulnerable / shipping with SELinux sandbox?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There's some history involved why Gentoo does not ship the SELinux sandbox (anymore).&lt;/p&gt;
&lt;p&gt;First of all, Gentoo already has a command that is called &lt;code&gt;sandbox&lt;/code&gt;, installed through
the &lt;code&gt;sys-apps/sandbox&lt;/code&gt; application. So back in the days that we still shipped with
the SELinux sandbox, we continuously had to patch &lt;code&gt;policycoreutils&lt;/code&gt; to use a
different name for the sandbox application (we used &lt;code&gt;sesandbox&lt;/code&gt; then).&lt;/p&gt;
&lt;p&gt;But then we had a couple of security issues with the SELinux sandbox application.
In 2011, &lt;a href="http://www.cvedetails.com/cve/CVE-2011-1011/"&gt;CVE-2011-1011&lt;/a&gt;
came up in which the &lt;code&gt;seunshare_mount&lt;/code&gt; function had a security issue. And in 2014,
&lt;a href="http://www.cvedetails.com/cve/CVE-2014-3215/"&gt;CVE-2014-3215&lt;/a&gt; came up with - again -
a security issue with &lt;code&gt;seunshare&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At that point, I had enough of this sandbox utility. First of all, it never quite worked
enough on Gentoo as it is (as it also requires a policy which is not part of the
upstream release) and given its wide open access approach (it was meant to contain
various types of workloads, so security concessions had to be made), I decided to
&lt;a href="http://blog.siphos.be/2014/05/dropping-sesandbox-support/"&gt;no longer support the SELinux sandbox in Gentoo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;None of the Gentoo SELinux users ever approached me with the question to add it back.&lt;/p&gt;
&lt;p&gt;And that is why Gentoo is not vulnerable to this specific issue.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="sandbox"></category><category term="gentoo"></category><category term="vulnerability"></category><category term="seunshare"></category></entry><entry><title>Mounting QEMU images</title><link href="https://blog.siphos.be/2016/09/mounting-qemu-images/" rel="alternate"></link><published>2016-09-26T19:26:00+02:00</published><updated>2016-09-26T19:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-09-26:/2016/09/mounting-qemu-images/</id><summary type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Mounting qemu images on the system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To resolve this, I want to mount the image on my system, and edit the &lt;code&gt;/etc/shadow&lt;/code&gt;
file so that the root account is accessible. Once that is accomplished, I can
log on through the console and start setting up the system further.&lt;/p&gt;
&lt;p&gt;Images that are in the qcow2 format can be mounted through the nbd driver, but that
would require some updates on my local SELinux policy that I am too lazy to do right
now (I'll get to them eventually, but first need to finish the book). Still, if you
are interested in using nbd, see &lt;a href="https://www.kumari.net/index.php/system-adminstration/49-mounting-a-qemu-image"&gt;these instructions&lt;/a&gt;
or a &lt;a href="https://forums.gentoo.org/viewtopic-t-822672.html"&gt;related thread&lt;/a&gt; on the Gentoo
Forums.&lt;/p&gt;
&lt;p&gt;Luckily, storage is cheap (even SSD disks), so I quickly converted the qcow2 images
into raw images:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ qemu-img convert root.qcow2 root.raw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the image now available in raw format, I can use the loop devices to mount
the image(s) on my system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# losetup /dev/loop0 root.raw
~# kpartx -a /dev/loop0
~# mount /dev/mapper/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;kpartx&lt;/code&gt; command will detect the partitions and ensure that those are
available: the first partition becomes available at &lt;code&gt;/dev/loop0p1&lt;/code&gt;, the
second &lt;code&gt;/dev/loop0p2&lt;/code&gt; and so forth.&lt;/p&gt;
&lt;p&gt;With the image now mounted, let's update the &lt;code&gt;/etc/shadow&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Placing a new password hash in the shadow file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A google search quickly revealed that the following command generates
a shadow-compatible hash for a password:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl passwd -1 MyMightyPassword
$1$BHbMVz9i$qYHmULtXIY3dqZkyfW/oO.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The challenge wasn't to find the hash though, but to edit it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# vim /mnt/etc/shadow
vim: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The image that I downloaded used SELinux (of course), which meant that the &lt;code&gt;shadow&lt;/code&gt;
file was labeled with &lt;code&gt;shadow_t&lt;/code&gt; which I am not allowed to access. And I didn't
want to put SELinux in permissive mode just for this (sometimes I /do/ have some
time left, apparently).&lt;/p&gt;
&lt;p&gt;So I remounted the image, but now with the &lt;code&gt;context=&lt;/code&gt; mount option, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -o context=&amp;quot;system_u:object_r:var_t:s0: /dev/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now all files are labeled with &lt;code&gt;var_t&lt;/code&gt; which I do have permissions to edit. But
I also need to take care that the files that I edited get the proper label again.
There are a number of ways to accomplish this. I chose to create a &lt;code&gt;.autorelabel&lt;/code&gt;
file in the root of the partition. Red Hat based distributions will pick this up
and force a file system relabeling operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unmounting the file system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After making the changes, I can now unmount the file system again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# umount /mnt
~# kpart -d /dev/loop0
~# losetup -d /dev/loop0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that done, I had root access to the image and could start testing out
my own set of commands.&lt;/p&gt;
&lt;p&gt;It did trigger my interest in the cloud-init setup though...&lt;/p&gt;</content><category term="Free-Software"></category><category term="qemu"></category></entry><entry><title>Comparing Hadoop with mainframe</title><link href="https://blog.siphos.be/2016/06/comparing-hadoop-with-mainframe/" rel="alternate"></link><published>2016-06-15T20:55:00+02:00</published><updated>2016-06-15T20:55:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-06-15:/2016/06/comparing-hadoop-with-mainframe/</id><summary type="html">&lt;p&gt;At my work, I have the pleasure of being involved in a big data project that
uses Hadoop as the primary platform for several services. As an architect, I
try to get to know the platform's capabilities, its potential use cases, its
surrounding ecosystem, etc. And although the implementation at work is not in
its final form (yay agile infrastructure releases) I do start to get a grasp of
where we might be going.&lt;/p&gt;
&lt;p&gt;For many analysts and architects, this Hadoop platform is a new kid on the block
so I have some work explaining what it is and what it is capable of. Not for the
fun of it, but to help the company make the right decisions, to support management
and operations, to lift the fear of new environments. One thing I've once said is
that "Hadoop is the poor man's mainframe", because I notice some high-level
similarities between the two.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;At my work, I have the pleasure of being involved in a big data project that
uses Hadoop as the primary platform for several services. As an architect, I
try to get to know the platform's capabilities, its potential use cases, its
surrounding ecosystem, etc. And although the implementation at work is not in
its final form (yay agile infrastructure releases) I do start to get a grasp of
where we might be going.&lt;/p&gt;
&lt;p&gt;For many analysts and architects, this Hadoop platform is a new kid on the block
so I have some work explaining what it is and what it is capable of. Not for the
fun of it, but to help the company make the right decisions, to support management
and operations, to lift the fear of new environments. One thing I've once said is
that "Hadoop is the poor man's mainframe", because I notice some high-level
similarities between the two.&lt;/p&gt;


&lt;p&gt;Somehow, it stuck, and I was asked to elaborate. So why not bring these points
into a nice blog post :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The big fat disclaimer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, before embarking on this comparison, I would like to state that I am &lt;strong&gt;not&lt;/strong&gt;
saying that Hadoop offers the same services, or even quality and functionality
of what can be found in mainframe environments. Considering how much time, effort
and experience was already put in the mainframe platform, it would be strange if
Hadoop could match the same. This post is to seek some similarities and, who knows,
learn a few more tricks from one or another.&lt;/p&gt;
&lt;p&gt;Second, I am not an avid mainframe knowledgeable person. I've been involved as
an IT architect in database and workload automation technical domains, which also
spanned the mainframe parts of it, but most of the effort was within the distributed
world. Mainframes remain somewhat opaque to me. Still, that shouldn't prevent me
from making any comparisons for those areas that I do have some grasp on.&lt;/p&gt;
&lt;p&gt;And if my current understanding is just wrong, I'm sure that I'll learn from the
comments that you can leave behind!&lt;/p&gt;
&lt;p&gt;With that being said, here it goes...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reliability, Availability, Serviceability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's start with some of the promises that both platforms make - and generally are
also able to deliver. Those promises are of reliability, availability and serviceability.&lt;/p&gt;
&lt;p&gt;For the mainframe platform, these quality attributes are shown as the &lt;a href="https://www.ibm.com/support/knowledgecenter/zosbasics/com.ibm.zos.zmainframe/zconc_RAS.htm"&gt;mainframe strengths&lt;/a&gt;.
The platform's hardware has extensive self-checking and self-recovery
capabilities, the systems can recover from failed components without service
interruption, and failures can be quickly determined and resolved. On the mainframes,
this is done through a good balance and alignment of hardware and software, design
decisions and - in my opinion - tight control over the various components and
services.&lt;/p&gt;
&lt;p&gt;I notice the same promises on Hadoop. Various components are checking the state
of the hardware and other components, and when something fails, it is often 
automatically recovered without impacting services. Instead of tight control
over the components and services, Hadoop uses a service architecture and APIs
with Java virtual machine abstractions.&lt;/p&gt;
&lt;p&gt;Let's consider hardware changes. &lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;hardware failure and component substitutions&lt;/strong&gt;, both platforms are capable
of dealing with those without service disruption.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mainframe probably has a better reputation in this matter, as its components
  have a very high Mean Time Between Failure (MTBF), and many - if not all - of
  the components are set up in a redundant fashion. Lots of error detection and
  failure detection processes try to detect if a component is close to failure,
  and ensure proper transitioning of any workload towards the other components
  without impact.&lt;/li&gt;
&lt;li&gt;Hadoop uses redundancy on a server level. If a complete server fails, Hadoop
  is usually able to deal with this without impact. Either the sensor-like
  services disable a node before it goes haywire, or the workload and data that
  was running on the failed node is restarted on a different node. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hardware (component) failures on the mainframe side will not impact the services
and running transactions. Component failures on Hadoop might have a noticeable
impact (especially if it is OLTP-like workload), but will be quickly recovered.&lt;/p&gt;
&lt;p&gt;Failures are more likely to happen on Hadoop clusters though, as it was designed
to work with many systems that have a worse MTBF design than a mainframe. The
focus within Hadoop is on resiliency and fast recoverability. Depending on the
service that is being used, active redundancy can be in use (so disruptions are
not visible to the user).&lt;/p&gt;
&lt;p&gt;If the Hadoop workload includes anything that resembles online transactional
processing, you're still better off with enterprise-grade hardware such as ECC
memory to at least allow improved hardware failure detection (and perform
proactive workload management). CPU failures are not that common (at least not
those without any upfront Machine Check Exception - MCE), and disk/controller
failures are handled through the abstraction of HDFS anyway.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;system substitutions&lt;/strong&gt;, I think both platforms can deal with this in a
dynamic fashion as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the mainframe side (and I'm guessing here) it is possible to switch
  machines with no service impact &lt;em&gt;if&lt;/em&gt; the services are running on LPARs
  that are joined together in a Parallel Sysplex setup (sort-of clustering
  through the use of the Coupling Facilities of mainframe, which is supported
  through high-speed data links and services for handling data sharing and
  IPC across LPARs). My company 
  &lt;a href="https://www-03.ibm.com/press/us/en/pressrelease/47812.wss"&gt;switched to the z13 mainframe&lt;/a&gt;
  last year, and was able to keep core services available during the migration.&lt;/li&gt;
&lt;li&gt;For Hadoop systems, the redundancy on system level is part of its design.
  Extending clusters, removing nodes, moving services, ... can be done with
  no impact. For instance, switching the active HiveServer2 instance means
  de-registering it in the ZooKeeper service. New client connects are then no
  longer served by that HiveServer2 instance, while active client connections
  remain until finished.
  There are also in-memory data grid solutions such as through the Ignite
  project, allowing for data sharing and IPC across nodes, as well as
  building up memory-based services with Arrow, allowing for efficient
  memory transfers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, also &lt;strong&gt;application level code failures&lt;/strong&gt; tend to only disrupt that
application, and not the other users. Be it because of different address
spaces and tight runtime control (mainframe) or the use of different
containers / JVMs for the applications (Hadoop), this is a good feat to have
(even though it is not something that differentiates these platforms from
other platforms or operating systems).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let's talk workloads&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When we look at a mainframe setup, we generally look at different workload
patterns as well. There are basically two main workload approaches for the
mainframe: batch, and On-Line Transactional Processing (OLTP) workload. In
the OLTP type, there is often an additional distinction between synchronous
OLTP and asynchronous OLTP (usually message-based). &lt;/p&gt;
&lt;p&gt;Well, we have the same on Hadoop. It was once a pure batch-driven platform
(and many of its components are still using batches or micro-batches in their
underlying designs) but now also provides OLTP workload capabilities. Most of
the OLTP workload on Hadoop is in the form of SQL-like or NoSQL database
management systems with transaction manager support though.&lt;/p&gt;
&lt;p&gt;To manage these (different) workloads, and to deal with prioritization of
the workload, both platforms offer the necessary services to make things both
managed as well as business (or "fit for purpose") focused.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using the Workload Manager (WLM) on the mainframe, policies can be set on
  the workload classes so that an over-demand of resources (cross-LPARs) results
  in the "right" amount of allocations for the "right" workload. To actually 
  manage jobs themselves, the Job Entry Subsystem (JES) to receive jobs and
  schedule then for processing on z/OS. For transactional workload, WLM
  provides the right resources to for instance the involved IMS regions.&lt;/li&gt;
&lt;li&gt;On Hadoop, workload management is done through Yet Another Resource 
  Negotiator (YARN), which uses (logical) queues for the different workloads.
  Workload (Application Containers) running through these queues can be, 
  resource-wise, controlled both on the queue level (high-level resource
  control) as well as process level (low-level resource control) through
  the use of Linux Control Groups (CGroups - when using Linux based systems
  course).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If I would try to compare both against each other, one might say that the
YARN queues are like WLMs service classes, and for batch applications, the
initiators on mainframe are like the Application Containers within YARN
queues. The latter can also be somewhat compared to IMS regions in case
of long-running Application Containers.&lt;/p&gt;
&lt;p&gt;The comparison will not hold completely though. WLM can be tuned based on
goals and will do dynamic decision making on the workloads depending on its
parameters, and even do live adjustments on the resources (through the 
System Resources Manager - SRM). Heavy focus on workload management on
mainframe environments is feasible because extending the available resources
on mainframes is usually expensive (additional Million Service Units - MSU).
On Hadoop, large cluster users who notice resource contention just tend
to extend the cluster further. It's a different approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Files and file access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another thing that tends to confuse some new users on Hadoop is its approach
to files. But when you know some things about the mainframe, this does remain
understandable.&lt;/p&gt;
&lt;p&gt;Both platforms have a sort-of master repository where data sets (mainframe)
or files (Hadoop) are registered in. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the mainframe, the catalog translates data set names into the right
  location (or points to other catalogs that do the same)&lt;/li&gt;
&lt;li&gt;On Hadoop, the Hadoop Distributed File System (HDFS) NameNode is
  responsible for tracking where files (well, blocks) are located across
  the various systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considering the use of the repository, both platforms thus require the
allocation of files and offer the necessary APIs to work with them. But
this small comparison does not end here.&lt;/p&gt;
&lt;p&gt;Depending on what you want to store (or access), the file format you use
is important as well.
- On mainframe, Virtual Storage Access Method (VSAM) provides both the
  methods (think of it as API) as well as format for a particular data
  organization. Inside a VSAM, multiple data entries can be stored in a
  structured way. Besides VSAM, there is also Partitioned Data Set/Extended
  (PDSE), which is more like a directory of sorts. Regular files are Physical
  Sequential (PS) data sets.
- On Hadoop, a number of file formats are supported which optimize the use of
  the files across the services. One is Avro, which holds both methods and
  format (not unlike VSAM), another is Optimized Row Columnar (ORC).  HDFS also
  has a number of options that can be enabled or set on certain locations (HDFS
  uses a folder-like structure) such as encryption, or on files themselves,
  such as replication factor.&lt;/p&gt;
&lt;p&gt;Although I don't say VSAM versus Avro are very similar (Hadoop focuses more on
the concept of files and then the file structure, whereas mainframe focuses on
the organization and allocation aspect if I'm not mistaken) they seem to be
sufficiently similar to get people's attention back on the table.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services all around&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What makes a platform tick is its multitude of supported services. And even
here can we find similarities between the two platforms.&lt;/p&gt;
&lt;p&gt;On mainframe, DBMS services can be offered my a multitude of softwares.
Relational DBMS services can be provided by IBM DB2, CA Datacom/DB, NOMAD, ...
while other database types are rendered by titles such as CA IDMS and ADABAS.
All these titles build upon the capabilities of the underlying components and
services to extend the platform's abilities.&lt;/p&gt;
&lt;p&gt;On Hadoop, several database technologies exist as well. Hive offers a SQL layer
on top of Hadoop managed data (so does Drill btw), HBase is a non-relational
database (mainly columnar store), Kylin provides distributed analytics, MapR-DB
offers a column-store NoSQL database, etc.&lt;/p&gt;
&lt;p&gt;When we look at transaction processing, the mainframe platform shows its decades
of experience with solutions such as CICS and IMS. Hadoop is still very much
at its infancy here, but with projects such as Omid or commercial software solutions
such as Splice Machine, transactional processing is coming here as well. Most
of these are based on underlying database management systems which are extended with
transactional properties.&lt;/p&gt;
&lt;p&gt;And services that offer messaging and queueing are also available on both
platforms: mainframe can enjoy Tibco Rendezvous and IBM WebSphere MQ, while
Hadoop is hitting the news with projects such as Kafka and Ignite.&lt;/p&gt;
&lt;p&gt;Services extend even beyond the ones that are directly user facing. For instance,
both platforms can easily be orchestrated using workload automation tooling.
Mainframe has a number of popular schedulers up its sleeve (such as IBM TWS,
BMC Control-M or CA Workload Automation) whereas Hadoop is generally easily
extended with the scheduling and workload automation software of the distributed
world (which, given its market, is dominated by the same vendors, although many
smaller ones exist as well). Hadoop also has its "own" little scheduling
infrastructure called Oozie.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Programming for the platforms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Platforms however are more than just the sum of the services and the properties
that it provides. Platforms are used to build solutions on, and that is true for
both mainframe as well as Hadoop.&lt;/p&gt;
&lt;p&gt;Let's first look at scripting - using interpreted languages. On mainframe, you
can use the Restructed Extended Executor (REXX) or CLIST (Command LIST). Hadoop
gives you Tez and Pig, as well as Python and R (through PySpark and SparkR).&lt;/p&gt;
&lt;p&gt;If you want to directly interact with the systems, mainframe offers the Time
Sharing Option/Extensions (TSO/E) and Interactive System Productivity Facility
(ISPF). For Hadoop, regular shells can be used, as well as service-specific
ones such as Spark shell. However, for end users, web-based services such 
as Ambari UI (Ambari Views) are generally better suited.&lt;/p&gt;
&lt;p&gt;If you're more fond of compiled code, mainframe supports you with COBOL, Java
(okay, it's "a bit" interpreted, but also compiled - don't shoot me here), C/C++
and all the other popular programming languages. Hadoop builds on top of Java,
but supports other languages such as Scala and allows you to run native
applications as well - it's all about using the right APIs.&lt;/p&gt;
&lt;p&gt;To support development efforts, Integrated Development Environments (IDEs) are
provided for both platforms as well. You can use Cobos, Micro Focus Enterprise
Developer, Rational Developer for System z, Topaz Workbench and more for
mainframe development. Hadoop has you covered with web-based notebook solutions
such as Zeppelin and JupyterHub, as well as client-level IDEs such as Eclipse
(with the Hadoop Development Tools plugins) and IntelliJ.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Governing and managing the platforms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, there is also the aspect of managing the platforms.&lt;/p&gt;
&lt;p&gt;When working on the mainframe, management tooling such as the Hardware
Management Console (HMC) and z/OS Management Facility (z/OSMF) cover operations
for both hardware and system resources. On Hadoop, central management
software such as Ambari, Cloudera Manager or Zettaset Orchestrator try to
cover the same needs - although most of these focus more on the software
side than on the hardware level.&lt;/p&gt;
&lt;p&gt;Both platforms also have a reasonable use for multiple roles: application
developers, end users, system engineers, database adminstrators, operators, 
system administrators, production control, etc. who all need some kind of access
to the platform to support their day-to-day duties. And when you talk roles,
you talk authorizations.&lt;/p&gt;
&lt;p&gt;On the mainframe, the Resource Access Control Facility (RACF) provides
access control and auditing facilities, and supports a multitude of services on
the mainframe (such as DB2, MQ, JES, ...). Many major Hadoop services, such
as HDFS, YARN, Hive and HBase support Ranger, providing a single pane for
security controls on the Hadoop platform.&lt;/p&gt;
&lt;p&gt;Both platforms also offer the necessary APIs or hooks through which system
developers can fine-tune the platform to fit the needs of the business, or
develop new integrated solutions - including security oriented ones. 
Hadoop's extensive plugin-based design (not explicitly
named) or mainframe's Security Access Facility (SAF) are just examples of this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Playing around&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Going for a mainframe or a Hadoop platform will always be a management decision.
Both platforms have specific roles and need particular profiles in order to
support them. They are both, in my opinion, also difficult to migrate away from
once you are really using them actively (lock-in) although it is more digestible
for Hadoop given its financial implications.&lt;/p&gt;
&lt;p&gt;Once you want to start meddling with it, getting access to a full platform used
to be hard (the coming age of cloud services makes that this is no longer the
case though), and both therefore had some potential "small deployment" uses.
Mainframe experience could be gained through the Hercules 390 emulator, whereas
most Hadoop distributions have a single-VM sandbox available for download.&lt;/p&gt;
&lt;p&gt;To do a full scale roll-out however is much harder to do by your own. You'll
need to have quite some experience or even expertise on so many levels that 
you will soon see that you need teams (plural) to get things done.&lt;/p&gt;
&lt;p&gt;This concludes my (apparently longer than expected) write-down of this matter.
If you don't agree, or are interested in some insights, be sure to comment!&lt;/p&gt;</content><category term="Hadoop"></category><category term="hadoop"></category><category term="mainframe"></category></entry><entry><title>Template was specified incorrectly</title><link href="https://blog.siphos.be/2016/03/template-was-specified-incorrectly/" rel="alternate"></link><published>2016-03-27T13:32:00+02:00</published><updated>2016-03-27T13:32:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-27:/2016/03/template-was-specified-incorrectly/</id><summary type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv 'testing', u'salt://top.sls'
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to 'merge' and multiple top files found. Top file merging order is undefined; for better results use 'same' option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what's all this about?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv &amp;#39;testing&amp;#39;, u&amp;#39;salt://top.sls&amp;#39;
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to &amp;#39;merge&amp;#39; and multiple top files found. Top file merging order is undefined; for better results use &amp;#39;same&amp;#39; option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So what's all this about?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;When using a single top file is preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to stick with a single top file, then the first error is (or at least, in my case)
caused by my environments not having a fall-back definition.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;/etc/salt/master&lt;/code&gt; configuration file had the following &lt;code&gt;file_roots&lt;/code&gt; setting:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The problem is that Salt expects ''a'' top file through the environment. What I had to do was to
set the fallback directory to the base directory again, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
    - /srv/salt/base
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this set, the error disappeared and both salt and myself were happy again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When multiple top files are preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you really want to use multiple top files (which is also a use case in my configuration),
then first we need to make sure that the top files of all environments correctly isolate the
minion matches. If two environments would match the same minion, then this approach becomes
more troublesome.&lt;/p&gt;
&lt;p&gt;On the one hand, we can just let saltstack merge the top files (default behavior) but the order
of the merging is undefined (and no, you can't set it using &lt;code&gt;env_order&lt;/code&gt;) which might result in 
salt states being executed in an unexpected order. If the definitions are done to such an extend
that this is not a problem, then you can just ignore the warning. See also
&lt;a href="https://github.com/saltstack/salt/issues/29104"&gt;bug 29104&lt;/a&gt; about the warning itself.&lt;/p&gt;
&lt;p&gt;But better would be to have the top files of the environment(s) isolated so that each environment
top file completely manages the entire environment. When that is the case, then we tell salt that
only the top file of the affected environment should be used. This is done using the following
setting in &lt;code&gt;/etc/salt/master&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;top_file_merging_strategy: same
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this is used, then the &lt;code&gt;env_order&lt;/code&gt; setting is used to define in which order the environments
are processed. &lt;/p&gt;
&lt;p&gt;Oh and if you're using &lt;code&gt;salt-ssh&lt;/code&gt;, then be sure to set the environment of the minion in the roster
file, as there is no running minion on the target system that informs salt about the environment 
to use otherwise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# In /etc/salt/roster
testserver:
  host: testserver.example.com
  minion_opts:
    environment: testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free-Software"></category><category term="salt"></category></entry><entry><title>Using salt-ssh with agent forwarding</title><link href="https://blog.siphos.be/2016/03/using-salt-ssh-with-agent-forwarding/" rel="alternate"></link><published>2016-03-26T19:57:00+01:00</published><updated>2016-03-26T19:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-26:/2016/03/using-salt-ssh-with-agent-forwarding/</id><summary type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Using SSH agent forwarding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anyway, I quickly got stuck with accessing minions over the SSH interface as it seemed that
salt requires its own SSH keys (I don't enable password-only authentication, most of the systems
use the &lt;a href="https://blog.flameeyes.eu/2013/03/openssh-6-2-adds-support-for-two-factor-authentication"&gt;AuthenticationMethods&lt;/a&gt;
approach to chain both key and passwords). But first things first, the current target uses regular
ssh key authentication (no chained approach, that's for later). But I don't want to assign
such a powerful key to my salt master (especially not if it would later also document the
passwords). I would like to use SSH agent forwarding.&lt;/p&gt;
&lt;p&gt;Luckily, salt does support that, it just &lt;a href="https://github.com/saltstack/salt/pull/31328/commits/024439186a0c51c0ac1242b38d6584d2abd1a534"&gt;forgot to document&lt;/a&gt;
it. Basically, what you need to do is update the roster file with the &lt;code&gt;priv:&lt;/code&gt; parameter
set to &lt;code&gt;agent-forwarding&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;myminion:
  host: myminion.example.com
  priv: agent-forwarding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It will use the &lt;code&gt;known_hosts&lt;/code&gt; file of the currently logged on user (the one executing
the &lt;code&gt;salt-ssh&lt;/code&gt; command) so make sure that the system's key is already known.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ salt-ssh myminion test.ping
myminion:
    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free-Software"></category><category term="salt"></category></entry><entry><title>Trying out imapsync</title><link href="https://blog.siphos.be/2016/03/trying-out-imapsync/" rel="alternate"></link><published>2016-03-13T12:57:00+01:00</published><updated>2016-03-13T12:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2016-03-13:/2016/03/trying-out-imapsync/</id><summary type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What I required&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The migration required that all mails, except for the spam and trash e-mails, were
migrated to another mail server. The migrated mails had to retain their status flags
(so unread mails had to remain unread while read mails had to remain read), and the
migration had to be done in two waves: one while the primary mail server was still
in use (where most of the mails where synchronized) and then, after switching the
mail servers (which was done through DNS changes) re-sync to fetch the final ones.&lt;/p&gt;
&lt;p&gt;I did not get access to the credentials of all mail boxes, but together with the
main administrator we enabled a sort-of shadow authentication system (a temporary
OpenLDAP installation) in which the same users were enabled, but with passwords that
will be used during the synchronization. The mailservers were then configured to
have a secondary interface available which used this OpenLDAP rather than the primary
authentication that was being used by the end users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using imapsync&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;imapsync&lt;/code&gt; is simple. It is a command-line application, and everything
configurable is done through command arguments. The basic ones are of course the
source and target definitions, as well as the authentication information for both
sides.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync \
  --host1 src-host --user1 src-user --password1 src-pw --authmech1 LOGIN --ssl1 \
  --host2 dst-host --user2 dst-user --password2 dst-pw --authmech2 LOGIN --ssl2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The use of the &lt;code&gt;--ssl1&lt;/code&gt; and &lt;code&gt;--ssl2&lt;/code&gt; is not to enable an older or newer version of
the SSL/TLS protocol. It just enables the use of SSL/TLS for the source host (&lt;code&gt;--ssl1&lt;/code&gt;)
and destination host (&lt;code&gt;--ssl2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This would just start synchronizing messages, but we need to include the necessary
directives to skip trash and spam mailboxes for instance. For this, the &lt;code&gt;--exclude&lt;/code&gt; parameter
can be used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --exclude &amp;quot;Trash|Spam|Drafts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is also possible to transform some mailbox names. For instance, if the source host
uses &lt;code&gt;Sent&lt;/code&gt; as the mailbox for sent mail, while the target has &lt;code&gt;Sent Items&lt;/code&gt;, then the
following would enable migrating mails between the right folders:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --folder &amp;quot;Sent&amp;quot; --regextrans2 &amp;#39;s/Sent/Sent Items/&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Conclusions and interesting resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the application was a breeze. I do recommend to create a test account on both sides
so that you can easily see the available folders, source and target naming conventions as
well as test if rerunning the application works flawlessly.&lt;/p&gt;
&lt;p&gt;In my case for instance, I had to add &lt;code&gt;--skipsize&lt;/code&gt; so that the application does not use the
mail sizes for comparing if a mail is already transferred or not, as the target mailserver
showed different mail sizes for the same mails. This was luckily often documented on the
various online tutorials about &lt;code&gt;imapsync&lt;/code&gt;, such as &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://seagrief.co.uk/2010/12/moving-to-google-apps-with-imapsync/"&gt;Moving to Google Apps with imapsync&lt;/a&gt; on seagrief.co.uk&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.zimbra.com/wiki/Guide_to_imapsync"&gt;Guide to imapsync&lt;/a&gt; on wiki.zimbra.com&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The migration took a while, but without major issues. Within a few hours, the mailboxes of all
users where correctly migrated.&lt;/p&gt;</content><category term="Free-Software"></category><category term="imapsync"></category></entry><entry><title>New cvechecker release</title><link href="https://blog.siphos.be/2015/11/new-cvechecker-release/" rel="alternate"></link><published>2015-11-07T11:07:00+01:00</published><updated>2015-11-07T11:07:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-11-07:/2015/11/new-cvechecker-release/</id><content type="html">&lt;p&gt;A short while ago I got the notification that pulling new CVE information was
no longer possible. The reason was that the NVD site did not support uncompressed
downloads anymore. The fix for cvechecker was simple, and it also gave me a reason
to push out a new release (after two years) which also includes various updates by
Christopher Warner.&lt;/p&gt;
&lt;p&gt;So &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker 3.6&lt;/a&gt; is now available
for general consumption.&lt;/p&gt;
</content><category term="Free-Software"></category><category term="cvechecker"></category></entry><entry><title>Switching focus at work</title><link href="https://blog.siphos.be/2015/09/switching-focus-at-work/" rel="alternate"></link><published>2015-09-20T13:29:00+02:00</published><updated>2015-09-20T13:29:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-20:/2015/09/switching-focus-at-work/</id><summary type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;


&lt;p&gt;This means looking at the investments related to those domains, implementation
details, standards of use, features that we will or will not use, positioning
of products and so forth. To do this from an architecture point of view means
that I not only had to focus on the details of the technology and understand 
all their use, but also become a sort-of subject matter expert on those topics.
Luckily, I had (well, still have) great teams of DBAs (for the databases) and
batch teams (for the scheduling/workload automation) to keep things in the right
direction. &lt;/p&gt;
&lt;p&gt;I helped them with a (hopefully sufficiently) clear roadmap, investment track,
procurement, contract/terms and conditions for use, architectural decisions and
positioning and what not. And they helped me with understanding the various
components, learn about the best use of these, and of course implement the 
improvements that we collaboratively put on the roadmap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Times, they are changing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last week, I flipped over a page at work. Although I remain an IT architect
within the same architecture team, my focus shifts entirely. Instead of a fixed
domain, my focus is now more volatile. I leave behind the stability of 
organizationally anchored technology domains and go forward in a more tense
environment.&lt;/p&gt;
&lt;p&gt;Instead of looking at just two technology domains, I need to look at all of them,
and find the right balance between high flexibility demands (which might not want
to use current "standard" offerings) which come up from a very agile context, and
the almost non-negotionable requirements that are typical for financial institutions.&lt;/p&gt;
&lt;p&gt;The focus is also not primarily technology oriented anymore. I'll be part of an 
enterprise architecture team with direct business involvement and although my
main focus will be on the technology side, it'll also involve information
management, business processes and applications.&lt;/p&gt;
&lt;p&gt;The end goal is to set up a future-proof architecture in an agile, fast-moving
environment (contradictio in terminis ?) which has a main focus in data analytics
and information gathering/management. Yes, "big data", but more applied than what
some of the vendors try to sell us ;-)&lt;/p&gt;
&lt;p&gt;I'm currently finishing off the high-level design and use of a Hadoop platform,
and the next focus will be on a possible micro-service architecture using Docker.
I've been working on this Hadoop design for a while now (but then it was together
with my previous function at work) and given the evolving nature of Hadoop (and
the various services that surround it) I'm confident that it will not be the last
time I'm looking at it. &lt;/p&gt;
&lt;p&gt;Now let me hope I can keep things manageable ;-)&lt;/p&gt;</content><category term="Architecture"></category><category term="work"></category><category term="hadoop"></category><category term="docker"></category></entry><entry><title>Getting su to work in init scripts</title><link href="https://blog.siphos.be/2015/09/getting-su-to-work-in-init-scripts/" rel="alternate"></link><published>2015-09-14T16:37:00+02:00</published><updated>2015-09-14T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-14:/2015/09/getting-su-to-work-in-init-scripts/</id><summary type="html">&lt;p&gt;While developing an init script which has to switch user, I got a couple of
errors from SELinux and the system itself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~# rc-service hadoop-namenode format&lt;/span&gt;
&lt;span class="go"&gt;Authenticating root.&lt;/span&gt;
&lt;span class="go"&gt; * Formatting HDFS ...&lt;/span&gt;
&lt;span class="go"&gt;su: Authentication service cannot retrieve authentication info&lt;/span&gt;
&lt;span class="gp gp-VirtualEnv"&gt;(Ignored)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</summary><content type="html">&lt;p&gt;While developing an init script which has to switch user, I got a couple of
errors from SELinux and the system itself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~# rc-service hadoop-namenode format&lt;/span&gt;
&lt;span class="go"&gt;Authenticating root.&lt;/span&gt;
&lt;span class="go"&gt; * Formatting HDFS ...&lt;/span&gt;
&lt;span class="go"&gt;su: Authentication service cannot retrieve authentication info&lt;/span&gt;
&lt;span class="gp gp-VirtualEnv"&gt;(Ignored)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;The authentication log shows entries such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sep 14 20:20:05 localhost unix_chkpwd[5522]: could not obtain user info (hdfs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I've always had issues with getting su to work properly again. Now that I have
what I think is a working set, let me document it for later (as I still need to
review why they are needed):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Allow initrc_t to use unix_chkpwd to check entries&lt;/span&gt;
&lt;span class="c1"&gt;# Without it gives the retrieval failure&lt;/span&gt;
&lt;span class="n"&gt;auth_domtrans_chk_passwd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Allow initrc_t to query selinux access, otherwise avc assertion&lt;/span&gt;
&lt;span class="n"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;self&lt;/span&gt;&lt;span class="ss"&gt;:netlink_selinux_socket&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="n"&gt;selinux_compute_access_vector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Allow initrc_t to honor the pam_rootok setting&lt;/span&gt;
&lt;span class="n"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;initrc_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;self&lt;/span&gt;&lt;span class="ss"&gt;:passwd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;passwd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rootok&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With these SELinux rules, switching the user works as expected from within an
init script.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="initrc"></category></entry><entry><title>Custom CIL SELinux policies in Gentoo</title><link href="https://blog.siphos.be/2015/09/custom-cil-selinux-policies-in-gentoo/" rel="alternate"></link><published>2015-09-10T07:13:00+02:00</published><updated>2015-09-10T07:13:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-10:/2015/09/custom-cil-selinux-policies-in-gentoo/</id><summary type="html">&lt;p&gt;In Gentoo, we have been supporting &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;custom policy packages&lt;/a&gt;
for a while now. Unlike most other distributions, which focus on binary packages,
Gentoo has always supported source-based packages as default (although 
&lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary packages&lt;/a&gt; are 
supported as well).&lt;/p&gt;
&lt;p&gt;A recent &lt;a href="https://gitweb.gentoo.org/repo/gentoo.git/commit/?id=8f2aa45db35bbf3a74f8db09ece9edac60e79ee4"&gt;commit&lt;/a&gt;
now also allows CIL files to be used.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In Gentoo, we have been supporting &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Tutorials/Creating_your_own_policy_module_file"&gt;custom policy packages&lt;/a&gt;
for a while now. Unlike most other distributions, which focus on binary packages,
Gentoo has always supported source-based packages as default (although 
&lt;a href="https://wiki.gentoo.org/wiki/Binary_package_guide"&gt;binary packages&lt;/a&gt; are 
supported as well).&lt;/p&gt;
&lt;p&gt;A recent &lt;a href="https://gitweb.gentoo.org/repo/gentoo.git/commit/?id=8f2aa45db35bbf3a74f8db09ece9edac60e79ee4"&gt;commit&lt;/a&gt;
now also allows CIL files to be used.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Policy ebuilds, how they work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gentoo provides its own SELinux policy, based on the &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Reference_policy"&gt;reference policy&lt;/a&gt;, 
and provides per-module ebuilds (packages). For instance, the SELinux policy for
the &lt;a href="https://packages.gentoo.org/package/app-misc/screen"&gt;screen&lt;/a&gt; package is
provided by the &lt;a href="https://packages.gentoo.org/package/sec-policy/selinux-screen"&gt;sec-policy/selinux-screen&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;The package itself is pretty straight forward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Copyright 1999-2015 Gentoo Foundation&lt;/span&gt;
&lt;span class="c1"&gt;# Distributed under the terms of the GNU General Public License v2&lt;/span&gt;
&lt;span class="c1"&gt;# $Id$&lt;/span&gt;
&lt;span class="nv"&gt;EAPI&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;IUSE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;MODS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;screen&amp;quot;&lt;/span&gt;

inherit&lt;span class="w"&gt; &lt;/span&gt;selinux-policy-2

&lt;span class="nv"&gt;DESCRIPTION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELinux policy for screen&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$PV&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;9999&lt;/span&gt;*&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;~amd64 ~x86&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The real workhorse lays within a &lt;a href="https://devmanual.gentoo.org/eclass-writing/"&gt;Gentoo eclass&lt;/a&gt;,
something that can be seen as a library for ebuilds. It allows consolidation of functions and
activities so that a large set of ebuilds can be simplified. The more ebuilds are standardized,
the more development can be put inside an eclass instead of in the ebuilds. As a result, some
ebuilds are extremely simple, and the SELinux policy ebuilds are a good example of this.&lt;/p&gt;
&lt;p&gt;The eclass for SELinux policy ebuilds is called &lt;a href="https://devmanual.gentoo.org/eclass-reference/selinux-policy-2.eclass/index.html"&gt;selinux-policy-2.eclass&lt;/a&gt;
and holds a number of functionalities. One of these (the one we focus on right now)
is to support custom SELinux policy modules.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom SELinux policy ebuilds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever a user has a SELinux policy that is not part of the Gentoo policy repository,
then the user might want to provide these policies through packages still. This has
the advantage that Portage (or whatever package manager is used) is aware of the
policies on the system, and proper dependencies can be built in.&lt;/p&gt;
&lt;p&gt;To use a custom policy, the user needs to create an ebuild which informs the eclass
not only about the module name (through the &lt;code&gt;MODS&lt;/code&gt; variable) but also about the
policy files themselves. These files are put in the &lt;code&gt;files/&lt;/code&gt; location of the ebuild,
and referred to through the &lt;code&gt;POLICY_FILES&lt;/code&gt; variable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Copyright 1999-2015 Gentoo Foundation&lt;/span&gt;
&lt;span class="c1"&gt;# Distributed under the terms of the GNU General Public License v2&lt;/span&gt;
&lt;span class="c1"&gt;# $Id$&lt;/span&gt;
&lt;span class="nv"&gt;EAPI&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;IUSE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;MODS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;oracle&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;POLICY_FILES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;oracle.te oracle.if oracle.fc&amp;quot;&lt;/span&gt;

inherit&lt;span class="w"&gt; &lt;/span&gt;selinux-policy-2

&lt;span class="nv"&gt;DESCRIPTION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELinux policy for screen&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$PV&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;9999&lt;/span&gt;*&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;KEYWORDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;~amd64 ~x86&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The eclass generally will try to build the policies, converting them into &lt;code&gt;.pp&lt;/code&gt;
files. With CIL, this is no longer needed. Instead, what we do is copy the &lt;code&gt;.cil&lt;/code&gt;
files straight into the location where we place the &lt;code&gt;.pp&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;From that point onwards, managing the &lt;code&gt;.cil&lt;/code&gt; files is similar to &lt;code&gt;.pp&lt;/code&gt; files.
They are loaded with &lt;code&gt;semodule -i&lt;/code&gt; and unloaded with &lt;code&gt;semodule -r&lt;/code&gt; when needed.&lt;/p&gt;
&lt;p&gt;Enabling CIL in our ebuilds is a small improvement (after the heavy workload
to support the 2.4 userspace) which allows Gentoo to stay ahead in the SELinux
world.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="cil"></category><category term="selinux"></category><category term="ebuild"></category><category term="eclass"></category></entry><entry><title>Using multiple OpenSSH daemons</title><link href="https://blog.siphos.be/2015/09/using-multiple-openssh-daemons/" rel="alternate"></link><published>2015-09-06T16:37:00+02:00</published><updated>2015-09-06T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-06:/2015/09/using-multiple-openssh-daemons/</id><summary type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Running multiple instances&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although I might get a similar result with a single OpenSSH instance, I
prefer to have multiple instances for this. The default OpenSSH port is used
for the non-administrative access whereas administrative access is on a
non-default port. This has a number of advantages...&lt;/p&gt;
&lt;p&gt;First of all, the SSH configurations are simple and clean. No complex
configurations, and more importantly: easy to manage through configuration
management tools like &lt;a href="http://saltstack.com/"&gt;SaltStack&lt;/a&gt;, my current favorite
orchestration/automation tool.&lt;/p&gt;
&lt;p&gt;Different instances also allow for different operational support services.
There is different monitoring for end-user SSH access versus administrative
SSH access. Also the &lt;a href="https://wiki.gentoo.org/wiki/Fail2ban"&gt;fail2ban&lt;/a&gt; configuration
is different for these instances.&lt;/p&gt;
&lt;p&gt;I can also easily shut down the non-administrative service while ensuring that
administrative access remains operational - something important in case of
changes and maintenance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with multiple instances and SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Beyond enabling a non-default port for SSH (i.e. by marking it as &lt;code&gt;ssh_port_t&lt;/code&gt;
as well) there is little additional tuning necessary, but that doesn't mean that
there is no additional tuning possible.&lt;/p&gt;
&lt;p&gt;For instance, we could leverage MCS' categories to only allow users (and thus the
SSH daemon) access to the files assigned only that category (and not the rest)
whereas the administrative SSH daemon can access all categories.&lt;/p&gt;
&lt;p&gt;On an MLS enabled system we could even use different sensitivity levels, allowing
the administrative SSH to access the full scala whereas the user-facing SSH can
only access the lowest sensitivity level. But as I don't use MLS myself, I won't go
into detail for this.&lt;/p&gt;
&lt;p&gt;A third possibility would be to fine-tune the permissions of the SSH daemons. However,
that would require different types for the daemon, which requires the daemons to be
started through different scripts (so that we first transition to dedicated 
types) before they execute the SSHd binary (which has the &lt;code&gt;sshd_exec_t&lt;/code&gt; type
assigned).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requiring pubkey and password authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recent OpenSSH daemons allow &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining multiple authentication methods&lt;/a&gt;
before access is granted. This allows the systems to force SSH key authentication first, and then -
after succesful authentication - require the password to be passed on as well. Or a
second step such as &lt;a href="https://wiki.archlinux.org/index.php/Google_Authenticator"&gt;Google Authenticator&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;AuthenticationMethods publickey,password
PasswordAuthentication yes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I don't use the Google Authenticator, but the &lt;a href="https://developers.yubico.com/yubico-pam/"&gt;Yubico PAM module&lt;/a&gt;
to require additional authentication through my U2F dongle (so ssh key, password
and u2f key). Don't consider this three-factor authentication: one thing I know
(password) and two things I have (U2F and ssh key). It's more that I have a couple
of devices with a valid SSH key (laptop, tablet, mobile) which are of course targets
for theft.&lt;/p&gt;
&lt;p&gt;The chance that both one of those devices is stolen &lt;em&gt;together&lt;/em&gt; with the U2F
dongle (which I don't keep attached to those devices of course) is somewhat less.&lt;/p&gt;</content><category term="Free-Software"></category><category term="openssh"></category><category term="ssh"></category><category term="u2f"></category><category term="selinux"></category></entry><entry><title>Maintaining packages and backporting</title><link href="https://blog.siphos.be/2015/09/maintaining-packages-and-backporting/" rel="alternate"></link><published>2015-09-02T20:33:00+02:00</published><updated>2015-09-02T20:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-02:/2015/09/maintaining-packages-and-backporting/</id><summary type="html">&lt;p&gt;A few days ago I committed a small update to &lt;code&gt;policycoreutils&lt;/code&gt;, a SELinux related
package that provides most of the management utilities for SELinux systems. The
fix was to get two patches (which are committed upstream) into the existing
release so that our users can benefit from the fixed issues without having to
wait for a new release.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few days ago I committed a small update to &lt;code&gt;policycoreutils&lt;/code&gt;, a SELinux related
package that provides most of the management utilities for SELinux systems. The
fix was to get two patches (which are committed upstream) into the existing
release so that our users can benefit from the fixed issues without having to
wait for a new release.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Getting the patches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To capture the patches, I used &lt;code&gt;git&lt;/code&gt; together with the commit id:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ git format-patch -n -1 73b7ff41&lt;/span&gt;
&lt;span class="go"&gt;0001-Only-invoke-RPM-on-RPM-enabled-Linux-distributions.patch&lt;/span&gt;
&lt;span class="go"&gt;~$ git format-patch -n -1 4fbc6623&lt;/span&gt;
&lt;span class="go"&gt;0001-Set-self.sename-to-sename-after-calling-semanage_seu.patch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The two generated patch files contain all information about the commit. Thanks
to the &lt;code&gt;epatch&lt;/code&gt; support in the &lt;code&gt;eutils.eclass&lt;/code&gt;, these patch files are
immediately usable within Gentoo's ebuilds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Updating the ebuilds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux userspace ebuilds in Gentoo all have &lt;a href="http://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/"&gt;live ebuilds&lt;/a&gt;
available which are immediately usable for releases. The idea with those live
ebuilds is that we can simply copy them and commit in order to make a new release.&lt;/p&gt;
&lt;p&gt;So, in case of the patch backporting, the necessary patch files are first moved
into the &lt;code&gt;files/&lt;/code&gt; subdirectory of the package. Then, the live ebuild is updated
to use the new patches:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gu"&gt;@@ -88,6 +85,8 @@ src_prepare() {&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0070-remove-symlink-attempt-fails-with-gentoo-sandbox-approach.patch&amp;quot;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0110-build-mcstrans-bug-472912.patch&amp;quot;
&lt;span class="w"&gt; &lt;/span&gt;               epatch &amp;quot;${FILESDIR}/0120-build-failure-for-mcscolor-for-CONTEXT__CONTAINS.patch&amp;quot;
&lt;span class="gi"&gt;+               epatch &amp;quot;${FILESDIR}/0130-Only-invoke-RPM-on-RPM-enabled-Linux-distributions-bug-534682.patch&amp;quot;&lt;/span&gt;
&lt;span class="gi"&gt;+               epatch &amp;quot;${FILESDIR}/0140-Set-self.sename-to-sename-after-calling-semanage-bug-557370.patch&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;       fi

&lt;span class="w"&gt; &lt;/span&gt;       # rlpkg is more useful than fixfiles
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The patches themselves do not apply for the live ebuilds themselves (they are
ignored there) as we want the live ebuilds to be as close to the upstream
project as possible. But because the ebuilds are immediately usable for
releases, we add the necessary information there first.&lt;/p&gt;
&lt;p&gt;Next, the new release is created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ cp policycoreutils-9999.ebuild policycoreutils-2.4-r2.ebuild&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Testing the changes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new release is then tested. I have a couple of scripts that I use
for automated testing. So first I update these scripts to also try out
the functionality that was failing before. On existing systems, these
tests should fail:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Running task semanage (Various semanage related operations).
  ...
    Executing step &amp;quot;perm_port_on   : Marking portage_t as a permissive domain                              &amp;quot; -&amp;gt; ok
    Executing step &amp;quot;perm_port_off  : Removing permissive mark from portage_t                               &amp;quot; -&amp;gt; ok
    Executing step &amp;quot;selogin_modify : Modifying a SELinux login definition                                  &amp;quot; -&amp;gt; failed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, on a test system where the new package has been installed, the same
testset is executed (together with all other tests) to validate if the problem
is fixed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pushing out the new release&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, with the fixes in and validated, the new release is pushed out (into
~arch first of course) and the bugs are marked as &lt;code&gt;RESOLVED:TEST-REQUEST&lt;/code&gt;. Users
can confirm that it works (which would move it to &lt;code&gt;VERIFIED:TEST-REQUEST&lt;/code&gt;) or
we stabilize it after the regular testing period is over (which moves it to
&lt;code&gt;RESOLVED:FIXED&lt;/code&gt; or &lt;code&gt;VERIFIED:FIXED&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I do still have to get used to Gentoo using git as its repository now. The
&lt;a href="https://wiki.gentoo.org/wiki/Gentoo_git_workflow"&gt;workflow&lt;/a&gt; to use is
documented though. Luckily, because I often get that the &lt;code&gt;git push&lt;/code&gt; fails
(due to changes to the tree since my last pull). So I need to run &lt;code&gt;git
pull --rebase=preserve&lt;/code&gt; followed by &lt;code&gt;repoman full&lt;/code&gt; and then the push again
sufficiently quick after each other).&lt;/p&gt;
&lt;p&gt;This simple flow is easy to get used to. Thanks to the existing foundation
for package maintenance (such as &lt;code&gt;epatch&lt;/code&gt; for patching, live ebuilds that
can be immediately used for releases and the ability to just cherry pick
patches towards our repository) we can serve updates with just a few minutes
of work.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="ebuild"></category><category term="patching"></category></entry><entry><title>Doing away with interfaces</title><link href="https://blog.siphos.be/2015/08/doing-away-with-interfaces/" rel="alternate"></link><published>2015-08-29T11:30:00+02:00</published><updated>2015-08-29T11:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-29:/2015/08/doing-away-with-interfaces/</id><summary type="html">&lt;p&gt;CIL is SELinux' Common Intermediate Language, which brings on a whole new set of
possibilities with policy development. I hardly know CIL but am (slowly)
learning. Of course, the best way to learn is to try and do lots of things with
it, but real-life work and time-to-market for now forces me to stick with the
M4-based refpolicy one.&lt;/p&gt;
&lt;p&gt;Still, I do try out some things here and there, and one of the things I wanted
to look into was how CIL policies would deal with interfaces.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;CIL is SELinux' Common Intermediate Language, which brings on a whole new set of
possibilities with policy development. I hardly know CIL but am (slowly)
learning. Of course, the best way to learn is to try and do lots of things with
it, but real-life work and time-to-market for now forces me to stick with the
M4-based refpolicy one.&lt;/p&gt;
&lt;p&gt;Still, I do try out some things here and there, and one of the things I wanted
to look into was how CIL policies would deal with interfaces.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Recap on interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the M4 based reference policy, interfaces are M4 macros that expand into
the standard SELinux rules. They are used by the reference policy to provide 
a way to isolate module-specific code and to have "public" calls.&lt;/p&gt;
&lt;p&gt;Policy modules are not allowed (by convention) to call types or domains that
are not defined by the same module. If they want to interact with those modules,
then they need to call the interface(s):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# module &amp;quot;ntp&amp;quot;
# domtrans: when executing an ntpd_exec_t binary, the resulting process 
#           runs in ntpd_t
interface(`ntp_domtrans&amp;#39;,`
  domtrans_pattern($1, ntpd_exec_t, ntpd_t)
)

# module &amp;quot;hal&amp;quot;
ntp_domtrans(hald_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the purpose is to have &lt;code&gt;hald_t&lt;/code&gt; be able to execute
binaries labeled as &lt;code&gt;ntpd_exec_t&lt;/code&gt; and have the resulting process run as the
&lt;code&gt;ntpd_t&lt;/code&gt; domain.&lt;/p&gt;
&lt;p&gt;The following would not be allowed inside the hal module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;domtrans_pattern(hald_t, ntpd_exec_t, ntpd_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This would imply that both &lt;code&gt;hald_t&lt;/code&gt;, &lt;code&gt;ntpd_exec_t&lt;/code&gt; and &lt;code&gt;ntpd_t&lt;/code&gt; are defined
by the same module, which is not the case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interfaces in CIL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It &lt;em&gt;seems&lt;/em&gt; that CIL will not use interface files. Perhaps some convention
surrounding it will be created - to know this, we'll have to wait until a
"cilrefpolicy" is created. However, functionally, this is no longer necessary.&lt;/p&gt;
&lt;p&gt;Consider the &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; declaration from a &lt;a href="http://blog.siphos.be/2015/08/filtering-network-access-per-application/"&gt;previous post&lt;/a&gt;.
In it, we wanted to allow &lt;code&gt;mozilla_t&lt;/code&gt; to send and receive these packets. The 
example didn't use an interface-like construction for this, so let's see
how this would be dealt with.&lt;/p&gt;
&lt;p&gt;First, the module is slightly adjusted to create a &lt;em&gt;macro&lt;/em&gt; called &lt;code&gt;myhttp_sendrecv_client_packet&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;macro&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_sendrecv_client_packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;cil_gen_require&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Another module would then call this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;call&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_sendrecv_client_packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's it. When the policy modules are both loaded, then the &lt;code&gt;mozilla_t&lt;/code&gt; domain is able
to send and receive &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; labeled packets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's more: namespaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But it doesn't end there. Whereas the reference policy had a single namespace
for the interfaces, CIL is able to use namespaces. It allows to create an almost
object-like approach for policy development.&lt;/p&gt;
&lt;p&gt;The above &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; definition could be written as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;block&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;; MyHTTP client packet&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;roletype&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;object_r&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;client_packet_type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;packet_type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;macro&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;sendrecv_client_packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;cil_gen_require&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;client_packet_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The other module looks as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;block&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;mozilla&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;cil_gen_require&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;call&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp.sendrecv_client_packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is similar, but not fully the same. The packet is no longer called
&lt;code&gt;myhttp_client_packet_t&lt;/code&gt; but &lt;code&gt;myhttp.client_packet_t&lt;/code&gt;. In other words, a period (&lt;code&gt;.&lt;/code&gt;)
is used to separate the object name (&lt;code&gt;myhttp&lt;/code&gt;) and the object/type (&lt;code&gt;client_packet_t&lt;/code&gt;)
as well as interface/macro (&lt;code&gt;sendrecv_client_packet&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ sesearch -s mozilla_t -c packet -p send -Ad&lt;/span&gt;
&lt;span class="go"&gt;  ...&lt;/span&gt;
&lt;span class="go"&gt;  allow mozilla_t myhttp.client_packet_t : packet { send recv };&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And it looks that namespace support goes even further than that, but I still
need to learn more about it first.&lt;/p&gt;
&lt;p&gt;Still, I find this a good evolution. With CIL interfaces are no longer separate
from the module definition: everything is inside the CIL file. I secretly hope
that tools such as &lt;code&gt;seinfo&lt;/code&gt; would support querying macros as well.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="cil"></category></entry><entry><title>Slowly converting from GuideXML to HTML</title><link href="https://blog.siphos.be/2015/08/slowly-converting-from-guidexml-to-html/" rel="alternate"></link><published>2015-08-25T11:30:00+02:00</published><updated>2015-08-25T11:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-25:/2015/08/slowly-converting-from-guidexml-to-html/</id><summary type="html">&lt;p&gt;Gentoo has removed its support of the older GuideXML format in favor of using
the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; and a new content management system
for the main site (or is it static pages, I don't have the faintest idea to be
honest). I do still have a few GuideXML pages in my development space, which I
am going to move to HTML pretty soon.&lt;/p&gt;
&lt;p&gt;In order to do so, I make use of the &lt;a href="https://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl?view=log"&gt;guidexml2wiki&lt;/a&gt;
stylesheet I &lt;a href="http://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"&gt;developed&lt;/a&gt;.
But instead of migrating it to wiki syntax, I want to end with HTML.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Gentoo has removed its support of the older GuideXML format in favor of using
the &lt;a href="https://wiki.gentoo.org"&gt;Gentoo Wiki&lt;/a&gt; and a new content management system
for the main site (or is it static pages, I don't have the faintest idea to be
honest). I do still have a few GuideXML pages in my development space, which I
am going to move to HTML pretty soon.&lt;/p&gt;
&lt;p&gt;In order to do so, I make use of the &lt;a href="https://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl?view=log"&gt;guidexml2wiki&lt;/a&gt;
stylesheet I &lt;a href="http://blog.siphos.be/2013/02/transforming-guidexml-to-wiki/"&gt;developed&lt;/a&gt;.
But instead of migrating it to wiki syntax, I want to end with HTML.&lt;/p&gt;


&lt;p&gt;So what I do is first convert the file from GuideXML to MediaWiki with &lt;code&gt;xsltproc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, I use &lt;code&gt;pandoc&lt;/code&gt; to convert this to restructured text. The idea is that the main
pages on my devpage are now restructured text based. I was hoping to use markdown, but
the conversion from markdown to HTML is not what I hoped it was.&lt;/p&gt;
&lt;p&gt;The restructured text is then converted to HTML using &lt;code&gt;rst2html.py&lt;/code&gt;. In the end,
I use the following function (for conversion, once):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Convert GuideXML to RestructedText and to HTML&lt;/span&gt;
gxml2html&lt;span class="o"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="p"&gt;%%.xml&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# Convert to Mediawiki syntax&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;xsltproc&lt;span class="w"&gt; &lt;/span&gt;~/dev-cvs/gentoo/xml/htdocs/xsl/guidexml2wiki.xsl&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;# Convert to restructured text&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;pandoc&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;mediawiki&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;rst&lt;span class="w"&gt; &lt;/span&gt;-s&lt;span class="w"&gt; &lt;/span&gt;-S&lt;span class="w"&gt; &lt;/span&gt;-o&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mediawiki&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;# Use your own stylesheet links (use full https URLs for this)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;rst2html.py&lt;span class="w"&gt;  &lt;/span&gt;--stylesheet&lt;span class="o"&gt;=&lt;/span&gt;link-to-bootstrap.min.css,link-to-tyrian.min.css&lt;span class="w"&gt; &lt;/span&gt;--link-stylesheet&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.rst&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;basefile&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.html
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Is it perfect? No, but &lt;a href="http://dev.gentoo.org/~swift/snapshots/"&gt;it works&lt;/a&gt;.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="guidexml"></category><category term="xml"></category><category term="xslt"></category><category term="rst"></category><category term="mediawiki"></category><category term="html"></category></entry><entry><title>Making the case for multi-instance support</title><link href="https://blog.siphos.be/2015/08/making-the-case-for-multi-instance-support/" rel="alternate"></link><published>2015-08-22T12:45:00+02:00</published><updated>2015-08-22T12:45:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-22:/2015/08/making-the-case-for-multi-instance-support/</id><summary type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Examples of multi-instance technologies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first example that I'm pretty familiar with is multi-instance database
deployments: Oracle DBs, SQL Servers, PostgreSQLs, etc. The consolidation
of databases while still keeping multiple instances around (instead of
consolidating into a single instance itself) is mainly for operational 
reasons (changes should not influence other database/schema's) or
technical reasons (different requirements in parameters, locales, etc.)&lt;/p&gt;
&lt;p&gt;Other examples are web servers (for web hosting companies), which next to
virtual host support (which is still part of a single instance) could
benefit from multi-instance deployments for security reasons (vulnerabilities
might be better contained then) as well as performance tuning. Same goes
for web application servers (such as TomCat deployments).&lt;/p&gt;
&lt;p&gt;But even other technologies like mail servers can benefit from multiple
instance deployments. Postfix has a &lt;a href="http://www.postfix.org/MULTI_INSTANCE_README.html"&gt;nice guide&lt;/a&gt;
on multi-instance deployments and also covers some of the use cases for it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages of multi-instance setups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The primary objective that most organizations have when dealing with multiple
instances is the consolidation to reduce cost. Especially expensive, 
propriatary software which is CPU licensed gains a lot from consolidation 
(and don't think a CPU is a CPU, each company
&lt;a href="http://www-01.ibm.com/software/passportadvantage/pvu_licensing_for_customers.html"&gt;has&lt;/a&gt;
&lt;a href="http://www.oracle.com/us/corporate/contracts/processor-core-factor-table-070634.pdf"&gt;its&lt;/a&gt; (PDF)
&lt;a href="go.microsoft.com/fwlink/?LinkID=229882"&gt;own&lt;/a&gt; (PDF) core weight table to
get the most money out of their customers).&lt;/p&gt;
&lt;p&gt;But beyond cost savings, using multi-instance deployments also provides for
resource sharing. A high-end server can be used to host the multiple instances,
with for instance SSD disks (or even flash cards), more memory, high-end CPUs,
high-speed network connnectivity and more. This improves performance considerably,
because most multi-instance technologies don't need all resources continuously.&lt;/p&gt;
&lt;p&gt;Another advantage, if properly designed, is that multi-instance capable software
can often leverage the multi-instance deployments for fast changes. A database
might be easily patched (remove vulnerabilities) by creating a second codebase
deployment, patching that codebase, and then migrating the database from one
instance to another. Although it often still requires downtime, it can be made
considerably less, and roll-back of such changes is very easy.&lt;/p&gt;
&lt;p&gt;A last advantage that I see is security. Instances can be running as different
runtime accounts, through different SELinux contexts, bound on different
interfaces or chrooted into different locations. This is not an advantage
compared to dedicated systems of course, but more an advantage compared
to full consolidation (everything in a single instance).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don't always focus on multi-instance setups though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Multiple instances isn't a silver bullet. Some technologies are generally much
better when there is a single instance on a single operating system. Personally,
I find that such technologies should know better. If they are really designed to
be suboptimal in case of multi-instance deployments, then there is a design error.&lt;/p&gt;
&lt;p&gt;But when the advantages of multiple instances do not exist (no license cost,
hardware cost is low, etc.) then organizations might focus on single-instance
deployments, because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-instance deployments might require more users to access the system
  (especially when it is multi-tenant)&lt;/li&gt;
&lt;li&gt;operational activities might impact other instances (for instance updating 
  kernel parameters for one instance requires a reboot which affects other
  instances)&lt;/li&gt;
&lt;li&gt;the software might not be properly "multi-instance aware" and as such
  starts fighting for resources with its own sigbling instances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that properly designed architectures are well capable of using
virtualization (and in the future containerization) moving towards
single-instance deployments becomes more and more interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What should multi-instance software consider?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Software should, imo, always consider multi-instance deployments. Even
when the administrator decides to stick with a single instance, all that
that takes is that the software ends up with a "single instance" setup
(it is &lt;em&gt;much&lt;/em&gt; easier to support multiple instances and deploy a single one,
than to support single instances and deploy multiple ones).&lt;/p&gt;
&lt;p&gt;The first thing software should take into account is that it might (and
will) run with different runtime accounts - service accounts if you whish.
That means that the software should be well aware that file locations are
separate, and that these locations will have different access control settings
on them (if not just a different owner).&lt;/p&gt;
&lt;p&gt;So instead of using &lt;code&gt;/etc/foo&lt;/code&gt; as the mandatory location, consider supporting
&lt;code&gt;/etc/foo/instance1&lt;/code&gt;, &lt;code&gt;/etc/foo/instance2&lt;/code&gt; if full directories are needed, or
just have &lt;code&gt;/etc/foo1.conf&lt;/code&gt; and &lt;code&gt;/etc/foo2.conf&lt;/code&gt;. I prefer the directory approach,
because it makes management much easier. It then also makes sense that the log
location is &lt;code&gt;/var/log/foo/instance1&lt;/code&gt;, the data files are at &lt;code&gt;/var/lib/foo/instance1&lt;/code&gt;,
etc.&lt;/p&gt;
&lt;p&gt;The second is that, if a service is network-facing (which most of them
are), it must be able to either use multihomed systems easily (bind to
different interfaces) or use different ports. The latter is a challenge
I often come across with software - the way to configure the software to
deal with multiple deployments and multiple ports is often a lengthy
trial-and-error setup.&lt;/p&gt;
&lt;p&gt;What's so difficult with using a &lt;em&gt;base port&lt;/em&gt; setting, and document how the
other ports are derived from this base port. &lt;a href="http://neo4j.com/docs/stable/ha-setup-tutorial.html"&gt;Neo4J&lt;/a&gt;
needs 3 ports for its enterprise services (transactions, cluster management
and online backup), but they all need to be explicitly configured if you
want a multi-instance deployment. What if one could just set &lt;code&gt;baseport = 5001&lt;/code&gt;
with the software automatically selecting 5002 and 5003 as other ports (or 6001
and 7001). If the software in the future needs another port, there is no need
to update the configuration (assuming the administrator leaves sufficient room).&lt;/p&gt;
&lt;p&gt;Also consider the service scripts (&lt;code&gt;/etc/init.d&lt;/code&gt;) or similar (depending on the
init system used). Don't provide a single one which only deals with one instance.
Instead, consider supporting symlinked service scripts which automatically obtain
the right configuration from its name.&lt;/p&gt;
&lt;p&gt;For instance, a service script called &lt;code&gt;pgsql-inst1&lt;/code&gt; which is a symlink to
&lt;code&gt;/etc/init.d/postgresql&lt;/code&gt; could then look for its configuration in &lt;code&gt;/var/lib/postgresql/pgsql-inst1&lt;/code&gt;
(or &lt;code&gt;/etc/postgresql/pgsql-inst1&lt;/code&gt;). &lt;/p&gt;
&lt;p&gt;Just like supporting &lt;a href="http://blog.siphos.be/2013/05/the-linux-d-approach/"&gt;.d directories&lt;/a&gt;,
I consider multi-instance support an important non-functional requirement for software.&lt;/p&gt;</content><category term="Architecture"></category></entry><entry><title>Switching OpenSSH to ed25519 keys</title><link href="https://blog.siphos.be/2015/08/switching-openssh-to-ed25519-keys/" rel="alternate"></link><published>2015-08-19T18:26:00+02:00</published><updated>2015-08-19T18:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-19:/2015/08/switching-openssh-to-ed25519-keys/</id><summary type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The issue with DSA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You might find DSA a bit slower than RSA:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl speed rsa1024 rsa2048 dsa1024 dsa2048
...
                  sign    verify    sign/s verify/s
rsa 1024 bits 0.000127s 0.000009s   7874.0 111147.6
rsa 2048 bits 0.000959s 0.000029s   1042.9  33956.0
                  sign    verify    sign/s verify/s
dsa 1024 bits 0.000098s 0.000103s  10213.9   9702.8
dsa 2048 bits 0.000293s 0.000339s   3407.9   2947.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, RSA verification outperforms DSA in verification, while signing
with DSA is better than RSA. But for what OpenSSH is concerned, this speed
difference should not be noticeable on the vast majority of OpenSSH servers.&lt;/p&gt;
&lt;p&gt;So no, it is not the speed, but the secure state of the DSS standard.&lt;/p&gt;
&lt;p&gt;The OpenSSH developers find that &lt;a href="http://www.openssh.com/legacy.html"&gt;ssh-dss (DSA) is too weak&lt;/a&gt;,
which is followed by &lt;a href="http://meyering.net/nuke-your-DSA-keys/"&gt;various&lt;/a&gt; 
&lt;a href="https://docs.moodle.org/dev/SSH_key"&gt;sources&lt;/a&gt;. Considering the impact of these keys,
it is important that they follow the state-of-the-art cryptographic services. &lt;/p&gt;
&lt;p&gt;Instead, they suggest to switch to elliptic curve cryptography based algorithms,
with Ed25519 and &lt;a href="https://en.wikipedia.org/wiki/Curve25519"&gt;Curve25519&lt;/a&gt; coming out
on top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Switch to RSA or ED25519?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that RSA is still considered very secure, one of the questions is of
course if &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; is the right choice here or not.
I don't consider myself anything in cryptography, but I do like to validate stuff
through academic and (hopefully) reputable sources for information (not that I don't
trust the OpenSSH and OpenSSL folks, but more from a broader interest in the subject).&lt;/p&gt;
&lt;p&gt;Ed25519 should be written fully as &lt;em&gt;Ed25519-SHA-512&lt;/em&gt; and is a signature
algorithm. It uses elliptic curve cryptography as explained on the
&lt;a href="https://en.wikipedia.org/wiki/EdDSA"&gt;EdDSA wikipedia page&lt;/a&gt;. An often cited
paper is &lt;a href="http://aspartame.shiftleft.org/papers/fff/fff.pdf"&gt;Fast and compact elliptic-curve cryptography&lt;/a&gt;
by Mike Hamburg, which talks about the performance improvements, but the main
paper is called &lt;a href="http://ed25519.cr.yp.to/ed25519-20110705.pdf"&gt;High-speed high-security signatures&lt;/a&gt;
which introduces the Ed25519 implementation.&lt;/p&gt;
&lt;p&gt;Of the references I was able to (quickly) go through (not all papers are
publicly reachable) none showed any concerns about the secure state of the 
algorithm. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The (simple) process of switching&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Switching to Ed25519 is simple. First, generate the (new) SSH key (below
just an example run):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh-keygen -t ed25519
Generating public/private ed25519 key pair.
Enter file in which to save the key (/home/testuser/.ssh/id_ed25519): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/testuser/.ssh/id_ed25519.
Your public key has been saved in /home/testuser/.ssh/id_ed25519.pub.
The key fingerprint is:
SHA256:RDaEw3tNAKBGMJ2S4wmN+6P3yDYIE+v90Hfzz/0r73M testuser@testserver
The key&amp;#39;s randomart image is:
+--[ED25519 256]--+
|o*...o.+*.       |
|*o+.  +o ..      |
|o++    o.o       |
|o+    ... .      |
| +     .S        |
|+ o .            |
|o+.o . . o       |
|oo+o. . . o ....E|
| oooo.     ..o+=*|
+----[SHA256]-----+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, make sure that the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; file contains the public key
(as generated as &lt;code&gt;id_ed25519.pub&lt;/code&gt;). Don't remove the other keys yet until the
communication is validated. For me, all I had to do was to update the file in
the Salt repository and have the master push the changes to all nodes (starting
with non-production first of course).&lt;/p&gt;
&lt;p&gt;Next, try to log on to the system using the Ed25519 key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -i ~/.ssh/id_ed25519 testuser@testserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Make sure that your SSH agent is not running as it might still try to revert
back to another key if the Ed25519 one does not work. You can validate if the
connection was using Ed25519 through the &lt;code&gt;auth.log&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ sudo tail -f auth.log&lt;/span&gt;
&lt;span class="go"&gt;Aug 17 21:20:48 localhost sshd[13962]: Accepted publickey for root from \&lt;/span&gt;
&lt;span class="go"&gt;  192.168.100.1 port 43152 ssh2: ED25519 SHA256:-------redacted----------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this communication succeeds, then you can remove the old key from the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;On the client level, you might want to hide &lt;code&gt;~/.ssh/id_dsa&lt;/code&gt; from the SSH agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Obsolete - keychain ~/.ssh/id_dsa&lt;/span&gt;
keychain&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If a server update was forgotten, then the authentication will fail and, depending
on the configuration, either fall back to the regular authentication or fail
immediately. This gives a nice heads-up to you to update the server, while keeping
the key handy just in case. Just refer to the old &lt;code&gt;id_dsa&lt;/code&gt; key during the authentication
and fix up the server.&lt;/p&gt;</content><category term="Free-Software"></category><category term="openssh"></category><category term="ssh"></category><category term="gentoo"></category></entry><entry><title>Updates on my Pelican adventure</title><link href="https://blog.siphos.be/2015/08/updates-on-my-pelican-adventure/" rel="alternate"></link><published>2015-08-16T19:50:00+02:00</published><updated>2015-08-16T19:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-16:/2015/08/updates-on-my-pelican-adventure/</id><summary type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The full article view on index page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the features I wanted was to have my latest blog post to be fully
readable from the front page (called the &lt;em&gt;index&lt;/em&gt; page within Pelican). Sadly,
I could not find a plugin of setting that would do this, but I did find
a plugin that I can use to work around this: the &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/summary"&gt;summary&lt;/a&gt;
plugin.&lt;/p&gt;
&lt;p&gt;Enabling the plugin was a breeze. Extract the plugin sources in the &lt;code&gt;plugin/&lt;/code&gt;
folder, and enable it in &lt;code&gt;pelicanconf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;summary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this plug-in, articles can use inline comments to tell the system at which
point the summary of the article stops. Usually, the summary (which is displayed
on index pages) is a first paragraph (or set of paragraphs). What I do is I now
manually set the summmary to the entire blog post for the latest post, and adjust
later when a new post comes up.&lt;/p&gt;
&lt;p&gt;It might be some manual labour, but it fits nicely and doesn't hack around in the
code too much.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Commenting with Disqus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I had some remarks that the &lt;a href="https://disqus.com/home/welcome/"&gt;Disqus&lt;/a&gt; integration
is not as intuitive as expected. Some readers had difficulties finding out how
to comment as a guest (without the need to log on through popular social media
or through Disqus itself).&lt;/p&gt;
&lt;p&gt;Agreed, it is not easy to see at first sight that people need to start typing
their name in the &lt;em&gt;Or sign up with disqus&lt;/em&gt; before they can select &lt;em&gt;I'd rather post
as guest&lt;/em&gt;. As I don't have any way of controlling the format and rendered code
with Disqus, I updated the theme a bit to add in two paragraphs on commenting.
The first paragraph tells how to comment as guest.&lt;/p&gt;
&lt;p&gt;The second paragraph for now informs readers that non-verified comments are put
in the moderation queue. Once I get a feeling of how the spam and bots act on the
commenting system, I will adjust the filters and also allow guest comments to be
readily accessible (no moderation queue). Give it a few more weeks to get myself
settled and I'll adjust it.&lt;/p&gt;
&lt;p&gt;If the performance of the site is slowed down due to the Disqus javascripts: both
Firefox (excuse me, Aurora) and Chromium have this at the initial load. Later, the
scripts are properly cached and load in relatively fast (a quick test shows
all pages I tried load in less than 2 seconds - WordPress was at 4). And if you're
not interested in commenting, then you can even use &lt;a href="https://noscript.net/"&gt;NoScript&lt;/a&gt;
or similar plugins to disallow any remote javascript.&lt;/p&gt;
&lt;p&gt;Still, I will continue to look at how to make commenting easier. I recently allowed
unmoderated comments (unless a number of keywords are added, and comments with links
are also put in the moderation queue). If someone knows of another comment-like
system that I could integrate I'm happy to hear about it as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My issue with Tipue Search has been fixed by reverting a change in &lt;code&gt;tipue_search.py&lt;/code&gt;
(the plugin) where the URL was assigned to the &lt;code&gt;loc&lt;/code&gt; key instead of &lt;code&gt;url&lt;/code&gt;. It is
probably a mismatch between the plugin and the theme (the change of the key was done
in May in Tipue Search itself).&lt;/p&gt;
&lt;p&gt;With this minor issue changed, the search capabilities are back on track on my blog.
Enabling is was a matter of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;tipue_search&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;DIRECT_TEMPLATES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;search&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tags and categories&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;WordPress supports multiple categories, but Pelican does not. So I went through
the various posts that had multiple categories and decided on a single one. While
doing so, I also reduced the &lt;a href="http://blog.siphos.be/categories.html"&gt;categories&lt;/a&gt; to
a small set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Databases&lt;/li&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;Free Software&lt;/li&gt;
&lt;li&gt;Gentoo&lt;/li&gt;
&lt;li&gt;Misc&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;SELinux&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will try to properly tag all posts so that, if someone is interested in a very
particular topic, such as &lt;a href="http://blog.siphos.be/tag/postgresql/index.html"&gt;PostgreSQL&lt;/a&gt;, he can reach
those posts through the tag.&lt;/p&gt;</content><category term="Free-Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>Finding a good compression utility</title><link href="https://blog.siphos.be/2015/08/finding-a-good-compression-utility/" rel="alternate"></link><published>2015-08-13T19:15:00+02:00</published><updated>2015-08-13T19:15:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-13:/2015/08/finding-a-good-compression-utility/</id><summary type="html">&lt;p&gt;I recently came across a &lt;a href="http://catchchallenger.first-world.info//wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO"&gt;wiki page&lt;/a&gt;
written by &lt;a href="http://catchchallenger.first-world.info/wiki/User:Alpha_one_x86"&gt;Herman Brule&lt;/a&gt;
which gives a quick benchmark on a couple of compression methods / algorithms.
It gave me the idea of writing a quick script that tests out a wide number of
compression utilities available in Gentoo (usually through the &lt;code&gt;app-arch&lt;/code&gt;
category), with also a number of options (in case multiple options are
possible).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I recently came across a &lt;a href="http://catchchallenger.first-world.info//wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO"&gt;wiki page&lt;/a&gt;
written by &lt;a href="http://catchchallenger.first-world.info/wiki/User:Alpha_one_x86"&gt;Herman Brule&lt;/a&gt;
which gives a quick benchmark on a couple of compression methods / algorithms.
It gave me the idea of writing a quick script that tests out a wide number of
compression utilities available in Gentoo (usually through the &lt;code&gt;app-arch&lt;/code&gt;
category), with also a number of options (in case multiple options are
possible).&lt;/p&gt;


&lt;p&gt;The currently supported packages are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;app-arch/bloscpack      app-arch/bzip2          app-arch/freeze
app-arch/gzip           app-arch/lha            app-arch/lrzip
app-arch/lz4            app-arch/lzip           app-arch/lzma
app-arch/lzop           app-arch/mscompress     app-arch/p7zip
app-arch/pigz           app-arch/pixz           app-arch/plzip
app-arch/pxz            app-arch/rar            app-arch/rzip
app-arch/xar            app-arch/xz-utils       app-arch/zopfli
app-arch/zpaq
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The script should keep the best compression information: duration, compression
ratio, compression command, as well as the compressed file itself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the "best" compression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is not my intention to find the most optimal compression, as that would
require heuristic optimizations (which has triggered my interest in seeking
such software, or writing it myself) while trying out various optimization
parameters.&lt;/p&gt;
&lt;p&gt;No, what I want is to find the "best" compression for a given file, with "best"
being either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;most reduced size (which I call &lt;em&gt;compression delta&lt;/em&gt; in my script)&lt;/li&gt;
&lt;li&gt;best reduction obtained per time unit (which I call the &lt;em&gt;efficiency&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For me personally, I think I would use it for the various raw image files that
I have through the photography hobby. Those image files are difficult to
compress (the Nikon DS3200 I use is an entry-level camera which applies
lossy compression already for its raw files) but their total size is considerable,
and it would allow me to better use the storage I have available both on my
laptop (which is SSD-only) as well as backup server.&lt;/p&gt;
&lt;p&gt;But next to the best compression ratio, the efficiency is also an important
metric as it shows how efficient the algorithm works in a certain time aspect.
If one compression method yields 80% reduction in 5 minutes, and another one
yields 80,5% in 45 minutes, then I might want to prefer the first one even
though that is not the best compression at all.&lt;/p&gt;
&lt;p&gt;Although the script could be used to get the most compression (without
resolving to an optimization algorithm for the compression commands) for 
each file, this is definitely not the use case. A single run can take hours
for files that are compressed in a handful of seconds. But it can show the
best algorithms for a particular file type (for instance, do a few runs on
a couple of raw image files and see which method is most succesful).&lt;/p&gt;
&lt;p&gt;Another use case I'm currently looking into is how much improvement I can
get when multiple files (all raw image files) are first grouped in a single
archive (&lt;code&gt;.tar&lt;/code&gt;). Theoretically, this should improve the compression, but
by how much?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How the script works&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The script does not contain much intelligence. It iterates over a wide set of
compression commands that I tested out, checks the final compressed file size,
and if it is better than a previous one it keeps this compressed file (and
its statistics).&lt;/p&gt;
&lt;p&gt;I tried to group some of the compressions together based on the algorithm used,
but as I don't really know the details of the algorithms (it's based on manual
pages and internet sites) and some of them combine multiple algorithms, it is
more of a high-level selection than anything else.&lt;/p&gt;
&lt;p&gt;The script can also only run the compressions of a single application (which I
use when I'm fine-tuning the parameter runs).&lt;/p&gt;
&lt;p&gt;A run shows something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Original file (test.nef) size 20958430 bytes
      package name                                                 command      duration                   size compr.Δ effic.:
      ------------                                                 -------      --------                   ---- ------- -------
app-arch/bloscpack                                               blpk -n 4           0.1               20947097 0.00054 0.00416
app-arch/bloscpack                                               blpk -n 8           0.1               20947097 0.00054 0.00492
app-arch/bloscpack                                              blpk -n 16           0.1               20947097 0.00054 0.00492
    app-arch/bzip2                                                   bzip2           2.0               19285616 0.07982 0.03991
    app-arch/bzip2                                                bzip2 -1           2.0               19881886 0.05137 0.02543
    app-arch/bzip2                                                bzip2 -2           1.9               19673083 0.06133 0.03211
...
    app-arch/p7zip                                      7za -tzip -mm=PPMd           5.9               19002882 0.09331 0.01592
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=24           5.7               19002882 0.09331 0.01640
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=25           6.4               18871933 0.09955 0.01551
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=26           7.7               18771632 0.10434 0.01364
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=27           9.0               18652402 0.11003 0.01224
    app-arch/p7zip                             7za -tzip -mm=PPMd -mmem=28          10.0               18521291 0.11628 0.01161
    app-arch/p7zip                                       7za -t7z -m0=PPMd           5.7               18999088 0.09349 0.01634
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=24           5.8               18999088 0.09349 0.01617
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=25           6.5               18868478 0.09972 0.01534
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=26           7.5               18770031 0.10442 0.01387
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=27           8.6               18651294 0.11008 0.01282
    app-arch/p7zip                                7za -t7z -m0=PPMd:mem=28          10.6               18518330 0.11643 0.01100
      app-arch/rar                                                     rar           0.9               20249470 0.03383 0.03980
      app-arch/rar                                                 rar -m0           0.0               20958497 -0.00000        -0.00008
      app-arch/rar                                                 rar -m1           0.2               20243598 0.03411 0.14829
      app-arch/rar                                                 rar -m2           0.8               20252266 0.03369 0.04433
      app-arch/rar                                                 rar -m3           0.8               20249470 0.03383 0.04027
      app-arch/rar                                                 rar -m4           0.9               20248859 0.03386 0.03983
      app-arch/rar                                                 rar -m5           0.8               20248577 0.03387 0.04181
    app-arch/lrzip                                                lrzip -z          13.1               19769417 0.05673 0.00432
     app-arch/zpaq                                                    zpaq           0.2               20970029 -0.00055        -0.00252
The best compression was found with 7za -t7z -m0=PPMd:mem=28.
The compression delta obtained was 0.11643 within 10.58 seconds.
This file is now available as test.nef.7z.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the test file was around 20 MByte. The best compression
compression command that the script found was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ 7za -t7z -m0=PPMd:mem=28 a test.nef.7z test.nef
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting file (&lt;code&gt;test.nef.7z&lt;/code&gt;) is 18 MByte, a reduction of 11,64%. The
compression command took almost 11 seconds to do its thing, which gave an
efficiency rating of 0,011, which is definitely not a fast one.&lt;/p&gt;
&lt;p&gt;Some other algorithms don't do bad either with a better efficiency. For
instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;   app-arch/pbzip2                                                  pbzip2           0.6               19287402 0.07973 0.13071
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the &lt;code&gt;pbzip2&lt;/code&gt; command got almost 8% reduction in less than a
second, which is considerably more efficient than the 11-seconds long &lt;code&gt;7za&lt;/code&gt;
run.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Want to try it out yourself?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've pushed the script to my &lt;a href="https://github.com/sjvermeu/small.coding/tree/master/sw_comprbest"&gt;github&lt;/a&gt;
location. Do a quick review of the code first (to see that I did not include
anything malicious) and then execute it to see how it works:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sw_comprbest -h
Usage: sw_comprbest --infile=&amp;lt;inputfile&amp;gt; [--family=&amp;lt;family&amp;gt;[,...]] [--command=&amp;lt;cmd&amp;gt;]
       sw_comprbest -i &amp;lt;inputfile&amp;gt; [-f &amp;lt;family&amp;gt;[,...]] [-c &amp;lt;cmd&amp;gt;]

Supported families: blosc bwt deflate lzma ppmd zpaq. These can be provided comma-separated.
Command is an additional filter - only the tests that use this base command are run.

The output shows
  - The package (in Gentoo) that the command belongs to
  - The command run
  - The duration (in seconds)
  - The size (in bytes) of the resulting file
  - The compression delta (percentage) showing how much is reduced (higher is better)
  - The efficiency ratio showing how much reduction (percentage) per second (higher is better)

When the command supports multithreading, we use the number of available cores on the system (as told by /proc/cpuinfo).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For instance, to try it out against a PDF file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ sw_comprbest -i MEA6-Sven_Vermeulen-Research_Summary.pdf
Original file (MEA6-Sven_Vermeulen-Research_Summary.pdf) size 117763 bytes
...
The best compression was found with zopfli --deflate.
The compression delta obtained was 0.00982 within 0.19 seconds.
This file is now available as MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So in this case, the resulting file is hardly better compressed - the PDF
itself is already compressed. Let's try it against the uncompressed PDF:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ pdftk MEA6-Sven_Vermeulen-Research_Summary.pdf output test.pdf uncompress
~$ sw_comprbest -i test.pdf
Original file (test.pdf) size 144670 bytes
...
The best compression was found with lrzip -z.
The compression delta obtained was 0.27739 within 0.18 seconds.
This file is now available as test.pdf.lrz.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is somewhat better:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ls -l MEA6-Sven_Vermeulen-Research_Summary.pdf* test.pdf*
-rw-r--r--. 1 swift swift 117763 Aug  7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf
-rw-r--r--. 1 swift swift 116606 Aug  7 14:32 MEA6-Sven_Vermeulen-Research_Summary.pdf.deflate
-rw-r--r--. 1 swift swift 144670 Aug  7 14:34 test.pdf
-rw-r--r--. 1 swift swift 104540 Aug  7 14:35 test.pdf.lrz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting file is 11,22% reduced from the original one.&lt;/p&gt;</content><category term="Gentoo"></category><category term="gentoo"></category><category term="compression"></category></entry><entry><title>Why we do confine Firefox</title><link href="https://blog.siphos.be/2015/08/why-we-do-confine-firefox/" rel="alternate"></link><published>2015-08-11T19:18:00+02:00</published><updated>2015-08-11T19:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-11:/2015/08/why-we-do-confine-firefox/</id><summary type="html">&lt;p&gt;If you're a bit following the SELinux development community you will know
&lt;a href="http://danwalsh.livejournal.com"&gt;Dan Walsh&lt;/a&gt;, a &lt;a href="http://people.redhat.com/dwalsh/"&gt;Red Hat&lt;/a&gt;
security engineer. Today he &lt;a href="http://danwalsh.livejournal.com/72697.html"&gt;blogged&lt;/a&gt; 
about &lt;em&gt;CVE-2015-4495 and SELinux, or why doesn't SELinux confine Firefox&lt;/em&gt;. He 
should've asked why the &lt;em&gt;reference policy&lt;/em&gt; or &lt;em&gt;Red Hat/Fedora policy&lt;/em&gt; does not
confine Firefox, because SELinux is, as I've
&lt;a href="http://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/"&gt;mentioned before&lt;/a&gt;,
not the same as its policy.&lt;/p&gt;
&lt;p&gt;In effect, Gentoo's SELinux policy &lt;em&gt;does&lt;/em&gt; confine Firefox by default. One of the
principles we focus on in Gentoo Hardened is to
&lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/Development_policy#Develop_desktop_policies"&gt;develop desktop policies&lt;/a&gt;
in order to reduce exposure and information leakage of user documents. We might
not have the manpower to confine all desktop applications, but I do think it is
worthwhile to at least attempt to do this, even though what Dan Walsh mentioned
is also correct: desktops are notoriously difficult to use a mandatory access
control system on.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;If you're a bit following the SELinux development community you will know
&lt;a href="http://danwalsh.livejournal.com"&gt;Dan Walsh&lt;/a&gt;, a &lt;a href="http://people.redhat.com/dwalsh/"&gt;Red Hat&lt;/a&gt;
security engineer. Today he &lt;a href="http://danwalsh.livejournal.com/72697.html"&gt;blogged&lt;/a&gt; 
about &lt;em&gt;CVE-2015-4495 and SELinux, or why doesn't SELinux confine Firefox&lt;/em&gt;. He 
should've asked why the &lt;em&gt;reference policy&lt;/em&gt; or &lt;em&gt;Red Hat/Fedora policy&lt;/em&gt; does not
confine Firefox, because SELinux is, as I've
&lt;a href="http://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/"&gt;mentioned before&lt;/a&gt;,
not the same as its policy.&lt;/p&gt;
&lt;p&gt;In effect, Gentoo's SELinux policy &lt;em&gt;does&lt;/em&gt; confine Firefox by default. One of the
principles we focus on in Gentoo Hardened is to
&lt;a href="https://wiki.gentoo.org/wiki/Project:SELinux/Development_policy#Develop_desktop_policies"&gt;develop desktop policies&lt;/a&gt;
in order to reduce exposure and information leakage of user documents. We might
not have the manpower to confine all desktop applications, but I do think it is
worthwhile to at least attempt to do this, even though what Dan Walsh mentioned
is also correct: desktops are notoriously difficult to use a mandatory access
control system on.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;How Gentoo wants to support more confined desktop applications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What Gentoo Hardened tries to do is to support the
&lt;a href="http://standards.freedesktop.org/basedir-spec/basedir-spec-0.8.html"&gt;XDG Base Directory Specification&lt;/a&gt;
for several documentation types. Downloads are marked as &lt;code&gt;xdg_downloads_home_t&lt;/code&gt;,
pictures are marked as &lt;code&gt;xdg_pictures_home_t&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;With those types defined, we grant the regular user domains full access to
those types, but start removing access to user content from applications. Rules
such as the following are commented out or removed from the policies:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# userdom_manage_user_home_content_dirs(mozilla_t)
# userdom_manage_user_home_content_files(mozilla_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Instead, we add in a call to a template we have defined ourselves:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;userdom_user_content_access_template(mozilla, { mozilla_t mozilla_plugin_t })
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This call makes access to user content optional through SELinux booleans. For
instance, for the &lt;code&gt;mozilla_t&lt;/code&gt; domain (which is used for Firefox), the following
booleans are created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Read generic (user_home_t) user content
mozilla_read_generic_user_content       -&amp;gt;      true

# Read all user content
mozilla_read_all_user_content           -&amp;gt;      false

# Manage generic (user_home_t) user content
mozilla_manage_generic_user_content     -&amp;gt;      false

# Manage all user content
mozilla_manage_all_user_content         -&amp;gt;      false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the default setting is that Firefox can read user content, but
only non-specific types. So &lt;code&gt;ssh_home_t&lt;/code&gt;, which is used for the SSH related
files, is not readable by Firefox with our policy &lt;em&gt;by default&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;By changing these booleans, the policy is fine-tuned to the requirements of
the administrator. On my systems, &lt;code&gt;mozilla_read_generic_user_content&lt;/code&gt; is switched
off.&lt;/p&gt;
&lt;p&gt;You might ask how we can then still support a browser if it cannot access user
content to upload or download. Well, as mentioned before, we support the XDG
types. The browser is allowed to manage &lt;code&gt;xdg_download_home_t&lt;/code&gt; files and
directories. For the majority of cases, this is sufficient. I also don't mind
copying over files to the &lt;code&gt;~/Downloads&lt;/code&gt; directory just for uploading files. But
I am well aware that this is not what the majority of users would want, which
is why the default is as it is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is much more work to be done sadly&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As said earlier, the default policy will allow &lt;em&gt;reading&lt;/em&gt; of user files if those
files are not typed specifically. Types that are protected by our policy (but not
by the reference policy standard) includes SSH related files at &lt;code&gt;~/.ssh&lt;/code&gt; and
GnuPG files at &lt;code&gt;~/.gnupg&lt;/code&gt;. Even other configuration files, such as for my Mutt
configuration (&lt;code&gt;~/.muttrc&lt;/code&gt;) which contains a password for an IMAP server I connect
to, are not reachable.&lt;/p&gt;
&lt;p&gt;However, it is still far from perfect. One of the reasons is that many desktop
applications are not "converted" yet to our desktop policy approach. Yes, Chromium
is also already converted, and policies we've added such as for Skype also do not
allow direct access unless the user explicitly enabled it. But Evolution for instance
isn't yet.&lt;/p&gt;
&lt;p&gt;Converting desktop policies to a more strict setup requires lots of testing, which
translates to many human resources. Within Gentoo, only a few developers and 
contributors are working on policies, and considering that this is not a change
that is already part of the (upstream) reference policy, some contributors also
do not want to put lots of focus on it either. But without having done the works,
it will not be easy (nor probably acceptable) to upstream this (the XDG patch has
been submitted a few times already but wasn't deemed ready yet then).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Having a more restrictive policy isn't the end&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As the blog post of Dan rightly mentioned, there are still quite some other
ways of accessing information that we might want to protect. An application 
might not have access to user files, but can be able to communicate (for instance
through DBus) with an application that does, and through that instruct it to
pass on the data.&lt;/p&gt;
&lt;p&gt;Plugins might require permissions which do not match with the principles set up
earlier. When we tried out Google Talk (needed for proper Google Hangouts support)
we noticed that it requires many, many more privileges. Luckily, we were able to
write down and develop a policy for the Google Talk plugin (&lt;code&gt;googletalk_plugin_t&lt;/code&gt;)
so it is still properly confined. But this is just a single plugin, and I'm sure
that more plugins exist which will have similar requirements. Which leads to more
policy development.&lt;/p&gt;
&lt;p&gt;But having workarounds does not make the effort we do worthless. Being able to
work around a firewall through application data does not make the firewall
useless, it is just one of the many security layers. The same is true with SELinux
policies.&lt;/p&gt;
&lt;p&gt;I am glad that we at least try to confine desktop applications more, and
that Gentoo Hardened users who use SELinux are at least somewhat more protected
from the vulnerability (even with the default case) and that our investment for
this is sound.&lt;/p&gt;</content><category term="SELinux"></category><category term="gentoo"></category><category term="selinux"></category><category term="policy"></category><category term="firefox"></category><category term="cve"></category><category term="vulnerability"></category><category term="xdg"></category></entry><entry><title>Can SELinux substitute DAC?</title><link href="https://blog.siphos.be/2015/08/can-selinux-substitute-dac/" rel="alternate"></link><published>2015-08-09T14:48:00+02:00</published><updated>2015-08-09T14:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-09:/2015/08/can-selinux-substitute-dac/</id><summary type="html">&lt;p&gt;A nice &lt;a href="https://twitter.com/sjvermeu/status/630107879123623936"&gt;twitter discussion&lt;/a&gt;
with &lt;a href="https://twitter.com/erlheldata"&gt;Erling Hellenäs&lt;/a&gt; caught my full attention later
when I was heading home: Can SELinux substitute DAC? I know it can't and doesn't
in the current implementation, but why not and what would be needed?&lt;/p&gt;
&lt;p&gt;SELinux is implemented through the &lt;a href="https://en.wikipedia.org/wiki/Linux_Security_Modules"&gt;Linux Security Modules framework&lt;/a&gt;
which allows for different security systems to be implemented and integrated
in the Linux kernel. Through LSM, various security-sensitive operations can be
secured further through &lt;em&gt;additional&lt;/em&gt; access checks. This criteria was made to
have LSM be as minimally invasive as possible.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A nice &lt;a href="https://twitter.com/sjvermeu/status/630107879123623936"&gt;twitter discussion&lt;/a&gt;
with &lt;a href="https://twitter.com/erlheldata"&gt;Erling Hellenäs&lt;/a&gt; caught my full attention later
when I was heading home: Can SELinux substitute DAC? I know it can't and doesn't
in the current implementation, but why not and what would be needed?&lt;/p&gt;
&lt;p&gt;SELinux is implemented through the &lt;a href="https://en.wikipedia.org/wiki/Linux_Security_Modules"&gt;Linux Security Modules framework&lt;/a&gt;
which allows for different security systems to be implemented and integrated
in the Linux kernel. Through LSM, various security-sensitive operations can be
secured further through &lt;em&gt;additional&lt;/em&gt; access checks. This criteria was made to
have LSM be as minimally invasive as possible.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The LSM design&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic LSM design paper, called &lt;a href="http://www.kroah.com/linux/talks/usenix_security_2002_lsm_paper/lsm.pdf"&gt;Linux Security Modules: General Security
Support for the Linux Kernel&lt;/a&gt;
as presented in 2002, is still one of the better references for learning and
understanding LSM. It does show that there was a whish-list from the community
where LSM hooks could override DAC checks, and that it has been partially
implemented through permissive hooks (not to be mistaken with SELinux' 
permissive mode).&lt;/p&gt;
&lt;p&gt;However, this definitely is &lt;em&gt;partially&lt;/em&gt; implemented because there are quite
a few restrictions. One of them is that, if a request is made towards a
resource and the UIDs match (see page 3, figure 2 of the paper) then
the LSM hook is not consulted. When they don't match, a permissive LSM
hook can be implemented. Support for permissive hooks is implemented
for capabilities, a powerful DAC control that Linux supports and which is
implemented &lt;a href="http://www.hep.by/gnu/kernel/lsm/cap.html"&gt;through LSM&lt;/a&gt; as
well. I have &lt;a href="http://blog.siphos.be/tag/capabilities/index.html"&gt;blogged&lt;/a&gt;
about this nice feature a while ago.&lt;/p&gt;
&lt;p&gt;These restrictions are also why some other security-conscious developers,
such as &lt;a href="http://grsecurity.net/lsm.php"&gt;grsecurity's team&lt;/a&gt; and &lt;a href="https://www.rsbac.org/documentation/why_rsbac_does_not_use_lsm"&gt;RSBAC&lt;/a&gt;
do not use the LSM system. Well, it's not only through these restrictions
of course - other reasons play a role in them as well. But knowing what
LSM can (and cannot) do also shows what SELinux can and cannot do.&lt;/p&gt;
&lt;p&gt;The LSM design itself is already a reason why SELinux cannot substitute
DAC controls. But perhaps we could disable DAC completely and thus only
rely on SELinux?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disabling DAC in Linux would be an excessive workload&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discretionary access controls in the Linux kernel are not easy to remove.
They are often part of the code itself (just grep through the source code
after &lt;code&gt;-EPERM&lt;/code&gt;). Some subsystems which use a common standard approach (such
as VFS operations) can rely on good integrated security controls, but these
too often allow the operation if DAC allows it, and will only consult the LSM
hooks otherwise.&lt;/p&gt;
&lt;p&gt;VFS operations are the most known ones, but DAC controls go beyond file access.
It also entails reading program memory, sending signals to applications,
accessing hardware and more. But let's focus on the easier controls (as in,
easier to use examples for), such as sharing files between users, restricting
access to personal documents and authorizing operations in applications based
on the user id (for instance, the owner can modify while other users can only
read the file).&lt;/p&gt;
&lt;p&gt;We could "work around" the Linux DAC controls by running everything as a single user
(the root user) and having all files and resources be fully accessible by this
user. But the problem with that is that SELinux would not be able to take
over controls either, because you will need some user-based access controls,
and within SELinux this implies that a mapping is done from a user to a 
SELinux user. Also, access controls based on the user id would no longer work,
and unless the application is made SELinux-aware it would lack any authorization
system (or would need to implement it itself).&lt;/p&gt;
&lt;p&gt;With DAC Linux also provides quite some "freedom" which is well established
in the Linux (and Unix) environment: a simple security model where the user
and group membership versus the owner-privileges, group-privileges and
"rest"-privileges are validated. Note that SELinux does not really know
what a "group" is. It knows SELinux users, roles, types and sensitivities.&lt;/p&gt;
&lt;p&gt;So, suppose we would keep multi-user support in Linux but completely remove
the DAC controls and rely solely on LSM (and SELinux). Is this something
reusable?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using SELinux for DAC-alike rules&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the use case of two users. One user wants another user to read a few
of his files. With DAC controls, he can "open up" the necessary resources
(files and directories) through &lt;a href="https://wiki.gentoo.org/wiki/Filesystem/Access_Control_List_Guide"&gt;extended access control lists&lt;/a&gt;
so that the other user can access it. No need to involve administrators.&lt;/p&gt;
&lt;p&gt;With a MAC(-only) system, updates on the MAC policy usually require the security
administrator to write additional policy rules to allow something. With SELinux
(and without DAC) it would require the users to be somewhat isolated from each
other (otherwise the users can just access everything from each other), which
SELinux can do through &lt;a href="https://wiki.gentoo.org/wiki/SELinux/Constraints#UBAC_-_User_Based_Access_Control"&gt;User Based Access Control&lt;/a&gt;,
but the target resource itself should be labeled with a type that is not managed
through the UBAC control. Which means that the users will need the privilege to
change labels to this type (which is possible!), &lt;em&gt;assuming&lt;/em&gt; such a type is already
made available for them. Users can't create new types themselves.&lt;/p&gt;
&lt;p&gt;UBAC is by default disabled in many distributions, because it has some nasty
side-effects that need to be taken into consideration. Just recently one of these
&lt;a href="http://oss.tresys.com/pipermail/refpolicy/2015-August/007704.html"&gt;came up on the refpolicy mailinglist&lt;/a&gt;.
But even with UBAC enabled (I have it enabled on most of my systems, but considering
that I only have a couple of users to manage and am administrator on these systems
to quickly "update" rules when necessary) it does not provide equal functionality as
DAC controls.&lt;/p&gt;
&lt;p&gt;As mentioned before, SELinux does not know group membership. In order to create
something group-like, we will probably need to consider roles. But in SELinux,
roles are used to define what types are transitionable towards - it is not a
membership approach. A type which is usable by two roles (for instance, the
&lt;code&gt;mozilla_t&lt;/code&gt; type which is allowed for &lt;code&gt;staff_r&lt;/code&gt; and &lt;code&gt;user_r&lt;/code&gt;) does not care about
the role. This is unlike group membership.&lt;/p&gt;
&lt;p&gt;Also, roles only focus on &lt;em&gt;transitionable&lt;/em&gt; types (known as domains). It does not
care about &lt;em&gt;accessible&lt;/em&gt; resources (regular file types for instance). In order to
allow one person to read a certain file type but not another, SELinux will need
to control that one person can read this file through a particular domain while
the other user can't. And given that domains are part of the SELinux policy, any
situation that the policy has not thought about before will not be easily adaptable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, we can't do it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, I'm pretty sure that a very extensive policy and set of rules can be made
for SELinux which would make a number of DAC permissions obsolete, and that we could
theoretically remove DAC from the Linux kernel.&lt;/p&gt;
&lt;p&gt;End users would require a huge training to work with this system, and it would not
be reusable across other systems in different environments, because the policy
will be too specific to the system (unlike the current reference policy based ones,
which are quite reusable across many distributions).&lt;/p&gt;
&lt;p&gt;Furthermore, the effort to create these policies would be extremely high, whereas
the DAC permissions are very simple to implement, and have been proven to be
well suitable for many secured systems. &lt;/p&gt;
&lt;p&gt;So no, unless you do massive engineering, I do not believe it is possible to
substitute DAC with SELinux-only controls.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="refpolicy"></category><category term="linux"></category><category term="dac"></category><category term="lsm"></category></entry><entry><title>Filtering network access per application</title><link href="https://blog.siphos.be/2015/08/filtering-network-access-per-application/" rel="alternate"></link><published>2015-08-07T03:49:00+02:00</published><updated>2015-08-07T03:49:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-07:/2015/08/filtering-network-access-per-application/</id><summary type="html">&lt;p&gt;Iptables (and the successor nftables) is a powerful packet filtering system in
the Linux kernel, able to create advanced firewall capabilities. One of the 
features that it &lt;em&gt;cannot&lt;/em&gt; provide is per-application filtering. Together with
SELinux however, it is possible to implement this on a &lt;em&gt;per domain&lt;/em&gt; basis.&lt;/p&gt;
&lt;p&gt;SELinux does not know applications, but it knows domains. If we ensure that each
application runs in its own domain, then we can leverage the firewall
capabilities with SELinux to only allow those domains access that we need.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Iptables (and the successor nftables) is a powerful packet filtering system in
the Linux kernel, able to create advanced firewall capabilities. One of the 
features that it &lt;em&gt;cannot&lt;/em&gt; provide is per-application filtering. Together with
SELinux however, it is possible to implement this on a &lt;em&gt;per domain&lt;/em&gt; basis.&lt;/p&gt;
&lt;p&gt;SELinux does not know applications, but it knows domains. If we ensure that each
application runs in its own domain, then we can leverage the firewall
capabilities with SELinux to only allow those domains access that we need.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;SELinux network control: packet types&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic network control we need to enable is SELinux' packet types. Most
default policies will grant application domains the right set of packet types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# sesearch -s mozilla_t -c packet -A
Found 13 semantic av rules:
   allow mozilla_t ipp_client_packet_t : packet { send recv } ; 
   allow mozilla_t soundd_client_packet_t : packet { send recv } ; 
   allow nsswitch_domain dns_client_packet_t : packet { send recv } ; 
   allow mozilla_t speech_client_packet_t : packet { send recv } ; 
   allow mozilla_t ftp_client_packet_t : packet { send recv } ; 
   allow mozilla_t http_client_packet_t : packet { send recv } ; 
   allow mozilla_t tor_client_packet_t : packet { send recv } ; 
   allow mozilla_t squid_client_packet_t : packet { send recv } ; 
   allow mozilla_t http_cache_client_packet_t : packet { send recv } ; 
 DT allow mozilla_t server_packet_type : packet recv ; [ mozilla_bind_all_unreserved_ports ]
 DT allow mozilla_t server_packet_type : packet send ; [ mozilla_bind_all_unreserved_ports ]
 DT allow nsswitch_domain ldap_client_packet_t : packet recv ; [ authlogin_nsswitch_use_ldap ]
 DT allow nsswitch_domain ldap_client_packet_t : packet send ; [ authlogin_nsswitch_use_ldap ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the &lt;code&gt;mozilla_t&lt;/code&gt; domain is able to send and receive packets of
type &lt;code&gt;ipp_client_packet_t&lt;/code&gt;, &lt;code&gt;soundd_client_packet_t&lt;/code&gt;, &lt;code&gt;dns_client_packet_t&lt;/code&gt;, 
&lt;code&gt;speech_client_packet_t&lt;/code&gt;, &lt;code&gt;ftp_client_packet_t&lt;/code&gt;, &lt;code&gt;http_client_packet_t&lt;/code&gt;, 
&lt;code&gt;tor_client_packet_t&lt;/code&gt;, &lt;code&gt;squid_client_packet_t&lt;/code&gt; and &lt;code&gt;http_cache_client_packet_t&lt;/code&gt;.
If the SELinux booleans mentioned at the end are enabled, additional packet
types are alloed to be used as well.&lt;/p&gt;
&lt;p&gt;But even with this default policy in place, SELinux is not being consulted for
filtering. To accomplish this, &lt;code&gt;iptables&lt;/code&gt; will need to be told to label the
incoming and outgoing packets. This is the &lt;a href="http://blog.siphos.be/2013/05/secmark-and-selinux/"&gt;SECMARK&lt;/a&gt;
functionality that I've blogged about earlier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling SECMARK filtering through iptables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To enable SECMARK filtering, we use the &lt;code&gt;iptables&lt;/code&gt; command and tell it to label
SSH incoming and outgoing packets as &lt;code&gt;ssh_server_packet_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -A INPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore
~# iptables -t mangle -A INPUT -p tcp --dport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0
~# iptables -t mangle -A OUTPUT -m state --state ESTABLISHED,RELATED -j CONNSECMARK --restore
~# iptables -t mangle -A OUTPUT -p tcp --sport 22 -j SECMARK --selctx system_u:object_r:ssh_server_packet_t:s0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But be warned: the moment iptables starts with its SECMARK support, &lt;em&gt;all packets&lt;/em&gt;
will be labeled. Those that are not explicitly labeled through one of the above
commands will be labeled with the &lt;code&gt;unlabeled_t&lt;/code&gt; type, and most domains are not
allowed any access to &lt;code&gt;unlabeled_t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are two things we can do to improve this situation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define the necessary SECMARK rules for all supported ports (which is something
   that &lt;a href="https://www.linux.com/learn/tutorials/421152:using-selinux-and-iptables-together"&gt;secmarkgen&lt;/a&gt;
   does), and/or&lt;/li&gt;
&lt;li&gt;Allow &lt;code&gt;unlabeled_t&lt;/code&gt; for all domains.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To allow the latter, we can load a SELinux rule like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;domain&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;unlabeled_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will allow all domains to send and receive packets of the &lt;code&gt;unlabeled_t&lt;/code&gt; type.
Although this is something that might be security-sensitive, it might be a good idea
to allow at start, together with proper auditing (you can use &lt;code&gt;(auditallow ...)&lt;/code&gt; to
audit all granted packet communication) so that the right set of packet types can be
enabled. This way, administrators can iteratively improve the SECMARK rules and finally
remove the &lt;code&gt;unlabeled_t&lt;/code&gt; privilege from the &lt;code&gt;domain&lt;/code&gt; attribute.&lt;/p&gt;
&lt;p&gt;To list the current SECMARK rules, list the firewall rules for the &lt;code&gt;mangle&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -nvL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Only granting one application network access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These two together allow for creating a firewall that only allows a single domain
access to a particular target.&lt;/p&gt;
&lt;p&gt;For instance, suppose that we only want the &lt;code&gt;mozilla_t&lt;/code&gt; domain to connect to the
company proxy (10.15.10.5). We can't enable the &lt;code&gt;http_client_packet_t&lt;/code&gt; for this
connection, as all other web browsers and other HTTP-aware applications will have
policy rules enabled to send and receive that packet type. Instead, we are going
to create a new packet type to use.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;;; Definition of myhttp_client_packet_t&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;roletype&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;object_r&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;client_packet_type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;packet_type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;;; Grant the use to mozilla_t&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;typeattributeset&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;cil_gen_require&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;allow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;mozilla_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;myhttp_client_packet_t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;packet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;send&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;recv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Putting the above in a &lt;code&gt;myhttppacket.cil&lt;/code&gt; file and loading it allows the type
to be used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i myhttppacket.cil
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, the &lt;code&gt;myhttp_client_packet_t&lt;/code&gt; type can be used in &lt;code&gt;iptables&lt;/code&gt; rules. Also, 
only the &lt;code&gt;mozilla_t&lt;/code&gt; domain is allowed to send and receive these packets,
effectively creating an application-based firewall, as all we now need to do
is to mark the outgoing packets towards the proxy as &lt;code&gt;myhttp_client_packet_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# iptables -t mangle -A OUTPUT -p tcp --dport 80 -d 10.15.10.5 -j SECMARK --selctx system_u:object_r:myhttp_client_packet_t:s0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This shows that it is &lt;em&gt;possible&lt;/em&gt; to create such firewall rules with SELinux. It
is however not an out-of-the-box solution, requiring thought and development of
both firewall rules and SELinux code constructions. Still, with some advanced
scripting experience this will lead to a powerful addition to a hardened
system.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="network"></category><category term="iptables"></category></entry><entry><title>My application base: Obnam</title><link href="https://blog.siphos.be/2015/08/my-application-base-obnam/" rel="alternate"></link><published>2015-08-05T22:35:00+02:00</published><updated>2015-08-05T22:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-05:/2015/08/my-application-base-obnam/</id><summary type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;


&lt;p&gt;I found attic to be faster, but with a small supporting community. Obnam was
slower, but seems to have a more active community which I find important for 
infrastructure that is meant to live quite long (you don't want to switch 
backup solutions every year). I also found it pretty easy to work with, and
to restore files back, and Gentoo provides the &lt;a href="https://packages.gentoo.org/package/app-backup/obnam"&gt;app-backup/obnam&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;I think both are decent solutions, so I had to make one choice and ended up
with obnam. So, how does it work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring what to backup&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic configuration file for obnam is &lt;code&gt;/etc/obnam.conf&lt;/code&gt;. Inside this file,
I tell which directories need to be backed up, as well as which subdirectories
or files (through expressions) can be left alone. For instance, I don't want
obnam to backup ISO files as those have been downloaded anyway.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[config]&lt;/span&gt;
&lt;span class="na"&gt;repository&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;/srv/backup&lt;/span&gt;
&lt;span class="na"&gt;root&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;/root, /etc, /var/lib/portage, /srv/virt/gentoo, /home&lt;/span&gt;
&lt;span class="na"&gt;exclude&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;\.img$, \.iso$, /home/[^/]*/Development/Centralized/.*&lt;/span&gt;
&lt;span class="na"&gt;exclude-caches&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;yes&lt;/span&gt;

&lt;span class="na"&gt;keep&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;8h,14d,10w,12m,10y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;root&lt;/code&gt; parameter tells obnam which directories (and subdirectories) to
back up. With &lt;code&gt;exclude&lt;/code&gt; a particular set of files or directories can be
excluded, for instance because these contain downloaded resources (and as such
do not need to be inside the backup archives).&lt;/p&gt;
&lt;p&gt;Obnam also supports the &lt;a href="http://www.brynosaurus.com/cachedir/spec.html"&gt;CACHEDIR.TAG&lt;/a&gt;
specification, which I use for the various cache directories. With the use of 
these cache tag files I do not need to update the &lt;code&gt;obnam.conf&lt;/code&gt; file with every
new cache directory (or software build directory).&lt;/p&gt;
&lt;p&gt;The last parameter in the configuration that I want to focus on is the &lt;code&gt;keep&lt;/code&gt;
parameter. Every time obnam takes a backup, it creates what it calls a new
&lt;em&gt;generation&lt;/em&gt;. When the backup storage becomes too big, administrators can run
&lt;code&gt;obnam forget&lt;/code&gt; to drop generations. The &lt;code&gt;keep&lt;/code&gt; parameter informs obnam which
generations can be removed and which ones can be kept.&lt;/p&gt;
&lt;p&gt;In my case, I want to keep one backup per hour for the last 8 hours (I normally
take one backup per day, but during some development sprees or photo
manipulations I back up multiple times), one per day for the last two weeks, 
one per week for the last 10 weeks, one per month for the last 12 months and
one per year for the last 10 years.&lt;/p&gt;
&lt;p&gt;Obnam will clean up only when &lt;code&gt;obnam forget&lt;/code&gt; is executed. As storage is cheap,
and the performance of obnam is sufficient for me, I do not need to call this
very often.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backing up and restoring files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My backup strategy is to backup to an external disk, and then synchronize this
disk with a personal backup server somewhere else. This backup server runs no
other software beyond OpenSSH (to allow secure transfer of the backups) and both
the backup server disks and the external disk is &lt;a href="https://wiki.gentoo.org/wiki/Dm-crypt"&gt;LUKS&lt;/a&gt;
encrypted. Considering that I don't have government secrets I opted not to encrypt
the backup files themselves, but Obnam does support that (through GnuPG).&lt;/p&gt;
&lt;p&gt;All backup enabled systems use cron jobs which execute &lt;code&gt;obnam backup&lt;/code&gt; to take
the backup, and use rsync to synchronize the finished backup with the backup
server. If I need to restore a file, I use &lt;code&gt;obnam ls&lt;/code&gt; to see which file(s) I
need to restore (add in a &lt;code&gt;--generation=&lt;/code&gt; to list the files of a different
backup generation than the last one).&lt;/p&gt;
&lt;p&gt;Then, the command to restore is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/var/restore /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or I can restore immediately to the directory again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/home/swift/Images/Processing /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To support multiple clients, obnam by default identifies each client through
the hostname. It is possible to use different names, but hostnames tend to be
a common best practice which I don't deviate from either. Obnam is able to share
blocks between clients (it is not mandatory, but supported nonetheless).&lt;/p&gt;</content><category term="Free-Software"></category><category term="mab"></category><category term="backup"></category><category term="obnam"></category></entry><entry><title>Don't confuse SELinux with its policy</title><link href="https://blog.siphos.be/2015/08/dont-confuse-selinux-with-its-policy/" rel="alternate"></link><published>2015-08-03T01:49:00+02:00</published><updated>2015-08-03T01:49:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-03:/2015/08/dont-confuse-selinux-with-its-policy/</id><summary type="html">&lt;p&gt;With the increased attention that SELinux is getting thanks to its inclusion in
recent &lt;a href="https://source.android.com/devices/tech/security/selinux/"&gt;Android&lt;/a&gt;
releases, more and more people are understanding that SELinux is not a singular
security solution. Many administrators are still disabling SELinux on their 
servers because it does not play well with their day-to-day operations. But
the Android inclusion shows that SELinux itself is not the culprit for this:
it is the policy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the increased attention that SELinux is getting thanks to its inclusion in
recent &lt;a href="https://source.android.com/devices/tech/security/selinux/"&gt;Android&lt;/a&gt;
releases, more and more people are understanding that SELinux is not a singular
security solution. Many administrators are still disabling SELinux on their 
servers because it does not play well with their day-to-day operations. But
the Android inclusion shows that SELinux itself is not the culprit for this:
it is the policy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Policy versus enforcement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SELinux has conceptually segregated the enforcement from the rules/policy. 
There is an in-kernel enforcement (the SELinux subsystem) which is configured
through an administrator-provided policy (the SELinux rules). As long as 
SELinux was being used on servers, chances are very high that the policy that
is being used is based on the &lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;SELinux Reference Policy&lt;/a&gt;
as this is, as far as I know, the only policy implementation for Linux systems
that is widely usable.&lt;/p&gt;
&lt;p&gt;The reference policy project aims to provide a well designed, broadly usable
yet still secure set of rules. And through this goal, it has to play ball with
all possible use cases that the various software titles require. Given the open
ecosystem of the free software world, and the Linux based ones in particular, 
managing such a policy is not for beginners. New policy development requires 
insight in the technology for which the policy is created, as well as knowledge
of how the reference policy works.&lt;/p&gt;
&lt;p&gt;Compare this to the Android environment. Applications have to follow more
rigid guidelines before they are accepted on Android systems. Communication
between applications and services is governed through Intents and Activities
which are managed by the &lt;a href="http://www.cubrid.org/blog/dev-platform/binder-communication-mechanism-of-android-processes/"&gt;Binder&lt;/a&gt;
application. Interactions with the user are based on well defined interfaces.
Heck, the Android OS even holds a number of permissions that applications
have to subscribe to before they can use it.&lt;/p&gt;
&lt;p&gt;Such an environment is much easier to create policies for, because it allows
policies to be created almost on-the-fly, with the application permissions
being mapped to predefined SELinux rules. Because the freedom of
implementations is limited (in order to create a manageable environment which
is used by millions of devices over the world) policies can be made more
strictly and yet enjoy the static nature of the environment: no continuous
updates on existing policies, something that Linux distributions have to do
on an almost daily basis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aiming for a policy development ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having SELinux active on Android shows that one should not confuse SELinux
with its policies. SELinux is a nice security subsystem in the Linux kernel,
and can be used and tuned to cover whatever use case is given to it. The slow
adoption of SELinux by Linux distributions might be attributed to its lack
of policy diversification, which results in few ecosystems where additional
(and perhaps innovative) policies could be developed.&lt;/p&gt;
&lt;p&gt;It is however a huge advantage that a reference policy exists, so that
distributions can enjoy a working policy without having to put resources
into its own policy development and maintenance. Perhaps we should try to
further enhance the existing policies while support new policy ecosystems
and development initiatives.&lt;/p&gt;
&lt;p&gt;The maturation of the &lt;a href="https://github.com/SELinuxProject/cil/wiki"&gt;CIL&lt;/a&gt;
language by the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux userland libraries and tools&lt;/a&gt;
might be a good catalyst for this. At one point, policies will need to be
migrated to CIL (although this can happen gradually as the userland utilities
can deal with CIL and other languages such as the legacy &lt;code&gt;.pp&lt;/code&gt; files 
simultaneously) and there are a few developers considering a renewal
of the reference policy. This would make use of the new benefits of the CIL
language and implementation: some restrictions that where applicable to the legacy
format no longer holds on CIL, such as rules which previously were only allowed
in the base policy which can now be made part of the modules as well.&lt;/p&gt;
&lt;p&gt;But next to renewing existing policies, there is plenty of room left for
innovative policy ideas and developments. The &lt;a href="http://selinuxproject.org/page/PolicyLanguage"&gt;SELinux language&lt;/a&gt;
is very versatile, and just like with programming languages we notice that only
a few set of constructs are used. Some applications might even benefit from
using SELinux as their decision and enforcement system (something that
&lt;a href="https://wiki.postgresql.org/wiki/SEPostgreSQL_Introduction"&gt;SEPostgreSQL&lt;/a&gt; has
tried).&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://freecomputerbooks.com/The-SELinux-Notebook-The-Foundations.html"&gt;SELinux Notebook&lt;/a&gt; by
Richard Haines is an excellent resource for developers that want to work more
closely with the SELinux language constructs. Just skimming through this resource
also shows how very open SELinux itself is, and that most of the users'
experience with SELinux is based on a singular policy implementation. This is
a prime reason why having a more open policy ecosystem makes perfect sense.&lt;/p&gt;
&lt;p&gt;If you don't like a particular car, do you ditch driving at all? No, you try out
another car. Let's create other cars in the SELinux world as well.&lt;/p&gt;</content><category term="SELinux"></category><category term="selinux"></category><category term="policy"></category><category term="cil"></category></entry><entry><title>Switching to Pelican</title><link href="https://blog.siphos.be/2015/08/switching-to-pelican/" rel="alternate"></link><published>2015-08-02T04:09:00+02:00</published><updated>2015-08-02T04:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-02:/2015/08/switching-to-pelican/</id><summary type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;


&lt;p&gt;Today I finally moved the blog, imported all past articles as well as
comments. For the commenting, I now use &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;disqus&lt;/a&gt;
which integrates nicely with Pelican and has a fluid feel to it. I wanted to
use the &lt;a href="http://www.tipue.com/search/"&gt;Tipue Search&lt;/a&gt; plug-in as well for
searching through the blog, but I had to put that on hold as I couldn't get
the results of a search to display nicely (all I got were links to
"undefined"). But I'll work on this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring Pelican&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pelican configuration is done through &lt;code&gt;pelicanconf.py&lt;/code&gt; and &lt;code&gt;publishconf.py&lt;/code&gt;. 
The former contains all definitions and settings for the site which are also
useful when previewing changes. The latter contains additional (or overruled)
settings related to publication.&lt;/p&gt;
&lt;p&gt;In order to keep the same links as before (to keep web crawlers happy, as well
as links to the blog from other sites and even the comments themselves) I did
had to update some variables, but the Internet was strong on this one and I had
little problems finding the right settings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Link structure of the site&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next challenges were (and still are, I will have to check if this is working
or not soon by checking the blog aggregation sites I am usually aggregated on)
the RSS and Atom feeds. From the access logs of my previous blog, I believe that
most of the aggregation sites are using the &lt;code&gt;/feed/&lt;/code&gt;, &lt;code&gt;/feed/atom&lt;/code&gt; and 
&lt;code&gt;/category/*/feed&lt;/code&gt; links.&lt;/p&gt;
&lt;p&gt;Now, I would like to move the aggregations to XML files, so that the RSS feed is
available at &lt;code&gt;/feed/rss.xml&lt;/code&gt; and the Atom feed at &lt;code&gt;/feed/atom.xml&lt;/code&gt;, but then the
existing aggregations would most likely fail because they currently don't use
these URLs. To fix this, I am now trying to generate the XML files as I would
like them to be, and create symbolic links afterwards from &lt;code&gt;index.html&lt;/code&gt; to the
right XML file.&lt;/p&gt;
&lt;p&gt;The RSS/ATOM settings I am currently using are as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;CATEGORY_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TRANSLATION_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Hopefully, the existing aggregations still work, and I can then start asking
the planets to move to the XML URL itself. Some tracking on the access logs
should allow me to see how well this is going.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing to make sure is happening correctly is the blog aggregation and
the comment system. Then, a few tweaks are still on the pipeline.&lt;/p&gt;
&lt;p&gt;One is to optimize the front page a bit. Right now, all articles are
summarized, and I would like to have the last (or last few) article(s) fully
expanded whereas the rest is summarized. If that isn't possible, I'll probably
switch to fully expanded articles (which is a matter of setting a single
variable).&lt;/p&gt;
&lt;p&gt;Next, I really want the search functionality to work again. Enabling the Tipue
search worked almost flawlessly - search worked as it should, and the resulting
search entries are all correct. The problem is that the URLs that the entries
point to (which is what users will click on) all point to an invalid
("undefined") URL.&lt;/p&gt;
&lt;p&gt;Finally, I want the printer-friendly one to be without the social / links on
the top right. This is theme-oriented, and I'm happily using
&lt;a href="https://github.com/DandyDev/pelican-bootstrap3"&gt;pelican-bootstrap3&lt;/a&gt; right now,
so I don't expect this to be much of a hassle. But considering that my blog is
mainly technology oriented for now (although I am planning on expanding that)
being able to have the articles saved in PDF or printed in a nice format is
an important use case for me.&lt;/p&gt;</content><category term="Free-Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>Loading CIL modules directly</title><link href="https://blog.siphos.be/2015/07/loading-cil-modules-directly/" rel="alternate"></link><published>2015-07-15T15:54:00+02:00</published><updated>2015-07-15T15:54:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-15:/2015/07/loading-cil-modules-directly/</id><summary type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/"&gt;previous
post&lt;/a&gt;
I used the &lt;code&gt;secilc&lt;/code&gt; binary to load an additional test policy. Little did
I know (and that's actually embarrassing because it was one of the
things I complained about) that you can just use the CIL policy as
modules directly.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;With this I mean that a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a &lt;a href="http://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/"&gt;previous
post&lt;/a&gt;
I used the &lt;code&gt;secilc&lt;/code&gt; binary to load an additional test policy. Little did
I know (and that's actually embarrassing because it was one of the
things I complained about) that you can just use the CIL policy as
modules directly.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;With this I mean that a CIL policy as mentioned in the previous post can
be loaded like a prebuilt &lt;code&gt;.pp&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i test.cil
~# semodule -l | grep test
test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's all that is to it. Loading the module resulted in the test port
to be immediately declared and available:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage port -l | grep test
test_port_t                    tcp      1440
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In hindsight, it makes sense that it is this easy. After all, support
for the old-style policy language is done by converting it into CIL when
calling &lt;code&gt;semodule&lt;/code&gt; so it makes sense to immediately put the module (in
CIL code) ready to be taken up.&lt;/p&gt;</content><category term="SELinux"></category><category term="cil"></category><category term="selinux"></category></entry><entry><title>Restricting even root access to a folder</title><link href="https://blog.siphos.be/2015/07/restricting-even-root-access-to-a-folder/" rel="alternate"></link><published>2015-07-11T14:09:00+02:00</published><updated>2015-07-11T14:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-11:/2015/07/restricting-even-root-access-to-a-folder/</id><summary type="html">&lt;p&gt;In a
&lt;a href="http://blog.siphos.be/2014/01/private-key-handling-and-selinux-protection/comment-page-1/#comment-143323"&gt;comment&lt;/a&gt;
Robert asked how to use SELinux to prevent even root access to a
directory. The trivial solution would be not to assign an administrative
role to the root account (which is definitely possible, but you want
some way to gain administrative access otherwise ;-)&lt;/p&gt;
&lt;p&gt;Restricting root is one of the commonly referred features of a MAC
(Mandatory Access Control) system. With a well designed user management
and sudo environment, it is fairly trivial - but if you need to start
from the premise that a user has direct root access, it requires some
thought to implement it correctly. The main "issue" is not that it is
difficult to implement policy-wise, but that most users will start from
a pre-existing policy (such as the reference policy) and build on top of
that.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In a
&lt;a href="http://blog.siphos.be/2014/01/private-key-handling-and-selinux-protection/comment-page-1/#comment-143323"&gt;comment&lt;/a&gt;
Robert asked how to use SELinux to prevent even root access to a
directory. The trivial solution would be not to assign an administrative
role to the root account (which is definitely possible, but you want
some way to gain administrative access otherwise ;-)&lt;/p&gt;
&lt;p&gt;Restricting root is one of the commonly referred features of a MAC
(Mandatory Access Control) system. With a well designed user management
and sudo environment, it is fairly trivial - but if you need to start
from the premise that a user has direct root access, it requires some
thought to implement it correctly. The main "issue" is not that it is
difficult to implement policy-wise, but that most users will start from
a pre-existing policy (such as the reference policy) and build on top of
that.&lt;/p&gt;


&lt;p&gt;The use of a pre-existing policy means that some roles are already
identified and privileges are already granted to users - often these
higher privileged roles are assigned to the Linux root user as not to
confuse users. But that does mean that restricting root access to a
folder means that some additional countermeasures need to be
implemented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The policy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But first things first. Let's look at a simple policy for restricting
access to &lt;code&gt;/etc/private&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(myprivate, 1.0)

type etc_private_t;
fs_associate(etc_private_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This simple policy introduces a type (&lt;code&gt;etc_private_t&lt;/code&gt;) which is allowed
to be used for files (it associates with a file system). &lt;em&gt;Do not&lt;/em&gt; use
the &lt;code&gt;files_type()&lt;/code&gt; interface as this would assign a set of attributes
that many user roles get read access on.&lt;/p&gt;
&lt;p&gt;Now, it is not sufficient to have the type available. If we want to
assign it to a type, someone or something needs to have the privileges
to change the security context of a file and directory to this type. If
we would just load this policy and try to do this from a privileged
account, it would fail:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# chcon -t etc_private_t /etc/private
chcon: failed to change context of &amp;#39;/etc/private&amp;#39; to &amp;#39;system_u:object_r:etc_private_t:s0&amp;#39;: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the following rule, the &lt;code&gt;sysadm_t&lt;/code&gt; domain (which I use for system
administration) is allowed to change the context to &lt;code&gt;etc_private_t&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;allow sysadm_t etc_private_t:{dir file} relabelto;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this in place, the administrator can label resources as
&lt;code&gt;etc_private_t&lt;/code&gt; without having read access to these resources
afterwards. Also, as long as there are no &lt;em&gt;relabelfrom&lt;/em&gt; privileges
assigned, the administrator cannot revert the context back to a type
that he has read access to.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The countermeasures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But this policy is not sufficient. One way that administrators can
easily access the resources is to disable SELinux controls (as in, put
the system in permissive mode):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# cat /etc/private/README
cat: /etc/private/README: Permission denied
~# setenforce 0
~# cat /etc/private/README
Hello World!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To prevent this, enable the &lt;em&gt;secure_mode_policyload&lt;/em&gt; SELinux boolean:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# setsebool secure_mode_policyload on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will prevent any policy and SELinux state manipulation... including
permissive mode, but also including loading additional SELinux policies
or changing booleans. Definitely experiment with this setting without
persisting (i.e. do not use &lt;code&gt;-P&lt;/code&gt; in the above command yet) to make sure
it is manageable for you.&lt;/p&gt;
&lt;p&gt;Still, this isn't sufficient. Don't forget that the administrator is
otherwise a full administrator - if he cannot access the &lt;code&gt;/etc/private&lt;/code&gt;
location directly, then he might be able to access it indirectly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the resource is on a non-critical file system, he can unmount the
    file system and remount it with a &lt;code&gt;context=&lt;/code&gt; mount option. This will
    override the file-level contexts. Bind-mounting does not seem to
    allow overriding the context.&lt;/li&gt;
&lt;li&gt;If the resource is on a file system that cannot be unmounted, the
    administrator can still reboot the system in a mode where he can
    access the file system regardless of SELinux controls (either
    through editing &lt;code&gt;/etc/selinux/config&lt;/code&gt; or by booting with
    &lt;code&gt;enforcing=0&lt;/code&gt;, etc.&lt;/li&gt;
&lt;li&gt;The administrator can still access the block device files on which
    the resources are directly. Specialized tools can allow for
    extracting files and directories without actually (re)mounting
    the device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more extensive list of methods to potentially gain access to such
resources is iterated in &lt;a href="http://blog.siphos.be/2013/12/limiting-file-access-with-selinux-alone/"&gt;Limiting file access with SELinux
alone&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This set of methods for gaining access is due to the administrative role
already assigned by the existing policy. To further mitigate these risks
with SELinux (although SELinux will never completely mitigate all risks)
the roles assigned to the users need to be carefully revisited. If you
grant people administrative access, but you don't want them to be able
to reboot the system, (re)mount file systems, access block devices, etc.
then create a user role that does not have these privileges at all.&lt;/p&gt;
&lt;p&gt;Creating such user roles does not require leaving behind the policy that
is already active. Additional user domains can be created and granted to
Linux accounts (including root). But in my experience, when you need to
allow a user to log on as the "root" account directly, you probably need
him to have true administrative privileges. Otherwise you'd work with
personal accounts and a well-designed &lt;code&gt;/etc/sudoers&lt;/code&gt; file.&lt;/p&gt;</content><category term="SELinux"></category></entry><entry><title>Intermediate policies</title><link href="https://blog.siphos.be/2015/07/intermediate-policies/" rel="alternate"></link><published>2015-07-05T18:17:00+02:00</published><updated>2015-07-05T18:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-07-05:/2015/07/intermediate-policies/</id><summary type="html">&lt;p&gt;When developing SELinux policies for new software (or existing ones
whose policies I don't agree with) it is often more difficult to finish
the policies so that they are broadly usable. When dealing with personal
policies, having them "just work" is often sufficient. To make the
policies reusable for distributions (or for the upstream project), a
number of things are necessary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try structuring the policy using the style as suggested by refpolicy
    or Gentoo&lt;/li&gt;
&lt;li&gt;Add the role interfaces that are most likely to be used or required,
    or which are in the current draft implemented differently&lt;/li&gt;
&lt;li&gt;Refactor some of the policies to use refpolicy/Gentoo style
    interfaces&lt;/li&gt;
&lt;li&gt;Remove the comments from the policies (as refpolicy does not want
    too verbose policies)&lt;/li&gt;
&lt;li&gt;Change or update the file context definitions for default
    installations (rather than the custom installations I use)&lt;/li&gt;
&lt;/ul&gt;
</summary><content type="html">&lt;p&gt;When developing SELinux policies for new software (or existing ones
whose policies I don't agree with) it is often more difficult to finish
the policies so that they are broadly usable. When dealing with personal
policies, having them "just work" is often sufficient. To make the
policies reusable for distributions (or for the upstream project), a
number of things are necessary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try structuring the policy using the style as suggested by refpolicy
    or Gentoo&lt;/li&gt;
&lt;li&gt;Add the role interfaces that are most likely to be used or required,
    or which are in the current draft implemented differently&lt;/li&gt;
&lt;li&gt;Refactor some of the policies to use refpolicy/Gentoo style
    interfaces&lt;/li&gt;
&lt;li&gt;Remove the comments from the policies (as refpolicy does not want
    too verbose policies)&lt;/li&gt;
&lt;li&gt;Change or update the file context definitions for default
    installations (rather than the custom installations I use)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This often takes quite some effort. Some of these changes (such as the
style updates and commenting) are even counterproductive for me
personally (in the sense that I don't gain any value from doing so and
would have to start maintaining two different policy files for the same
policy), and necessary only for upstreaming policies. As a result, I
often finish with policies that I just leave for me personally or
somewhere on a public repository (like these
&lt;a href="https://github.com/sjvermeu/small.coding/tree/master/selinux-modules/neo4j"&gt;Neo4J&lt;/a&gt;
and
&lt;a href="https://github.com/sjvermeu/small.coding/tree/master/selinux-modules/ceph"&gt;Ceph&lt;/a&gt;
policies), without any activities already scheduled to attempt to
upstream those.&lt;/p&gt;
&lt;p&gt;But not contributing the policies to a broader public means that the
effort is not known, and other contributors might be struggling with
creating policies for their favorite (or necessary) technologies. So the
majority of policies that I write I still hope to eventually push them
out. But I noticed that these last few steps for upstreaming (the ones
mentioned above) might only take a few hours of work, but take me over 6
months (or more) to accomplish (as I often find other stuff more
interesting to do).&lt;/p&gt;
&lt;p&gt;I don't know yet how to change the process to make it more interesting
to use. However, I do have a couple of wishes that might make it easier
for me, and perhaps others, to contribute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of reacting on contribution suggestions, work on a common
    repository together. Just like with a wiki, where we don't aim for a
    100% correct and well designed document from the start, we should
    use the strength of the community to continuously improve policies
    (and to allow multiple people to work on the same policy). Right
    now, policies are a one-man publication with a number of people
    commenting on the suggested changes and asking the one person to
    refactor or update the change himself.&lt;/li&gt;
&lt;li&gt;Document the style guide properly, but don't disallow contributions
    if they do not adhere to the style guide completely. Instead, merge
    and update. On successful wikis there are even people that update
    styles without content updates, and their help is greatly
    appreciated by the community.&lt;/li&gt;
&lt;li&gt;If a naming convention is to be followed (which is the case
    with policies) make it clear. Too often the name of an interface is
    something that takes a few days of discussion. That's not productive
    for policy development.&lt;/li&gt;
&lt;li&gt;Find a way to truly create a "core" part of the policy and a
    modular/serviceable approach to handle additional policies. The idea
    of the &lt;code&gt;contrib/&lt;/code&gt; repository was like that, but failed to live up to
    its expectations: the number of people who have commit access to the
    contrib is almost the same as to the core, a few exceptions
    notwithstanding, and whenever policies are added to contrib they
    often require changes on the core as well. Perhaps even support
    overlay-type approaches to policies so that intermediate policies
    can be "staged" and tested by a larger audience before they are
    vetted into the upstream reference policy.&lt;/li&gt;
&lt;li&gt;Settle on how to deal with networking controls. My suggestion would
    be to immediately support the TCP/UDP ports as assigned by IANA (or
    another set of sources) so that additional policies do not need to
    wait for the base policy to support the ports. Or find and support a
    way for contributions to declare the port types themselves (we
    probably need to focus on CIL for this).&lt;/li&gt;
&lt;li&gt;Document "best practices" on policy development where certain types
    of policies are documented in more detail. For instance, desktop
    application profiles, networked daemons, user roles, etc. These best
    practices should not be mandatory and should in fact support a broad
    set of privilege isolation. With the latter, I mean that there are
    policies who cover an entire category of systems (init systems, web
    servers), a single software package or even the sub-commands and
    sub-daemons of that package. It would surprise me if this can't be
    supported better out-of-the-box (as in, through a well
    thought-through base policy framework and styleguide).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I believe that this might create a more active community surrounding
policy development.&lt;/p&gt;</content><category term="SELinux"></category><category term="community"></category><category term="contributions"></category><category term="policy-development"></category><category term="selinux"></category></entry><entry><title>Where does CIL play in the SELinux system?</title><link href="https://blog.siphos.be/2015/06/where-does-cil-play-in-the-selinux-system/" rel="alternate"></link><published>2015-06-13T23:12:00+02:00</published><updated>2015-06-13T23:12:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-06-13:/2015/06/where-does-cil-play-in-the-selinux-system/</id><summary type="html">&lt;p&gt;SELinux policy developers already have a number of file formats to work
with. Currently, policy code is written in a set of three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.te&lt;/code&gt; file contains the SELinux policy code (type
    enforcement rules)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.if&lt;/code&gt; file contains functions which turn a set of arguments into
    blocks of SELinux policy code (interfaces). These functions are
    called by other interface files or type enforcement files&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.fc&lt;/code&gt; file contains mappings of file path expressions towards
    labels (file contexts)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These files are compiled into loadable modules (or a base module) which
are then transformed to an active policy. But this is not a single-step
approach.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;SELinux policy developers already have a number of file formats to work
with. Currently, policy code is written in a set of three files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.te&lt;/code&gt; file contains the SELinux policy code (type
    enforcement rules)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.if&lt;/code&gt; file contains functions which turn a set of arguments into
    blocks of SELinux policy code (interfaces). These functions are
    called by other interface files or type enforcement files&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.fc&lt;/code&gt; file contains mappings of file path expressions towards
    labels (file contexts)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These files are compiled into loadable modules (or a base module) which
are then transformed to an active policy. But this is not a single-step
approach.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Transforming policy code into policy file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the Linux kernel SELinux subsystem, only a single file matters - the
&lt;code&gt;policy.##&lt;/code&gt; file (for instance &lt;code&gt;policy.29&lt;/code&gt;). The suffix denotes the
binary format used as higher numbers mean that additional SELinux
features are supported which require different binary formats for the
SELinux code in the Linux kernel.&lt;/p&gt;
&lt;p&gt;With the 2.4 userspace, the transformation of the initial files as
mentioned above towards a policy file is done as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SELinux transformation diagram" src="http://dev.gentoo.org/~swift/blog/201506/formats_selinux.png"&gt;&lt;/p&gt;
&lt;p&gt;When a developer builds a policy module, first &lt;code&gt;checkmodule&lt;/code&gt; is used to
build a &lt;code&gt;.mod&lt;/code&gt; intermediate file. This file contains the type
enforcement rules with the expanded rules of the various interface
files. Next, &lt;code&gt;semodule_package&lt;/code&gt; is called which transforms this
intermediate file, together with the file context file, into a &lt;code&gt;.pp&lt;/code&gt;
file.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;.pp&lt;/code&gt; file is, in the 2.4 userspace, called a "high level language"
file. There is little high-level about it, but the idea is that such
high-level language files are then transformed into &lt;code&gt;.cil&lt;/code&gt; files (CIL
stands for &lt;em&gt;Common Intermediate Language&lt;/em&gt;). If at any moment other
frameworks come around, they could create high-level languages
themselves and provide a transformation engine to convert these HLL
files into CIL files.&lt;/p&gt;
&lt;p&gt;For the current &lt;code&gt;.pp&lt;/code&gt; files, this transformation is supported through
the &lt;code&gt;/usr/libexec/selinux/hll/pp&lt;/code&gt; binary which, given a &lt;code&gt;.pp&lt;/code&gt; file,
outputs CIL code.&lt;/p&gt;
&lt;p&gt;Finally, all CIL files (together) are compiled into a binary &lt;code&gt;policy.29&lt;/code&gt;
file. All the steps coming from a &lt;code&gt;.pp&lt;/code&gt; file towards the final binary
file are handled by the &lt;code&gt;semodule&lt;/code&gt; command. For instance, if an
administrator loads an additional &lt;code&gt;.pp&lt;/code&gt; file, its (generated) CIL code
is added to the other active CIL code and together, a new policy binary
file is created.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adding some CIL code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SELinux userspace development repository contains a &lt;code&gt;secilc&lt;/code&gt; command
which can compile CIL code into a binary policy file. As such, it can
perform the (very) last step of the file conversions above. However, it
is not &lt;em&gt;integrated&lt;/em&gt; in the sense that, if additional code is added, the
administrator can "play" with it as he would with SELinux policy
modules.&lt;/p&gt;
&lt;p&gt;Still, that shouldn't prohibit us from playing around with it to
experiment with the CIL language construct. Consider the following CIL
SELinux policy code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;; Declare a test_port_t type
(type test_port_t)
; Assign the type to the object_r role
(roletype object_r test_port_t)

; Assign the right set of attributes to the port
(typeattributeset defined_port_type test_port_t)
(typeattributeset port_type test_port_t)

; Declare tcp:1440 as test_port_t
(portcon tcp 1440 (system_u object_r test_port_t ((s0) (s0))))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code declares a port type (&lt;code&gt;test_port_t&lt;/code&gt;) and uses it for the TCP
port 1440.&lt;/p&gt;
&lt;p&gt;In order to use this code, we have to build a policy file which includes
all currently active CIL code, together with the test code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ secilc -c 29 /var/lib/selinux/mcs/active/modules/400/*/cil testport.cil
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is a &lt;code&gt;policy.29&lt;/code&gt; (the command forces version 29 as the
current Linux kernel used on this system does not support version 30)
file, which can now be copied to &lt;code&gt;/etc/selinux/mcs/policy&lt;/code&gt;. Then, after
having copied the file, load the new policy file using &lt;code&gt;load_policy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;And lo and behold, the port type is now available:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage port -l | grep 1440
test_port_t           tcp      1440
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To verify that it really is available and not just parsed by the
userspace, let's connect to it and hope for a nice denial message:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -p 1440 localhost
ssh: connect to host localhost port 1440: Permission denied

~$ sudo ausearch -ts recent
time-&amp;gt;Thu Jun 11 19:35:45 2015
type=PROCTITLE msg=audit(1434044145.829:296): proctitle=737368002D700031343430006C6F63616C686F7374
type=SOCKADDR msg=audit(1434044145.829:296): saddr=0A0005A0000000000000000000000000000000000000000100000000
type=SYSCALL msg=audit(1434044145.829:296): arch=c000003e syscall=42 success=no exit=-13 a0=3 a1=6d4d1ce050 a2=1c a3=0 items=0 ppid=2005 pid=18045 auid=1001 uid=1001 gid=1001 euid=1001 suid=1001 fsuid=1001 egid=1001 sgid=1001 fsgid=1001 tty=pts0 ses=1 comm=&amp;quot;ssh&amp;quot; exe=&amp;quot;/usr/bin/ssh&amp;quot; subj=staff_u:staff_r:ssh_t:s0 key=(null)
type=AVC msg=audit(1434044145.829:296): avc:  denied  { name_connect } for  pid=18045 comm=&amp;quot;ssh&amp;quot; dest=1440 scontext=staff_u:staff_r:ssh_t:s0 tcontext=system_u:object_r:test_port_t:s0 tclass=tcp_socket permissive=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="cil"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Live SELinux userspace ebuilds</title><link href="https://blog.siphos.be/2015/06/live-selinux-userspace-ebuilds/" rel="alternate"></link><published>2015-06-10T20:07:00+02:00</published><updated>2015-06-10T20:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-06-10:/2015/06/live-selinux-userspace-ebuilds/</id><summary type="html">&lt;p&gt;In between courses, I pushed out live ebuilds for the SELinux userspace
applications: libselinux, policycoreutils, libsemanage, libsepol,
sepolgen, checkpolicy and secilc. These live ebuilds (with Gentoo
version 9999) pull in the current development code of the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux
userspace&lt;/a&gt; so that developers
and contributors can already work with in-progress code developments as
well as see how they work on a Gentoo platform.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In between courses, I pushed out live ebuilds for the SELinux userspace
applications: libselinux, policycoreutils, libsemanage, libsepol,
sepolgen, checkpolicy and secilc. These live ebuilds (with Gentoo
version 9999) pull in the current development code of the &lt;a href="https://github.com/SELinuxProject/selinux"&gt;SELinux
userspace&lt;/a&gt; so that developers
and contributors can already work with in-progress code developments as
well as see how they work on a Gentoo platform.&lt;/p&gt;


&lt;p&gt;That being said, I do not recommend using the live ebuilds for anyone
else except developers and contributors in development zones (definitely
not on production). One of the reasons is that the ebuilds do not apply
Gentoo-specific patches to the ebuilds. I would also like to remove the
Gentoo-specific manipulations that we do, such as small Makefile
adjustments, but let's start with just ignoring the Gentoo patches.&lt;/p&gt;
&lt;p&gt;Dropping the patches makes sure that we track upstream libraries and
userspace closely, and allows developers to try and send out patches to
the SELinux project to fix Gentoo related build problems. But as not all
packages can be deployed successfully on a Gentoo system some patches
need to be applied anyway. For this, users can drop the necessary
patches inside &lt;code&gt;/etc/portage/patches&lt;/code&gt; as all userspace ebuilds use the
&lt;em&gt;epatch_user&lt;/em&gt; method.&lt;/p&gt;
&lt;p&gt;Finally, observant users will notice that "secilc" is also provided.
This is a new package, which is probably going to have an official
release with a new userspace release. It allows for building CIL-based
SELinux policy code, and was one of the drivers for me to create the
live ebuilds as I'm experimenting with the CIL constructions. So expect
more on that later.&lt;/p&gt;</content><category term="Gentoo"></category><category term="cil"></category><category term="Gentoo"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>PostgreSQL with central authentication and authorization</title><link href="https://blog.siphos.be/2015/05/postgresql-with-central-authentication-and-authorization/" rel="alternate"></link><published>2015-05-25T12:07:00+02:00</published><updated>2015-05-25T12:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-25:/2015/05/postgresql-with-central-authentication-and-authorization/</id><summary type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;


&lt;p&gt;Centralized management is an important principle whenever deployments
become very dispersed. For authentication and authorization, having a
high-available LDAP is one of the more powerful components in any
architecture. It isn't the only method though - it is also possible to
use a distributed approach where the master data is centrally managed,
but the proper data is distributed to the various systems that need it.
Such a distributed approach allows for high availability without the
need for a highly available central infrastructure (user ids, group
membership and passwords are distributed to the servers rather than
queried centrally). Here, I'm going to focus on a mixture of both
methods: central authentication for password verification, and
distributed authorization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PostgreSQL default uses in-database credentials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By default, PostgreSQL uses in-database credentials for the
authentication and authorization. When a &lt;code&gt;CREATE ROLE&lt;/code&gt; (or
&lt;code&gt;CREATE USER&lt;/code&gt;) command is issued with a password, it is stored in the
&lt;code&gt;pg_catalog.pg_authid&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# select rolname, rolpassword from pg_catalog.pg_authid;
    rolname     |             rolpassword             
----------------+-------------------------------------
 postgres_admin | 
 dmvsl          | 
 johan          | 
 hdc_owner      | 
 hdc_reader     | 
 hdc_readwrite  | 
 hadoop         | 
 swift          | 
 sean           | 
 hdpreport      | 
 postgres       | md5c127bc9fc185daf0e06e785876e38484
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;this cannot be moved outside):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# \l db_hadoop
                                   List of databases
   Name    |   Owner   | Encoding |  Collate   |   Ctype    |     Access privileges     
-----------+-----------+----------+------------+------------+---------------------------
 db_hadoop | hdc_owner | UTF8     | en_US.utf8 | en_US.utf8 | hdc_owner=CTc/hdc_owner  +
           |           |          |            |            | hdc_reader=c/hdc_owner   +
           |           |          |            |            | hdc_readwrite=c/hdc_owner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Furthermore, PostgreSQL has some additional access controls through its
&lt;code&gt;pg_hba.conf&lt;/code&gt; file, in which the access towards the PostgreSQL service
itself can be governed based on context information (such as originating
IP address, target database, etc.).&lt;/p&gt;
&lt;p&gt;For more information about the standard setups for PostgreSQL,
&lt;em&gt;definitely&lt;/em&gt; go through the &lt;a href="http://www.postgresql.org/docs/9.4/static/index.html"&gt;official PostgreSQL
documentation&lt;/a&gt; as
it is well documented and kept up-to-date.&lt;/p&gt;
&lt;p&gt;Now, for central management, in-database settings become more difficult
to handle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using PAM for authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first step to move the management of authentication and
authorization outside the database is to look at a way to authenticate
users (password verification) outside the database. I tend not to use a
distributed password approach (where a central component is responsible
for changing passwords on multiple targets), instead relying on a
high-available LDAP setup, but with local caching (to catch short-lived
network hick-ups) and local password use for last-hope accounts (such as
root and admin accounts).&lt;/p&gt;
&lt;p&gt;PostgreSQL can be configured to directly interact with an LDAP, but I
like to use &lt;a href="http://www.linux-pam.org/"&gt;Linux PAM&lt;/a&gt; whenever I can. For
my systems, it is a standard way of managing the authentication of many
services, so the same goes for PostgreSQL. And with the
&lt;a href="https://packages.gentoo.org/package/sys-auth/pam_ldap"&gt;sys-auth/pam_ldap&lt;/a&gt;
package integrating multiple services with LDAP is a breeze. So the
first step is to have PostgreSQL use PAM for authentication. This is
handled through its &lt;code&gt;pg_hba.conf&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# TYPE  DATABASE        USER    ADDRESS         METHOD          [OPTIONS]
local   all             all                     md5
host    all             all     all             pam             pamservice=postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will have PostgreSQL use the &lt;code&gt;postgresql&lt;/code&gt; PAM service for
authentication. The PAM configuration is thus in
&lt;code&gt;/etc/pam.d/postgresql&lt;/code&gt;. In it, we can either directly use the LDAP PAM
modules, or use the SSSD modules and have SSSD work with LDAP.&lt;/p&gt;
&lt;p&gt;Yet, this isn't sufficient. We still need to tell PostgreSQL which users
can be authenticated - the users need to be defined in the database
(just without password credentials because that is handled externally
now). This is done together with the authorization handling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Users and group membership&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every service on the systems I maintain has dedicated groups in which
for instance its administrators are listed. For instance, for the
PostgreSQL services:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqladmin
gpgsqladmin:x:413843:swift,dmvsl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A local batch job (ran through cron) queries this group (which I call
the &lt;em&gt;masterlist&lt;/em&gt;, as well as queries which users in PostgreSQL are
assigned the &lt;code&gt;postgres_admin&lt;/code&gt; role (which is a superuser role like
postgres and is used as the intermediate role to assign to
administrators of a PostgreSQL service), known as the &lt;em&gt;slavelist&lt;/em&gt;.
Delta's are then used to add the user or remove it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Note: membersToAdd / membersToRemove / _psql are custom functions
#       so do not vainly search for them on your system ;-)
for member in $(membersToAdd ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;CREATE USER ${member} LOGIN INHERIT;&amp;quot; postgres
  _psql &amp;quot;GRANT postgres_admin TO ${member};&amp;quot; postgres
done

for member in $(membersToRemove ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;REVOKE postgres_admin FROM ${member};&amp;quot; postgres
  _psql &amp;quot;DROP USER ${member};&amp;quot; postgres
done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;postgres_admin&lt;/code&gt; role is created whenever I create a PostgreSQL
instance. Likewise, for databases, a number of roles are added as well.
For instance, for the &lt;code&gt;db_hadoop&lt;/code&gt; database, the &lt;code&gt;hdc_owner&lt;/code&gt;,
&lt;code&gt;hdc_reader&lt;/code&gt; and &lt;code&gt;hdc_readwrite&lt;/code&gt; roles are created with the right set of
privileges. Users are then granted this role if they belong to the right
group in the LDAP. For instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqlhdc_own
gpgsqlhdc_own:x:413850:hadoop,johan,christov,sean
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this simple approach, granting users access to a database is a
matter of adding the user to the right group (like &lt;code&gt;gpgsqlhdc_ro&lt;/code&gt; for
read-only access to the Hadoop related database(s)) and either wait for
the cron-job to add it, or manually run the authorization
synchronization. By standardizing on infrastructural roles (admin,
auditor) and data roles (owner, rw, ro) managing multiple databases is a
breeze.&lt;/p&gt;</content><category term="Free-Software"></category><category term="postgresql"></category></entry><entry><title>Testing with permissive domains</title><link href="https://blog.siphos.be/2015/05/testing-with-permissive-domains/" rel="alternate"></link><published>2015-05-18T13:40:00+02:00</published><updated>2015-05-18T13:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-18:/2015/05/testing-with-permissive-domains/</id><summary type="html">&lt;p&gt;When testing out new technologies or new setups, not having (proper)
SELinux policies can be a nuisance. Not only are the number of SELinux
policies that are available through the standard repositories limited,
some of these policies are not even written with the same level of
confinement that an administrator might expect. Or perhaps the
technology to be tested is used in a completely different manner.&lt;/p&gt;
&lt;p&gt;Without proper policies, any attempt to start such a daemon or
application might or will cause permission violations. In many cases,
developers or users tend to disable SELinux enforcing then so that they
can continue playing with the new technology. And why not? After all,
policy development is to be done &lt;em&gt;after&lt;/em&gt; the technology is understood.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When testing out new technologies or new setups, not having (proper)
SELinux policies can be a nuisance. Not only are the number of SELinux
policies that are available through the standard repositories limited,
some of these policies are not even written with the same level of
confinement that an administrator might expect. Or perhaps the
technology to be tested is used in a completely different manner.&lt;/p&gt;
&lt;p&gt;Without proper policies, any attempt to start such a daemon or
application might or will cause permission violations. In many cases,
developers or users tend to disable SELinux enforcing then so that they
can continue playing with the new technology. And why not? After all,
policy development is to be done &lt;em&gt;after&lt;/em&gt; the technology is understood.&lt;/p&gt;


&lt;p&gt;But completely putting the system in permissive mode is overshooting. It
is much easier to make a very simple policy to start with, and then mark
the domain as a permissive domain. What happens is that the software
then, after transitioning into the "simple" domain, is not part of the
SELinux enforcements anymore whereas the rest of the system remains in
SELinux enforcing mode.&lt;/p&gt;
&lt;p&gt;For instance, create a minuscule policy like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;policy_module(testdom, 1.0)

type testdom_t;
type testdom_exec_t;
init_daemon_domain(testdom_t, testdom_exec_t)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Mark the executable for the daemon as &lt;code&gt;testdom_exec_t&lt;/code&gt; (after building
and loading the minuscule policy):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# chcon -t testdom_exec_t /opt/something/bin/daemond
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, tell SELinux that &lt;code&gt;testdom_t&lt;/code&gt; is to be seen as a permissive
domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage permissive -a testdom_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When finished, don't forget to remove the permissive bit
(&lt;code&gt;semanage permissive -d testdom_t&lt;/code&gt;) and unload/remove the SELinux
policy module.&lt;/p&gt;
&lt;p&gt;And that's it. If the daemon is now started (through a standard init
script) it will run as &lt;code&gt;testdom_t&lt;/code&gt; and everything it does will be
logged, but not enforced by SELinux. That might even help in
understanding the application better.&lt;/p&gt;</content><category term="SELinux"></category><category term="permissive"></category><category term="policy"></category><category term="selinux"></category><category term="semanage"></category><category term="test"></category></entry><entry><title>Audit buffering and rate limiting</title><link href="https://blog.siphos.be/2015/05/audit-buffering-and-rate-limiting/" rel="alternate"></link><published>2015-05-10T14:18:00+02:00</published><updated>2015-05-10T14:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-05-10:/2015/05/audit-buffering-and-rate-limiting/</id><summary type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be logged
through the audit subsystem. Depending on the system configuration, they
might be either ignored, sent through the kernel logging infrastructure
or even have the system panic. And if the messages are sent to the
kernel log then they might show up, but even that log has its
limitations, which can lead to output similar to the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;__ratelimit: 53 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this post, I want to give some pointers in configuring the audit
subsystem as well as understand these messages...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is auditd and kauditd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you take a look at the audit processes running on the system, you'll
notice that (assuming Linux auditing is used of course) two processes
are running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# ps -ef | grep audit
root      1483     1  0 10:11 ?        00:00:00 /sbin/auditd
root      1486     2  0 10:11 ?        00:00:00 [kauditd]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;/sbin/auditd&lt;/code&gt; daemon is the user-space audit daemon. It &lt;a href="http://man7.org/linux/man-pages/man3/audit_open.3.html"&gt;registers
itself&lt;/a&gt; with the
Linux kernel audit subsystem (through the audit netlink system), which
responds with spawning the &lt;code&gt;kauditd&lt;/code&gt; kernel thread/process. The fact
that the process is a kernel-level one is why the &lt;code&gt;kauditd&lt;/code&gt; is
surrounded by brackets in the &lt;code&gt;ps&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;Once this is done, audit messages are communicated through the netlink
socket to the user-space audit daemon. For the detail-oriented people
amongst you, look for the &lt;em&gt;kauditd_send_skb()&lt;/em&gt; method in the
&lt;a href="http://lxr.free-electrons.com/source/kernel/audit.c"&gt;kernel/audit.c&lt;/a&gt;
file. Now, generated audit event messages are not directly relayed to
the audit daemon - they are first queued in a sort-of backlog, which is
where the backlog-related messages above come from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audit backlog queue&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the kernel-level audit subsystem, a socket buffer queue is used to
hold audit events. Whenever a new audit event is received, it is logged
and prepared to be added to this queue. Adding to this queue can be
controlled through a few parameters.&lt;/p&gt;
&lt;p&gt;The first parameter is the backlog limit. Be it through a kernel boot
parameter (&lt;code&gt;audit_backlog_limit=N&lt;/code&gt;) or through a message relayed by the
user-space audit daemon (&lt;code&gt;auditctl -b N&lt;/code&gt;), this limit will ensure that a
queue cannot grow beyond a certain size (expressed in the amount of
messages). If an audit event is logged which would grow the queue beyond
this limit, then a failure occurs and is handled according to the system
configuration (more on that later).&lt;/p&gt;
&lt;p&gt;The second parameter is the rate limit. When more audit events are
logged within a second than set through this parameter (which can be
controlled through a message relayed by the user-space audit system,
using &lt;code&gt;auditctl -r N&lt;/code&gt;) then those audit events are not added to the
queue. Instead, a failure occurs and is handled according to the system
configuration.&lt;/p&gt;
&lt;p&gt;Only when the limits are not reached is the message added to the queue,
allowing the user-space audit daemon to consume those events and log
those according to the audit configuration. There are some good
resources on audit configuration available on the Internet. I find &lt;a href="http://webapp5.rrz.uni-hamburg.de/SuSe-Dokumentation/manual/sles-manuals_en/cha.audit.comp.html"&gt;this
SuSe
chapter&lt;/a&gt;
worth reading, but many others exist as well.&lt;/p&gt;
&lt;p&gt;There is a useful command related to the subject of the audit backlog
queue. It queries the audit subsystem for its current status:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# auditctl -s
AUDIT_STATUS: enabled=1 flag=1 pid=1483 rate_limit=0 backlog_limit=8192 lost=3 backlog=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The command displays not only the audit state (enabled or not) but also
the settings for rate limits (on the audit backlog) and backlog limit.
It also shows how many events are currently still waiting in the backlog
queue (which is zero in our case, so the audit user-space daemon has
properly consumed and logged the audit events).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Failure handling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If an audit event cannot be logged, then this failure needs to be
resolved. The Linux audit subsystem can be configured do either silently
discard the message, switch to the kernel log subsystem, or panic. This
can be configured through the audit user-space (&lt;code&gt;auditctl -f [0..2]&lt;/code&gt;),
but is usually left at the default (which is 1, being to switch to the
kernel log subsystem).&lt;/p&gt;
&lt;p&gt;Before that is done, the message is displayed which reveals the cause of
the failure handling:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the backlog queue was set to contain at most 320 entries
(which is low for a production system) and more messages were being
added (the Linux kernel in certain cases allows to have a few more
entries than configured for performance and consistency reasons). The
number of events already lost is displayed, as well as the current
limitation settings. The message "backlog limit exceeded" can be "rate
limit exceeded" if that was the limitation that was triggered.&lt;/p&gt;
&lt;p&gt;Now, if the system is not configured to silently discard it, or to panic
the system, then the "dropped" messages are sent to the kernel log
subsystem. The calls however are also governed through a configurable
limitation: it uses a rate limit which can be set through &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# sysctl -a | grep kernel.printk_rate
kernel.printk_ratelimit = 5
kernel.printk_ratelimit_burst = 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, this system allows one message every 5 seconds,
but does allow a burst of up to 10 messages at once. When the rate
limitation kicks in, then the kernel will log (at most one per second)
the number of suppressed events:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[40676.545099] __ratelimit: 246 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although this limit is kernel-wide, not all kernel log events are
governed through it. It is the caller subsystem (in our case, the audit
subsystem) which is responsible for having its events governed through
this rate limitation or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing up&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before waving goodbye, I would like to point out that the backlog queue
is a memory queue (and not &lt;a href="https://access.redhat.com/solutions/19327"&gt;on disk, Red
Hat&lt;/a&gt;), just in case it wasn't
obvious. Increasing the queue size can result in more kernel memory
consumption. Apparently, a &lt;a href="https://www.redhat.com/archives/linux-audit/2011-October/msg00007.html"&gt;practical size
estimate&lt;/a&gt;
is around 9000 bytes per message. On production systems, it is advised
not to make this setting too low. I personally set it to 8192.&lt;/p&gt;
&lt;p&gt;Lost audit events might result in difficulties for troubleshooting,
which is the case when dealing with new or experimental SELinux
policies. It would also result in missing security-important events. It
is the audit subsystem, after all. So tune it properly, and enjoy the
power of Linux' audit subsystem.&lt;/p&gt;</content><category term="Free-Software"></category><category term="audit"></category><category term="kernel"></category><category term="security"></category><category term="selinux"></category></entry><entry><title>Use change management when you are using SELinux to its fullest</title><link href="https://blog.siphos.be/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/" rel="alternate"></link><published>2015-04-30T20:58:00+02:00</published><updated>2015-04-30T20:58:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-04-30:/2015/04/use-change-management-when-you-are-using-selinux-to-its-fullest/</id><summary type="html">&lt;p&gt;If you are using SELinux on production systems (with which I mean
systems that you offer services with towards customers or other parties
beyond you, yourself and your ego), please consider proper change
management if you don't do already. SELinux is a very sensitive security
subsystem - not in the sense …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are using SELinux on production systems (with which I mean
systems that you offer services with towards customers or other parties
beyond you, yourself and your ego), please consider proper change
management if you don't do already. SELinux is a very sensitive security
subsystem - not in the sense that it easily fails, but because it is
very fine-grained and as such can easily stop applications from running
when their behavior changes just a tiny bit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sensitivity of SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SELinux is a wonderful security measure for Linux systems that can
prevent successful exploitation of vulnerabilities or misconfigurations.
Of course, it is not the sole security measure that systems should take.
Proper secure configuration of services, least privilege accounts,
kernel-level mitigations such as grSecurity and more are other measures
that certainly need to be taken if you really find system security to be
a worthy goal to attain. But I'm not going to talk about those others
right now. What I am going to focus on is SELinux, and how sensitive it
is to changes.&lt;/p&gt;
&lt;p&gt;An important functionality of SELinux to understand is that it
segregates the security control system itself (the SELinux subsystem)
from its configuration (the policy). The security control system itself
is relatively small, and focuses on enforcement of the policy and
logging (either because the policy asks to log something, or because
something is prevented, or because an error occurred). The most
difficult part of handling SELinux on a system is not enabling or
interacting with it. No, it is its policy.&lt;/p&gt;
&lt;p&gt;The policy is also what makes SELinux so darn sensitive for small system
changes (or behavior that is not either normal, or at least not allowed
through the existing policy). Let me explain with a small situation that
I recently had.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case in point: Switching an IP address&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A case that beautifully shows how sensitive SELinux can be is an IP
address change. My systems all obtain their IP address (at least for
IPv4) from a DHCP system. This is of course acceptable behavior as
otherwise my systems would never be able to boot up successfully anyway.
The SELinux policy that I run also allows this without any hindrance. So
that was not a problem.&lt;/p&gt;
&lt;p&gt;Yet recently I had to switch an IP address for a system in production.
All the services I run are set up in a dual-active mode, so I started
with the change by draining the services to the second system, shutting
down the service and then (after reconfiguring the DHCP system to now
provide a different IP address) reload the network configuration. And
then it happened - the DHCP client just stalled.&lt;/p&gt;
&lt;p&gt;As the change failed, I updated the DHCP system again to deliver the old
IP address and then reloaded the network configuration on the client.
Again, it failed. Dumbstruck, I looked at the AVC denials and lo and
behold, I notice a &lt;code&gt;dig&lt;/code&gt; process running in a DHCP client related domain
that is trying to do UDP binds, which the policy (at that time) did not
allow. But why now suddenly, after all - this system was running happily
for more than a year already (and with occasional reboots for kernel
updates).&lt;/p&gt;
&lt;p&gt;I won't bore you with the investigation. It boils down to the fact that
the DHCP client detected a change compared to previous startups, and was
configured to run a few hooks as additional steps in the IP lease setup.
As these hooks were never ran previously, the policy was never
challenged to face this. And since the address change occurred a revert
to the previous situation didn't work either (as its previous state
information was already deleted).&lt;/p&gt;
&lt;p&gt;I was able to revert the client (which is a virtual guest in KVM) to the
situation right before the change (thank you &lt;code&gt;savevm&lt;/code&gt; and &lt;code&gt;loadvm&lt;/code&gt;
functionality) so that I could work on the policy first in a
non-production environment so that the next change attempt was
successful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The previous situation might be "solved" by temporarily putting the DHCP
client domain in permissive mode just for the change and then back. But
that is ignoring the issue, and unless you have perfect operational
documentation that you always read before making system or configuration
changes, I doubt that you'll remember this for the next time.&lt;/p&gt;
&lt;p&gt;The case is also a good example on the sensitivity of SELinux. It is not
just when software is being upgraded. Every change (be it in
configuration, behavior or operational activity) might result in a
situation that is new for the loaded SELinux policy. As the default
action in SELinux is to deny everything, this will result in unexpected
results on the system. Sometimes very visible (no IP address obtained),
sometimes hidden behind some weird behavior (hostname correctly set but
not the domainname) or perhaps not even noticed until far later. Compare
it to the firewall rule configurations: you might be able to easily
confirm that standard flows are still passed through, but how are you
certain that fallback flows or one-in-a-month connection setups are not
suddenly prevented from happening.&lt;/p&gt;
&lt;p&gt;A somewhat better solution than just temporarily disabling SELinux
access controls for a domain is to look into proper change management.
Whenever a change has to be done, make sure that you&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can easily revert the change back to the previous
    situation (backups!)&lt;/li&gt;
&lt;li&gt;have tested the change on a non-vital (preproduction) system first&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two principles are pretty vital when you are serious about using
SELinux in production. I'm not talking about a system that hardly has
any fine-grained policies, like where most of the system's services are
running in "unconfined" domains (although that's still better than not
running with SELinux at all), but where you are truly trying to put a
least privilege policy in place for all processes and services.&lt;/p&gt;
&lt;p&gt;Being able to revert a change allows you to quickly get a service up and
running again so that customers are not affected by the change (and
potential issues) for long time. First fix the service, then fix the
problem. If you are an engineer like me, you might rather focus on the
problem (and a permanent, correct solution) first. But that's wrong -
always first make sure that the customers are not affected by it. Revert
and put the service back up, and then investigate so that the next
change attempt will not go wrong anymore.&lt;/p&gt;
&lt;p&gt;Having a multi-master setup might give some more leeway into
investigating issues (as the service itself is not disrupted) so in the
case mentioned above I would probably have tried fixing the issue
immediately anyway if it wasn't policy-based. But most users do not have
truly multi-master service setups.&lt;/p&gt;
&lt;p&gt;Being able to test (and retest) changes in non-production also allows
you to focus on automation (so that changes can be done faster and in a
repeated, predictable and qualitative manner), regression testing as
well as change accumulation testing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You don't have time for that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Be honest with yourself. If you support services for others (be it in a
paid-for manner or because you support an organization in your free
time) you'll quickly learn that service availability is one of the most
qualitative aspects of what you do. No matter what mess is behind it,
most users don't see all that. All they see is the service itself (and
its performance / features). If a change you wanted to make made a
service unavailable for hours, users will notice. And if the change
wasn't communicated up front or it is the n-th time that this downtime
occurs, they will start asking questions you rather not hear.&lt;/p&gt;
&lt;p&gt;Using a non-production environment is not that much of an issue if the
infrastructure you work with supports bare metal restores, or
snapshot/cloning (in case of VMs). After doing those a couple of times,
you'll easily find that you can create a non-production environment from
the production one. Or, you can go for a permanent non-production
environment (although you'll need to take care that this environment is
at all times representative for the production systems).&lt;/p&gt;
&lt;p&gt;And regarding qualitative changes, I really recommend to use a
configuration management system. I recently switched from Puppet to
Saltstack and have yet to use the latter to its fullest set (most of
what I do is still scripted), but it is growing on me and I'm pretty
convinced that I'll have the majority of my change management scripts
removed by the end of this year towards Saltstack-based configurations.
And that'll allow me to automate changes and thus provide a more
qualitative service offering.&lt;/p&gt;
&lt;p&gt;With SELinux, of course.&lt;/p&gt;</content><category term="SELinux"></category><category term="change-management"></category><category term="policy"></category><category term="selinux"></category></entry><entry><title>Moving closer to 2.4 stabilization</title><link href="https://blog.siphos.be/2015/04/moving-closer-to-2-4-stabilization/" rel="alternate"></link><published>2015-04-27T19:18:00+02:00</published><updated>2015-04-27T19:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-04-27:/2015/04/moving-closer-to-2-4-stabilization/</id><summary type="html">&lt;p&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki"&gt;SELinux userspace&lt;/a&gt;
project has released version 2.4 in february this year, after release
candidates have been tested for half a year. After its release, we at
the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened&lt;/a&gt;
project have been working hard to integrate it within Gentoo. This
effort has been made a bit more difficult …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki"&gt;SELinux userspace&lt;/a&gt;
project has released version 2.4 in february this year, after release
candidates have been tested for half a year. After its release, we at
the &lt;a href="https://wiki.gentoo.org/wiki/Project:Hardened"&gt;Gentoo Hardened&lt;/a&gt;
project have been working hard to integrate it within Gentoo. This
effort has been made a bit more difficult due to the migration of the
policy store from one location to another while at the same time
switching to HLL- and CIL based builds.&lt;/p&gt;
&lt;p&gt;Lately, 2.4 itself has been pretty stable, and we're focusing on the
proper migration from 2.3 to 2.4. The SELinux policy has been adjusted
to allow the migrations to work, and a few final fixes are being tested
so that we can safely transition our stable users from 2.3 to 2.4.
Hopefully we'll be able to stabilize the userspace this month or
beginning of next month.&lt;/p&gt;</content><category term="Gentoo"></category><category term="2.4"></category><category term="Gentoo"></category><category term="hardened"></category><category term="selinux"></category><category term="userspace"></category></entry><entry><title>Trying out Pelican, part one</title><link href="https://blog.siphos.be/2015/03/trying-out-pelican-part-one/" rel="alternate"></link><published>2015-03-06T20:02:00+01:00</published><updated>2015-03-06T20:02:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-03-06:/2015/03/trying-out-pelican-part-one/</id><summary type="html">&lt;p&gt;One of the goals I've set myself to do this year (not as a new year
resolution though, I *really* want to accomplish this ;-) is to move
my blog from Wordpress to a statically built website. And
&lt;a href="http://docs.getpelican.com/en/3.5.0/"&gt;Pelican&lt;/a&gt; looks to be a good
solution to do so. It's based on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the goals I've set myself to do this year (not as a new year
resolution though, I *really* want to accomplish this ;-) is to move
my blog from Wordpress to a statically built website. And
&lt;a href="http://docs.getpelican.com/en/3.5.0/"&gt;Pelican&lt;/a&gt; looks to be a good
solution to do so. It's based on Python, which is readily available and
supported on Gentoo, and is quite readable. Also, it looks to be very
active in development and support. And also: it supports taking data
from an existing Wordpress installation, so that none of the posts are
lost (with some rounding error that's inherit to such migrations of
course).&lt;/p&gt;
&lt;p&gt;Before getting Pelican ready (which is available through Gentoo btw) I
also needed to install &lt;a href="http://johnmacfarlane.net/pandoc/"&gt;pandoc&lt;/a&gt;, and
that became more troublesome than expected. While installing &lt;code&gt;pandoc&lt;/code&gt; I
got hit by its massive amount of dependencies towards &lt;code&gt;dev-haskell/*&lt;/code&gt;
packages, and many of those packages really failed to install. It does
some internal dependency checking and fails, informing me to run
&lt;code&gt;haskell-updater&lt;/code&gt;. Sadly, multiple re-runs of said command did not
resolve the issue. In fact, it wasn't until I hit a &lt;a href="http://forums.gentoo.org/viewtopic-p-7712250.html?sid=7707e62264dadf8bad4b8a1273b19f77"&gt;forum post about
the same
issue&lt;/a&gt;
that a first step to a working solution was found.&lt;/p&gt;
&lt;p&gt;It turns out that the &lt;code&gt;~arch&lt;/code&gt; versions of the haskell packages are
better working. So I enabled &lt;code&gt;dev-haskell/*&lt;/code&gt; in my
&lt;code&gt;package.accept_keywords&lt;/code&gt; file. And then started updating the
packages... which also failed. Then I ran &lt;code&gt;haskell-updater&lt;/code&gt; multiple
times, but that also failed. After a while, I had to run the following
set of commands (in random order) just to get everything to build fine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# emerge -u $(qlist -IC dev-haskell) --keep-going
~# for n in $(qlist -IC dev-haskell); do emerge -u $n; done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It took quite some reruns, but it finally got through. I never thought I
had this much Haskell-related packages installed on my system (89
packages here to be exact), as I never intended to do any Haskell
development since I left the university. Still, I finally got &lt;code&gt;pandoc&lt;/code&gt;
to work. So, on to the migration of my Wordpress site... I thought.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a good time to ask for stabilization requests (I'll look into
it myself as well of course) but also to see if you can help out our
arch testing teams to support the stabilization requests on Gentoo! We
need you!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I started with the &lt;a href="http://docs.getpelican.com/en/latest/importer.html"&gt;official docs on
importing&lt;/a&gt;. Looks
promising, but it didn't turn out too well for me. Importing was okay,
but then immediately building the site again resulted in issues about
wrong arguments (file names being interpreted as an argument name or
function when an underscore was used) and interpretation of code inside
the posts. Then I found Jason Antman's &lt;a href="http://blog.jasonantman.com/2014/02/converting-wordpress-posts-to-pelican-markdown/"&gt;converting wordpress posts to
pelican
markdown&lt;/a&gt;
post to inform me I had to try using markdown instead of restructured
text. And lo and behold - that's much better.&lt;/p&gt;
&lt;p&gt;The first builds look promising. Of all the posts that I made on
Wordpress, only one gives a build failure. The next thing to investigate
is theming, as well as seeing how good the migration goes (it isn't
because there are no errors otherwise that the migration is successful
of course) so that I know how much manual labor I have to take into
consideration when I finally switch (right now, I'm still running
Wordpress).&lt;/p&gt;</content><category term="Gentoo"></category><category term="blog"></category><category term="Gentoo"></category><category term="haskell"></category><category term="pandoc"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>CIL and attributes</title><link href="https://blog.siphos.be/2015/02/cil-and-attributes/" rel="alternate"></link><published>2015-02-15T15:49:00+01:00</published><updated>2015-02-15T15:49:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-02-15:/2015/02/cil-and-attributes/</id><summary type="html">&lt;p&gt;I keep on struggling to remember this, so let's make a blog post out of
it ;-)&lt;/p&gt;
&lt;p&gt;When the SELinux policy is being built, recent userspace (2.4 and
higher) will convert the policy into CIL language, and then build the
binary policy. When the policy supports type attributes, these are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I keep on struggling to remember this, so let's make a blog post out of
it ;-)&lt;/p&gt;
&lt;p&gt;When the SELinux policy is being built, recent userspace (2.4 and
higher) will convert the policy into CIL language, and then build the
binary policy. When the policy supports type attributes, these are of
course also made available in the CIL code. For instance the
&lt;code&gt;admindomain&lt;/code&gt; attribute from the &lt;code&gt;userdomain&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;...
(typeattribute admindomain)
(typeattribute userdomain)
(typeattribute unpriv_userdomain)
(typeattribute user_home_content_type)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Interfaces provided by the module are also applied. You won't find the
interface CIL code in &lt;code&gt;/var/lib/selinux/mcs/active/modules&lt;/code&gt; though; the
code at that location is already "expanded" and filled in. So for the
&lt;code&gt;sysadm_t&lt;/code&gt; domain we have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Equivalent of
# gen_require(`
#   attribute admindomain;
#   attribute userdomain;
# &amp;#39;)
# typeattribute sysadm_t admindomain;
# typeattribute sysadm_t userdomain;

(typeattributeset cil_gen_require admindomain)
(typeattributeset admindomain (sysadm_t ))
(typeattributeset cil_gen_require userdomain)
(typeattributeset userdomain (sysadm_t ))
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, when checking which domains use the &lt;code&gt;admindomain&lt;/code&gt; attribute,
notice the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# seinfo -aadmindomain -x
ERROR: Provided attribute (admindomain) is not a valid attribute name.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But don't panic - this has a reason: as long as there is no SELinux rule
applied towards the &lt;code&gt;admindomain&lt;/code&gt; attribute, then the SELinux policy
compiler will drop the attribute from the final policy. This can be
confirmed by adding a single, cosmetic rule, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## allow admindomain admindomain:process sigchld;

~# seinfo -aadmindomain -x
   admindomain
      sysadm_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So there you go. That does mean that if something previously used the
attribute assignation for any decisions (like "for each domain assigned
the userdomain attribute, do something") will need to make sure that the
attribute is really used in a policy rule.&lt;/p&gt;</content><category term="SELinux"></category><category term="attribute"></category><category term="cil"></category><category term="selinux"></category></entry><entry><title>Have dhcpcd wait before backgrounding</title><link href="https://blog.siphos.be/2015/02/have-dhcpcd-wait-before-backgrounding/" rel="alternate"></link><published>2015-02-08T16:50:00+01:00</published><updated>2015-02-08T16:50:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-02-08:/2015/02/have-dhcpcd-wait-before-backgrounding/</id><summary type="html">&lt;p&gt;Many of my systems use DHCP for obtaining IP addresses. Even though they
all receive a static IP address, it allows me to have them moved over
(migrations), use TFTP boot, cloning (in case of quick testing), etc.
But one of the things that was making my efforts somewhat more …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many of my systems use DHCP for obtaining IP addresses. Even though they
all receive a static IP address, it allows me to have them moved over
(migrations), use TFTP boot, cloning (in case of quick testing), etc.
But one of the things that was making my efforts somewhat more difficult
was that the &lt;code&gt;dhcpcd&lt;/code&gt; service continued (the &lt;code&gt;dhcpcd&lt;/code&gt; daemon immediately
went in the background) even though no IP address was received yet.
Subsequent service scripts that required a working network connection
failed to start then.&lt;/p&gt;
&lt;p&gt;The solution is to configure &lt;code&gt;dhcpcd&lt;/code&gt; to wait for an IP address. This is
done through the &lt;code&gt;-w&lt;/code&gt; option, or the &lt;code&gt;waitip&lt;/code&gt; instruction in the
&lt;code&gt;dhcpcd.conf&lt;/code&gt; file. With that in place, the service script now waits
until an IP address is assigned.&lt;/p&gt;</content><category term="Gentoo"></category><category term="dhcp"></category><category term="dhcpcd"></category><category term="Gentoo"></category></entry><entry><title>Old Gentoo system? Not a problem...</title><link href="https://blog.siphos.be/2015/01/old-gentoo-system-not-a-problem/" rel="alternate"></link><published>2015-01-21T23:05:00+01:00</published><updated>2015-01-21T23:05:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-21:/2015/01/old-gentoo-system-not-a-problem/</id><summary type="html">&lt;p&gt;If you have a very old Gentoo system that you want to upgrade, you might
have some issues with too old software and Portage which can't just
upgrade to a recent state. Although many methods exist to work around
it, one that I have found to be very useful is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you have a very old Gentoo system that you want to upgrade, you might
have some issues with too old software and Portage which can't just
upgrade to a recent state. Although many methods exist to work around
it, one that I have found to be very useful is to have access to old
Portage snapshots. It often allows the administrator to upgrade the
system in stages (say in 6-months blocks), perhaps not the entire world
but at least the system set.&lt;/p&gt;
&lt;p&gt;Finding old snapshots might be difficult though, so at one point I
decided to create &lt;a href="http://dev.gentoo.org/~swift/snapshots/"&gt;a list of old
snapshots&lt;/a&gt;, two months apart,
together with the GPG signature (so people can verify that the snapshot
was not tampered with by me in an attempt to create a Gentoo botnet). I
haven't needed it in a while anymore, but I still try to update the list
every two months, which I just did with the snapshot of January 20th
this year.&lt;/p&gt;
&lt;p&gt;I hope it at least helps a few other admins out there.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="portage"></category><category term="snapshot"></category><category term="tree"></category></entry><entry><title>SELinux is great for enterprises (but many don't know it yet)</title><link href="https://blog.siphos.be/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/" rel="alternate"></link><published>2015-01-03T13:36:00+01:00</published><updated>2015-01-03T13:36:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-03:/2015/01/selinux-is-great-for-enterprises-but-many-dont-know-it-yet/</id><summary type="html">&lt;p&gt;Large companies that handle their own IT often have internal support
teams for many of the technologies that they use. Most of the time, this
is for reusable components like database technologies, web application
servers, operating systems, middleware components (like file transfers,
messaging infrastructure, ...) and more. All components that are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Large companies that handle their own IT often have internal support
teams for many of the technologies that they use. Most of the time, this
is for reusable components like database technologies, web application
servers, operating systems, middleware components (like file transfers,
messaging infrastructure, ...) and more. All components that are used
and deployed multiple times, and thus warrant the expenses of a
dedicated engineering team.&lt;/p&gt;
&lt;p&gt;Such teams often have (or need to write) secure configuration deployment
guides, so that these components are installed in the organization with
as little misconfigurations as possible. A wrongly configured component
is often worse than a vulnerable component, because vulnerabilities are
often fixed with the software upgrades (you do patch your software,
right?) whereas misconfigurations survive these updates and remain
exploitable for longer periods. Also, misuse of components is harder to
detect than exploiting vulnerabilities because they are often seen as
regular user behavior.&lt;/p&gt;
&lt;p&gt;But next to the redeployable components, most business services are
provided by a single application. Most companies don't have the budget
and resources to put dedicated engineering teams on each and every
application that is deployed in the organization. Even worse, many
companies hire external consultants to help in the deployment of the
component, and then the consultants hand over the maintenance of that
software to internal teams. Some consultants don't fully bother with
secure configuration deployment guides, or even feel the need to disable
security constraints put forth by the organization (policies and
standards) because "it is needed". A deployment is often seen as
successful when the software functionally works, which not necessarily
means that it is misconfiguration-free.&lt;/p&gt;
&lt;p&gt;As a recent example that I came across, consider an application that
needs &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt;. A consultancy firm is hired to set
up the infrastructure, and given full administrative rights on the
operating system to make sure that this particular component is deployed
fast (because the company wants to have the infrastructure in production
before the end of the week). Security is initially seen as less of a
concern, and the consultancy firm informs the customer (without any
guarantees though) that it will be set up "according to common best
practices". The company itself has no engineering team for Node.js nor
wants to invest in the appropriate resources (such as training) for
security engineers to review Node.js configurations. Yet the application
that is deployed on the Node.js application server is internet-facing,
so has a higher risk associated with it than a purely internal
deployment.&lt;/p&gt;
&lt;p&gt;So, how to ensure that these applications cannot be exploited or, if an
exploit is done, how to ensure that the risks involved with the exploit
are contained? Well, this is where I believe SELinux has a great
potential. And although I'm talking about SELinux here, the same goes
for other similar technologies like &lt;a href="http://en.wikipedia.org/wiki/TOMOYO_Linux"&gt;TOMOYO
Linux&lt;/a&gt;, &lt;a href="http://en.wikibooks.org/wiki/Grsecurity/The_RBAC_System"&gt;grSecurity's RBAC
system&lt;/a&gt;,
&lt;a href="http://www.rsbac.org/"&gt;RSBAC&lt;/a&gt; and more.&lt;/p&gt;
&lt;p&gt;SELinux can provide a container, decoupled from the application itself
(but of course built for that particular application) which restricts
the behavior of that application on the system to those activities that
are expected. The application itself is not SELinux-aware (or does not
need to be - some applications are, but those that I am focusing on here
usually don't), but the SELinux access controls ensure that exploits on
the application cannot reach beyond those activities/capabilities that
are granted to it.&lt;/p&gt;
&lt;p&gt;Consider the Node.js deployment from before. The Node.js application
server might need to connect to a &lt;a href="http://www.mongodb.org/"&gt;MongoDB&lt;/a&gt;
cluster, so we can configure SELinux to allow just that, but all other
connections that originate from the Node.js deployment should be
forbidden. Worms (if any) cannot use this deployment then to spread out.
Same with access to files - the Node.js application probably only needs
access to the application files and not to other system files. Instead
of trying to run the application in a chroot (which requires engineering
effort from those people implementing Node.js, which could be a
consultancy firm that does not know or want to deploy within a chroot)
SELinux is configured to disallow any file access beyond the application
files.&lt;/p&gt;
&lt;p&gt;With SELinux, the application can be deployed relatively safely while
ensuring that exploits (or abuse of misconfigurations) cannot spread.
All that the company itself has to do is to provide resources for a
SELinux engineering team (which can be just a responsibility of the
Linux engineering teams, but can be specialized as well). Such a team
does not need to be big, as policy development effort is usually only
needed during changes (for instance when the application is updated to
also send e-mails, in which case the SELinux policy can be adjusted to
allow that as well), and given enough experience, the SELinux
engineering team can build flexible policies that the administration
teams (those that do the maintenance of the servers) can tune the policy
as needed (for instance through SELinux booleans) without the need to
have the SELinux team work on the policies again.&lt;/p&gt;
&lt;p&gt;Using SELinux also has a number of additional advantages which other,
sometimes commercial tools (like Symantecs SPE/SCSP - really Symantec,
you ask customers to disable SELinux?) severly lack.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SELinux is part of a default Linux installation in many cases.
    RedHat Enterprise Linux ships with SELinux by default, and actively
    supports SELinux when customers have any problems with it. This also
    improves the likelihood for SELinux to be accepted, as other, third
    party solutions might not be supported. Ever tried getting support
    for a system on which both McAfee AV for Linux and Symantec SCSP are
    running (if you got it to work together at all)? At least McAfee
    gives pointers to how to update &lt;a href="https://kc.mcafee.com/corporate/index?page=content&amp;amp;id=KB67360"&gt;SELinux
    settings&lt;/a&gt;
    when they would interfere with McAfee processes.&lt;/li&gt;
&lt;li&gt;SELinux is widely known and many resources exist for users,
    administrators and engineers to learn more about it. The resources
    are freely available, and often kept up2date by a very
    motivated community. Unlike commercial products, whose support pages
    are hidden behind paywalls, customers are usually prevented from
    interacting with each other and tips and tricks for using the
    product are often not found on the Internet, SELinux information can
    be found almost everywhere. And if you like books, I have a couple
    for you to read: &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
    Administration&lt;/a&gt;
    and &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
    Cookbook&lt;/a&gt;,
    written by yours truly.&lt;/li&gt;
&lt;li&gt;Using SELinux is widely supported by third party configuration
    management tools, especially in the free software world.
    &lt;a href="http://puppetlabs.com/"&gt;Puppet&lt;/a&gt;, &lt;a href="https://www.chef.io/chef/"&gt;Chef&lt;/a&gt;,
    &lt;a href="http://www.ansible.com/home"&gt;Ansible&lt;/a&gt;,
    &lt;a href="http://www.saltstack.com/"&gt;SaltStack&lt;/a&gt; and others all support
    SELinux and/or have modules that integrate SELinux support in the
    management system.&lt;/li&gt;
&lt;li&gt;Using SELinux incurs no additional licensing costs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, SELinux is definitely not a holy grail. It has its limitations, so
security should still be seen as a global approach where SELinux is just
playing one specific role in. For instance, SELinux does not prevent
application behavior that is allowed by the policy. If a user abuses a
configuration and can have an application expose information that the
user usually does not have access to, but the application itself does
(for instance because other users on that application might) SELinux
cannot do anything about it (well, not as long as the application is not
made SELinux-aware). Also, vulnerabilities that exploit application
internals are not controlled by SELinux access controls. It is the
application behavior ("external view") that SELinux controls. To
mitigate in-application vulnerabilities, other approaches need to be
considered (such as memory protections for free software solutions,
which can protect against some kinds of exploits - see
&lt;a href="http://grsecurity.net/"&gt;grsecurity&lt;/a&gt; as one of the solutions that could
be used).&lt;/p&gt;
&lt;p&gt;Still, I believe that SELinux can definitely provide additional
protections for such "one-time deployments" where a company cannot
invest in resources to provide engineering services on those
deployments. The SELinux security controls do not require engineering on
the application side, making investments in SELinux engineering very
much reusable.&lt;/p&gt;</content><category term="SELinux"></category><category term="companies"></category><category term="configuration"></category><category term="engineering"></category><category term="enterprise"></category><category term="selinux"></category></entry><entry><title>Gentoo Wiki is growing</title><link href="https://blog.siphos.be/2015/01/gentoo-wiki-is-growing/" rel="alternate"></link><published>2015-01-03T10:09:00+01:00</published><updated>2015-01-03T10:09:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-01-03:/2015/01/gentoo-wiki-is-growing/</id><summary type="html">&lt;p&gt;Perhaps it is because of the winter holidays, but the last weeks I've
noticed a lot of updates and edits on the Gentoo wiki.&lt;/p&gt;
&lt;p&gt;The move to the
&lt;a href="https://wiki.gentoo.org/wiki/Project:Website/Tyrian"&gt;Tyrian&lt;/a&gt; layout,
whose purpose is to eventually become the unified layout for all Gentoo
resources, happened first. Then, three common templates (&lt;code&gt;Code …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Perhaps it is because of the winter holidays, but the last weeks I've
noticed a lot of updates and edits on the Gentoo wiki.&lt;/p&gt;
&lt;p&gt;The move to the
&lt;a href="https://wiki.gentoo.org/wiki/Project:Website/Tyrian"&gt;Tyrian&lt;/a&gt; layout,
whose purpose is to eventually become the unified layout for all Gentoo
resources, happened first. Then, three common templates (&lt;code&gt;Code&lt;/code&gt;, &lt;code&gt;File&lt;/code&gt;
and &lt;code&gt;Kernel&lt;/code&gt;) where deprecated in favor of their "*Box" counterparts
(&lt;code&gt;CodeBox&lt;/code&gt;, &lt;code&gt;FileBox&lt;/code&gt; and &lt;code&gt;KernelBox&lt;/code&gt;). These provide better parameter
support (which should make future updates on the templates easier to
implement) as well as syntax highlighting.&lt;/p&gt;
&lt;p&gt;But the wiki also saw a number of contributions being added. I added a
short article on &lt;a href="https://wiki.gentoo.org/wiki/Efibootmgr"&gt;Efibootmgr&lt;/a&gt;
as the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbook&lt;/a&gt; now also uses
it for its EFI related instructions, but other users added quite a few
additional articles as well. As they come along, articles are being
marked by editors for translation. For me, that's a trigger.&lt;/p&gt;
&lt;p&gt;Whenever a wiki article is marked for translations, it shows up on the
&lt;a href="https://wiki.gentoo.org/wiki/Special:PageTranslation"&gt;PageTranslation&lt;/a&gt;
list. When I have time, I pick one of these articles and try to update
it to move to a common style (the
&lt;a href="https://wiki.gentoo.org/wiki/Gentoo_Wiki:Guidelines"&gt;Guidelines&lt;/a&gt; page
is the "official" one, and I have a
&lt;a href="https://wiki.gentoo.org/wiki/User:SwifT/Styleguide"&gt;Styleguide&lt;/a&gt; in
which I elaborate a bit more on the use). Having a common style gives a
better look and feel to the articles (as they are then more alike),
gives a common documentation development approach (so everyone can join
in and update documentation in a similar layout/structure) and - most
importantly - reduces the number of edits that do little more than
switch from one formatting to another.&lt;/p&gt;
&lt;p&gt;When an article has been edited, I mark it for translation, and then the
real workhorse on the wiki starts. We have several active translators on
the Gentoo wiki, who we cannot thank hard enough for their work (I used
to start at Gentoo as a translator, I have some feeling about their
work). They make the Gentoo documentation reachable for a broader
audience. Thanks to the use of the translation extension (kindly offered
by the Gentoo wiki admins, who have been working quite hard the last few
weeks on improving the wiki infrastructure) translations are easier to
handle and follow through.&lt;/p&gt;
&lt;p&gt;The advantage of a translation-marked article is that any change on the
article also shows up on the list again, allowing me to look at the
change and perform edits when necessary. For the end user, this is
behind the scenes - an update on an article shows up immediately, which
is fine. But for me (and perhaps other editors as well) this gives a
nice overview of changes to articles (watchlists can only go so far) and
also shows the changes in a simple yet efficient manner. Thanks to this
approach, we can more actively follow up on edits and improve where
necessary.&lt;/p&gt;
&lt;p&gt;Now, editing is not always just a few minutes of work. Consider the
&lt;a href="https://wiki.gentoo.org/wiki/GRUB2"&gt;GRUB2&lt;/a&gt; article on the wiki. It was
marked for translation, but had some issues with its style. It was very
verbose (which is not a bad thing, but suggests to split information
towards multiple articles) and quite a few open discussions on its
&lt;a href="https://wiki.gentoo.org/wiki/Talk:GRUB2"&gt;Discussions&lt;/a&gt; page. I started
editing the article around 13.12h local time, and ended at 19.40h.
Unlike with offline documentation, the entire process of the editing can
be followed through the page'
&lt;a href="https://wiki.gentoo.org/index.php?title=GRUB2&amp;amp;offset=&amp;amp;limit=100&amp;amp;action=history"&gt;history&lt;/a&gt;).
And although I'm still not 100% satisfied with the result, it is imo
easier to follow through and read.&lt;/p&gt;
&lt;p&gt;However, don't get me wrong - I do not feel that the article was wrong
in any way. Although I would appreciate articles that immediately follow
a style, I rather see more contributions (which we can then edit towards
the new style) than that we would start penalizing contributors that
don't use the style. That would work contra-productive, because it is
far easier to update the style of an article than to write articles. We
should try and get more contributors to document aspects of their Gentoo
journey.&lt;/p&gt;
&lt;p&gt;So, please keep them coming. If you find a lack of (good) information
for something, start jotting down what you know in an article. We'll
gladly help you out with editing and improving the article then, but the
content is something you are probably best to write down.&lt;/p&gt;</content><category term="Documentation"></category><category term="documentation"></category><category term="Gentoo"></category><category term="wiki"></category></entry><entry><title>Why does it access /etc/shadow?</title><link href="https://blog.siphos.be/2014/12/why-does-it-access-etcshadow/" rel="alternate"></link><published>2014-12-30T22:48:00+01:00</published><updated>2014-12-30T22:48:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-30:/2014/12/why-does-it-access-etcshadow/</id><summary type="html">&lt;p&gt;While updating the SELinux policy for the Courier IMAP daemon, I noticed
that it (well, the authdaemon that is part of Courier) wanted to access
&lt;code&gt;/etc/shadow&lt;/code&gt;, which is of course a big no-no. It doesn't take long to
know that this is through the PAM support (more specifically,
&lt;code&gt;pam_unix …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;While updating the SELinux policy for the Courier IMAP daemon, I noticed
that it (well, the authdaemon that is part of Courier) wanted to access
&lt;code&gt;/etc/shadow&lt;/code&gt;, which is of course a big no-no. It doesn't take long to
know that this is through the PAM support (more specifically,
&lt;code&gt;pam_unix.so&lt;/code&gt;). But why? After all, &lt;code&gt;pam_unix.so&lt;/code&gt; should try to execute
&lt;code&gt;unix_chkpwd&lt;/code&gt; to verify a password and not read in the shadow file
directly (which would require all PAM-aware applications to be granted
access to the shadow file).&lt;/p&gt;
&lt;p&gt;So I dived into the PAM-Linux sources (yay free software).&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;pam_unix_passwd.c&lt;/code&gt;, the &lt;em&gt;_unix_run_verify_binary()&lt;/em&gt; method is
called but only if the &lt;em&gt;get_account_info()&lt;/em&gt; method returns
&lt;code&gt;PAM_UNIX_RUN_HELPER&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;static int _unix_verify_shadow(pam_handle_t *pamh, const char *user, unsigned int ctrl)
{
...
        retval = get_account_info(pamh, user, &amp;amp;pwent, &amp;amp;spent);
...
        if (retval == PAM_UNIX_RUN_HELPER) {
                retval = _unix_run_verify_binary(pamh, ctrl, user, &amp;amp;daysleft);
                if (retval == PAM_AUTH_ERR || retval == PAM_USER_UNKNOWN)
                        return retval;
        }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code&gt;passverify.c&lt;/code&gt; this method will check the password entry file and, if
the entry is a shadow file, will return &lt;code&gt;PAM_UNIX_RUN_HELPER&lt;/code&gt; if the
current user id is not root, or if SELinux is enabled:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;PAMH_ARG_DECL(int get_account_info,
        const char *name, struct passwd **pwd, struct spwd **spwdent)
{
        /* UNIX passwords area */
        *pwd = pam_modutil_getpwnam(pamh, name);        /* Get password file entry... */
        *spwdent = NULL;

        if (*pwd != NULL) {
...
                } else if (is_pwd_shadowed(*pwd)) {
                        /*
                         * ...and shadow password file entry for this user,
                         * if shadowing is enabled
                         */
#ifndef HELPER_COMPILE
                        if (geteuid() || SELINUX_ENABLED)
                                return PAM_UNIX_RUN_HELPER;
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;SELINUX_ENABLED&lt;/code&gt; is a C macro defined in the same file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#ifdef WITH_SELINUX
#include 
#define SELINUX_ENABLED is_selinux_enabled()&amp;gt;0
#else
#define SELINUX_ENABLED 0
#endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this is where my "aha" moment came forth: the Courier authdaemon
runs as root, so its user id is 0. The &lt;em&gt;geteuid()&lt;/em&gt; method will return 0,
so the &lt;code&gt;SELINUX_ENABLED&lt;/code&gt; macro must return non-zero for the proper path
to be followed. A quick check in the audit logs, after disabling
&lt;em&gt;dontaudit&lt;/em&gt; lines, showed that the Courier IMAPd daemon wants to get the
attribute(s) of the &lt;code&gt;security_t&lt;/code&gt; file system (on which the SELinux
information is exposed). As this was denied, the call to
&lt;em&gt;is_selinux_enabled()&lt;/em&gt; returns -1 (error) which, through the macro,
becomes 0.&lt;/p&gt;
&lt;p&gt;So granting &lt;em&gt;selinux_getattr_fs(courier_authdaemon_t)&lt;/em&gt; was enough to
get it to use the &lt;code&gt;unix_chkpwd&lt;/code&gt; binary again.&lt;/p&gt;
&lt;p&gt;To fix this properly, we need to grant this to all PAM using
applications. There is an interface called &lt;em&gt;auth_use_pam()&lt;/em&gt; in the
policies, but that isn't used by the Courier policy. Until now, that is
;-)&lt;/p&gt;</content><category term="SELinux"></category><category term="chkpwd"></category><category term="pam"></category><category term="selinux"></category><category term="shadow"></category><category term="unix_chkpwd"></category></entry><entry><title>Added UEFI instructions to AMD64/x86 handbooks</title><link href="https://blog.siphos.be/2014/12/added-uefi-instructions-to-amd64x86-handbooks/" rel="alternate"></link><published>2014-12-23T18:08:00+01:00</published><updated>2014-12-23T18:08:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-23:/2014/12/added-uefi-instructions-to-amd64x86-handbooks/</id><summary type="html">&lt;p&gt;I just finished up adding some UEFI instructions to the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbooks&lt;/a&gt; for AMD64
and x86 (I don't know how many systems are still using x86 instead of
the AMD64 one, and if those support UEFI, but the instructions are
shared and they don't collide). The entire EFI stuff can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just finished up adding some UEFI instructions to the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
handbooks&lt;/a&gt; for AMD64
and x86 (I don't know how many systems are still using x86 instead of
the AMD64 one, and if those support UEFI, but the instructions are
shared and they don't collide). The entire EFI stuff can probably be
improved a lot, but basically the things that were added are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;boot the system using UEFI already if possible (which is needed for
    efibootmgr to access the EFI variables). This is not entirely
    mandatory (as efibootmgr is not mandatory to boot a system)
    but recommended.&lt;/li&gt;
&lt;li&gt;use vfat for the &lt;code&gt;/boot/&lt;/code&gt; location, as this now becomes the EFI
    System Partition.&lt;/li&gt;
&lt;li&gt;configure the Linux kernel to support EFI stub and EFI variables&lt;/li&gt;
&lt;li&gt;install the Linux kernel as the &lt;code&gt;bootx64.efi&lt;/code&gt; file to boot the
    system with&lt;/li&gt;
&lt;li&gt;use efibootmgr to add boot options (if required) and create an EFI
    boot entry called "Gentoo"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you find grave errors, please do mention them (either on a talk page
on the wiki, as a &lt;a href="https://bugs.gentoo.org"&gt;bug&lt;/a&gt; or through IRC) so it
is picked up. All developers and trusted contributors on the wiki have
access to the files so can edit where needed (but do take care that, if
something is edited, that it is either architecture-specific or shared
across all architectures - check the page when editing; if it is
&lt;em&gt;Handbook:Parts&lt;/em&gt; then it is shared, and &lt;em&gt;Handbook:AMD64&lt;/em&gt; is specific for
the architecture). And if I'm online I'll of course act on it quickly.&lt;/p&gt;
&lt;p&gt;Oh, and no - it is not a bug that there is a (now not used) &lt;code&gt;/dev/sda1&lt;/code&gt;
"bios" partition. Due to the differences with the possible installation
alternatives, it is easier for us (me) to just document a common
partition layout than to try and write everything out (making it just
harder for new users to follow the instructions).&lt;/p&gt;</content><category term="Documentation"></category><category term="efi"></category><category term="Gentoo"></category><category term="handbook"></category><category term="uefi"></category></entry><entry><title>Handbooks moved</title><link href="https://blog.siphos.be/2014/12/handbooks-moved/" rel="alternate"></link><published>2014-12-14T14:42:00+01:00</published><updated>2014-12-14T14:42:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-14:/2014/12/handbooks-moved/</id><summary type="html">&lt;p&gt;Yesterday the move of the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
Wiki&lt;/a&gt; for the Gentoo
handbooks (whose most important part are the installation instructions
for the various supported architectures) has been concluded, with a
last-minute addition being the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page#Viewing_the_handbook"&gt;one-page
views&lt;/a&gt;
so that users who want to can view the installation instructions
completely within one view …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday the move of the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;Gentoo
Wiki&lt;/a&gt; for the Gentoo
handbooks (whose most important part are the installation instructions
for the various supported architectures) has been concluded, with a
last-minute addition being the &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page#Viewing_the_handbook"&gt;one-page
views&lt;/a&gt;
so that users who want to can view the installation instructions
completely within one view.&lt;/p&gt;
&lt;p&gt;Because we use lots of
&lt;a href="http://www.mediawiki.org/wiki/Transclusion"&gt;transclusions&lt;/a&gt; (i.e.
including different wiki articles inside another article) to support a
common documentation base for the various architectures, I did hit a
limit that prevented me from creating a single-page for the entire
handbook (i.e. "Installing Gentoo Linux", "Working with Gentoo",
"Working with portage" and "Network configuration" together), but I
could settle with one page per part. I think that matches most of the
use cases.&lt;/p&gt;
&lt;p&gt;With the move now done, it is time to start tackling the various bugs
that were reported against the handbook, as well as initiate
improvements where needed.&lt;/p&gt;
&lt;p&gt;I did make a (probably more - but this one is fresh in my memory)
mistake in the move though. I had to do a lot of the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Content went missing when switching blog technology :-(
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Without this, transcluded parts would suddenly show the translation tags
as regular text. Only afterwards (I'm talking about more than &lt;a href="https://wiki.gentoo.org/wiki/Project:Documentation/HandbookDevelopment"&gt;400
different
pages&lt;/a&gt;)
did I read that I should transclude the &lt;code&gt;/en&lt;/code&gt; pages (like
&lt;code&gt;Handbook:Parts/Installation/About/en&lt;/code&gt; instead of
&lt;code&gt;Handbook:Parts/Installation/About&lt;/code&gt;) as those do not have the
translation specifics in them. Sigh.&lt;/p&gt;</content><category term="Documentation"></category><category term="Gentoo"></category><category term="handbook"></category><category term="wiki"></category></entry><entry><title>Gentoo Handbooks almost moved to wiki</title><link href="https://blog.siphos.be/2014/12/gentoo-handbooks-almost-moved-to-wiki/" rel="alternate"></link><published>2014-12-12T17:35:00+01:00</published><updated>2014-12-12T17:35:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-12:/2014/12/gentoo-handbooks-almost-moved-to-wiki/</id><summary type="html">&lt;p&gt;Content-wise, the move is done. I've done a few checks on the content to
see if the structure still holds, translations are enabled on all pages,
the use of partitions is sufficiently consistent for each architecture,
and so on. The result can be seen on &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;the gentoo handbook main
page …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Content-wise, the move is done. I've done a few checks on the content to
see if the structure still holds, translations are enabled on all pages,
the use of partitions is sufficiently consistent for each architecture,
and so on. The result can be seen on &lt;a href="https://wiki.gentoo.org/wiki/Handbook:Main_Page"&gt;the gentoo handbook main
page&lt;/a&gt;, from which the
various architectural handbooks are linked.&lt;/p&gt;
&lt;p&gt;I sent a &lt;a href="http://thread.gmane.org/gmane.linux.gentoo.project/4141"&gt;sort-of
announcement&lt;/a&gt;
to the gentoo-project mailinglist (which also includes the motivation of
the move). If there are no objections, I will update the current
handbooks to link to the wiki ones, as well as update the links on the
website (and in wiki articles) to point to the wiki.&lt;/p&gt;</content><category term="Gentoo"></category><category term="Gentoo"></category><category term="handbook"></category><category term="wiki"></category></entry><entry><title>Sometimes I forget how important communication is</title><link href="https://blog.siphos.be/2014/12/sometimes-i-forget-how-important-communication-is/" rel="alternate"></link><published>2014-12-10T20:38:00+01:00</published><updated>2014-12-10T20:38:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-12-10:/2014/12/sometimes-i-forget-how-important-communication-is/</id><summary type="html">&lt;p&gt;Free software (and documentation) developers don't always have all the
time they want. Instead, they grab whatever time they have to do what
they believe is the most productive - be it documentation editing,
programming, updating ebuilds, SELinux policy improvements and what not.
But they often don't take the time to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Free software (and documentation) developers don't always have all the
time they want. Instead, they grab whatever time they have to do what
they believe is the most productive - be it documentation editing,
programming, updating ebuilds, SELinux policy improvements and what not.
But they often don't take the time to communicate. And communication is
important.&lt;/p&gt;
&lt;p&gt;For one, communication is needed to reach a larger audience than those
that follow the commit history in whatever repository work is being
done. Yes, there are developers that follow &lt;a href="http://news.gmane.org/gmane.linux.gentoo.cvs"&gt;each
commit&lt;/a&gt;, but development
isn't just done for developers, it is also for end users. And end users
deserve frequent updates and feedback. Be it through blog posts, Google+
posts, tweets or instragrams (well, I'm not sure how to communicate a
software or documentation change through Instagram, but I'm sure people
find lots of creative ways to do so), telling the broader world what has
changed is important.&lt;/p&gt;
&lt;p&gt;Perhaps a (silent or not) user was waiting for this change. Perhaps he
or she is even actually trying to fix things himself/herself but is
struggling with it, and would really benefit (time-wise) from a quick
fix. Without communicating about the change, (s)he does not know that no
further attempts are needed, actually reducing the efficiency in
overall.&lt;/p&gt;
&lt;p&gt;But communication is just one-way. Better is to get feedback as well. In
that sense, communication is just one part of the feedback loop - once
developers receive feedback on what they are doing (or did recently)
they might even improve results faster. With feedback loops, the wisdom
of the crowd (in the positive sense) can be used to improve solutions
beyond what the developer originally intended. And even a simple "cool"
and "I like" is good information for a developer or contributor.&lt;/p&gt;
&lt;p&gt;Still, I often forget to do it - or don't have the time to focus on
communication. And that's bad. So, let me quickly state what things I
forgot to communicate more broadly about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.project/4129"&gt;new
    developer&lt;/a&gt;
    joined the Gentoo ranks: Jason Zaman. Now developers join Gentoo
    more often than just once in a while, but Jason is one of
    my "recruits". In a sense, he became a developer because I was tired
    of pulling his changes in and proxy-committing stuff. Of course,
    that's only half the truth; he is also a very active contributor in
    other areas (and was already a maintainer for a few packages through
    the proxy-maintainer project) and is a tremendous help in the Gentoo
    Hardened project. So welcome onboard Jason (or perfinion as he calls
    himself online).&lt;/li&gt;
&lt;li&gt;I've started with &lt;a href="https://wiki.gentoo.org/wiki/Project:Documentation/HandbookDevelopment"&gt;copying the Gentoo handbook to the
    wiki&lt;/a&gt;.
    This is still an on-going project, but was long overdue. There are
    many reasons why the move to the wiki is interesting. For me
    personally, it is to attract a larger audience to update
    the handbook. Although the document will be restricted for editing
    by developers and trusted contributors only (it does contain the
    installation instructions and is a primary entry point for
    many users) that's still a whole lot more than when just a handful
    (one or two actually) developers update the handbook.&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://github.com/SELinuxProject/selinux/wiki/Releases"&gt;SELinux
    userspace&lt;/a&gt;
    (2.4 release) is looking more stable; there are no specific
    regressions anymore (upstream is at release candidate 7) although I
    must admit that I have not implemented it on the majority of test
    systems that I maintain. Not due to fears, but mostly because I
    struggle a bit with available time so I can do without testing
    upgrades that are not needed. I do plan on moving towards 2.4 in a
    week or two.&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;reference
    policy&lt;/a&gt; has
    released a new version of the policy. Gentoo quickly followed
    through (Jason did the honors of creating the ebuilds).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, apologies for not communicating sooner, and I promise I'll try to
uplift the communication frequency.&lt;/p&gt;</content><category term="Gentoo"></category><category term="communication"></category><category term="developer"></category><category term="Gentoo"></category><category term="selinux"></category><category term="time"></category></entry><entry><title>No more DEPENDs for SELinux policy package dependencies</title><link href="https://blog.siphos.be/2014/11/no-more-depends-for-selinux-policy-package-dependencies/" rel="alternate"></link><published>2014-11-02T14:51:00+01:00</published><updated>2014-11-02T14:51:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-11-02:/2014/11/no-more-depends-for-selinux-policy-package-dependencies/</id><summary type="html">&lt;p&gt;I just finished updating 102 packages. The change? Removing the
following from the ebuilds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DEPEND=&amp;quot;selinux? ( sec-policy/selinux-${packagename} )&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the past, we needed this construction in both DEPEND and RDEPEND.
Recently however, the SELinux eclass got updated with some logic to
relabel files after the policy package is deployed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just finished updating 102 packages. The change? Removing the
following from the ebuilds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DEPEND=&amp;quot;selinux? ( sec-policy/selinux-${packagename} )&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the past, we needed this construction in both DEPEND and RDEPEND.
Recently however, the SELinux eclass got updated with some logic to
relabel files after the policy package is deployed. As a result, the
DEPEND variable no longer needs to refer to the SELinux policy package.&lt;/p&gt;
&lt;p&gt;This change also means that for those moving from a regular Gentoo
installation to an SELinux installation will have much less packages to
rebuild. In the past, getting &lt;code&gt;USE="selinux"&lt;/code&gt; (through the SELinux
profiles) would rebuild all packages that have a DEPEND dependency to
the SELinux policy package. No more - only packages that depend on the
SELinux libraries (like &lt;code&gt;libselinux&lt;/code&gt;) or utilities rebuild. The rest
will just pull in the proper policy package.&lt;/p&gt;</content><category term="Gentoo"></category><category term="DEPEND"></category><category term="ebuild"></category><category term="Gentoo"></category><category term="RDEPEND"></category><category term="selinux"></category></entry><entry><title>Using multiple priorities with modules</title><link href="https://blog.siphos.be/2014/10/using-multiple-priorities-with-modules/" rel="alternate"></link><published>2014-10-31T18:24:00+01:00</published><updated>2014-10-31T18:24:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-31:/2014/10/using-multiple-priorities-with-modules/</id><summary type="html">&lt;p&gt;One of the new features of the 2.4 SELinux userspace is support for
module priorities. The idea is that distributions and administrators can
override a (pre)loaded SELinux policy module with another module without
removing the previous module. This lower-version module will remain in
the store, but will not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the new features of the 2.4 SELinux userspace is support for
module priorities. The idea is that distributions and administrators can
override a (pre)loaded SELinux policy module with another module without
removing the previous module. This lower-version module will remain in
the store, but will not be active until the higher-priority module is
disabled or removed again.&lt;/p&gt;
&lt;p&gt;The "old" modules (pre-2.4) are loaded with priority 100. When policy
modules with the 2.4 SELinux userspace series are loaded, they get
loaded with priority 400. As a result, the following message occurs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -i screen.pp
libsemanage.semanage_direct_install_info: Overriding screen module at lower priority 100 with module at priority 400
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So unlike the previous situation, where the older module is substituted
with the new one, we now have two "screen" modules loaded; the last one
gets priority 400 and is active. To see all installed modules and
priorities, use the &lt;code&gt;--list-modules&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule --list-modules=all | grep screen
100 screen     pp
400 screen     pp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Older versions of modules can be removed by specifying the priority:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semodule -X 100 -r screen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="SELinux"></category><category term="priorities"></category><category term="priority"></category><category term="selinux"></category><category term="semodule"></category></entry><entry><title>Migrating to SELinux userspace 2.4 (small warning for users)</title><link href="https://blog.siphos.be/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/" rel="alternate"></link><published>2014-10-30T19:44:00+01:00</published><updated>2014-10-30T19:44:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-30:/2014/10/migrating-to-selinux-userspace-2-4-small-warning-for-users/</id><summary type="html">&lt;p&gt;In a few moments, SELinux users which have the \~arch KEYWORDS set
(either globally or for the SELinux utilities in particular) will notice
that the SELinux userspace will upgrade to version 2.4 (release
candidate 5 for now). This upgrade comes with a manual step that needs
to be performed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a few moments, SELinux users which have the \~arch KEYWORDS set
(either globally or for the SELinux utilities in particular) will notice
that the SELinux userspace will upgrade to version 2.4 (release
candidate 5 for now). This upgrade comes with a manual step that needs
to be performed after upgrade. The information is mentioned as
post-installation message of the &lt;code&gt;policycoreutils&lt;/code&gt; package, and
basically sais that you need to execute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The reason is that the SELinux utilities expect the SELinux policy
module store (and the semanage related files) to be in
&lt;code&gt;/var/lib/selinux&lt;/code&gt; and no longer in &lt;code&gt;/etc/selinux&lt;/code&gt;. Note that this does
not mean that the SELinux policy itself is moved outside of that
location, nor is the basic configuration file (&lt;code&gt;/etc/selinux/config&lt;/code&gt;).
It is what tools such as &lt;strong&gt;semanage&lt;/strong&gt; manage that is moved outside that
location.&lt;/p&gt;
&lt;p&gt;I tried to automate the migration as part of the packages themselves,
but this would require the &lt;code&gt;portage_t&lt;/code&gt; domain to be able to move,
rebuild and load policies, which it can't (and to be honest, shouldn't).
Instead of augmenting the policy or making updates to the migration
script as delivered by the upstream project, we currently decided to
have the migration done manually. It is a one-time migration anyway.&lt;/p&gt;
&lt;p&gt;If for some reason end users forget to do the migration, then that does
not mean that the system breaks or becomes unusable. SELinux still
works, SELinux aware applications still work; the only thing that will
fail are updates on the SELinux configuration through tools like
&lt;strong&gt;semanage&lt;/strong&gt; or &lt;strong&gt;setsebool&lt;/strong&gt; - the latter when you want to persist
boolean changes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# semanage fcontext -l
ValueError: SELinux policy is not managed or store cannot be accessed.

~# setsebool -P allow_ptrace on
Cannot set persistent booleans without managed policy.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you get those errors or warnings, all that is left to do is to do the
migration. Note in the following that there is a warning about 'else'
blocks that are no longer supported: that's okay, as far as I know (and
it was mentioned on the upstream mailinglist as well as not something to
worry about) it does not have any impact.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store
Migrating from /etc/selinux/mcs/modules/active to /var/lib/selinux/mcs/active
Attempting to rebuild policy from /var/lib/selinux
sysnetwork: Warning: &amp;#39;else&amp;#39; blocks in optional statements are unsupported in CIL. Dropping from output.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also add in &lt;code&gt;-c&lt;/code&gt; so that the old policy module store is cleaned
up. You can also rerun the command multiple times:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# /usr/libexec/selinux/semanage_migrate_store -c
warning: Policy type mcs has already been migrated, but modules still exist in the old store. Skipping store.
Attempting to rebuild policy from /var/lib/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can manually clean up the old policy module store like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rm -rf /etc/selinux/mcs/modules
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So... don't worry - the change is small and does not break stuff. And
for those wondering about CIL I'll talk about it in one of my next
posts.&lt;/p&gt;</content><category term="Gentoo"></category><category term="cil"></category><category term="Gentoo"></category><category term="migrate"></category><category term="selinux"></category><category term="semanage"></category><category term="upgrade"></category><category term="userspace"></category></entry><entry><title>Lots of new challenges ahead</title><link href="https://blog.siphos.be/2014/10/lots-of-new-challenges-ahead/" rel="alternate"></link><published>2014-10-19T16:01:00+02:00</published><updated>2014-10-19T16:01:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-10-19:/2014/10/lots-of-new-challenges-ahead/</id><summary type="html">&lt;p&gt;I've been pretty busy lately, albeit behind the corners, which leads to
a lower activity within the free software communities that I'm active
in. Still, I'm not planning any exit, on the contrary. Lots of ideas are
just waiting for some free time to engage. So what are the challenges …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been pretty busy lately, albeit behind the corners, which leads to
a lower activity within the free software communities that I'm active
in. Still, I'm not planning any exit, on the contrary. Lots of ideas are
just waiting for some free time to engage. So what are the challenges
that have been taking up my time?&lt;/p&gt;
&lt;p&gt;One of them is that I recently moved. And with moving comes a lot of
work in getting the place into a good shape and getting settled. Today I
finished the last job that I wanted to finish in my appartment in a
short amount of time, so that's one thing off my TODO list.&lt;/p&gt;
&lt;p&gt;Another one is that I started an intensive master-after-master programme
with the subject of &lt;em&gt;Enterprise Architecture&lt;/em&gt;. This not only takes up
quite some ex-cathedra time, but also additional hours of studying (and
for the moment also exams). But I'm really satisfied that I can take up
this course, as I've been wandering around in the world of enterprise
architecture for some time now and want to grow even further in this
field.&lt;/p&gt;
&lt;p&gt;But that's not all. One of my side activities has been blooming a lot,
and I recently reached the 200th server that I'm administering (although
I think this number will reduce to about 120 as I'm helping one
organization with handing over management of their 80+ systems to their
own IT staff). Together with some friends (who also have non-profit
customers' IT infrastructure management as their side-business) we're
now looking at consolidating our approach to system administration (and
engineering).&lt;/p&gt;
&lt;p&gt;I'm also looking at investing time and resources in a start-up,
depending on the business plan and required efforts. But more
information on this later when things are more clear :-)&lt;/p&gt;</content><category term="Misc"></category></entry><entry><title>After SELinux System Administration, now the SELinux Cookbook</title><link href="https://blog.siphos.be/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/" rel="alternate"></link><published>2014-09-24T20:10:00+02:00</published><updated>2014-09-24T20:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2014-09-24:/2014/09/after-selinux-system-administration-now-the-selinux-cookbook/</id><summary type="html">&lt;p&gt;Almost an entire year ago (just a few days apart) I
&lt;a href="http://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/"&gt;announced&lt;/a&gt;
my first published book, called &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
Administration&lt;/a&gt;.
The book covered SELinux administration commands and focuses on Linux
administrators that need to interact with SELinux-enabled systems.&lt;/p&gt;
&lt;p&gt;An important part of SELinux was only covered very briefly in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Almost an entire year ago (just a few days apart) I
&lt;a href="http://blog.siphos.be/2013/09/it-has-finally-arrived-selinux-system-administration/"&gt;announced&lt;/a&gt;
my first published book, called &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration"&gt;SELinux System
Administration&lt;/a&gt;.
The book covered SELinux administration commands and focuses on Linux
administrators that need to interact with SELinux-enabled systems.&lt;/p&gt;
&lt;p&gt;An important part of SELinux was only covered very briefly in the book:
policy development. So in the spring this year, Packt approached me and
asked if I was interested in authoring a second book for them, called
&lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
Cookbook&lt;/a&gt;.
This book focuses on policy development and tuning of SELinux to fit the
needs of the administrator or engineer, and as such is a logical
follow-up to the previous book. Of course, given my affinity with the
wonderful Gentoo Linux distribution, it is mentioned in the book (and
even the reference platform) even though the book itself is checked
against Red Hat Enterprise Linux and Fedora as well, ensuring that every
recipe in the book works on all distributions. Luckily (or perhaps not
surprisingly) the approach is quite distribution-agnostic.&lt;/p&gt;
&lt;p&gt;Today, I got word that the &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-cookbook"&gt;SELinux
Cookbook&lt;/a&gt;
is now officially published. The book uses a recipe-based approach to
SELinux development and tuning, so it is quickly hands-on. It gives my
view on SELinux policy development while keeping the methods and
processes aligned with the upstream policy development project (the
&lt;a href="https://github.com/TresysTechnology/refpolicy/wiki"&gt;reference policy&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It's been a pleasure (but also somewhat a pain, as this is done in free
time, which is scarce already) to author the book. Unlike the first
book, where I struggled a bit to keep the page count to the requested
amount, this book was not limited. Also, I think the various stages of
the book development contributed well to the final result (something
that I overlooked a bit in the first time, so I re-re-reviewed changes
over and over again this time - after the first editorial reviews, then
after the content reviews, then after the language reviews, then after
the code reviews).&lt;/p&gt;
&lt;p&gt;You'll see me blog a bit more about the book later (as the marketing
phase is now starting) but for me, this is a major milestone which
allowed me to write down more of my SELinux knowledge and experience. I
hope it is as good a read for you as I hope it to be.&lt;/p&gt;</content><category term="SELinux"></category></entry></feed>