<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art... - Free Software</title><link href="http://192.168.1.71:8000/" rel="alternate"></link><link href="http://192.168.1.71:8000/feeds/free-software.atom.xml" rel="self"></link><id>http://192.168.1.71:8000/</id><updated>2018-09-09T13:20:00+02:00</updated><entry><title>cvechecker 3.9 released</title><link href="http://192.168.1.71:8000/2018/09/cvechecker-3.9-released/" rel="alternate"></link><published>2018-09-09T13:20:00+02:00</published><updated>2018-09-09T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2018-09-09:/2018/09/cvechecker-3.9-released/</id><content type="html">&lt;p&gt;Thanks to updates from Vignesh Jayaraman, Anton Hillebrand and Rolf Eike Beer,
a new release of &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; is
now made available.&lt;/p&gt;
&lt;p&gt;This new release (v3.9) is a bugfix release.&lt;/p&gt;
</content><category term="Free Software"></category><category term="cvechecker"></category></entry><entry><title>cvechecker 3.8 released</title><link href="http://192.168.1.71:8000/2017/03/cvechecker-3.8-released/" rel="alternate"></link><published>2017-03-27T19:00:00+02:00</published><updated>2017-03-27T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2017-03-27:/2017/03/cvechecker-3.8-released/</id><summary type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A new release is now available for the &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; application.
This is a stupid yet important bugfix release: the 3.7 release saw all newly released CVEs as being already
known, so it did not take them up to the database. As a result, systems would never check for the new CVEs.&lt;/p&gt;


&lt;p&gt;It is recommended to remove any historical files from &lt;code&gt;/var/lib/cvechecker/cache&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# rm /var/lib/cvechecker/cache/nvdcve-2.0-2017.*
~# rm /var/lib/cvechecker/cache/nvdcve-2.0-modified.*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will make sure that the next run of &lt;code&gt;pullcves pull&lt;/code&gt; will re-download those files, and attempt to load
the resulting CVEs back in the database.&lt;/p&gt;
&lt;p&gt;Sorry for this issue :-(&lt;/p&gt;</content><category term="Free Software"></category><category term="cvechecker"></category></entry><entry><title>cvechecker 3.7 released</title><link href="http://192.168.1.71:8000/2017/03/cvechecker-3.7-released/" rel="alternate"></link><published>2017-03-02T10:00:00+01:00</published><updated>2017-03-02T10:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2017-03-02:/2017/03/cvechecker-3.7-released/</id><summary type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long time of getting too little attention from me, I decided to make a 
new &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker&lt;/a&gt; release. There are
few changes in it, but I am planning on making a new release soon with lots of
clean-ups.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What has been changed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, what has changed? With this release (now at version 3.7) two bugs have been
fixed, one having a wrong URL in the CVE download and the other about the CVE
sequence numbers.&lt;/p&gt;
&lt;p&gt;The first bug was an annoying one, which I should have fixed a long time ago.
Well, it was fixed in the repository, but I didn't make a new release for it. 
When downloading the &lt;code&gt;nvdcve-2.0-Modified.xml&lt;/code&gt; file, the &lt;code&gt;pullcves&lt;/code&gt; command used
the lowercase filename, which doesn't exist.&lt;/p&gt;
&lt;p&gt;The second bug is about parsing the CVE sequence. On &lt;a href="https://cve.mitre.org/cve/identifiers/syntaxchange.html"&gt;January 2014&lt;/a&gt;
the syntax changed to allow for sequence identifiers longer than 4 digits. The
cvechecker tool however did a hard validation on the length of the identifier,
and cut off longer fields.&lt;/p&gt;
&lt;p&gt;That means that some CVE reports failed to parse in cvechecker, and thus cvechecker
didn't "know" about these vulnerabilities. This has been fixed in this release,
although I am not fully satisfied...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What still needs to be done&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The codebase for cvechecker is from 2010, and is actually based on a prototype
that I wrote which I decided not to rewrite into proper code. As a result, the
code is not up to par.&lt;/p&gt;
&lt;p&gt;I'm going to gradually improve and clean up the code in the next few [insert
timeperiod here]. I don't know if there will be feature improvements in the
next few releases (not that there aren't many feature enhancements needed) but
I hope that, once the code is improved, new functionality can be added more
easily.&lt;/p&gt;
&lt;p&gt;But that's for another time. Right now, enjoy the new release.&lt;/p&gt;</content><category term="Free Software"></category><category term="cvechecker"></category></entry><entry><title>GnuPG: private key suddenly missing?</title><link href="http://192.168.1.71:8000/2016/10/gnupg-private-key-suddenly-missing/" rel="alternate"></link><published>2016-10-12T18:56:00+02:00</published><updated>2016-10-12T18:56:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2016-10-12:/2016/10/gnupg-private-key-suddenly-missing/</id><summary type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can't find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After updating my workstation, I noticed that keychain reported that it could
not load one of the GnuPG keys I passed it on.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; * keychain 2.8.1 ~ http://www.funtoo.org
 * Found existing ssh-agent: 2167
 * Found existing gpg-agent: 2194
 * Warning: can&amp;#39;t find 0xB7BD4B0DE76AC6A4; skipping
 * Known ssh key: /home/swift/.ssh/id_dsa
 * Known ssh key: /home/swift/.ssh/id_ed25519
 * Known gpg key: 0x22899E947878B0CE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I did not modify my key store at all, so what happened?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;GnuPG upgrade to 2.1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The update I did also upgraded GnuPG to the 2.1 series. This version has &lt;a href="https://www.gnupg.org/faq/whats-new-in-2.1.html"&gt;quite
a few updates&lt;/a&gt;, one of which is
a change towards a new private key storage approach. I thought that it might have
done a wrong conversion, or that the key which was used was of a particular method
or strength that suddenly wasn't supported anymore (PGP-2 is mentioned in the
article).&lt;/p&gt;
&lt;p&gt;But the key is a relatively standard RSA4096 one. Yet still, when I listed my
private keys, I did not see this key. I even tried to re-import the &lt;code&gt;secring.gpg&lt;/code&gt;
file, but it only found private keys that it already saw previously.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I'm blind - the key never disappeared&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Luckily, when I tried to sign something with the key, &lt;code&gt;gpg-agent&lt;/code&gt; still asked me
for the passphraze that I had used for a while on that key. So it isn't gone. What
happened?&lt;/p&gt;
&lt;p&gt;Well, the key id is not my private key id, but the key id of one of the subkeys.
Previously, &lt;code&gt;gpg-agent&lt;/code&gt; sought and found the private key associated with the subkey,
but now it no longer does. I don't know if this is a bug in the past that I accidentally
used, or if this is a bug in the new version. I might investigate that a bit more,
but right now I'm happy that I found it.&lt;/p&gt;
&lt;p&gt;All I had to do was use the right key id in keychain, and things worked again.&lt;/p&gt;
&lt;p&gt;Good, now I can continue debugging networking issues with an azure-hosted system...&lt;/p&gt;</content><category term="Free Software"></category><category term="gnupg"></category></entry><entry><title>Mounting QEMU images</title><link href="http://192.168.1.71:8000/2016/09/mounting-qemu-images/" rel="alternate"></link><published>2016-09-26T19:26:00+02:00</published><updated>2016-09-26T19:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2016-09-26:/2016/09/mounting-qemu-images/</id><summary type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;While working on the second edition of my first book, &lt;a href="https://www.packtpub.com/networking-and-servers/selinux-system-administration-second-edition"&gt;SELinux System Administration - Second Edition&lt;/a&gt;
I had to test out a few commands on different Linux distributions to make sure
that I don't create instructions that only work on Gentoo Linux. After all, as
awesome as Gentoo might be, the Linux world is a bit bigger. So I downloaded a
few live systems to run in Qemu/KVM.&lt;/p&gt;
&lt;p&gt;Some of these systems however use &lt;a href="https://cloudinit.readthedocs.io/en/latest/"&gt;cloud-init&lt;/a&gt;
which, while interesting to use, is not set up on my system yet. And without 
support for cloud-init, how can I get access to the system?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Mounting qemu images on the system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To resolve this, I want to mount the image on my system, and edit the &lt;code&gt;/etc/shadow&lt;/code&gt;
file so that the root account is accessible. Once that is accomplished, I can
log on through the console and start setting up the system further.&lt;/p&gt;
&lt;p&gt;Images that are in the qcow2 format can be mounted through the nbd driver, but that
would require some updates on my local SELinux policy that I am too lazy to do right
now (I'll get to them eventually, but first need to finish the book). Still, if you
are interested in using nbd, see &lt;a href="https://www.kumari.net/index.php/system-adminstration/49-mounting-a-qemu-image"&gt;these instructions&lt;/a&gt;
or a &lt;a href="https://forums.gentoo.org/viewtopic-t-822672.html"&gt;related thread&lt;/a&gt; on the Gentoo
Forums.&lt;/p&gt;
&lt;p&gt;Luckily, storage is cheap (even SSD disks), so I quickly converted the qcow2 images
into raw images:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ qemu-img convert root.qcow2 root.raw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the image now available in raw format, I can use the loop devices to mount
the image(s) on my system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# losetup /dev/loop0 root.raw
~# kpartx -a /dev/loop0
~# mount /dev/mapper/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;kpartx&lt;/code&gt; command will detect the partitions and ensure that those are
available: the first partition becomes available at &lt;code&gt;/dev/loop0p1&lt;/code&gt;, the
second &lt;code&gt;/dev/loop0p2&lt;/code&gt; and so forth.&lt;/p&gt;
&lt;p&gt;With the image now mounted, let's update the &lt;code&gt;/etc/shadow&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Placing a new password hash in the shadow file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A google search quickly revealed that the following command generates
a shadow-compatible hash for a password:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl passwd -1 MyMightyPassword
$1$BHbMVz9i$qYHmULtXIY3dqZkyfW/oO.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The challenge wasn't to find the hash though, but to edit it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# vim /mnt/etc/shadow
vim: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The image that I downloaded used SELinux (of course), which meant that the &lt;code&gt;shadow&lt;/code&gt;
file was labeled with &lt;code&gt;shadow_t&lt;/code&gt; which I am not allowed to access. And I didn't
want to put SELinux in permissive mode just for this (sometimes I /do/ have some
time left, apparently).&lt;/p&gt;
&lt;p&gt;So I remounted the image, but now with the &lt;code&gt;context=&lt;/code&gt; mount option, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# mount -o context=&amp;quot;system_u:object_r:var_t:s0: /dev/loop0p1 /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now all files are labeled with &lt;code&gt;var_t&lt;/code&gt; which I do have permissions to edit. But
I also need to take care that the files that I edited get the proper label again.
There are a number of ways to accomplish this. I chose to create a &lt;code&gt;.autorelabel&lt;/code&gt;
file in the root of the partition. Red Hat based distributions will pick this up
and force a file system relabeling operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unmounting the file system&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After making the changes, I can now unmount the file system again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# umount /mnt
~# kpart -d /dev/loop0
~# losetup -d /dev/loop0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With that done, I had root access to the image and could start testing out
my own set of commands.&lt;/p&gt;
&lt;p&gt;It did trigger my interest in the cloud-init setup though...&lt;/p&gt;</content><category term="Free Software"></category><category term="qemu"></category></entry><entry><title>Template was specified incorrectly</title><link href="http://192.168.1.71:8000/2016/03/template-was-specified-incorrectly/" rel="alternate"></link><published>2016-03-27T13:32:00+02:00</published><updated>2016-03-27T13:32:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2016-03-27:/2016/03/template-was-specified-incorrectly/</id><summary type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv 'testing', u'salt://top.sls'
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to 'merge' and multiple top files found. Top file merging order is undefined; for better results use 'same' option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what's all this about?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After reorganizing my salt configuration, I received the following error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Enabling some debugging on the command gave me a slight pointer why this occurred:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[DEBUG   ] Could not find file from saltenv &amp;#39;testing&amp;#39;, u&amp;#39;salt://top.sls&amp;#39;
[DEBUG   ] No contents loaded for env: testing
[DEBUG   ] compile template: False
[ERROR   ] Template was specified incorrectly: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I was using a single top file as recommended by Salt, but apparently it was still
looking for top files in the other environments.&lt;/p&gt;
&lt;p&gt;Yet, if I split the top files across the environments, I got the following warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[WARNING ] Top file merge strategy set to &amp;#39;merge&amp;#39; and multiple top files found. Top file merging order is undefined; for better results use &amp;#39;same&amp;#39; option
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So what's all this about?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;When using a single top file is preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to stick with a single top file, then the first error is (or at least, in my case)
caused by my environments not having a fall-back definition.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;/etc/salt/master&lt;/code&gt; configuration file had the following &lt;code&gt;file_roots&lt;/code&gt; setting:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The problem is that Salt expects ''a'' top file through the environment. What I had to do was to
set the fallback directory to the base directory again, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;file_roots:
  base:
    - /srv/salt/base
  testing:
    - /srv/salt/testing
    - /srv/salt/base
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this set, the error disappeared and both salt and myself were happy again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When multiple top files are preferred&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you really want to use multiple top files (which is also a use case in my configuration),
then first we need to make sure that the top files of all environments correctly isolate the
minion matches. If two environments would match the same minion, then this approach becomes
more troublesome.&lt;/p&gt;
&lt;p&gt;On the one hand, we can just let saltstack merge the top files (default behavior) but the order
of the merging is undefined (and no, you can't set it using &lt;code&gt;env_order&lt;/code&gt;) which might result in 
salt states being executed in an unexpected order. If the definitions are done to such an extend
that this is not a problem, then you can just ignore the warning. See also
&lt;a href="https://github.com/saltstack/salt/issues/29104"&gt;bug 29104&lt;/a&gt; about the warning itself.&lt;/p&gt;
&lt;p&gt;But better would be to have the top files of the environment(s) isolated so that each environment
top file completely manages the entire environment. When that is the case, then we tell salt that
only the top file of the affected environment should be used. This is done using the following
setting in &lt;code&gt;/etc/salt/master&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;top_file_merging_strategy: same
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this is used, then the &lt;code&gt;env_order&lt;/code&gt; setting is used to define in which order the environments
are processed. &lt;/p&gt;
&lt;p&gt;Oh and if you're using &lt;code&gt;salt-ssh&lt;/code&gt;, then be sure to set the environment of the minion in the roster
file, as there is no running minion on the target system that informs salt about the environment 
to use otherwise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# In /etc/salt/roster
testserver:
  host: testserver.example.com
  minion_opts:
    environment: testing
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free Software"></category><category term="salt"></category></entry><entry><title>Using salt-ssh with agent forwarding</title><link href="http://192.168.1.71:8000/2016/03/using-salt-ssh-with-agent-forwarding/" rel="alternate"></link><published>2016-03-26T19:57:00+01:00</published><updated>2016-03-26T19:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2016-03-26:/2016/03/using-salt-ssh-with-agent-forwarding/</id><summary type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Part of a system's security is to reduce the attack surface. Following this principle,
I want to see if I can switch from using regular salt minions for a saltstack managed
system set towards &lt;code&gt;salt-ssh&lt;/code&gt;. This would allow to do some system management over SSH
instead of ZeroMQ.&lt;/p&gt;
&lt;p&gt;I'm not confident yet that this is a solid approach to take (as performance is also
important, which is greatly reduced with &lt;code&gt;salt-ssh&lt;/code&gt;), and the security exposure of the
salt minions over ZeroMQ is also not that insecure (especially not when a local firewall
ensures that only connections from the salt master are allowed). But playing doesn't hurt.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Using SSH agent forwarding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anyway, I quickly got stuck with accessing minions over the SSH interface as it seemed that
salt requires its own SSH keys (I don't enable password-only authentication, most of the systems
use the &lt;a href="https://blog.flameeyes.eu/2013/03/openssh-6-2-adds-support-for-two-factor-authentication"&gt;AuthenticationMethods&lt;/a&gt;
approach to chain both key and passwords). But first things first, the current target uses regular
ssh key authentication (no chained approach, that's for later). But I don't want to assign
such a powerful key to my salt master (especially not if it would later also document the
passwords). I would like to use SSH agent forwarding.&lt;/p&gt;
&lt;p&gt;Luckily, salt does support that, it just &lt;a href="https://github.com/saltstack/salt/pull/31328/commits/024439186a0c51c0ac1242b38d6584d2abd1a534"&gt;forgot to document&lt;/a&gt;
it. Basically, what you need to do is update the roster file with the &lt;code&gt;priv:&lt;/code&gt; parameter
set to &lt;code&gt;agent-forwarding&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;myminion:
  host: myminion.example.com
  priv: agent-forwarding
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It will use the &lt;code&gt;known_hosts&lt;/code&gt; file of the currently logged on user (the one executing
the &lt;code&gt;salt-ssh&lt;/code&gt; command) so make sure that the system's key is already known.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ salt-ssh myminion test.ping
myminion:
    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free Software"></category><category term="salt"></category></entry><entry><title>Trying out imapsync</title><link href="http://192.168.1.71:8000/2016/03/trying-out-imapsync/" rel="alternate"></link><published>2016-03-13T12:57:00+01:00</published><updated>2016-03-13T12:57:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2016-03-13:/2016/03/trying-out-imapsync/</id><summary type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Recently, I had to migrate mail boxes for a couple of users from one mail provider to
another. Both mail providers used IMAP, so I looked into IMAP related synchronization
methods. I quickly found the &lt;a href="https://github.com/imapsync/imapsync"&gt;imapsync&lt;/a&gt; application,
also supported through Gentoo's repository.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What I required&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The migration required that all mails, except for the spam and trash e-mails, were
migrated to another mail server. The migrated mails had to retain their status flags
(so unread mails had to remain unread while read mails had to remain read), and the
migration had to be done in two waves: one while the primary mail server was still
in use (where most of the mails where synchronized) and then, after switching the
mail servers (which was done through DNS changes) re-sync to fetch the final ones.&lt;/p&gt;
&lt;p&gt;I did not get access to the credentials of all mail boxes, but together with the
main administrator we enabled a sort-of shadow authentication system (a temporary
OpenLDAP installation) in which the same users were enabled, but with passwords that
will be used during the synchronization. The mailservers were then configured to
have a secondary interface available which used this OpenLDAP rather than the primary
authentication that was being used by the end users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using imapsync&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;imapsync&lt;/code&gt; is simple. It is a command-line application, and everything
configurable is done through command arguments. The basic ones are of course the
source and target definitions, as well as the authentication information for both
sides.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync \
  --host1 src-host --user1 src-user --password1 src-pw --authmech1 LOGIN --ssl1 \
  --host2 dst-host --user2 dst-user --password2 dst-pw --authmech2 LOGIN --ssl2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The use of the &lt;code&gt;--ssl1&lt;/code&gt; and &lt;code&gt;--ssl2&lt;/code&gt; is not to enable an older or newer version of
the SSL/TLS protocol. It just enables the use of SSL/TLS for the source host (&lt;code&gt;--ssl1&lt;/code&gt;)
and destination host (&lt;code&gt;--ssl2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This would just start synchronizing messages, but we need to include the necessary
directives to skip trash and spam mailboxes for instance. For this, the &lt;code&gt;--exclude&lt;/code&gt; parameter
can be used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --exclude &amp;quot;Trash|Spam|Drafts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is also possible to transform some mailbox names. For instance, if the source host
uses &lt;code&gt;Sent&lt;/code&gt; as the mailbox for sent mail, while the target has &lt;code&gt;Sent Items&lt;/code&gt;, then the
following would enable migrating mails between the right folders:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ imapsync ... --folder &amp;quot;Sent&amp;quot; --regextrans2 &amp;#39;s/Sent/Sent Items/&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Conclusions and interesting resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the application was a breeze. I do recommend to create a test account on both sides
so that you can easily see the available folders, source and target naming conventions as
well as test if rerunning the application works flawlessly.&lt;/p&gt;
&lt;p&gt;In my case for instance, I had to add &lt;code&gt;--skipsize&lt;/code&gt; so that the application does not use the
mail sizes for comparing if a mail is already transferred or not, as the target mailserver
showed different mail sizes for the same mails. This was luckily often documented on the
various online tutorials about &lt;code&gt;imapsync&lt;/code&gt;, such as &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://seagrief.co.uk/2010/12/moving-to-google-apps-with-imapsync/"&gt;Moving to Google Apps with imapsync&lt;/a&gt; on seagrief.co.uk&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.zimbra.com/wiki/Guide_to_imapsync"&gt;Guide to imapsync&lt;/a&gt; on wiki.zimbra.com&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The migration took a while, but without major issues. Within a few hours, the mailboxes of all
users where correctly migrated.&lt;/p&gt;</content><category term="Free Software"></category><category term="imapsync"></category></entry><entry><title>New cvechecker release</title><link href="http://192.168.1.71:8000/2015/11/new-cvechecker-release/" rel="alternate"></link><published>2015-11-07T11:07:00+01:00</published><updated>2015-11-07T11:07:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-11-07:/2015/11/new-cvechecker-release/</id><content type="html">&lt;p&gt;A short while ago I got the notification that pulling new CVE information was
no longer possible. The reason was that the NVD site did not support uncompressed
downloads anymore. The fix for cvechecker was simple, and it also gave me a reason
to push out a new release (after two years) which also includes various updates by
Christopher Warner.&lt;/p&gt;
&lt;p&gt;So &lt;a href="https://github.com/sjvermeu/cvechecker/wiki"&gt;cvechecker 3.6&lt;/a&gt; is now available
for general consumption.&lt;/p&gt;
</content><category term="Free Software"></category><category term="cvechecker"></category></entry><entry><title>Using multiple OpenSSH daemons</title><link href="http://192.168.1.71:8000/2015/09/using-multiple-openssh-daemons/" rel="alternate"></link><published>2015-09-06T16:37:00+02:00</published><updated>2015-09-06T16:37:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-09-06:/2015/09/using-multiple-openssh-daemons/</id><summary type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I administer a couple of systems which provide interactive access by end users,
and for this interactive access I position &lt;a href="http://www.openssh.com/"&gt;OpenSSH&lt;/a&gt;. 
However, I also use this for administrative access to the system, and I tend to
have harder security requirements for OpenSSH than most users do.&lt;/p&gt;
&lt;p&gt;For instance, on one system, end users with a userid + password use the
sFTP server for publishing static websites. Other access is prohibited,
so I really like this OpenSSH configuration to use chrooted users, internal
sftp support, whereas a different OpenSSH is used for administrative access
(which is only accessible by myself and some trusted parties).&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Running multiple instances&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although I might get a similar result with a single OpenSSH instance, I
prefer to have multiple instances for this. The default OpenSSH port is used
for the non-administrative access whereas administrative access is on a
non-default port. This has a number of advantages...&lt;/p&gt;
&lt;p&gt;First of all, the SSH configurations are simple and clean. No complex
configurations, and more importantly: easy to manage through configuration
management tools like &lt;a href="http://saltstack.com/"&gt;SaltStack&lt;/a&gt;, my current favorite
orchestration/automation tool.&lt;/p&gt;
&lt;p&gt;Different instances also allow for different operational support services.
There is different monitoring for end-user SSH access versus administrative
SSH access. Also the &lt;a href="https://wiki.gentoo.org/wiki/Fail2ban"&gt;fail2ban&lt;/a&gt; configuration
is different for these instances.&lt;/p&gt;
&lt;p&gt;I can also easily shut down the non-administrative service while ensuring that
administrative access remains operational - something important in case of
changes and maintenance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with multiple instances and SELinux&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Beyond enabling a non-default port for SSH (i.e. by marking it as &lt;code&gt;ssh_port_t&lt;/code&gt;
as well) there is little additional tuning necessary, but that doesn't mean that
there is no additional tuning possible.&lt;/p&gt;
&lt;p&gt;For instance, we could leverage MCS' categories to only allow users (and thus the
SSH daemon) access to the files assigned only that category (and not the rest)
whereas the administrative SSH daemon can access all categories.&lt;/p&gt;
&lt;p&gt;On an MLS enabled system we could even use different sensitivity levels, allowing
the administrative SSH to access the full scala whereas the user-facing SSH can
only access the lowest sensitivity level. But as I don't use MLS myself, I won't go
into detail for this.&lt;/p&gt;
&lt;p&gt;A third possibility would be to fine-tune the permissions of the SSH daemons. However,
that would require different types for the daemon, which requires the daemons to be
started through different scripts (so that we first transition to dedicated 
types) before they execute the SSHd binary (which has the &lt;code&gt;sshd_exec_t&lt;/code&gt; type
assigned).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requiring pubkey and password authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recent OpenSSH daemons allow &lt;a href="https://lwn.net/Articles/544640/"&gt;chaining multiple authentication methods&lt;/a&gt;
before access is granted. This allows the systems to force SSH key authentication first, and then -
after succesful authentication - require the password to be passed on as well. Or a
second step such as &lt;a href="https://wiki.archlinux.org/index.php/Google_Authenticator"&gt;Google Authenticator&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;AuthenticationMethods publickey,password
PasswordAuthentication yes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I don't use the Google Authenticator, but the &lt;a href="https://developers.yubico.com/yubico-pam/"&gt;Yubico PAM module&lt;/a&gt;
to require additional authentication through my U2F dongle (so ssh key, password
and u2f key). Don't consider this three-factor authentication: one thing I know
(password) and two things I have (U2F and ssh key). It's more that I have a couple
of devices with a valid SSH key (laptop, tablet, mobile) which are of course targets
for theft.&lt;/p&gt;
&lt;p&gt;The chance that both one of those devices is stolen &lt;em&gt;together&lt;/em&gt; with the U2F
dongle (which I don't keep attached to those devices of course) is somewhat less.&lt;/p&gt;</content><category term="Free Software"></category><category term="openssh"></category><category term="ssh"></category><category term="u2f"></category><category term="selinux"></category></entry><entry><title>Switching OpenSSH to ed25519 keys</title><link href="http://192.168.1.71:8000/2015/08/switching-openssh-to-ed25519-keys/" rel="alternate"></link><published>2015-08-19T18:26:00+02:00</published><updated>2015-08-19T18:26:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-08-19:/2015/08/switching-openssh-to-ed25519-keys/</id><summary type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With Mike's &lt;a href="http://comments.gmane.org/gmane.linux.gentoo.devel/96896"&gt;news item&lt;/a&gt;
on OpenSSH's deprecation of the &lt;a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm"&gt;DSA algorithm&lt;/a&gt;
for the public key authentication, I started switching the few keys I still had
using DSA to the suggested &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; algorithm. Of
course, I wouldn't be a security-interested party if I did not do some additional
investigation into the DSA versus Ed25519 discussion.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The issue with DSA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You might find DSA a bit slower than RSA:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ openssl speed rsa1024 rsa2048 dsa1024 dsa2048
...
                  sign    verify    sign/s verify/s
rsa 1024 bits 0.000127s 0.000009s   7874.0 111147.6
rsa 2048 bits 0.000959s 0.000029s   1042.9  33956.0
                  sign    verify    sign/s verify/s
dsa 1024 bits 0.000098s 0.000103s  10213.9   9702.8
dsa 2048 bits 0.000293s 0.000339s   3407.9   2947.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, RSA verification outperforms DSA in verification, while signing
with DSA is better than RSA. But for what OpenSSH is concerned, this speed
difference should not be noticeable on the vast majority of OpenSSH servers.&lt;/p&gt;
&lt;p&gt;So no, it is not the speed, but the secure state of the DSS standard.&lt;/p&gt;
&lt;p&gt;The OpenSSH developers find that &lt;a href="http://www.openssh.com/legacy.html"&gt;ssh-dss (DSA) is too weak&lt;/a&gt;,
which is followed by &lt;a href="http://meyering.net/nuke-your-DSA-keys/"&gt;various&lt;/a&gt; 
&lt;a href="https://docs.moodle.org/dev/SSH_key"&gt;sources&lt;/a&gt;. Considering the impact of these keys,
it is important that they follow the state-of-the-art cryptographic services. &lt;/p&gt;
&lt;p&gt;Instead, they suggest to switch to elliptic curve cryptography based algorithms,
with Ed25519 and &lt;a href="https://en.wikipedia.org/wiki/Curve25519"&gt;Curve25519&lt;/a&gt; coming out
on top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Switch to RSA or ED25519?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that RSA is still considered very secure, one of the questions is of
course if &lt;a href="http://ed25519.cr.yp.to/"&gt;ED25519&lt;/a&gt; is the right choice here or not.
I don't consider myself anything in cryptography, but I do like to validate stuff
through academic and (hopefully) reputable sources for information (not that I don't
trust the OpenSSH and OpenSSL folks, but more from a broader interest in the subject).&lt;/p&gt;
&lt;p&gt;Ed25519 should be written fully as &lt;em&gt;Ed25519-SHA-512&lt;/em&gt; and is a signature
algorithm. It uses elliptic curve cryptography as explained on the
&lt;a href="https://en.wikipedia.org/wiki/EdDSA"&gt;EdDSA wikipedia page&lt;/a&gt;. An often cited
paper is &lt;a href="http://aspartame.shiftleft.org/papers/fff/fff.pdf"&gt;Fast and compact elliptic-curve cryptography&lt;/a&gt;
by Mike Hamburg, which talks about the performance improvements, but the main
paper is called &lt;a href="http://ed25519.cr.yp.to/ed25519-20110705.pdf"&gt;High-speed high-security signatures&lt;/a&gt;
which introduces the Ed25519 implementation.&lt;/p&gt;
&lt;p&gt;Of the references I was able to (quickly) go through (not all papers are
publicly reachable) none showed any concerns about the secure state of the 
algorithm. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The (simple) process of switching&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Switching to Ed25519 is simple. First, generate the (new) SSH key (below
just an example run):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh-keygen -t ed25519
Generating public/private ed25519 key pair.
Enter file in which to save the key (/home/testuser/.ssh/id_ed25519): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/testuser/.ssh/id_ed25519.
Your public key has been saved in /home/testuser/.ssh/id_ed25519.pub.
The key fingerprint is:
SHA256:RDaEw3tNAKBGMJ2S4wmN+6P3yDYIE+v90Hfzz/0r73M testuser@testserver
The key&amp;#39;s randomart image is:
+--[ED25519 256]--+
|o*...o.+*.       |
|*o+.  +o ..      |
|o++    o.o       |
|o+    ... .      |
| +     .S        |
|+ o .            |
|o+.o . . o       |
|oo+o. . . o ....E|
| oooo.     ..o+=*|
+----[SHA256]-----+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, make sure that the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; file contains the public key
(as generated as &lt;code&gt;id_ed25519.pub&lt;/code&gt;). Don't remove the other keys yet until the
communication is validated. For me, all I had to do was to update the file in
the Salt repository and have the master push the changes to all nodes (starting
with non-production first of course).&lt;/p&gt;
&lt;p&gt;Next, try to log on to the system using the Ed25519 key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ssh -i ~/.ssh/id_ed25519 testuser@testserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Make sure that your SSH agent is not running as it might still try to revert
back to another key if the Ed25519 one does not work. You can validate if the
connection was using Ed25519 through the &lt;code&gt;auth.log&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;~$ sudo tail -f auth.log&lt;/span&gt;
&lt;span class="go"&gt;Aug 17 21:20:48 localhost sshd[13962]: Accepted publickey for root from \&lt;/span&gt;
&lt;span class="go"&gt;  192.168.100.1 port 43152 ssh2: ED25519 SHA256:-------redacted----------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this communication succeeds, then you can remove the old key from the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;On the client level, you might want to hide &lt;code&gt;~/.ssh/id_dsa&lt;/code&gt; from the SSH agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Obsolete - keychain ~/.ssh/id_dsa&lt;/span&gt;
keychain ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If a server update was forgotten, then the authentication will fail and, depending
on the configuration, either fall back to the regular authentication or fail
immediately. This gives a nice heads-up to you to update the server, while keeping
the key handy just in case. Just refer to the old &lt;code&gt;id_dsa&lt;/code&gt; key during the authentication
and fix up the server.&lt;/p&gt;</content><category term="Free Software"></category><category term="openssh"></category><category term="ssh"></category><category term="gentoo"></category></entry><entry><title>Updates on my Pelican adventure</title><link href="http://192.168.1.71:8000/2015/08/updates-on-my-pelican-adventure/" rel="alternate"></link><published>2015-08-16T19:50:00+02:00</published><updated>2015-08-16T19:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-08-16:/2015/08/updates-on-my-pelican-adventure/</id><summary type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It's been a few weeks that I &lt;a href="http://blog.siphos.be/2015/08/switching-to-pelican/"&gt;switched&lt;/a&gt;
my blog to &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a static site generator build
with Python. A number of adjustments have been made since, which I'll happily
talk about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;The full article view on index page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the features I wanted was to have my latest blog post to be fully
readable from the front page (called the &lt;em&gt;index&lt;/em&gt; page within Pelican). Sadly,
I could not find a plugin of setting that would do this, but I did find
a plugin that I can use to work around this: the &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/summary"&gt;summary&lt;/a&gt;
plugin.&lt;/p&gt;
&lt;p&gt;Enabling the plugin was a breeze. Extract the plugin sources in the &lt;code&gt;plugin/&lt;/code&gt;
folder, and enable it in &lt;code&gt;pelicanconf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;summary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this plug-in, articles can use inline comments to tell the system at which
point the summary of the article stops. Usually, the summary (which is displayed
on index pages) is a first paragraph (or set of paragraphs). What I do is I now
manually set the summmary to the entire blog post for the latest post, and adjust
later when a new post comes up.&lt;/p&gt;
&lt;p&gt;It might be some manual labour, but it fits nicely and doesn't hack around in the
code too much.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Commenting with Disqus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I had some remarks that the &lt;a href="https://disqus.com/home/welcome/"&gt;Disqus&lt;/a&gt; integration
is not as intuitive as expected. Some readers had difficulties finding out how
to comment as a guest (without the need to log on through popular social media
or through Disqus itself).&lt;/p&gt;
&lt;p&gt;Agreed, it is not easy to see at first sight that people need to start typing
their name in the &lt;em&gt;Or sign up with disqus&lt;/em&gt; before they can select &lt;em&gt;I'd rather post
as guest&lt;/em&gt;. As I don't have any way of controlling the format and rendered code
with Disqus, I updated the theme a bit to add in two paragraphs on commenting.
The first paragraph tells how to comment as guest.&lt;/p&gt;
&lt;p&gt;The second paragraph for now informs readers that non-verified comments are put
in the moderation queue. Once I get a feeling of how the spam and bots act on the
commenting system, I will adjust the filters and also allow guest comments to be
readily accessible (no moderation queue). Give it a few more weeks to get myself
settled and I'll adjust it.&lt;/p&gt;
&lt;p&gt;If the performance of the site is slowed down due to the Disqus javascripts: both
Firefox (excuse me, Aurora) and Chromium have this at the initial load. Later, the
scripts are properly cached and load in relatively fast (a quick test shows
all pages I tried load in less than 2 seconds - WordPress was at 4). And if you're
not interested in commenting, then you can even use &lt;a href="https://noscript.net/"&gt;NoScript&lt;/a&gt;
or similar plugins to disallow any remote javascript.&lt;/p&gt;
&lt;p&gt;Still, I will continue to look at how to make commenting easier. I recently allowed
unmoderated comments (unless a number of keywords are added, and comments with links
are also put in the moderation queue). If someone knows of another comment-like
system that I could integrate I'm happy to hear about it as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My issue with Tipue Search has been fixed by reverting a change in &lt;code&gt;tipue_search.py&lt;/code&gt;
(the plugin) where the URL was assigned to the &lt;code&gt;loc&lt;/code&gt; key instead of &lt;code&gt;url&lt;/code&gt;. It is
probably a mismatch between the plugin and the theme (the change of the key was done
in May in Tipue Search itself).&lt;/p&gt;
&lt;p&gt;With this minor issue changed, the search capabilities are back on track on my blog.
Enabling is was a matter of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;tipue_search&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;DIRECT_TEMPLATES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;search&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tags and categories&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;WordPress supports multiple categories, but Pelican does not. So I went through
the various posts that had multiple categories and decided on a single one. While
doing so, I also reduced the &lt;a href="http://blog.siphos.be/categories.html"&gt;categories&lt;/a&gt; to
a small set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Databases&lt;/li&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;Free Software&lt;/li&gt;
&lt;li&gt;Gentoo&lt;/li&gt;
&lt;li&gt;Misc&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;SELinux&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will try to properly tag all posts so that, if someone is interested in a very
particular topic, such as &lt;a href="http://blog.siphos.be/tag/postgresql/index.html"&gt;PostgreSQL&lt;/a&gt;, he can reach
those posts through the tag.&lt;/p&gt;</content><category term="Free Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>My application base: Obnam</title><link href="http://192.168.1.71:8000/2015/08/my-application-base-obnam/" rel="alternate"></link><published>2015-08-05T22:35:00+02:00</published><updated>2015-08-05T22:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-08-05:/2015/08/my-application-base-obnam/</id><summary type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;It is often said, yet too often forgotten: taking backups (and verifying that 
they work). Taking backups is not purely for companies and organizations.
Individuals should also take backups to ensure that, in case of errors or
calamities, the all important files are readily recoverable.&lt;/p&gt;
&lt;p&gt;For backing up files and directories, I personally use &lt;a href="http://obnam.org/"&gt;obnam&lt;/a&gt;,
after playing around with &lt;a href="http://www.bacula.org/"&gt;Bacula&lt;/a&gt; and
&lt;a href="https://attic-backup.org/"&gt;attic&lt;/a&gt;. Bacula is more meant for large
distributed environments (although I also tend to use obnam for my server
infrastructure) and was too complex for my taste. The choice between obnam and
attic is even more personally-oriented.&lt;/p&gt;


&lt;p&gt;I found attic to be faster, but with a small supporting community. Obnam was
slower, but seems to have a more active community which I find important for 
infrastructure that is meant to live quite long (you don't want to switch 
backup solutions every year). I also found it pretty easy to work with, and
to restore files back, and Gentoo provides the &lt;a href="https://packages.gentoo.org/package/app-backup/obnam"&gt;app-backup/obnam&lt;/a&gt;
package.&lt;/p&gt;
&lt;p&gt;I think both are decent solutions, so I had to make one choice and ended up
with obnam. So, how does it work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring what to backup&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The basic configuration file for obnam is &lt;code&gt;/etc/obnam.conf&lt;/code&gt;. Inside this file,
I tell which directories need to be backed up, as well as which subdirectories
or files (through expressions) can be left alone. For instance, I don't want
obnam to backup ISO files as those have been downloaded anyway.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[config]&lt;/span&gt;
&lt;span class="na"&gt;repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;/srv/backup&lt;/span&gt;
&lt;span class="na"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;/root, /etc, /var/lib/portage, /srv/virt/gentoo, /home&lt;/span&gt;
&lt;span class="na"&gt;exclude&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;\.img$, \.iso$, /home/[^/]*/Development/Centralized/.*&lt;/span&gt;
&lt;span class="na"&gt;exclude-caches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;yes&lt;/span&gt;

&lt;span class="na"&gt;keep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;8h,14d,10w,12m,10y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;root&lt;/code&gt; parameter tells obnam which directories (and subdirectories) to
back up. With &lt;code&gt;exclude&lt;/code&gt; a particular set of files or directories can be
excluded, for instance because these contain downloaded resources (and as such
do not need to be inside the backup archives).&lt;/p&gt;
&lt;p&gt;Obnam also supports the &lt;a href="http://www.brynosaurus.com/cachedir/spec.html"&gt;CACHEDIR.TAG&lt;/a&gt;
specification, which I use for the various cache directories. With the use of 
these cache tag files I do not need to update the &lt;code&gt;obnam.conf&lt;/code&gt; file with every
new cache directory (or software build directory).&lt;/p&gt;
&lt;p&gt;The last parameter in the configuration that I want to focus on is the &lt;code&gt;keep&lt;/code&gt;
parameter. Every time obnam takes a backup, it creates what it calls a new
&lt;em&gt;generation&lt;/em&gt;. When the backup storage becomes too big, administrators can run
&lt;code&gt;obnam forget&lt;/code&gt; to drop generations. The &lt;code&gt;keep&lt;/code&gt; parameter informs obnam which
generations can be removed and which ones can be kept.&lt;/p&gt;
&lt;p&gt;In my case, I want to keep one backup per hour for the last 8 hours (I normally
take one backup per day, but during some development sprees or photo
manipulations I back up multiple times), one per day for the last two weeks, 
one per week for the last 10 weeks, one per month for the last 12 months and
one per year for the last 10 years.&lt;/p&gt;
&lt;p&gt;Obnam will clean up only when &lt;code&gt;obnam forget&lt;/code&gt; is executed. As storage is cheap,
and the performance of obnam is sufficient for me, I do not need to call this
very often.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backing up and restoring files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My backup strategy is to backup to an external disk, and then synchronize this
disk with a personal backup server somewhere else. This backup server runs no
other software beyond OpenSSH (to allow secure transfer of the backups) and both
the backup server disks and the external disk is &lt;a href="https://wiki.gentoo.org/wiki/Dm-crypt"&gt;LUKS&lt;/a&gt;
encrypted. Considering that I don't have government secrets I opted not to encrypt
the backup files themselves, but Obnam does support that (through GnuPG).&lt;/p&gt;
&lt;p&gt;All backup enabled systems use cron jobs which execute &lt;code&gt;obnam backup&lt;/code&gt; to take
the backup, and use rsync to synchronize the finished backup with the backup
server. If I need to restore a file, I use &lt;code&gt;obnam ls&lt;/code&gt; to see which file(s) I
need to restore (add in a &lt;code&gt;--generation=&lt;/code&gt; to list the files of a different
backup generation than the last one).&lt;/p&gt;
&lt;p&gt;Then, the command to restore is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/var/restore /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or I can restore immediately to the directory again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# obnam restore --to=/home/swift/Images/Processing /home/swift/Images/Processing/*.NCF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To support multiple clients, obnam by default identifies each client through
the hostname. It is possible to use different names, but hostnames tend to be
a common best practice which I don't deviate from either. Obnam is able to share
blocks between clients (it is not mandatory, but supported nonetheless).&lt;/p&gt;</content><category term="Free Software"></category><category term="mab"></category><category term="backup"></category><category term="obnam"></category></entry><entry><title>Switching to Pelican</title><link href="http://192.168.1.71:8000/2015/08/switching-to-pelican/" rel="alternate"></link><published>2015-08-02T04:09:00+02:00</published><updated>2015-08-02T04:09:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-08-02:/2015/08/switching-to-pelican/</id><summary type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nothing beats a few hours of flying to get things moving on stuff. Being
offline for a few hours with a good workstation helps to not be disturbed
by external actions (air pockets notwithstanding).&lt;/p&gt;
&lt;p&gt;Early this year, I expressed my &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;intentions to move to Pelican&lt;/a&gt;
from WordPress. I wasn't actually unhappy with WordPress, but the security
concerns I had were a bit too much for blog as simple as mine. Running a
PHP-enabled site with a database for something that I can easily handle through
a static site, well, I had to try.&lt;/p&gt;


&lt;p&gt;Today I finally moved the blog, imported all past articles as well as
comments. For the commenting, I now use &lt;a href="http://blog.siphos.be/2015/03/trying-out-pelican-part-one/"&gt;disqus&lt;/a&gt;
which integrates nicely with Pelican and has a fluid feel to it. I wanted to
use the &lt;a href="http://www.tipue.com/search/"&gt;Tipue Search&lt;/a&gt; plug-in as well for
searching through the blog, but I had to put that on hold as I couldn't get
the results of a search to display nicely (all I got were links to
"undefined"). But I'll work on this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring Pelican&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pelican configuration is done through &lt;code&gt;pelicanconf.py&lt;/code&gt; and &lt;code&gt;publishconf.py&lt;/code&gt;. 
The former contains all definitions and settings for the site which are also
useful when previewing changes. The latter contains additional (or overruled)
settings related to publication.&lt;/p&gt;
&lt;p&gt;In order to keep the same links as before (to keep web crawlers happy, as well
as links to the blog from other sites and even the comments themselves) I did
had to update some variables, but the Internet was strong on this one and I had
little problems finding the right settings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Link structure of the site&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;{slug}&lt;/span&gt;&lt;span class="s1"&gt;/index.html&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next challenges were (and still are, I will have to check if this is working
or not soon by checking the blog aggregation sites I am usually aggregated on)
the RSS and Atom feeds. From the access logs of my previous blog, I believe that
most of the aggregation sites are using the &lt;code&gt;/feed/&lt;/code&gt;, &lt;code&gt;/feed/atom&lt;/code&gt; and 
&lt;code&gt;/category/*/feed&lt;/code&gt; links.&lt;/p&gt;
&lt;p&gt;Now, I would like to move the aggregations to XML files, so that the RSS feed is
available at &lt;code&gt;/feed/rss.xml&lt;/code&gt; and the Atom feed at &lt;code&gt;/feed/atom.xml&lt;/code&gt;, but then the
existing aggregations would most likely fail because they currently don't use
these URLs. To fix this, I am now trying to generate the XML files as I would
like them to be, and create symbolic links afterwards from &lt;code&gt;index.html&lt;/code&gt; to the
right XML file.&lt;/p&gt;
&lt;p&gt;The RSS/ATOM settings I am currently using are as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;CATEGORY_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;CATEGORY_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FEED_ALL_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feed/all.rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/atom.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TAG_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tag/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;/feed/rss.xml&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;TRANSLATION_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_ATOM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;AUTHOR_FEED_RSS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Hopefully, the existing aggregations still work, and I can then start asking
the planets to move to the XML URL itself. Some tracking on the access logs
should allow me to see how well this is going.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing to make sure is happening correctly is the blog aggregation and
the comment system. Then, a few tweaks are still on the pipeline.&lt;/p&gt;
&lt;p&gt;One is to optimize the front page a bit. Right now, all articles are
summarized, and I would like to have the last (or last few) article(s) fully
expanded whereas the rest is summarized. If that isn't possible, I'll probably
switch to fully expanded articles (which is a matter of setting a single
variable).&lt;/p&gt;
&lt;p&gt;Next, I really want the search functionality to work again. Enabling the Tipue
search worked almost flawlessly - search worked as it should, and the resulting
search entries are all correct. The problem is that the URLs that the entries
point to (which is what users will click on) all point to an invalid
("undefined") URL.&lt;/p&gt;
&lt;p&gt;Finally, I want the printer-friendly one to be without the social / links on
the top right. This is theme-oriented, and I'm happily using
&lt;a href="https://github.com/DandyDev/pelican-bootstrap3"&gt;pelican-bootstrap3&lt;/a&gt; right now,
so I don't expect this to be much of a hassle. But considering that my blog is
mainly technology oriented for now (although I am planning on expanding that)
being able to have the articles saved in PDF or printed in a nice format is
an important use case for me.&lt;/p&gt;</content><category term="Free Software"></category><category term="blog"></category><category term="pelican"></category><category term="wordpress"></category></entry><entry><title>PostgreSQL with central authentication and authorization</title><link href="http://192.168.1.71:8000/2015/05/postgresql-with-central-authentication-and-authorization/" rel="alternate"></link><published>2015-05-25T12:07:00+02:00</published><updated>2015-05-25T12:07:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-05-25:/2015/05/postgresql-with-central-authentication-and-authorization/</id><summary type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I have been running a PostgreSQL cluster for a while as the primary
backend for many services. The database system is very robust, well
supported by the community and very powerful. In this post, I'm going to
show how I use central authentication and authorization with PostgreSQL.&lt;/p&gt;


&lt;p&gt;Centralized management is an important principle whenever deployments
become very dispersed. For authentication and authorization, having a
high-available LDAP is one of the more powerful components in any
architecture. It isn't the only method though - it is also possible to
use a distributed approach where the master data is centrally managed,
but the proper data is distributed to the various systems that need it.
Such a distributed approach allows for high availability without the
need for a highly available central infrastructure (user ids, group
membership and passwords are distributed to the servers rather than
queried centrally). Here, I'm going to focus on a mixture of both
methods: central authentication for password verification, and
distributed authorization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PostgreSQL default uses in-database credentials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By default, PostgreSQL uses in-database credentials for the
authentication and authorization. When a &lt;code&gt;CREATE ROLE&lt;/code&gt; (or
&lt;code&gt;CREATE USER&lt;/code&gt;) command is issued with a password, it is stored in the
&lt;code&gt;pg_catalog.pg_authid&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# select rolname, rolpassword from pg_catalog.pg_authid;
    rolname     |             rolpassword             
----------------+-------------------------------------
 postgres_admin | 
 dmvsl          | 
 johan          | 
 hdc_owner      | 
 hdc_reader     | 
 hdc_readwrite  | 
 hadoop         | 
 swift          | 
 sean           | 
 hdpreport      | 
 postgres       | md5c127bc9fc185daf0e06e785876e38484
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;this cannot be moved outside):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;postgres# \l db_hadoop
                                   List of databases
   Name    |   Owner   | Encoding |  Collate   |   Ctype    |     Access privileges     
-----------+-----------+----------+------------+------------+---------------------------
 db_hadoop | hdc_owner | UTF8     | en_US.utf8 | en_US.utf8 | hdc_owner=CTc/hdc_owner  +
           |           |          |            |            | hdc_reader=c/hdc_owner   +
           |           |          |            |            | hdc_readwrite=c/hdc_owner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Furthermore, PostgreSQL has some additional access controls through its
&lt;code&gt;pg_hba.conf&lt;/code&gt; file, in which the access towards the PostgreSQL service
itself can be governed based on context information (such as originating
IP address, target database, etc.).&lt;/p&gt;
&lt;p&gt;For more information about the standard setups for PostgreSQL,
&lt;em&gt;definitely&lt;/em&gt; go through the &lt;a href="http://www.postgresql.org/docs/9.4/static/index.html"&gt;official PostgreSQL
documentation&lt;/a&gt; as
it is well documented and kept up-to-date.&lt;/p&gt;
&lt;p&gt;Now, for central management, in-database settings become more difficult
to handle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using PAM for authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first step to move the management of authentication and
authorization outside the database is to look at a way to authenticate
users (password verification) outside the database. I tend not to use a
distributed password approach (where a central component is responsible
for changing passwords on multiple targets), instead relying on a
high-available LDAP setup, but with local caching (to catch short-lived
network hick-ups) and local password use for last-hope accounts (such as
root and admin accounts).&lt;/p&gt;
&lt;p&gt;PostgreSQL can be configured to directly interact with an LDAP, but I
like to use &lt;a href="http://www.linux-pam.org/"&gt;Linux PAM&lt;/a&gt; whenever I can. For
my systems, it is a standard way of managing the authentication of many
services, so the same goes for PostgreSQL. And with the
&lt;a href="https://packages.gentoo.org/package/sys-auth/pam_ldap"&gt;sys-auth/pam_ldap&lt;/a&gt;
package integrating multiple services with LDAP is a breeze. So the
first step is to have PostgreSQL use PAM for authentication. This is
handled through its &lt;code&gt;pg_hba.conf&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# TYPE  DATABASE        USER    ADDRESS         METHOD          [OPTIONS]
local   all             all                     md5
host    all             all     all             pam             pamservice=postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will have PostgreSQL use the &lt;code&gt;postgresql&lt;/code&gt; PAM service for
authentication. The PAM configuration is thus in
&lt;code&gt;/etc/pam.d/postgresql&lt;/code&gt;. In it, we can either directly use the LDAP PAM
modules, or use the SSSD modules and have SSSD work with LDAP.&lt;/p&gt;
&lt;p&gt;Yet, this isn't sufficient. We still need to tell PostgreSQL which users
can be authenticated - the users need to be defined in the database
(just without password credentials because that is handled externally
now). This is done together with the authorization handling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Users and group membership&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every service on the systems I maintain has dedicated groups in which
for instance its administrators are listed. For instance, for the
PostgreSQL services:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqladmin
gpgsqladmin:x:413843:swift,dmvsl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A local batch job (ran through cron) queries this group (which I call
the &lt;em&gt;masterlist&lt;/em&gt;, as well as queries which users in PostgreSQL are
assigned the &lt;code&gt;postgres_admin&lt;/code&gt; role (which is a superuser role like
postgres and is used as the intermediate role to assign to
administrators of a PostgreSQL service), known as the &lt;em&gt;slavelist&lt;/em&gt;.
Delta's are then used to add the user or remove it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Note: membersToAdd / membersToRemove / _psql are custom functions
#       so do not vainly search for them on your system ;-)
for member in $(membersToAdd ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;CREATE USER ${member} LOGIN INHERIT;&amp;quot; postgres
  _psql &amp;quot;GRANT postgres_admin TO ${member};&amp;quot; postgres
done

for member in $(membersToRemove ${masterlist} ${slavelist}) ; do
  _psql &amp;quot;REVOKE postgres_admin FROM ${member};&amp;quot; postgres
  _psql &amp;quot;DROP USER ${member};&amp;quot; postgres
done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;postgres_admin&lt;/code&gt; role is created whenever I create a PostgreSQL
instance. Likewise, for databases, a number of roles are added as well.
For instance, for the &lt;code&gt;db_hadoop&lt;/code&gt; database, the &lt;code&gt;hdc_owner&lt;/code&gt;,
&lt;code&gt;hdc_reader&lt;/code&gt; and &lt;code&gt;hdc_readwrite&lt;/code&gt; roles are created with the right set of
privileges. Users are then granted this role if they belong to the right
group in the LDAP. For instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# getent group gpgsqlhdc_own
gpgsqlhdc_own:x:413850:hadoop,johan,christov,sean
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this simple approach, granting users access to a database is a
matter of adding the user to the right group (like &lt;code&gt;gpgsqlhdc_ro&lt;/code&gt; for
read-only access to the Hadoop related database(s)) and either wait for
the cron-job to add it, or manually run the authorization
synchronization. By standardizing on infrastructural roles (admin,
auditor) and data roles (owner, rw, ro) managing multiple databases is a
breeze.&lt;/p&gt;</content><category term="Free Software"></category><category term="postgresql"></category></entry><entry><title>Audit buffering and rate limiting</title><link href="http://192.168.1.71:8000/2015/05/audit-buffering-and-rate-limiting/" rel="alternate"></link><published>2015-05-10T14:18:00+02:00</published><updated>2015-05-10T14:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2015-05-10:/2015/05/audit-buffering-and-rate-limiting/</id><summary type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be &lt;/p&gt;</summary><content type="html">&lt;p&gt;Be it because of SELinux experiments, or through general audit
experiments, sometimes you'll get in touch with a message similar to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;!-- PELICAN_END_SUMMMARY --&gt;

&lt;p&gt;The message shows up when certain audit events could not be logged
through the audit subsystem. Depending on the system configuration, they
might be either ignored, sent through the kernel logging infrastructure
or even have the system panic. And if the messages are sent to the
kernel log then they might show up, but even that log has its
limitations, which can lead to output similar to the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;__ratelimit: 53 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this post, I want to give some pointers in configuring the audit
subsystem as well as understand these messages...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is auditd and kauditd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you take a look at the audit processes running on the system, you'll
notice that (assuming Linux auditing is used of course) two processes
are running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# ps -ef | grep audit
root      1483     1  0 10:11 ?        00:00:00 /sbin/auditd
root      1486     2  0 10:11 ?        00:00:00 [kauditd]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;/sbin/auditd&lt;/code&gt; daemon is the user-space audit daemon. It &lt;a href="http://man7.org/linux/man-pages/man3/audit_open.3.html"&gt;registers
itself&lt;/a&gt; with the
Linux kernel audit subsystem (through the audit netlink system), which
responds with spawning the &lt;code&gt;kauditd&lt;/code&gt; kernel thread/process. The fact
that the process is a kernel-level one is why the &lt;code&gt;kauditd&lt;/code&gt; is
surrounded by brackets in the &lt;code&gt;ps&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;Once this is done, audit messages are communicated through the netlink
socket to the user-space audit daemon. For the detail-oriented people
amongst you, look for the &lt;em&gt;kauditd_send_skb()&lt;/em&gt; method in the
&lt;a href="http://lxr.free-electrons.com/source/kernel/audit.c"&gt;kernel/audit.c&lt;/a&gt;
file. Now, generated audit event messages are not directly relayed to
the audit daemon - they are first queued in a sort-of backlog, which is
where the backlog-related messages above come from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audit backlog queue&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the kernel-level audit subsystem, a socket buffer queue is used to
hold audit events. Whenever a new audit event is received, it is logged
and prepared to be added to this queue. Adding to this queue can be
controlled through a few parameters.&lt;/p&gt;
&lt;p&gt;The first parameter is the backlog limit. Be it through a kernel boot
parameter (&lt;code&gt;audit_backlog_limit=N&lt;/code&gt;) or through a message relayed by the
user-space audit daemon (&lt;code&gt;auditctl -b N&lt;/code&gt;), this limit will ensure that a
queue cannot grow beyond a certain size (expressed in the amount of
messages). If an audit event is logged which would grow the queue beyond
this limit, then a failure occurs and is handled according to the system
configuration (more on that later).&lt;/p&gt;
&lt;p&gt;The second parameter is the rate limit. When more audit events are
logged within a second than set through this parameter (which can be
controlled through a message relayed by the user-space audit system,
using &lt;code&gt;auditctl -r N&lt;/code&gt;) then those audit events are not added to the
queue. Instead, a failure occurs and is handled according to the system
configuration.&lt;/p&gt;
&lt;p&gt;Only when the limits are not reached is the message added to the queue,
allowing the user-space audit daemon to consume those events and log
those according to the audit configuration. There are some good
resources on audit configuration available on the Internet. I find &lt;a href="http://webapp5.rrz.uni-hamburg.de/SuSe-Dokumentation/manual/sles-manuals_en/cha.audit.comp.html"&gt;this
SuSe
chapter&lt;/a&gt;
worth reading, but many others exist as well.&lt;/p&gt;
&lt;p&gt;There is a useful command related to the subject of the audit backlog
queue. It queries the audit subsystem for its current status:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# auditctl -s
AUDIT_STATUS: enabled=1 flag=1 pid=1483 rate_limit=0 backlog_limit=8192 lost=3 backlog=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The command displays not only the audit state (enabled or not) but also
the settings for rate limits (on the audit backlog) and backlog limit.
It also shows how many events are currently still waiting in the backlog
queue (which is zero in our case, so the audit user-space daemon has
properly consumed and logged the audit events).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Failure handling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If an audit event cannot be logged, then this failure needs to be
resolved. The Linux audit subsystem can be configured do either silently
discard the message, switch to the kernel log subsystem, or panic. This
can be configured through the audit user-space (&lt;code&gt;auditctl -f [0..2]&lt;/code&gt;),
but is usually left at the default (which is 1, being to switch to the
kernel log subsystem).&lt;/p&gt;
&lt;p&gt;Before that is done, the message is displayed which reveals the cause of
the failure handling:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;audit: audit_backlog=321 &amp;gt; audit_backlog_limit=320
audit: audit_lost=44395 audit_rate_limit=0 audit_backlog_limit=320
audit: backlog limit exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the backlog queue was set to contain at most 320 entries
(which is low for a production system) and more messages were being
added (the Linux kernel in certain cases allows to have a few more
entries than configured for performance and consistency reasons). The
number of events already lost is displayed, as well as the current
limitation settings. The message "backlog limit exceeded" can be "rate
limit exceeded" if that was the limitation that was triggered.&lt;/p&gt;
&lt;p&gt;Now, if the system is not configured to silently discard it, or to panic
the system, then the "dropped" messages are sent to the kernel log
subsystem. The calls however are also governed through a configurable
limitation: it uses a rate limit which can be set through &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# sysctl -a | grep kernel.printk_rate
kernel.printk_ratelimit = 5
kernel.printk_ratelimit_burst = 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, this system allows one message every 5 seconds,
but does allow a burst of up to 10 messages at once. When the rate
limitation kicks in, then the kernel will log (at most one per second)
the number of suppressed events:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[40676.545099] __ratelimit: 246 callbacks suppressed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although this limit is kernel-wide, not all kernel log events are
governed through it. It is the caller subsystem (in our case, the audit
subsystem) which is responsible for having its events governed through
this rate limitation or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing up&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before waving goodbye, I would like to point out that the backlog queue
is a memory queue (and not &lt;a href="https://access.redhat.com/solutions/19327"&gt;on disk, Red
Hat&lt;/a&gt;), just in case it wasn't
obvious. Increasing the queue size can result in more kernel memory
consumption. Apparently, a &lt;a href="https://www.redhat.com/archives/linux-audit/2011-October/msg00007.html"&gt;practical size
estimate&lt;/a&gt;
is around 9000 bytes per message. On production systems, it is advised
not to make this setting too low. I personally set it to 8192.&lt;/p&gt;
&lt;p&gt;Lost audit events might result in difficulties for troubleshooting,
which is the case when dealing with new or experimental SELinux
policies. It would also result in missing security-important events. It
is the audit subsystem, after all. So tune it properly, and enjoy the
power of Linux' audit subsystem.&lt;/p&gt;</content><category term="Free Software"></category><category term="audit"></category><category term="kernel"></category><category term="security"></category><category term="selinux"></category></entry><entry><title>D-Bus, quick recap</title><link href="http://192.168.1.71:8000/2014/06/d-bus-quick-recap/" rel="alternate"></link><published>2014-06-29T19:16:00+02:00</published><updated>2014-06-29T19:16:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2014-06-29:/2014/06/d-bus-quick-recap/</id><summary type="html">&lt;p&gt;I've never fully investigated the what and how of D-Bus. I know it is
some sort of IPC, but higher level than the POSIX IPC methods. After
some reading, I think I start to understand how it works and how
administrators can work with it. So a quick write-down is &lt;/p&gt;</summary><content type="html">&lt;p&gt;I've never fully investigated the what and how of D-Bus. I know it is
some sort of IPC, but higher level than the POSIX IPC methods. After
some reading, I think I start to understand how it works and how
administrators can work with it. So a quick write-down is in place so I
don't forget in the future.&lt;/p&gt;
&lt;p&gt;There is one &lt;em&gt;system&lt;/em&gt; bus and, for each X session of a user, also a
&lt;em&gt;session&lt;/em&gt; bus.&lt;/p&gt;
&lt;p&gt;A bus is governed by a &lt;code&gt;dbus-daemon&lt;/code&gt; process. A bus itself has objects
on it, which are represented through path-like constructs (like
&lt;code&gt;/org/freedesktop/ConsoleKit&lt;/code&gt;). These objects are provided by a service
(application). Applications "own" such services, and identify these
through a namespace-like value (such as &lt;code&gt;org.freedesktop.ConsoleKit&lt;/code&gt;).&lt;br&gt;
Applications can send signals to the bus, or messages through methods
exposed by the service. If methods are invoked (i.e. messages send) then
the application must specify the interface (such as
&lt;code&gt;org.freedesktop.ConsoleKit.Manager.Stop&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Administrators can monitor the bus through &lt;strong&gt;dbus-monitor&lt;/strong&gt;, or send
messages through &lt;strong&gt;dbus-send&lt;/strong&gt;. For instance, the following command
invokes the &lt;code&gt;org.freedesktop.ConsoleKit.Manager.Stop&lt;/code&gt; method provided by
the object at &lt;code&gt;/org/freedesktop/ConsoleKit&lt;/code&gt; owned by the
service/application at &lt;code&gt;org.freedesktop.ConsoleKit&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ dbus-send --system --print-reply 
  --dest=org.freedesktop.ConsoleKit 
  /org/freedesktop/ConsoleKit/Manager 
  org.freedesktop.ConsoleKit.Manager.Stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What I found most interesting however was to query the busses. You can
do this with &lt;strong&gt;dbus-send&lt;/strong&gt; although it is much easier to use tools such
as &lt;strong&gt;d-feet&lt;/strong&gt; or &lt;strong&gt;qdbus&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To list current services on the system bus:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system
:1.1
 org.freedesktop.ConsoleKit
:1.10
:1.2
:1.3
 org.freedesktop.PolicyKit1
:1.36
 fi.epitest.hostap.WPASupplicant
 fi.w1.wpa_supplicant1
:1.4
:1.42
:1.5
:1.6
:1.7
 org.freedesktop.UPower
:1.8
:1.9
org.freedesktop.DBus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The numbers are generated by D-Bus itself, the namespace-like strings
are taken by the objects. To see what is provided by a particular
service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system org.freedesktop.PolicyKit1
/
/org
/org/freedesktop
/org/freedesktop/PolicyKit1
/org/freedesktop/PolicyKit1/Authority
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The methods made available through one of these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~# qdbus --system org.freedesktop.PolicyKit1 /org/freedesktop/PolicyKit1/Authority
method QDBusVariant org.freedesktop.DBus.Properties.Get(QString interface_name, QString property_name)
method QVariantMap org.freedesktop.DBus.Properties.GetAll(QString interface_name)
...
property read uint org.freedesktop.PolicyKit1.Authority.BackendFeatures
property read QString org.freedesktop.PolicyKit1.Authority.BackendName
property read QString org.freedesktop.PolicyKit1.Authority.BackendVersion
method void org.freedesktop.PolicyKit1.Authority.AuthenticationAgentResponse(QString cookie, QDBusRawType::(sa{sv} identity)
method void org.freedesktop.PolicyKit1.Authority.CancelCheckAuthorization(QString cancellation_id)
signal void org.freedesktop.PolicyKit1.Authority.Changed()
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Access to methods and interfaces is governed through XML files in
&lt;code&gt;/etc/dbus-1/system.d&lt;/code&gt; (or &lt;code&gt;session.d&lt;/code&gt; depending on the bus). Let's look
at &lt;code&gt;/etc/dbus-1/system.d/dnsmasq.conf&lt;/code&gt; as an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;busconfig&amp;gt;
        &amp;lt;policy user=&amp;quot;root&amp;quot;&amp;gt;
                &amp;lt;allow own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;allow send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
        &amp;lt;policy user=&amp;quot;dnsmasq&amp;quot;&amp;gt;
                &amp;lt;allow own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;allow send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
        &amp;lt;policy context=&amp;quot;default&amp;quot;&amp;gt;
                &amp;lt;deny own=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
                &amp;lt;deny send_destination=&amp;quot;uk.org.thekelleys.dnsmasq&amp;quot;/&amp;gt;
        &amp;lt;/policy&amp;gt;
&amp;lt;/busconfig&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The configuration mentions that only the root Linux user can 'assign' a
service/application to the &lt;code&gt;uk.org.thekelleys.dnsmasq&lt;/code&gt; name, and root
can send messages to this same service/application name. The default is
that no-one can own and send to this service/application name. As a
result, only the Linux root user can interact with this object.&lt;/p&gt;
&lt;p&gt;D-Bus also supports starting of services when a method is invoked
(instead of running this service immediately). This is configured
through &lt;code&gt;*.service&lt;/code&gt; files inside &lt;code&gt;/usr/share/dbus-1/system-services/&lt;/code&gt;.&lt;/p&gt;</content><category term="Free Software"></category><category term="dbus"></category><category term="linux"></category></entry><entry><title>What is that net-pf-## thingie?</title><link href="http://192.168.1.71:8000/2014/04/what-is-that-net-pf-thingie/" rel="alternate"></link><published>2014-04-01T19:46:00+02:00</published><updated>2014-04-01T19:46:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2014-04-01:/2014/04/what-is-that-net-pf-thingie/</id><summary type="html">&lt;p&gt;When checking audit logs, you might come across applications that
request loading of a &lt;code&gt;net-pf-##&lt;/code&gt; module, with &lt;code&gt;##&lt;/code&gt; being an integer.
Having requests for &lt;code&gt;net-pf-10&lt;/code&gt; is a more known cause (enable IPv6) but
what about &lt;code&gt;net-pf-34&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;The answer can be found in &lt;code&gt;/usr/src/linux/include/linux/socket.h&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#define AF_ATMPVC &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;When checking audit logs, you might come across applications that
request loading of a &lt;code&gt;net-pf-##&lt;/code&gt; module, with &lt;code&gt;##&lt;/code&gt; being an integer.
Having requests for &lt;code&gt;net-pf-10&lt;/code&gt; is a more known cause (enable IPv6) but
what about &lt;code&gt;net-pf-34&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;The answer can be found in &lt;code&gt;/usr/src/linux/include/linux/socket.h&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#define AF_ATMPVC       8       /* ATM PVCs                     */
#define AF_X25          9       /* Reserved for X.25 project    */
#define AF_INET6        10      /* IP version 6                 */
#define AF_ROSE         11      /* Amateur Radio X.25 PLP       */
#define AF_DECnet       12      /* Reserved for DECnet project  */
...
#define AF_BLUETOOTH    31      /* Bluetooth sockets            */
#define AF_IUCV         32      /* IUCV sockets                 */
#define AF_RXRPC        33      /* RxRPC sockets                */
#define AF_ISDN         34      /* mISDN sockets                */
#define AF_PHONET       35      /* Phonet sockets               */
#define AF_IEEE802154   36      /* IEEE802154 sockets           */
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So next time you get such a weird module load request, check &lt;code&gt;socket.h&lt;/code&gt;
for more information.&lt;/p&gt;</content><category term="Free Software"></category><category term="linux"></category><category term="module_request"></category><category term="net-pf"></category></entry><entry><title>Managing Inter-Process Communication (IPC)</title><link href="http://192.168.1.71:8000/2014/03/managing-inter-process-communication-ipc/" rel="alternate"></link><published>2014-03-30T12:50:00+02:00</published><updated>2014-03-30T12:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2014-03-30:/2014/03/managing-inter-process-communication-ipc/</id><summary type="html">&lt;p&gt;As a Linux administrator, you'll eventually need to concern you about
&lt;em&gt;Inter-Process Communication (IPC)&lt;/em&gt;. The IPC primitives that most POSIX
operating systems provide are semaphores, shared memory and message
queues. On Linux, the first utility that helps you with those primitives
is &lt;strong&gt;ipcs&lt;/strong&gt;. Let's start with semaphores first.&lt;/p&gt;
&lt;p&gt;Semaphores in &lt;/p&gt;</summary><content type="html">&lt;p&gt;As a Linux administrator, you'll eventually need to concern you about
&lt;em&gt;Inter-Process Communication (IPC)&lt;/em&gt;. The IPC primitives that most POSIX
operating systems provide are semaphores, shared memory and message
queues. On Linux, the first utility that helps you with those primitives
is &lt;strong&gt;ipcs&lt;/strong&gt;. Let's start with semaphores first.&lt;/p&gt;
&lt;p&gt;Semaphores in general are integer variables that have a positive value,
and are accessible by multiple processes (users/tasks/whatever). The
idea behind a semaphore is that it is used to streamline access to a
shared resource. For instance, a device' control channel might be used
by multiple applications, but only one application at a time is allowed
to put something on the channel. Through semaphores, applications check
the semaphore value. If it is zero, they wait. If it is higher, they
attempt decrement the semaphore. If it fails (because another
application in the mean time has decremented the semaphore) then the
application waits, otherwise it continues as it has successfully
decremented the semaphore. In effect, it acts as a sort-of lock towards
a common resource.&lt;/p&gt;
&lt;p&gt;An example you can come across is with ALSA. Some of the ALSA plugins
(such as dmix) use IPC semaphores to allow multiple ALSA applications to
connect to and use the sound subsystem. When an ALSA-enabled application
is using the sound system, you'll see that a semaphore is active:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -s
------ Semaphore Arrays --------
key        semid      owner      perms      nsems     
0x0056a4d5 32768      swift      660        1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;More information about a particular semaphore can be obtained using
&lt;strong&gt;ipcs -s -i SEMID&lt;/strong&gt; where &lt;code&gt;SEMID&lt;/code&gt; is the value in the &lt;em&gt;semid&lt;/em&gt; column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -s -i 32768
Semaphore Array semid=32768
uid=1001         gid=18  cuid=1001       cgid=100
mode=0660, access_perms=0660
nsems = 1
otime = Sun Mar 30 12:33:46 2014  
ctime = Sun Mar 30 12:33:38 2014  
semnum     value      ncount     zcount     pid       
0          0          0          0          32061
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As with all IPC resources, we have information about the owner of the
semaphore (&lt;code&gt;uid&lt;/code&gt; and &lt;code&gt;gid&lt;/code&gt;), the creator of the semaphore (&lt;code&gt;cuid&lt;/code&gt; and
&lt;code&gt;cgid&lt;/code&gt;) as well as its access mask, similar to the file access mask on
Linux systems (&lt;code&gt;mode&lt;/code&gt; and &lt;code&gt;access_perms&lt;/code&gt;). Specific to the IPC
semaphore, you can also notice the &lt;code&gt;nsems = 1&lt;/code&gt;. Unlike the general
semaphores, IPC semaphores are actually a wrapper around one or more
"real" semaphores. The &lt;code&gt;nsems&lt;/code&gt; variable shows how many "real" semaphores
are handled by the IPC semaphore.&lt;/p&gt;
&lt;p&gt;Another very popular IPC resource is &lt;em&gt;shared memory&lt;/em&gt;. This is memory
that is accessible by multiple applications, and provides a very
versatile approach to sharing information and collaboration between
processes. Usually, a semaphore is also used to govern writes and reads
to the shared memory, so that one process that wants to update a part of
the shared memory takes a semaphore (a sort-of lock), makes the updates,
and then increments the semaphore again.&lt;/p&gt;
&lt;p&gt;You can see the currently defined shared memory using &lt;strong&gt;ipcs -m&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status      
0x00000000 655370     swift      600        393216     2          dest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Again, more information can be obtained through &lt;strong&gt;-i SHMID&lt;/strong&gt;. An
interesting value to look at as well is the creator PID (just in case
the process still runs, or through the audit logs) and the last PID used
to operate on the shared memory (which also might no longer exist, but
is still an important value to investigate).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;~$ ipcs -m -p
------ Shared Memory Creator/Last-op PIDs --------
shmid      owner      cpid       lpid      
655370     swift      6147       6017

~$ ps -ef | grep -E &amp;#39;(6147|6017)&amp;#39;
root      6017  6016  0 09:49 tty1     00:01:30 /usr/bin/X -nolisten tcp :0 -auth /home/swift/.serverauth.6000
swift     6147     1  2 09:50 tty1     00:05:10 firefox
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the shared memory is most likely used to handle the UI of
firefox towards the X server.&lt;/p&gt;
&lt;p&gt;A last IPC resource are message queues, through which processes can put
messages on a queue and remove messages (by reading them) from the
queue. I don't have an example at hand for the moment, but just like
semaphores and shared memory, queues can be looked at through &lt;strong&gt;ipcs
-q&lt;/strong&gt; with more information being available through &lt;strong&gt;ipcs -q -i MSQID&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now what if you need to operate these? For this, you can use &lt;strong&gt;ipcrm&lt;/strong&gt;
to remove an IPC resource whereas &lt;strong&gt;ipcmk&lt;/strong&gt; can be used to create one
(although the latter is not that often used for administrative purposes,
whereas &lt;strong&gt;ipcrm&lt;/strong&gt; can help you troubleshoot and fix issues without
having to reboot a system). Of course, removing IPC resources from the
system should only be done when there is a bug in the application(s)
that use it (for instance, a process decreased a semaphore and then
crashed - in that case, remove the semaphore and start one of the
application(s) that also operates on the semaphore as they usually
recreate it and continue happily).&lt;/p&gt;
&lt;p&gt;Now before finishing this post, I do need to tell you about the
difference between an IPC resource key and its identifier. The &lt;em&gt;key&lt;/em&gt; is
like a path or URL, and is a value used by the applications to find and
obtain existing IPC resources (something like, "give me the list of
semaphores that I can access with key 12345"). The &lt;em&gt;identifier&lt;/em&gt; is a
unique ID generated by the Linux kernel at the moment that the IPC
resource is created. Unlike the key, which can be used for multiple IPC
resources, the identifier is unique. This is why the identifier is used
in the &lt;strong&gt;ipcs -i&lt;/strong&gt; command rather than the key. Also, that means that if
applications would properly document their IPC usage then we would
easily know what an IPC resource is used for.&lt;/p&gt;</content><category term="Free Software"></category><category term="ipc"></category><category term="ipcrm"></category><category term="ipcs"></category><category term="linux"></category><category term="msg"></category><category term="sem"></category><category term="shmem"></category></entry><entry><title>Creating a poor man central SCAP system</title><link href="http://192.168.1.71:8000/2013/09/creating-a-poor-man-central-scap-system/" rel="alternate"></link><published>2013-09-24T13:35:00+02:00</published><updated>2013-09-24T13:35:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-09-24:/2013/09/creating-a-poor-man-central-scap-system/</id><summary type="html">&lt;p&gt;A few weeks ago, I was asked to give some explanation about how SCAP
content can be used in companies to improve their infrastructure
knowledge. The focus back then was to look at benchmarks (secure states)
and violations, but other functionality should not be ignored. I'm not
going to talk &lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, I was asked to give some explanation about how SCAP
content can be used in companies to improve their infrastructure
knowledge. The focus back then was to look at benchmarks (secure states)
and violations, but other functionality should not be ignored. I'm not
going to talk about how SCAP can be used in various cases - that'll be
for later - but one of the remarks came how SCAP can be used in larger
environments. The question was not all that weird, as I explained the
functionality through &lt;a href="http://www.open-scap.org"&gt;Open-SCAP&lt;/a&gt; which
currently uses a local scanning approach, and for larger environments
you want to have remote scanning capabilities.&lt;/p&gt;
&lt;p&gt;There are many commercial products available that provide remote
scanning (so you centrally manage the SCAP content and it is "played"
against remote systems), but as a free software advocate I want to see
how this can be achieved in free software. There is
&lt;a href="http://spacewalk.redhat.com/"&gt;spacewalk&lt;/a&gt;, the upstream project of the
RedHat Network Sattelite project, but that looked a bit too complex for
"just" handling SCAP content on remote systems. That, and I'm wondering
if it would be that usable for non-RedHat systems.&lt;/p&gt;
&lt;p&gt;So I decided to make a quick prototype of how I would approach handling
SCAP content in a larger environment. I pushed the result to a
&lt;a href="https://github.com/sjvermeu/pmcs"&gt;github&lt;/a&gt; project, called &lt;em&gt;poor man
central SCAP&lt;/em&gt;. Or, in abbreviated form, &lt;em&gt;pmcs&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The idea is simple: have a central configuration repository that defines
the SCAP content to be played on the remote systems, and have the remote
systems pull this information, evaluating the SCAP content and sending
it to a central reporting repository (which of course can be the same
target where the central configuration is stored). After a few hours of
coding and writing documentation, I have a workable solution with some
nice features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pmcs supports a local-configuration-less approach on the
    target systems. The only thing you need to do is schedule the pmcs
    agent (currently only available as a shell script, but I'll be
    working on a perl or python equivalent soon so that I can support
    other platforms than Unix) with one URL - which is where the
    configuration is stored. No need for local configuration files.&lt;/li&gt;
&lt;li&gt;pmcs uses local SCAP scanning software (that needs to be available
    on the target platforms) but has no strict requirements to that
    software other than that it has to be triggered command-line. This
    allows us to leverage the existing knowledge and features available
    in tools like open-scap or
    &lt;a href="http://sourceforge.net/projects/ovaldi/"&gt;ovaldi&lt;/a&gt; or
    &lt;a href="http://joval.org/"&gt;jOVAL&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;By using a somewhat hierarchical configuration structure and
    keywording support, administrators can fine-tune which SCAP content
    is evaluated on which systems&lt;/li&gt;
&lt;li&gt;pmcs also supports a "daemonized" approach where administrators can
    ask for the immediate evaluation of some SCAP content. This allows
    admins to quickly obtain system information using OVAL/XCCDF.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information, consult the
&lt;a href="https://github.com/sjvermeu/pmcs/blob/master/README.md"&gt;README&lt;/a&gt; or
&lt;a href="https://github.com/sjvermeu/pmcs/blob/master/docs/DESIGN.md"&gt;DESIGN&lt;/a&gt;
document. I'm working on a &lt;a href="https://github.com/sjvermeu/pmcs/blob/master/docs/USES.md"&gt;use
case&lt;/a&gt;
document as well. The tool hardly contains much coding (thanks to the
available KISS tools on many Unix/Linux systems) and is GPL-3.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>My application base: graphviz</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-graphviz/" rel="alternate"></link><published>2013-06-09T03:50:00+02:00</published><updated>2013-06-09T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-09:/2013/06/my-application-base-graphviz/</id><summary type="html">&lt;p&gt;Visualization of data is often needed in order to understand what the
data means. When data needs to be visualized automatically, I often use
the &lt;a href="http://www.graphviz.org/"&gt;graphviz&lt;/a&gt; tools. Not that they are
extremely pretty, but it works very well and is made to be automated.&lt;/p&gt;
&lt;p&gt;Let me give a few examples &lt;/p&gt;</summary><content type="html">&lt;p&gt;Visualization of data is often needed in order to understand what the
data means. When data needs to be visualized automatically, I often use
the &lt;a href="http://www.graphviz.org/"&gt;graphviz&lt;/a&gt; tools. Not that they are
extremely pretty, but it works very well and is made to be automated.&lt;/p&gt;
&lt;p&gt;Let me give a few examples of when visualization helps...&lt;/p&gt;
&lt;p&gt;In SELinux, there is the notion of domain transitions: security contexts
that can transition to another security context (and thus change the
permissions that the application/process has). Knowing where domains can
transition to (and how) as well as how domains can be transitioned to
(so input/output, if you may) is an important aspect to validate the
security of a system. The information can be obtained from tools such as
&lt;strong&gt;sesearch&lt;/strong&gt;, but even on a small system you easily find hundreds of
transitions that can occur. Visualizing the transitions in a graph
(using &lt;strong&gt;dot&lt;/strong&gt; or &lt;strong&gt;neato&lt;/strong&gt;) shows how a starting point can move (or
cannot move - equally important to know ;-) to another domain. So a
simple &lt;strong&gt;sesearch&lt;/strong&gt; with a few &lt;strong&gt;awk&lt;/strong&gt; statements in the middle and a
&lt;strong&gt;dot&lt;/strong&gt; at the end produces a nice graph in PNG format to analyze
further.&lt;/p&gt;
&lt;p&gt;A second visualization is about dependencies. Be it package dependencies
or library dependencies, or even architectural dependencies (in IT
architecturing, abstraction of assets and such also provides a
dependency-like structure), with the Graphviz tools the generation of
dependency graphs can be done automatically. At work, I sometimes use a
simple home-brew web-based API to generate the data (similar to
&lt;a href="http://ashitani.jp/gv/"&gt;Ashitani's Ajax/Graphviz&lt;/a&gt;) since the
workstations don't allow installation of your own software - and they're
windows.&lt;/p&gt;
&lt;p&gt;Another purpose I use graphviz for is to quickly visualize processes
during the design. Of course, this can be done using Visio or Draw.io
easily as well, but these have the disadvantage that you already require
some idea on how the process will evolve. With the dot language, I can
just start writing processes in a simple way, combining steps into
clusters (or in scheduling terms: streams or applications ;-) and let
Graphviz visualize it for me. When the process is almost finished, I can
either copy the result in Draw.io to generate a nicer drawing or use the
Graphviz result (especially when the purpose was just rapid
prototyping).&lt;/p&gt;
&lt;p&gt;And sometimes it is just fun to generate graphs based on data. For
instance, I can take the IRC logs of #gentoo or #gentoo-hardened to
generate graphs showing interactions between people (who speaks to who
and how frequently) or to find out the strength of topics (get the
keywords and generate communication graphs based on those keywords).&lt;/p&gt;</content><category term="Free Software"></category><category term="dependencies"></category><category term="dot"></category><category term="graphviz"></category><category term="mab"></category><category term="neato"></category><category term="scheduling"></category><category term="visualization"></category><category term="visualize"></category></entry><entry><title>My application base: LibreOffice</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-libreoffice/" rel="alternate"></link><published>2013-06-08T03:50:00+02:00</published><updated>2013-06-08T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-08:/2013/06/my-application-base-libreoffice/</id><summary type="html">&lt;p&gt;Of course, working with a Linux desktop eventually requires you to work
with an office suite. Although I have used alternatives like AbiWord and
Calligra in the past, and although I do think that Google Docs might
eventually become powerful enough to use instead, I'm currently using
&lt;a href="https://www.libreoffice.org/"&gt;LibreOffice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The use &lt;/p&gt;</summary><content type="html">&lt;p&gt;Of course, working with a Linux desktop eventually requires you to work
with an office suite. Although I have used alternatives like AbiWord and
Calligra in the past, and although I do think that Google Docs might
eventually become powerful enough to use instead, I'm currently using
&lt;a href="https://www.libreoffice.org/"&gt;LibreOffice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The use of LibreOffice for Linux users is well known: it has decent
Microsoft Office support (although I hardly ever need it; most users
don't mind exporting the files in an open document format and publishers
often support OpenOffice/LibreOffice formats themselves) and its
features are becoming more and more powerful, such as the CMIS support
(for online collaboration through content management systems). It also
has a huge community, sharing templates and other documents that make
life with LibreOffice even much prettier. Don't forget to check out its
&lt;a href="https://www.libreoffice.org/get-help/documentation/"&gt;extensive
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The aspects of LibreOffice I use the most are of course its writer (word
processor) and calc (spreadsheet application). The writer-part is for
when I do technical writing, whereas the spreadsheet application is for
generating simple management sheets for startups and households that
want to keep track of things (such as budgets, creating invoices, data
for mail-merge, etc.). At my work, Excel is one of the most used "end
user computing" tools, so I happen to get acquainted with quite a few
spreadsheet tips and tricks that are beneficial for small companies or
organizations ;-) Also, Calc has support for macro-like enhancements,
which makes it a good start for fast application development (until the
requests of the user/client has been stabilized, after which I usually
suggest a &lt;em&gt;real&lt;/em&gt; application development ;-)&lt;/p&gt;
&lt;p&gt;I generally don't use its presentation part much though - if I get a
powerpoint, I first see if Google Docs doesn't show it sufficiently
well. If not, then I try it out in LibreOffice. But usually, if someone
sends me a presentation, I tend to ask for a PDF version.&lt;/p&gt;</content><category term="Free Software"></category><category term="excel"></category><category term="libreoffice"></category><category term="mab"></category><category term="openoffice"></category><category term="word"></category></entry><entry><title>My application base: firefox</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-firefox/" rel="alternate"></link><published>2013-06-07T03:50:00+02:00</published><updated>2013-06-07T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-07:/2013/06/my-application-base-firefox/</id><summary type="html">&lt;p&gt;Browsers are becoming application disclosure frameworks rather than the
visualization tools they were in the past. More and more services, like
the
&lt;a href="http://blog.siphos.be/2013/06/my-application-base-draw-io/"&gt;Draw.io&lt;/a&gt;
one I discussed not that long ago, are using browsers are their client
side while retaining the full capabilities of end clients (such as drag
and &lt;/p&gt;</summary><content type="html">&lt;p&gt;Browsers are becoming application disclosure frameworks rather than the
visualization tools they were in the past. More and more services, like
the
&lt;a href="http://blog.siphos.be/2013/06/my-application-base-draw-io/"&gt;Draw.io&lt;/a&gt;
one I discussed not that long ago, are using browsers are their client
side while retaining the full capabilities of end clients (such as drag
and drop, file management, editing capabilities and more).&lt;/p&gt;
&lt;p&gt;The browser I use consistently is
&lt;a href="https://www.mozilla.org/en-US/firefox/fx/"&gt;Firefox&lt;/a&gt;. I do think I will
move to Chromium (or at least use it more actively) sooner or later, but
firefox at this point in time covers all my needs. It isn't just the
browser itself though, but also the wide support in add-ons that I am
relying upon. This did make me push out SELinux policies to restrict the
actions that firefox can do, because it has become almost an entire
operating system by itself (like ChromeOS versus Chrome/Chromium). With
a few tunable settings (SELinux booleans) I can enable/disable access to
system devices (such as webcams), often vulnerable plugins (flash,
java), access to sensitive user information (I don't allow firefox
access to regular user files, only to the downloaded content) and more.&lt;/p&gt;
&lt;p&gt;One of the add-ons that is keeping me with Firefox for now is
&lt;a href="http://noscript.net/"&gt;NoScript&lt;/a&gt;. Being a security-conscious guy, being
able to limit the exposure of my surfing habits to advertisement
companies (and others) is very important to me. The NoScript add-on does
this perfectly. The add-on is very extensible (although I don't use that
- just the temporary/permanent allow) and easy to work with: on a site
where you notice some functionality isn't working, right-click and seek
the proper domain to allow methods from. Try-out a few of them
temporarily until you find the "sweet spot" and then allow those for
future reference.&lt;/p&gt;
&lt;p&gt;Another extension I use often (not often enough) is the spelling checker
capabilities. On multi-line fields, this gives me enough feedback about
what I am typing and if it doesn't use a mixture of American English and
British English. But with a simple javascript bookmarklet, I can even
enable spell check on a rendered page (simple javascript that sets the
designMode variable and the contentEditable variable to true), which is
perfect for the Gorg integration while developing Gentoo documentation.&lt;/p&gt;
&lt;p&gt;The abilities of a browser are endless: I have extensions that offer
ePub reading capabilities, full web development capabilities (to
edit/verify CSS and HTML changes), HTTPS Everywhere (to enforce SSL when
the site supports it), SQLite manager, Tamper Data (to track and
manipulate HTTP headers) and more. With the GoogleTalk plugins, doing
video chats and such is all done through the browser.&lt;/p&gt;
&lt;p&gt;This entire eco-system of plugins and extensions make the browser a big
but powerful interface, but also an important resource to properly
manage: keep it up-to-date, backup your settings (including auto-stored
passwords if you enable that), verify its integrity and ensure it runs
in its confined SELinux domain.&lt;/p&gt;</content><category term="Free Software"></category><category term="browser"></category><category term="firefox"></category><category term="mab"></category></entry><entry><title>My application base: bash and kiss tools</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-bash-and-kiss-tools/" rel="alternate"></link><published>2013-06-06T03:50:00+02:00</published><updated>2013-06-06T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-06:/2013/06/my-application-base-bash-and-kiss-tools/</id><summary type="html">&lt;p&gt;Okay, this just had to be here. I'm an automation guy - partially
because of my job in which I'm responsible for the long-term strategy
behind batch, scheduling and workload automation, but also because I
believe proper automation makes life just that much easier. And for
personal work, why not automate &lt;/p&gt;</summary><content type="html">&lt;p&gt;Okay, this just had to be here. I'm an automation guy - partially
because of my job in which I'm responsible for the long-term strategy
behind batch, scheduling and workload automation, but also because I
believe proper automation makes life just that much easier. And for
personal work, why not automate the majority of stuff as well? For most
of the automation I use, I use bash scripts (or POSIX sh scripts that I
try out with the dash shell if I need to export the scripts to non-bash
users).&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://tiswww.case.edu/php/chet/bash/bashtop.html"&gt;Bourne-Again
SHell&lt;/a&gt; (or &lt;strong&gt;bash&lt;/strong&gt;)
is the default shell on Gentoo Linux systems, and is a powerful shell in
features as well. There are numerous resources available on bash
scripting, such as the &lt;a href="http://tldp.org/LDP/abs/html/"&gt;Advanced Bash-Scripting
Guide&lt;/a&gt; or the
&lt;a href="http://www.commandlinefu.com/commands/browse"&gt;commandlinefu.com&lt;/a&gt; (not
purely bash), and specific features of Bash have several posts and
articles all over the web.&lt;/p&gt;
&lt;p&gt;Shell scripts are easy to write, but their power comes from the various
tools that a Linux system contains (including the often forgotten
GNU-provided ones, of which bash is one of them). My system is filled
with scripts, some small, some large, all with a specific function that
I imagined I would need to use again later. I prefix almost all my
scripts with &lt;code&gt;sw&lt;/code&gt; (first letters of SwifT) or &lt;code&gt;mgl&lt;/code&gt; (in case the scripts
have the potential to be used by others) so I can easily find them (if
they are within my &lt;code&gt;${PATH}&lt;/code&gt; of course, not all of them are): just type
the first letters followed by two tabs and bash shows me the list of
scripts I have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sw\t\t
swbackup               swdocbook2html      swsandboxfirefox    swletter      swpics
swstartvm              swstripcomment      swvmconsole         swgenpdf      swcheckmistakes
swdoctransaccuracy     swhardened-secmerge swmailman2mbox      swmassrename  swmassstable
swmovepics             swbumpmod           swsecontribmerge    swseduplicate swfindbracket
swmergeoverlay         swshowtree          swsetvid            swfileprocmon swlocalize
swgendigest            swgenmkbase         swgenfinoverview    swmatchcve

$ mgl\t\t
mglshow                mglverify         mglgxml2docbook       mglautogenif  mgltellabout
mgltellhowto           mgltellwhynot     mglgenmodoverview     mglgenoval    mglgensetup
mglcertcli             mglcleannode      mglwaitforfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the proper basic template, I can keep the scripts sane and well
documented. None of the scripts execute something without arguments, and
"-h" and "--help" are always mapped to the help information. Those that
(re)move files often have a "-p" (or "--pretend") flag that instead of
executing the logic, echo's it to the screen.&lt;/p&gt;
&lt;p&gt;A simple example is the swpics script. It mounts the SD card, moves the
images to a first location (&lt;code&gt;Pictures/local/raw&lt;/code&gt;), unmounts the SD card,
renames the pictures based on the metadata information, finds duplicates
based on two checksums (in case I forgot to wipe the SD card afterwards
- I don't wipe it from the script) and removes the duplicates, converts
the raws into JPEGs and moves these to a minidlna-served location so I
can review the images from DLNA-compliant devices when I want and then
starts the Geeqie application. When the Geeqie application has finished,
it searches for the removed raws and removes those from the
minidlna-served location as well. It's simple, nothing fancy, and saves
me a few minutes of work every time.&lt;/p&gt;
&lt;p&gt;The kiss tools are not really a toolset that is called kiss, but rather
a set of commands that are simple in their use. Examples are exiv2 (to
manage JPEG EXIF information, including renaming them based on the EXIF
timestamp), inotifywait (passive waiting for file modification/writes),
sipcalc (calculating IP addresses and subnetwork ranges), socat (network
socket "cat" tool), screen (or tmux, to implement virtual sessions), git
(okay, not that KISS, but perfect for what it does - versioning stuff)
and more. Because these applications just do what they are supposed to,
without too many bells and whistles, it makes it easy to "glue" them
together to get an automated flow.&lt;/p&gt;
&lt;p&gt;Automation saves you from performing repetitive steps manually, so is a
real time-saver. And bash is a perfect scripting language for it.&lt;/p&gt;</content><category term="Free Software"></category><category term="bash"></category><category term="dash"></category><category term="mab"></category><category term="scripting"></category></entry><entry><title>My application base: geekie</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-geekie/" rel="alternate"></link><published>2013-06-05T03:50:00+02:00</published><updated>2013-06-05T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-05:/2013/06/my-application-base-geekie/</id><summary type="html">&lt;p&gt;In the past, when I had to manage my images (pictures) I used
&lt;a href="http://gqview.sourceforge.net/"&gt;GQview&lt;/a&gt; (which started back in
&lt;a href="http://blog.siphos.be/2008/08/playing-with-gqview/"&gt;2008&lt;/a&gt;). But the
application doesn't get many updates, and if an application does not get
many updates, it either means it is no longer maintained or that it does
its job perfectly &lt;/p&gt;</summary><content type="html">&lt;p&gt;In the past, when I had to manage my images (pictures) I used
&lt;a href="http://gqview.sourceforge.net/"&gt;GQview&lt;/a&gt; (which started back in
&lt;a href="http://blog.siphos.be/2008/08/playing-with-gqview/"&gt;2008&lt;/a&gt;). But the
application doesn't get many updates, and if an application does not get
many updates, it either means it is no longer maintained or that it does
its job perfectly. Sadly, for GQview, it is the unmaintained reason
(even though the application seems to work pretty well for most tasks).
Enter Geeqie, a fork of GQview to keep evolution on the application up
to speed.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://geeqie.sourceforge.net/"&gt;Geeqie&lt;/a&gt; image viewer is a simple
viewer that allows to easily manipulate images (like rotation). I launch
it the moment I insert my camera's SD card into my laptop for image
processing. It quickly shows the thumbnails of all images and I start
processing them to see which ones are eligible for manipulations later
on (or are just perfect - not that that occurs frequently) and which can
be deleted immediately. You can also quickly set Exif information (to
annotate the image further) and view some basic aspects of the picture
(such as histogram information).&lt;/p&gt;
&lt;p&gt;Two features however are what is keeping me with this image viewer:
finding duplicates, and side-by-side comparison.&lt;/p&gt;
&lt;p&gt;With the duplicate feature, geekie can compare images by name, size,
date, dimensions, checksum, path and - most interestingly, similarity.
If you start working on images, you often create intermediate snapshots
or tryouts. Or, when you start taking pictures, you take several ones in
a short time-frame. With the "find duplicate" feature, you can search
through the images to find all images that had the same base (or are
taking quickly after each other) and see them all simultaneously. That
allows you to remove those you don't need anymore and keep the good
ones. I also use this feature often when people come with their external
hard drive filled with images - none of them having any exif information
anymore and not in any way structured - and ask to see if there are any
duplicates on it. A simple checksum might reveal the obvious ones, but
the similarity search of geeqie goes much, much further.&lt;/p&gt;
&lt;p&gt;The side-by-side comparison creates a split view of the application, in
which each pane has another image. This feature I use when I have two
pictures that are taken closely after another (so very, very similar in
nature) and I need to see which one is better. With the side-by-side
comparison, I can look at artifacts in the image or the consequences of
the different aperture, ISO and shutter speed.&lt;/p&gt;
&lt;p&gt;And the moment I start working on images, Gimp and Darktable are just a
single click away.&lt;/p&gt;</content><category term="Free Software"></category><category term="geeqie"></category><category term="gimp"></category><category term="gqview"></category><category term="images"></category><category term="mab"></category></entry><entry><title>My application base: freemind</title><link href="http://192.168.1.71:8000/2013/06/my-application-base-freemind/" rel="alternate"></link><published>2013-06-04T03:50:00+02:00</published><updated>2013-06-04T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-04:/2013/06/my-application-base-freemind/</id><summary type="html">&lt;p&gt;Anyone who is even remotely busy with innovation will know what mindmaps
are. They are a means to visualize information, ideas or tasks in
whatever structure you like. By using graphical annotations the
information is easier to look through, even when the mindmap becomes
very large. In the commercial world &lt;/p&gt;</summary><content type="html">&lt;p&gt;Anyone who is even remotely busy with innovation will know what mindmaps
are. They are a means to visualize information, ideas or tasks in
whatever structure you like. By using graphical annotations the
information is easier to look through, even when the mindmap becomes
very large. In the commercial world, mindmapping software such as
&lt;a href="http://www.xmind.net/"&gt;XMind&lt;/a&gt; and
&lt;a href="http://www.mindjet.com/products/mindmanager/"&gt;Mindmanager&lt;/a&gt; are often
used. But these companies should really start looking into Freemind.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://freemind.sourceforge.net/wiki/index.php/Main_Page"&gt;Freemind&lt;/a&gt;
software is a java-based mind map software, running perfectly on
Windows, Linux or other platforms. Installation is a breeze (if you are
allowed to on your work, you can just launch it from a USB drive if you
want, so no installation hassles whatsoever) and its interface is very
intuitive. For all the whistles and bells that the commercial ones
provide, I just want to create my mindmaps and export them into a format
that others can easily use and view.&lt;/p&gt;
&lt;p&gt;At my real-time job, we (have to) use XMind. If someone shares a mindmap
("their mind" map as I often see it - I seem to have a different mind
than most others in how I structure things, except for one colleague who
imo does not structure things at all) they just share the XMind file and
hope that the recipients can read it. Although XMind can export mindmaps
just fine, I do like the freemind approach where a simple java applet
can show the entire mindmap as interactively as you would navigate
through the application itself. This makes it perfect for discussing
ideas because you can close and open branches easily.&lt;/p&gt;
&lt;p&gt;The
&lt;a href="http://freemind.sourceforge.net/wiki/index.php/Import_and_export"&gt;export/import&lt;/a&gt;
capabilities of freemind are also interesting. Before being forced to
use XMind, we were using Mindmanager and I could just easily import the
mindmaps into freemind. The file format that freemind uses is an
XML-based one, so translating those onto other formats is not that
difficult if you know some XSLT.&lt;/p&gt;
&lt;p&gt;I personally use freemind when I embark on a new project, to structure
the approach, centralize all information, keep track of problems (and
their solutions), etc. The only thing I am missing is a nice interface
for mobile devices though.&lt;/p&gt;</content><category term="Free Software"></category><category term="freemind"></category><category term="java"></category><category term="mab"></category><category term="mindmanager"></category><category term="mindmap"></category><category term="structure"></category><category term="xmind"></category></entry><entry><title>Using extended attributes for custom information</title><link href="http://192.168.1.71:8000/2013/06/using-extended-attributes-for-custom-information/" rel="alternate"></link><published>2013-06-02T03:50:00+02:00</published><updated>2013-06-02T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-06-02:/2013/06/using-extended-attributes-for-custom-information/</id><summary type="html">&lt;p&gt;One of the things I have been meaning to implement on my system is a way
to properly "remove" old files from the system. Currently, I do this
through frequently listing all files, going through them and deleting
those I feel I no longer need (in any case, I can &lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the things I have been meaning to implement on my system is a way
to properly "remove" old files from the system. Currently, I do this
through frequently listing all files, going through them and deleting
those I feel I no longer need (in any case, I can retrieve them back
from the backup within 60 days). But this isn't always easy since it
requires me to reopen the files and consider what I want to do with
them... again.&lt;/p&gt;
&lt;p&gt;Most of the time, when files are created, you generally know how long
they are needed on the system. For instance, an attachment you download
from an e-mail to view usually has a very short lifespan (you can always
re-retrieve it from the e-mail as long as the e-mail itself isn't
removed). Same with output you captured from a shell command, a strace
logfile, etc. So I'm wondering if I can't create a simple method for
keeping track of &lt;em&gt;expiration dates&lt;/em&gt; on files, similar to the expiration
dates supported for z/OS data sets. And to implement this, I am
considering to use extended attributes.&lt;/p&gt;
&lt;p&gt;The idea is simple: when working with a file, I want to be able to
immediately set an expiration date to it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ strace -o strace.log ...
$ expdate +7d strace.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This would set an extended attribute named &lt;code&gt;user.expiration&lt;/code&gt; with the
value being the number of seconds since epoch (which you can obtain
through &lt;strong&gt;date +%s&lt;/strong&gt; if you want) on which the file can be expired (and
thus deleted from the system). A system cronjob can then regularly scan
the system for files with the extended attribute set and, if the
expiration date is beyond the current date, the file can be removed from
the system (perhaps first into a specific area where it lingers for an
additional while just in case).&lt;/p&gt;
&lt;p&gt;It is just an example of course. The idea is that the extended
attributes keep information about the file close to the file itself. I'm
probably going to have an additional layer on top if it, checking
SELinux contexts and automatically identifying expiration dates based on
their last modification time. Setting the expiration dates manually
after creating the files is prone to be forgotten after a while. And
perhaps introduce the flexibility of setting an &lt;code&gt;user.expire_after&lt;/code&gt;
attribute is well, telling that the file can be removed if it hasn't
been touched (modification time) in at least XX number of days.&lt;/p&gt;</content><category term="Free Software"></category><category term="attributes"></category><category term="expiration"></category><category term="extended attributes"></category><category term="linux"></category><category term="xattr"></category></entry><entry><title>Public support channels: irc</title><link href="http://192.168.1.71:8000/2013/05/public-support-channels-irc/" rel="alternate"></link><published>2013-05-16T03:50:00+02:00</published><updated>2013-05-16T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-05-16:/2013/05/public-support-channels-irc/</id><summary type="html">&lt;p&gt;I've &lt;a href="http://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/"&gt;said
it&lt;/a&gt;
before - support channels for free software are often (imo) superior to
the commercial support that you might get with vendors. And although
those vendors often try to use "modern" techniques, I fail to see why
the old, but proven/stable methods would be wrong.&lt;/p&gt;
&lt;p&gt;Consider the "Chat &lt;/p&gt;</summary><content type="html">&lt;p&gt;I've &lt;a href="http://blog.siphos.be/2012/12/why-would-paid-for-support-be-better/"&gt;said
it&lt;/a&gt;
before - support channels for free software are often (imo) superior to
the commercial support that you might get with vendors. And although
those vendors often try to use "modern" techniques, I fail to see why
the old, but proven/stable methods would be wrong.&lt;/p&gt;
&lt;p&gt;Consider the "Chat with Support" feature that many vendors have on their
site. Often, these services use a webbrowser, AJAX-driven method for
talking with support engineers. The problem with this that I see is that
it is difficult to keep track of the feedback you got over time (unless
you manually copy/paste the information), and again that it isn't
public. With free software communities, we still often redirect such
"online" support requests to IRC.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Internet Relay Chat&lt;/em&gt; has been around for ages
(&lt;a href="https://en.wikipedia.org/wiki/IRC"&gt;1988&lt;/a&gt; according to wikipedia) and
still quite active. Gentoo has all of its support channels on the
&lt;a href="http://www.freenode.net"&gt;freenode&lt;/a&gt; IRC network: a community-driven,
active &lt;code&gt;#gentoo&lt;/code&gt; channel with often crosses the 1000 users, a
&lt;code&gt;#gentoo-dev&lt;/code&gt; development-related channel where many developers
communicate, the &lt;code&gt;#gentoo-hardened&lt;/code&gt; channel for all questions and
support regarding Gentoo Hardened specifics, etc.&lt;/p&gt;
&lt;p&gt;Using IRC has many advantages. One is that logs can be kept (either
individually or by the project itself) that can be queried later by the
people who want to provide support (to see if questions have already
been popping up, see what the common questions are for the last few
days, etc.) or get support (to see if their question was already
answered in the past). Of course, these logs can be made public through
web interfaces quite easily. For users, such log functionality is
offered through the IRC client. Another very simple, yet interesting
feature is &lt;em&gt;highlighting&lt;/em&gt;: give the set of terms for which you want to
be notified (usually through a highlight and a specific notification in
the client), making it easier to be on multiple channels without having
to constantly follow-up on all discussions.&lt;/p&gt;
&lt;p&gt;Another advantage is that there is such a thing like "bots". Most Gentoo
related channels do not allow active bots on the channels except for the
project-approved ones (such as &lt;em&gt;willikens&lt;/em&gt;). These bots can provide
project-specific help to users and developers alike:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give one-line information about bugs reported on bugzilla (id,
    assignee, status, but also the URL where the user/developer can view
    the bug etc.)&lt;/li&gt;
&lt;li&gt;Give meta information about a package (maintainer, herd, etc.), herd
    (members), GLSA details, dependency information, etc.&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Allow users to query if a developer is
    &lt;a href="https://dev.gentoo.org/devaway/"&gt;away&lt;/a&gt; or not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create notes (messages) for users that are not online yet but for
    which you know they come online later (and know their nickname or
    registered username)&lt;/li&gt;
&lt;li&gt;Notify when commits are made, or when tweets are sent that match a
    particular expression, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, the IRC protocol has many features that are very
interesting to use in free software communities as well. You can still
do private chats (when potentially confidential data is exchanged) for
instance, or even exchange files (although that is less common to use in
free software communities). There is also still some hierarchy in case
of abuse (channel operators can remove users from the chat or even ban
them for a while) and one can even quiet a channel when for instance
online team meetings are held (although using a different channel for
that might be an alternative).&lt;/p&gt;
&lt;p&gt;IRC also has the advantage that connecting to the IRC channels has a
very low requirement (software-wise): one can use console-only chat
clients (in case users cannot get their graphical environment to work -
example is irssi) or even &lt;a href="http://webchat.freenode.net/"&gt;webbrowser&lt;/a&gt;
based ones (if one wants to chat from other systems). Even smartphones
have good IRC applications, like &lt;a href="http://www.andchat.net/"&gt;AndChat&lt;/a&gt; for
Android.&lt;/p&gt;
&lt;p&gt;IRC is also distributed: an IRC network consists of many interconnected
servers who pass on all IRC traffic. If one node goes down, users can
access a different node and continue. That makes IRC quite
high-available. IRC network operators do need to try and keep the
network from splitting ("netsplit") which occurs when one part of the
distributed network gets segregated from the other part and thus two
"independent" IRC networks are formed. When that occurs, IRC operators
will try to join them back as fast as possible. I'm not going to explain
the details on this - it suffices to understand that IRC is a
distributed manner and thus often much more available than the "support
chat" sites that vendors provide.&lt;/p&gt;
&lt;p&gt;So although IRC looks archaic, it is a very good match for support
channel requirements.&lt;/p&gt;</content><category term="Free Software"></category><category term="chat"></category><category term="irc"></category><category term="support"></category></entry><entry><title>Enabling Kernel Samepage Merging (KSM)</title><link href="http://192.168.1.71:8000/2013/05/enabling-kernel-samepage-merging-ksm/" rel="alternate"></link><published>2013-05-09T03:50:00+02:00</published><updated>2013-05-09T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-05-09:/2013/05/enabling-kernel-samepage-merging-ksm/</id><summary type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The &lt;/p&gt;</summary><content type="html">&lt;p&gt;When using virtualization extensively, you will pretty soon hit the
limits of your system (at least, the resources on it). When the
virtualization is used primarily for testing (such as in my case), the
limit is memory. So it makes sense to seek memory optimization
strategies on such systems. The first thing to enable is KSM or &lt;em&gt;Kernel
Samepage Merging&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This Linux feature looks for memory pages that the applications have
marked as being a possible candidate for optimization (sharing) which
are then reused across multiple processes. The idea is that, especially
for virtualized environments (but KSM is not limited to that), some
processes will have the same contents in memory. Without any sharing
abilities, these memory pages will be unique (meaning at different
locations in your system's memory). With KSM, such memory pages are
consolidated to a single page which is then referred to by the various
processes. When one process wants to modify the page, it is "unshared"
so that there is no corruption or unwanted modification of data for the
other processes.&lt;/p&gt;
&lt;p&gt;Such features are not new - VMWare has it named TPS (&lt;em&gt;Transparent Page
Sharing&lt;/em&gt;) and Xen calls it "Memory CoW" (Copy-on-Write). One advantage
of KSM is that it is simple to setup and advantageous for other
processes as well. For instance, if you host multiple instances of the
same service (web service, database, tomcat, whatever) there is a high
chance that several of its memory pages are prime candidates for
sharing.&lt;/p&gt;
&lt;p&gt;Now before I do mention that this sharing is only enabled when the
application has marked it as such. This is done through the &lt;em&gt;madvise()&lt;/em&gt;
method, where applications mark the memory with &lt;em&gt;MADV_MERGEABLE&lt;/em&gt;,
meaning that the applications explicitly need to support KSM in order
for it to be successful. There is work on the way to support transparent
KSM (such as
&lt;a href="http://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTEzMTI"&gt;UKSM&lt;/a&gt; and
&lt;a href="https://code.google.com/p/pksm/"&gt;PKSM&lt;/a&gt;) where no &lt;em&gt;madvise&lt;/em&gt; calls would
be needed anymore. But beyond quickly reading the home pages (or
translated home pages in case of UKSM ;-) I have no experience with
those projects.&lt;/p&gt;
&lt;p&gt;So let's get back to KSM. I am currently running three virtual machines
(all configured to take at most 1.5 Gb of memory). Together, they take
just a little over 1 Gb of memory (sum of their resident set sizes).
When I consult KSM, I get the following information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; # grep -H &amp;#39;&amp;#39; /sys/kernel/mm/ksm/pages_*
/sys/kernel/mm/ksm/pages_shared:48911
/sys/kernel/mm/ksm/pages_sharing:90090
/sys/kernel/mm/ksm/pages_to_scan:100
/sys/kernel/mm/ksm/pages_unshared:123002
/sys/kernel/mm/ksm/pages_volatile:1035
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;pages_shared&lt;/code&gt; tells me that 48911 pages are shared (which means
about 191 Mb) through 90090 references (&lt;code&gt;pages_sharing&lt;/code&gt; - meaning the
various processes have in total 90090 references to pages that are being
shared). That means a gain of 41179 pages (160 Mb). Note that the
resident set sizes do not take into account shared pages, so the sum of
the RSS has to be subtracted with this to find the "real" memory
consumption. The &lt;code&gt;pages_unshared&lt;/code&gt; value tells me that 123002 pages are
marked with the &lt;code&gt;MADV_MERGEABLE&lt;/code&gt; advise flag but are not used by other
processes.&lt;/p&gt;
&lt;p&gt;If you want to use KSM yourself, configure your kernel with &lt;code&gt;CONFIG_KSM&lt;/code&gt;
and start KSM by echo'ing the value "1" into &lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt;.
That's all there is to it.&lt;/p&gt;</content><category term="Free Software"></category><category term="cow"></category><category term="ksm"></category><category term="kvm"></category><category term="linux"></category><category term="virtualization"></category></entry><entry><title>The Linux ".d" approach</title><link href="http://192.168.1.71:8000/2013/05/the-linux-d-approach/" rel="alternate"></link><published>2013-05-08T03:50:00+02:00</published><updated>2013-05-08T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-05-08:/2013/05/the-linux-d-approach/</id><summary type="html">&lt;p&gt;Many services on a Linux system use a &lt;code&gt;*.d&lt;/code&gt; directory approach to make
their configuration easily configurable by other services. This is a
remarkably simple yet efficient method for exposing services towards
other applications. Let's look into how this &lt;code&gt;.d&lt;/code&gt; approach works.&lt;/p&gt;
&lt;p&gt;Take a look at the &lt;code&gt;/etc/pam.d &lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many services on a Linux system use a &lt;code&gt;*.d&lt;/code&gt; directory approach to make
their configuration easily configurable by other services. This is a
remarkably simple yet efficient method for exposing services towards
other applications. Let's look into how this &lt;code&gt;.d&lt;/code&gt; approach works.&lt;/p&gt;
&lt;p&gt;Take a look at the &lt;code&gt;/etc/pam.d&lt;/code&gt; structure: services that are PAM-aware
can place their PAM configuration files in this location, without
needing any additional configuration steps or registration. Same with
&lt;code&gt;/etc/cron.d&lt;/code&gt;: applications that need specific cronjobs do not need to
edit &lt;code&gt;/etc/crontab&lt;/code&gt; directly (with the problem of concurrent access,
overwriting changes, etc.) but instead can place their definitions in
the &lt;code&gt;cron.d&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;This approach is getting more traction, as can be seen from the
available "dot-d" directories on a system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ ls -d /etc/*.d
/etc/bash_completion.d  /etc/ld.so.conf.d  /etc/pam.d          /etc/sysctl.d
/etc/conf.d             /etc/local.d       /etc/profile.d      /etc/wgetpaste.d
/etc/dracut.conf.d      /etc/logrotate.d   /etc/request-key.d  /etc/xinetd.d
/etc/env.d              /etc/makedev.d     /etc/sandbox.d      /etc/cron.d
/etc/init.d             /etc/modprobe.d    /etc/sudoers.d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;An application can place its configuration files in these directories,
automatically "plugging" it in into the operating system and the
services that it provides. And the more services adopt this approach,
the easier it is for applications to be pluggable within the operating
system. Even complex systems such as database systems can easily
configure themselves this way. And for larger organizations, this is a
very interesting approach.&lt;/p&gt;
&lt;p&gt;Consider the need to deploy a database server on a Linux system in a
larger organization. Each organization has its standards for file system
locations, policies for log file management, etc. With the &lt;code&gt;*.d&lt;/code&gt;
approach, these organizations only need to put files on the file system
(a rather primitive feature that every organization supports) and manage
these files instead of using specific, proprietary interfaces to
configure the environment. But to properly control this flexibility, a
few attention points need to be taken into account.&lt;/p&gt;
&lt;p&gt;The first is to use a proper &lt;em&gt;naming convention&lt;/em&gt;. If the organization
has a data management structure, it might have specific names for
services. These names are then used throughout the organization to
properly identify owners or responsibilities. When using the &lt;code&gt;*.d&lt;/code&gt;
directories, these naming conventions also allow administrators to
easily know who to contact if a malfunctioning definition is placed. For
instance, if a log rotation definition has a wrong entry, a file called
&lt;code&gt;mylogrotation&lt;/code&gt; does not reveal much information. However,
&lt;code&gt;CDBM-postgres-querylogs&lt;/code&gt; might reveal that the file is placed there by
the customer database management team for a postgresql database. And it
isn't only about knowing who to contact (because that could easily be
done by comments as well), but also to ensure no conflicts occur. On a
shared database system, it is much more likely that two different teams
place a &lt;code&gt;postgresql&lt;/code&gt; file (which would overwrite the file already there)
unless they use a proper naming convention.&lt;/p&gt;
&lt;p&gt;The second is to use something identifying where the file comes from. A
best practice when using Puppet for instance is to add in a comment to
the file such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# This file is managed by Puppet through the org-pgsl-def module
# Please do not modify manually
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This informs the administrator how the file is put there; you might even
want to include version information.&lt;/p&gt;
&lt;p&gt;A third one is when the order of configuration entries is important.
Most &lt;code&gt;*.d&lt;/code&gt; supporting tools do not really care about ordering, but some,
like udev, do. When that is the case, the common consensus is to use
numbers in the beginning of the file name. The numbers then provide a
good ordering of the files.&lt;/p&gt;
&lt;p&gt;Not all services already offer &lt;code&gt;*.d&lt;/code&gt; functionality, although it isn't
that difficult to provide it as well. Consider the Linux audit daemon,
whose rules are managed in the &lt;code&gt;/etc/audit/audit.rules&lt;/code&gt; file. Not that
flexible, isn't it? But one can create a &lt;code&gt;/etc/audit/audit.rules.d&lt;/code&gt;
location and have the audit init script read these files (in
alphanumeric order), creating the same functionality.&lt;/p&gt;
&lt;p&gt;Given enough service adoption, software distribution can be sufficient
to configure an application completely and integrate it with all
services used by the operating system. And even services that do not
support &lt;code&gt;*.d&lt;/code&gt; directories can still be easily wrapped around so that
their configuration file itself is generated based on the information in
such directories. Consider a hypothetical
&lt;a href="https://wiki.gentoo.org/wiki/AIDE"&gt;AIDE&lt;/a&gt; configuration, where the
&lt;code&gt;aide.conf&lt;/code&gt; is generated based on the &lt;code&gt;aide.conf.head&lt;/code&gt;, &lt;code&gt;aide.d/*&lt;/code&gt; and
&lt;code&gt;aide.conf.tail&lt;/code&gt; files (similar to how &lt;code&gt;resolv.conf&lt;/code&gt; is sometimes
managed). The generation is triggered right before &lt;strong&gt;aide&lt;/strong&gt; itself is
called (perhaps all in a single script).&lt;/p&gt;
&lt;p&gt;Such an approach allows full integration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A PAM configuration file is placed, allowing the service
    authentication to be easily managed by administrators. Changes on
    the authentication (for instance, switch to an LDAP authentication
    or introduce some trust relation) is done by placing an
    updated file.&lt;/li&gt;
&lt;li&gt;A log rotation configuration file is placed, making sure that the
    log files for the service do not eventually fill the partitions&lt;/li&gt;
&lt;li&gt;A syslog configuration is provided, allowing for some events to be
    sent to a different server instead of keeping it local - or perhaps
    both&lt;/li&gt;
&lt;li&gt;A cron configuration is stored so that statistics and other
    house-cleaning jobs for the service can run at night&lt;/li&gt;
&lt;li&gt;An audit configuration snippet is added to ensure critical commands
    and configuration files are properly checked&lt;/li&gt;
&lt;li&gt;Intrusion detection rules are added when needed&lt;/li&gt;
&lt;li&gt;Monitoring information is placed on the file system, causing
    additional monitoring metrics to be automatically picked up&lt;/li&gt;
&lt;li&gt;Firewall definitions are extended based on the snippets placed on
    the system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;etc. And all this by only placing files on the file system. Keep It
Simple, and efficient ;-)&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>Qemu-KVM monitor tips and tricks</title><link href="http://192.168.1.71:8000/2013/04/qemu-kvm-monitor-tips-and-tricks/" rel="alternate"></link><published>2013-04-30T03:50:00+02:00</published><updated>2013-04-30T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-04-30:/2013/04/qemu-kvm-monitor-tips-and-tricks/</id><summary type="html">&lt;p&gt;When running KVM guests, the &lt;a href="https://en.wikibooks.org/wiki/QEMU/Monitor"&gt;Qemu/KVM
monitor&lt;/a&gt; is a nice interface
to interact with the VM and do specific maintenance tasks on. If you run
the KVM guests with VNC, then you can get to this monitor through
&lt;code&gt;Ctrl-Alt-2&lt;/code&gt; (and &lt;code&gt;Ctrl-Alt-1&lt;/code&gt; to get back to the VM display). I &lt;/p&gt;</summary><content type="html">&lt;p&gt;When running KVM guests, the &lt;a href="https://en.wikibooks.org/wiki/QEMU/Monitor"&gt;Qemu/KVM
monitor&lt;/a&gt; is a nice interface
to interact with the VM and do specific maintenance tasks on. If you run
the KVM guests with VNC, then you can get to this monitor through
&lt;code&gt;Ctrl-Alt-2&lt;/code&gt; (and &lt;code&gt;Ctrl-Alt-1&lt;/code&gt; to get back to the VM display). I
personally run with the monitor on the standard input/output where the
VM is launched as its output is often large and scrolling in the VNC
doesn't seem to work well.&lt;/p&gt;
&lt;p&gt;I decided to give you a few tricks that I use often on the monitor to
handle the VMs.&lt;/p&gt;
&lt;p&gt;When I do not start the VNC server associated with the VM by default, I
can enable it on the monitor using &lt;strong&gt;change vnc&lt;/strong&gt; while getting details
is done using &lt;strong&gt;info vnc&lt;/strong&gt;. To disable VNC again, use &lt;strong&gt;change vnc
none&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) info vnc
Server: disabled
(qemu) change vnc 127.0.0.1:20
(qemu) change vnc password
Password: ******
(qemu) info vnc
Server:
     address: 127.0.0.1:5920
        auth: vnc
Client: none
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly, if you need to enable remote debugging, you can use the
&lt;strong&gt;gdbserver&lt;/strong&gt; option.&lt;/p&gt;
&lt;p&gt;Getting information using &lt;strong&gt;info&lt;/strong&gt; is dead-easy, and supports a wide
area of categories: balloon info, block devices, character devices,
cpus, memory mappings, network information, etcetera etcetera... Just
enter &lt;strong&gt;info&lt;/strong&gt; to get an overview of all supported commands.&lt;/p&gt;
&lt;p&gt;To easily manage block devices, you can see the current state of devices
using &lt;strong&gt;info block&lt;/strong&gt; and then use &lt;strong&gt;change &amp;lt;blockdevice&amp;gt;
&amp;lt;path&amp;gt;&lt;/strong&gt; to update it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) info block
virtio0: removable=0 io-status=ok file=/srv/virt/gentoo/hardened2selinux/selinux-base.img ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0
ide1-cd0: removable=1 locked=0 tray-open=0 io-status=ok [not inserted]
floppy0: removable=1 locked=0 tray-open=0 [not inserted]
sd0: removable=1 locked=0 tray-open=0 [not inserted]
(qemu) change ide1-cd0 /srv/virt/media/systemrescuecd-x86-2.2.0.iso
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To powerdown the system, use &lt;strong&gt;system_powerdown&lt;/strong&gt;. If that fails, you
can use &lt;strong&gt;quit&lt;/strong&gt; to immediately shut down (terminate) the VM. To reset
it, use &lt;strong&gt;system_reset&lt;/strong&gt;. You can also hot-add PCI devices and
manipulate CPU states, or even perform &lt;a href="http://www.linux-kvm.org/page/Migration"&gt;live
migrations&lt;/a&gt; between systems.&lt;/p&gt;
&lt;p&gt;When you use qcow2 image formats, you can take a full VM snapshot using
&lt;strong&gt;savevm&lt;/strong&gt; and, when you later want to return to this point again, use
&lt;strong&gt;loadvm&lt;/strong&gt;. This is interesting when you want to do potentially harmful
changes on the system and want to easily revert back if things got
broken.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(qemu) savevm 20130419
(qemu) info snapshots
     ID        TAG                 VM SIZE                DATE       VM CLOCK
     1         20130419               224M 2013-04-19 12:05:16   00:00:17.294
(qemu) loadvm 20130419
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Free Software"></category><category term="kvm"></category><category term="monitor"></category><category term="qemu"></category></entry><entry><title>photorec to the rescue</title><link href="http://192.168.1.71:8000/2013/04/photorec-to-the-rescue/" rel="alternate"></link><published>2013-04-29T03:50:00+02:00</published><updated>2013-04-29T03:50:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-04-29:/2013/04/photorec-to-the-rescue/</id><summary type="html">&lt;p&gt;Once again
&lt;a href="http://www.cgsecurity.org/wiki/PhotoRec_Step_By_Step"&gt;PhotoRec&lt;/a&gt; has
been able to save files from a corrupt FAT USB drive. The application
scans the partition, looking for known files (based on the file magic)
and then restores those files. The files are not named as they were
though, so there is still some manual work &lt;/p&gt;</summary><content type="html">&lt;p&gt;Once again
&lt;a href="http://www.cgsecurity.org/wiki/PhotoRec_Step_By_Step"&gt;PhotoRec&lt;/a&gt; has
been able to save files from a corrupt FAT USB drive. The application
scans the partition, looking for known files (based on the file magic)
and then restores those files. The files are not named as they were
though, so there is still some manual work left, but the recovery works
pretty well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;PhotoRec 6.12, Data Recovery Utility, May 2011
Christophe GRENIER 
http://www.cgsecurity.org

Disk /dev/sdc1 - 1000 GB / 931 GiB (RO) - WD My Book
     Partition                  Start        End    Size in sectors
     No partition             0   0  1 121600 253 63 1953520002 [Whole disk]


Pass 1 - Reading sector  464342462/1953520002, 10738 files found
Elapsed time 2h46m01s - Estimated time to completion 8h52m25
jpg: 7429 recovered
txt: 961 recovered
mp3: 558 recovered
tx?: 373 recovered
riff: 297 recovered
gif: 218 recovered
exe: 151 recovered
ifo: 126 recovered
mpg: 91 recovered
pdf: 83 recovered
others: 451 recovered
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In Gentoo, you can find the package as part of &lt;code&gt;app-admin/testdisk&lt;/code&gt;. To
recover the files, I ran the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ photorec /log /d /path/to/recovery/dest /dev/sdc1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While skimming through the recovered files, I found a few ones that I
deleted a long time ago but apparently never got overwritten (the data,
that is). Scary to see how easy such recovery is... Makes me remember
that, if you really want to delete files in a less recoverable manner,
you can use &lt;strong&gt;shred&lt;/strong&gt; for that.&lt;/p&gt;
&lt;p&gt;And for those out there yelling to backup this data - you're absolutely
correct, but no. I backup my systems and important files daily, but this
disk contained (mainly) raw picture images and videorecordings. The
manipulated, finished images and recordings are backed up (or at least
on a disk &lt;em&gt;and&lt;/em&gt; somewhere online), but the raw images and recordings are
often too much to introduce a backup for, and if I would really lost
them, I wouldn't shed a tear (nor panic).&lt;/p&gt;</content><category term="Free Software"></category><category term="corruption"></category><category term="photorec"></category><category term="recovery"></category><category term="shred"></category></entry><entry><title>Comparing performance with sysbench: performance analysis</title><link href="http://192.168.1.71:8000/2013/04/comparing-performance-with-sysbench-part-3/" rel="alternate"></link><published>2013-04-19T16:22:00+02:00</published><updated>2013-04-19T16:22:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-3/</id><summary type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with &lt;/p&gt;</summary><content type="html">&lt;p&gt;So in the past few posts I discussed how &lt;strong&gt;sysbench&lt;/strong&gt; can be used to
simulate some workloads, specific to a particular set of tasks. I used
the benchmark application to look at the differences between the guest
and host on my main laptop, and saw a major performance regression with
the &lt;em&gt;memory&lt;/em&gt; workload test. Let's view this again, using parameters more
optimized to view the regressions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=32M --memory-block-size=64 run
Host:
  Operations performed: 524288 (2988653.44 ops/sec)
  32.00 MB transferred (182.41 MB/sec)

Guest:
  Operations performed: 524288 (24920.74 ops/sec)
  32.00 MB transferred (1.52 MB/sec)

$ sysbench --test=memory --memory-total-size=32M --memory-block-size=32M run
Host:
  Operations performed: 1 (  116.36 ops/sec)
  32.00 MB transferred (3723.36 MB/sec)

Guest:
  Operations performed: 1 (   89.27 ops/sec)
  32.00 MB transferred (2856.77 MB/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From looking at the code (gotta love Gentoo for making this obvious ;-)
we know that the memory workload, with a single thread, does something
like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;total_bytes = 0;
repeat until total_bytes &amp;gt;= memory-total-size:
  thread_mutex_lock()
  total_bytes += memory-block-size
  thread_mutex_unlock()

  (start event timer)
  pointer -&amp;gt; buffer;
  while pointer &amp;lt;-&amp;gt; end-of(buffer)
    write somevalue at pointer
    pointer++
  (stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Given that the regression is most noticeable when the memory-block-size
is very small, the part of the code whose execution count is much
different between the two runs is the mutex locking, global memory
increment and the start/stop of event timer.&lt;/p&gt;
&lt;p&gt;In a second phase, we also saw that mutex locking itself is not
impacted. In the above case, we have 524288 executions. However, if we
run the mutex workload this number of times, we see that this hardly has
any effect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=524288 --mutex-loops=0 run
Host:      total time:        0.0275s
Guest:     total time:        0.0286s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code for the mutex workload, knowing that we run with one thread,
looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mutex_locks = 524288
(start event timer)
do
  lock = get_mutex()
  thread_mutex_lock()
  global_var++
  thread_mutex_unlock()
  mutex_locks--
until mutex_locks = 0;
(stop event timer)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check if the timer might be the culprit, let's look for a benchmark
that mainly does timer checks. The &lt;em&gt;cpu&lt;/em&gt; workload can be used, when we
tell sysbench that the prime to check is 3 (as its internal loop runs
from 3 till the given number, and when the given number is 3 it skips
the loop completely) and we ask for 524288 executions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=3 --max-requests=524288 run
Host:  total time:  0.1640s
Guest: total time: 21.0306s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Gotcha! Now, the event timer (again from looking at the code) contains
two parts: getting the current time (using &lt;code&gt;clock_gettime()&lt;/code&gt;) and
logging the start/stop (which is done in memory structures). Let's make
a small test application that gets the current time (using the real-time
clock as the sysbench application does) and see if we get similar
results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cat test.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;time.h&amp;gt;

int main(int argc, char **argv, char **arge) {
  struct timespec tps;
  long int i = 524288;
  while (i-- &amp;gt; 0)
    clock_gettime(CLOCK_REALTIME, &amp;amp;tps);
}

$ gcc -lrt -o test test.c
$ time ./test
Host:  0m0.019s
Guest: 0m5.030s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So given that the &lt;code&gt;clock_gettime()&lt;/code&gt; is ran twice in the sysbench, we
already have 10 seconds of overhead on the guest (and only 0,04s on the
host). When such time-related functions are slow, it is wise to take a
look at the clock source configured on the system. On Linux, you can
check this by looking at &lt;code&gt;/sys/devices/system/clocksource/*&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# cd /sys/devices/system/clocksource/clocksource0
# cat current_clocksource
kvm-clock
# cat available_clocksource
kvm-clock tsc hpet acpi_pm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although kvm-clock is supposed to be the best clock source, let's switch
to the tsc clock:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# echo tsc &amp;gt; current_clocksource
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If we rerun our test application, we get a much more appreciative
result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ time ./test
Host:  0m0.019s
Guest: 0m0.024s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So, what does that mean for our previous benchmark results?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
Host:            35,3049 sec
Guest (before):  36,5582 sec
Guest (now):     35,6416 sec

$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
Host:            1,8424 MB/sec
Guest (before):  1,5591 MB/sec
Guest (now):     1,5912 MB/sec

$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host:            3959,78 MB/sec
Guest (before)   3079,29 MB/sec
Guest (now):     3821,89 MB/sec

$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:            9765 executions
Guest (before):   512 executions
Guest (now):      529 executions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So we notice that this small change has nice effects on some of the
tests. The CPU benchmark improves from 3,55% overhead to 0,95%; fileio
is the same (from 15,38% to 13,63%), memory improves from 22,24%
overhead to 3,48% and threads remains about status quo (from 94,76%
slower to 94,58%).&lt;/p&gt;
&lt;p&gt;That doesn't mean that the VM is now suddenly faster or better than
before - what we changed was how fast a certain time measurement takes,
which the benchmark software itself uses rigorously. This goes to show
how important it is to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;understand fully how the benchmark software works and measures&lt;/li&gt;
&lt;li&gt;realize the importance of access to source code is not to be
    misunderstood&lt;/li&gt;
&lt;li&gt;know that performance benchmarks give figures, but do not tell you
    how your users will experience the system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it for the sysbench benchmark for now (the MySQL part will need
to wait until a later stage).&lt;/p&gt;</content><category term="Free Software"></category><category term="performance"></category><category term="sysbench"></category></entry><entry><title>Comparing performance with sysbench: memory, threads and mutexes</title><link href="http://192.168.1.71:8000/2013/04/comparing-performance-with-sysbench-part-2/" rel="alternate"></link><published>2013-04-19T04:11:00+02:00</published><updated>2013-04-19T04:11:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-04-19:/2013/04/comparing-performance-with-sysbench-part-2/</id><summary type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each &lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, I gave some feedback on the cpu and fileio
workload tests that &lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; can handle. Next
on the agenda are the &lt;em&gt;memory&lt;/em&gt;, &lt;em&gt;threads&lt;/em&gt; and &lt;em&gt;mutex&lt;/em&gt; workloads.&lt;/p&gt;
&lt;p&gt;When using the &lt;em&gt;memory&lt;/em&gt; workload, &lt;strong&gt;sysbench&lt;/strong&gt; will allocate a buffer
(provided through the &lt;em&gt;--memory-block-size&lt;/em&gt; parameter, defaults to
1kbyte) and each execution will read or write to this memory
(&lt;em&gt;--memory-oper&lt;/em&gt;, defaults to write) in a random or sequential manner
(&lt;em&gt;--memory-access-mode&lt;/em&gt;, defaults to &lt;strong&gt;seq&lt;/strong&gt;uential).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-block-size=1M --memory-total-size=10G run
Host throughput, 1M:  3959,78 MB/sec
Guest throughput, 1M: 3079,29 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The guest has a lower throughput (about 77% of the host), which is lower
than what most online posts provide on KVM performance. We'll get back
to that later. Let's look at the default block size of 1k (meaning that
the benchmark will do a lot more executions before it reaches the total
memory (in load):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=memory --memory-total-size=1G run
Host throughput, 1k:  1702,59 MB/sec
Guest throughput, 1k:   23,67 MB/sec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is a lot worse: the guest' throughput is only 1,4% of the host
throughput! The &lt;code&gt;qemu-kvm&lt;/code&gt; process on the host is also taking up a lot
of CPU.&lt;/p&gt;
&lt;p&gt;Now let's take a look at the other workload, &lt;em&gt;threads&lt;/em&gt;. In this
particular workload, you identify the number of threads
(&lt;em&gt;--num-threads&lt;/em&gt;), the number of locks (&lt;em&gt;--thread-locks&lt;/em&gt;) and the number
of times a thread should run its 'lock-yield..unlock' workload
(&lt;em&gt;--thread-yields&lt;/em&gt;). The more locks you identify, the less number of
threads will have the same lock (each thread is allocated a single lock
during an execution, but every new execution will give it a new lock so
the threads do not always take the same lock).&lt;/p&gt;
&lt;p&gt;Note that parts of this is also handled by the other tests: mutex'es are
used when a new operation (execution) for the thread is prepared. In
case of the memory-related workload above, the smaller the buffer size,
the more frequent thread operations are needed. In the last run we did
(with the bad performance), millions of operations were executed
(although no yields were performed). Something similar can be simulated
using a single lock, single thread and a very high number of operations
and no yields:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=1 --thread-yields=0 --max-requests=1000000 --thread-locks=1 run
Host runtime:    0,3267 s  (event:    0,2278)
Guest runtime:  40,7672 s  (event:   30,6084)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This means that the guest "throughput" problems from the memory
identified above seem to be related to this rather than memory-specific
regressions. To verify if the scheduler itself also shows regressions,
we can run more threads concurrently. For instance, running 128 threads
simultaneously, using the otherwise default settings, during 10 seconds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=threads --num-threads=128 --max-time=10s run
Host:   9765 executions (events)
Guest:   512 executions (events)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we get only 5% throughput.&lt;/p&gt;
&lt;p&gt;Let's focus on the mutex again, as sysbench has an additional mutex
workload test. The workload has each thread running a local fast loop
(simple increments, &lt;em&gt;--mutex-loops&lt;/em&gt;) after which it takes a random mutex
(one of &lt;em&gt;--mutex-num&lt;/em&gt;), locks it, increments a global variable and then
releases the mutex again. This is repeated for the number of locks
identified (&lt;em&gt;--mutex-locks&lt;/em&gt;). If mutex operations would be the cause of
the performance issues above, then we would notice that the mutex
operations are a major performance regression on my system.&lt;/p&gt;
&lt;p&gt;Let's run that workload with a single thread (default), no loops and a
single mutex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=mutex --mutex-num=1 --mutex-locks=50000000 --mutex-loops=1 run
Host (duration):   2600,57 ms
Guest (duration):  2571,44 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, we see that the mutex operations are almost at the same
speed (99%) of the host, so pure mutex operations are not likely to be
the cause of the performance regressions earlier on. So what does give
the performance problems? Well, that investigation will be for the third
and last post in this series ;-)&lt;/p&gt;</content><category term="Free Software"></category><category term="memory"></category><category term="mutex"></category><category term="performance"></category><category term="sysbench"></category><category term="threading"></category><category term="threads"></category></entry><entry><title>Comparing performance with sysbench: cpu and fileio</title><link href="http://192.168.1.71:8000/2013/04/comparing-performance-with-sysbench/" rel="alternate"></link><published>2013-04-18T21:31:00+02:00</published><updated>2013-04-18T21:31:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2013-04-18:/2013/04/comparing-performance-with-sysbench/</id><summary type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out &lt;/p&gt;</summary><content type="html">&lt;p&gt;Being busy with virtualization and additional security measures, I
frequently come in contact with people asking me what the performance
impact is. Now, you won't find the performance impact of SELinux here as
I have no guests nor hosts that run without SELinux. But I did want to
find out what one can do to compare system (and later application)
performance, so I decided to take a look at the various benchmark
utilities available. In this first post, I'll take a look at
&lt;a href="http://sysbench.sf.net"&gt;sysbench&lt;/a&gt; (using 0.4.12, released on March 2009
- unlike what you would think from the looks of the site alone) to
compare the performance of my KVM guest versus host.&lt;/p&gt;
&lt;p&gt;The obligatory system information: the host is a HP Pavilion dv7 3160eb
with an Intel Core i5-430M processor (dual-core with 2 threads per
core). Frequency scaling is disabled - the CPU is fixed at 2.13 Ghz. The
system has 4Gb of memory (DDR3), the internal hard disks are configured
as a software RAID1 and with LVM on top (except for the file system that
hosts the virtual guest images, which is a plain software RAID1). The
guests run with the boot options given below, meaning 1.5Gb of memory, 2
virtual CPUs of the KVM64 type. The CFLAGS for both are given below as
well, together with the expanded set given by &lt;strong&gt;gcc \${CFLAGS} -E -v -
&lt;/dev&gt;&amp;amp;1 | grep cc1&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/bin/qemu-kvm -monitor stdio -nographic -gdb tcp::1301   
 -vnc 127.0.0.1:14   
 -net nic,model=virtio,macaddr=00:11:22:33:44:b3,vlan=0   
 -net vde,vlan=0   
 -drive file=/srv/virt/gentoo/test/pg1.img,if=virtio,cache=none   
 -k nl-be -m 1536 -cpu kvm64 -smp 2

# For host
CFLAGS=&amp;quot;-march=core2 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-D_FORTIFY_SOURCE=2 -fno-strict-overflow -march=core2   
        -fPIE -O2 -fstack-protector-all&amp;quot;
# For guest
CFLAGS=&amp;quot;-march=x86-64 -O2 -pipe&amp;quot;
#CFLAGS=&amp;quot;-fno-strict-overflow -march=x86-64 -fPIE -O2   
        -fstack-protector-all&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I am aware that the CFLAGS between the two are not the same (duh), and I
know as well that the expansion given above isn't entirely accurate. But
still, it gives some idea on the differences.&lt;/p&gt;
&lt;p&gt;Now before I go on to the results, please keep in mind that I am &lt;em&gt;not a
performance expert&lt;/em&gt;, not even a &lt;em&gt;performance experienced&lt;/em&gt; or even
&lt;em&gt;performance wanna-be experienced&lt;/em&gt; person: the more I learn about the
inner workings of an operating system such as Linux, the more complex it
becomes. And when you throw in additional layers such as virtualization,
I'm almost completely lost. In my day-job, some people think they can
"prove" the inefficiency of a hypervisor by counting from 1 to 100'000
and adding the numbers, and then take a look at how long this takes. I
think this is short-sighted, as this puts load on a system that does not
simulate reality. If you really want to do performance measures for
particular workloads, you need to run those workloads and not some small
script you hacked up. That is why I tend to focus on applications that
use workload simulations for infrastructural performance measurements
(like &lt;a href="http://hammerora.sf.net"&gt;HammerDB&lt;/a&gt; for performance testing
databases). But for this blog post series, I'm first going to start with
basic operations and later posts will go into more detail for particular
workloads (such as database performance measurements).&lt;/p&gt;
&lt;p&gt;Oh, and BTW, when I display figures with a comma (","), that comma means
decimal (so "1,00" = "1").&lt;/p&gt;
&lt;p&gt;The figures below are numbers that can be interpreted in many ways, and
can prove everything. I'll sometimes give my interpretation to it, but
don't expect to learn much from it - there are probably much better
guides out there for this. The posts are more of a way to describe how
&lt;strong&gt;sysbench&lt;/strong&gt; works and what you should take into account when doing
performance benchmarks.&lt;/p&gt;
&lt;p&gt;So the testing is done using &lt;strong&gt;sysbench&lt;/strong&gt;, which is capable of running
CPU, I/O, memory, threading, mutex and MySQL tests. The first run of it
that I did was a single-thread run for CPU performance testing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=cpu --cpu-max-prime=20000 run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This test verifies prime numbers by dividing the number with
sequentially increasing numbers and verifying that the remainder (modulo
calculation) is zero. If it is, then the number is not prime and the
calculation goes on to the next number; otherwise, if none have a
remainder of 0, then the number is prime. The maximum number that it
divides by is calculated by taking the integer part of the square root
of the number (so for 17, this is 4). This algorithm is very simple, so
you should also take into account that during the compilation of the
benchmark, the compiler might already have optimized some of it.&lt;/p&gt;
&lt;p&gt;Let's look at the numbers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   35,4331   37,0528
     e.total   35,4312   36,8917
1.2    total   35,1482   36,1951
     e.total   35,1462   36,0405
1.3    total   35,3334   36,4266
     e.total   35,3314   36,2640
================================
avg    total   35,3049   36,5582
     e.total   35,3029   36,3987
med    total   35,3334   36,4266
     e.total   35,3314   36,2640
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On average (I did three runs on each system), the guest took 3,55% more
time to finish the test than the host (&lt;code&gt;total&lt;/code&gt;). If we look at the pure
calculation (so not the remaining overhead of the inner workings -
&lt;code&gt;e.total&lt;/code&gt;) then the guest took 3,10% more time. The median however (the
run that wasn't the fastest nor the slowest of the three) has the guest
taking 3,09% more time (total) and 2,64% more time (e.total).&lt;/p&gt;
&lt;p&gt;Let's look at the two-thread results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Run     Stat     Host      Guest
1.1    total   17,5185   18,0905
     e.total   35,0296   36,0217
1.2    total   17,8084   18,1070
     e.total   35,6131   36,0518
1.3    total   18,0683   18,0921
     e.total   36,1322   36,0194
================================
avg    total   17,5185   18,0965
     e.total   35,0296   36,0310
med    total   17,8084   18,0921
     e.total   35,6131   36,0194
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With these figures, we notice that the guest average total run time
takes 1,67% more time to complete, and the event time only 1,23%. I was
personally expecting that the guest would have a higher percentage than
previously (gut feeling - never trust it when dealing with complex
matter) but was happy to see that the difference wasn't higher. I'm not
going to start analyze this in more detail and just go to the next test:
fileio.&lt;/p&gt;
&lt;p&gt;In case of fileio testing, I assume that the hypervisor will take up
&lt;a href="http://www.linux-kvm.org/page/Virtio/Block/Latency"&gt;more overhead&lt;/a&gt;, but
keep in mind that you also need to consider the environmental factors:
LVM or not, RAID1 or not, mount options, etc. Since I am comparing
guests versus hosts here, I should look for a somewhat comparable setup.
Hence, I will look for the performance of the host (software raid, LVM,
ext4 file system with data=ordered) and the guest (images on software
raid, ext4 file system with data=ordered and barrier=0, and LVM in
guest).&lt;/p&gt;
&lt;p&gt;Furthermore, running a sysbench test suggests a file that is much larger
than the available RAM. I'm going to run the tests on a 6Gb file size,
but enable O_DIRECT for writes so that some caches (page cache) are not
used. This can be done using &lt;em&gt;--file-extra-flags=direct&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As with all I/O-related benchmarks, you need to define which kind of
load you want to test with. Are the I/Os sequential (like reading or
writing a large file completely) or random? For databases, you are most
likely interested in random reads (data, for selects) and sequential
writes (into transaction logs). A file server usually has random
read/write. In the below test, I'll use a combined &lt;strong&gt;r&lt;/strong&gt;a&lt;strong&gt;nd&lt;/strong&gt;om
&lt;strong&gt;r&lt;/strong&gt;ead/&lt;strong&gt;w&lt;/strong&gt;rite.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sysbench --test=fileio --file-total-size=6G prepare
$ sysbench --test=fileio --file-total-size=6G --file-test-mode=rndrw --max-time=300 --max-requests=0 --file-extra-flags=direct run
$ sysbench --test=fileio --file-total-size=6G cleanup
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the output, the throughput seems to be most important:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Operations performed:  4348 Read, 2898 Write, 9216 Other = 16462 Total
Read 67.938Mb  Written 45.281Mb  Total transferred 113.22Mb  (1.8869Mb/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above case, the throughput is 1,8869 Mbps. So let's look at the
(averaged) results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Host:  1,8424 Mbps
Guest: 1,5591 Mbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above figures (which are an average of 3 runs) tell us that the
guest has a throughput of about 84,75% (so we take about 15% performance
hit on random read/write I/O). Now I used sysbench here for some I/O
validation of guest between host, but other usages apply as well. For
instance, let's look at the impact of &lt;code&gt;data=ordered&lt;/code&gt; versus
&lt;code&gt;data=journal&lt;/code&gt; (taken on the host):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;6G, data=ordered, barrier=1: 1,8435 Mbps
6G, data=ordered, barrier=0: 2,1328 Mbps
6G, data=journal, barrier=1: 599,85 Kbps
6G, data=journal, barrier=0: 767,93 Kbps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From the figures, we can see that the &lt;code&gt;data=journal&lt;/code&gt; option slows down
the throughput to a final figure about 30% of the original throughput
(70% decrease!). Also, disabling barriers has a positive impact on
performance, giving about 15% throughput gain. This is also why some
people report performance improvements when switching to LVM, as - as
far as I can tell (but finding a good source on this is difficult) - LVM
&lt;em&gt;by default&lt;/em&gt; disables barriers (but does honor the &lt;code&gt;barrier=1&lt;/code&gt; mount
option if you provide it).&lt;/p&gt;
&lt;p&gt;That's about it for now - the next post will be about the memory and
threads tests within sysbench.&lt;/p&gt;</content><category term="Free Software"></category><category term="cpu"></category><category term="hypervisor"></category><category term="io"></category><category term="kvm"></category><category term="performance"></category><category term="sysbench"></category></entry><entry><title>Why would paid-for support be better?</title><link href="http://192.168.1.71:8000/2012/12/why-would-paid-for-support-be-better/" rel="alternate"></link><published>2012-12-31T22:46:00+01:00</published><updated>2012-12-31T22:46:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2012-12-31:/2012/12/why-would-paid-for-support-be-better/</id><summary type="html">&lt;p&gt;Last Saturday evening, I sent an e-mail to a low-volume mailinglist
regarding IMA problems that I'm facing. I wasn't expecting an answer
very fast of course, being holidays, weekend and a low-volume
mailinglist. But hey - it is the free software world, so I should expect
some slack on this, right &lt;/p&gt;</summary><content type="html">&lt;p&gt;Last Saturday evening, I sent an e-mail to a low-volume mailinglist
regarding IMA problems that I'm facing. I wasn't expecting an answer
very fast of course, being holidays, weekend and a low-volume
mailinglist. But hey - it is the free software world, so I should expect
some slack on this, right?&lt;/p&gt;
&lt;p&gt;Well, not really. I got a reply on sunday - and not just an
acknowledgement e-mail, but a to-the-point answer. It was immediately
correct and described why, and helped me figure out things further. And
this is not a unique case in the free software world: because you are
dealing with the developers and users that have written the code that
you are running/testing, you get a bunch of very motivated souls, all
looking at your request when they can, and giving input when they can.&lt;/p&gt;
&lt;p&gt;Compare that to commercial support from bigger vendors: in these cases,
your request probably gets read by a single person whose state of mind
is difficult to know (but from the communication you often get the
impression that they either couldn't care less or they are swamped with
request tasks so they cannot devote enough time on your request). In
most cases, they check the request for containing the right amount of
information in the right format on the right fields, or even ignore that
you did all that right and just ask you for (the same) information
again. And who knows how many times I had to "state your business
impact".&lt;/p&gt;
&lt;p&gt;Now, I know that commercial support from bigger vendor has the burden of
a huge overload in requests, but is that truely that different in the
free software world? Mailinglists such as the Linux kernel mailinglist
(for kernel development) gets hundreds (thousands?) mails a day, and
those with request for feedback or with questions get a reply quite
swiftly. Mailinglists for distribution users get a lot of traffic as
well, and each and every request is handled with due care and responded
to within a very good timeframe (24h or less most of the time, sometimes
a few days if the user is using a strange or exotic environment that not
everyone knows how to handle).&lt;/p&gt;
&lt;p&gt;I think one of the biggest advantages of the free software world is that
the requests are public. That both teaches the many users on those
mailinglists and fora on how to handle problems they haven't seen
before, as well as allows users to first look for a problem before
reporting it. Everybody wins with this. And because it is public, many
users are happily answering more and more questions because they get the
visibility (with acknowledgements) they deserve: they gain a specific
position in that particular area that others respect, because we can see
how much effort (and good results) they gave earlier on.&lt;/p&gt;
&lt;p&gt;So kudos to the free software world, a happy new year - and keep going
forward.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>nginx as reverse SMTP proxy</title><link href="http://192.168.1.71:8000/2012/12/nginx-as-reverse-smtp-proxy/" rel="alternate"></link><published>2012-12-06T00:03:00+01:00</published><updated>2012-12-06T00:03:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2012-12-06:/2012/12/nginx-as-reverse-smtp-proxy/</id><summary type="html">&lt;p&gt;I've noticed that not that many resources are online telling you how you
can use nginx as a reverse SMTP proxy. Using a reverse SMTP proxy makes
sense even if you have just one mail server back-end, either because you
can easily switch towards another one, or because you want &lt;/p&gt;</summary><content type="html">&lt;p&gt;I've noticed that not that many resources are online telling you how you
can use nginx as a reverse SMTP proxy. Using a reverse SMTP proxy makes
sense even if you have just one mail server back-end, either because you
can easily switch towards another one, or because you want to put
additional checks before handing off the mail to the back-end.&lt;/p&gt;
&lt;p&gt;In the below example, a back-end mail server is running on localhost (in
my case it's a Postfix back-end, but that doesn't matter). Mails
received by Nginx will be forwarded to this server.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;user nginx nginx;
worker_processes 1;

error_log /var/log/nginx/error_log debug;

events {
        worker_connections 1024;
        use epoll;
}
http {

        log_format main
                &amp;#39;$remote_addr - $remote_user [$time_local] &amp;#39;
                &amp;#39;&amp;quot;$request&amp;quot; $status $bytes_sent &amp;#39;
                &amp;#39;&amp;quot;$http_referer&amp;quot; &amp;quot;$http_user_agent&amp;quot; &amp;#39;
                &amp;#39;&amp;quot;$gzip_ratio&amp;quot;&amp;#39;;


        server {
                listen 127.0.0.1:8008;
                server_name localhost;
                access_log /var/log/nginx/localhost.access_log main;
                error_log /var/log/nginx/localhost.error_log info;

                root /var/www/localhost/htdocs;

                location ~ .php$ {
                        add_header Auth-Server 127.0.0.1;
                        add_header Auth-Port 25;
                        return 200;
                }
        }
}

mail {
        server_name localhost;

        auth_http localhost:8008/auth-smtppass.php;

        server {
                listen 192.168.100.102:25;
                protocol smtp;
                timeout 5s;
                proxy on;
                xclient off;
                smtp_auth none;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you first look at the &lt;em&gt;mail&lt;/em&gt; setting, you notice that I include an
&lt;em&gt;auth_http&lt;/em&gt; directive. This is needed by Nginx as it will consult this
back-end service on what to do with the mail (the moment that it
receives the recipient information). The URL I use is arbitrarily chosen
here, as I don't really run a PHP service in the background (yet).&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;http&lt;/em&gt; section, I create the same resource that the mails'
auth_http wants to connect to. I then declare the two return headers
that Nginx needs (Auth-Server and Auth-Port) with the back-end
information (127.0.0.1:25). If I ever need to do load balancing or other
tricks, I'll write up a simple PHP script and serve it from PHP-FPM or
so.&lt;/p&gt;
&lt;p&gt;Next on the list is to enable SSL (not difficult) with client
authentication (which isn't supported by Nginx for the mail module (yet)
sadly, so I'll need to look at a different approach for that).&lt;/p&gt;
&lt;p&gt;BTW, this is all on a simple Gentoo Hardened with SELinux enabled. The
following booleans were set to true: &lt;em&gt;nginx_enable_http_server&lt;/em&gt;,
&lt;em&gt;nginx_enable_smtp_server&lt;/em&gt; and &lt;em&gt;nginx_can_network_connect_http&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This page has been translated into
&lt;a href="http://www.webhostinghub.com/support/es/misc/nginx-como-poder"&gt;Spanish&lt;/a&gt;
language by Maria Ramos from
&lt;a href="http://www.webhostinghub.com/support/edu"&gt;Webhostinghub.com/support/edu&lt;/a&gt;.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>devops - how hard can it/it can be</title><link href="http://192.168.1.71:8000/2010/09/devops-how-hard-can-itit-can-be/" rel="alternate"></link><published>2010-09-04T09:17:00+02:00</published><updated>2010-09-04T09:17:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2010-09-04:/2010/09/devops-how-hard-can-itit-can-be/</id><summary type="html">&lt;p&gt;Dieter made a good reference to &lt;a href="http://dieter.plaetinck.be/what_the_open_source_community_can_learn_from_devops"&gt;devops and the open source
community&lt;/a&gt;
and (correctly) points out that, even in a more collaborative scene such
as the free software communities', there is still distinction between
development and operations. And it isn't hard to see commonalities
between enterprise organizations and free software &lt;/p&gt;</summary><content type="html">&lt;p&gt;Dieter made a good reference to &lt;a href="http://dieter.plaetinck.be/what_the_open_source_community_can_learn_from_devops"&gt;devops and the open source
community&lt;/a&gt;
and (correctly) points out that, even in a more collaborative scene such
as the free software communities', there is still distinction between
development and operations. And it isn't hard to see commonalities
between enterprise organizations and free software communities in that
respect.&lt;/p&gt;
&lt;p&gt;But is the comparison correct? If you look at a distribution as an
enterprise, then surely the distinction between upstream (project
development) and "downstream" (distribution) should be compared with the
relations between an enterprise and its ISVs, not its internal
development / operational divisions. If we look at internal divisions,
then distributions tend to provide better integration between (internal)
projects and the distribution. I cannot talk for every distribution, but
in those I do know, the infrastructure team ("operations") has a firm
grip on the infrastructure, yet leaves out sufficient space for
development to do their releases/production activity: uploading files,
changing documentation, ...&lt;/p&gt;
&lt;p&gt;This works, if the provided interface does not allow for developers to
harm the principles that infrastructure has. This is what many
(enterprise) organizations are still lacking, but there is no simple
solution for this. Often, the operations team has principles that are
difficult to match with the goals of development. Finding the correct
balance between development and operations in that respect is quite a
challenge - usually, free software communities can get there faster,
often because their mass is sufficiently low. With a total 'employee'
count of a few hundreds it is statistically easier to find a balance
than within enterprises of a few thousand employees.&lt;/p&gt;
&lt;p&gt;I believe that both teams should write down their principles, policies
and standards, and see if they can find matches (which is good) and
mutually exclusive distinctions (which is challenging) where more
investigation can be done. Both teams should be allowed to question
decisions made by the other (but without pretending to know better) and
make suggestions. This should lead to the emergence of interfaces where
a team has sufficient freedom to get to their own goals autonomously.&lt;/p&gt;
&lt;p&gt;With such interfaces, people will start thinking that devops is growing
apart (after all, they're starting to work autonomously and
independently of each other). That isn't true. In my opinion, devops is
about interacting on a high level (which is less time-delimited) so that
interactions on a low level (which is very time-limited and focused on
releasing, releasing, releasing) aren't necessary. Less interaction
means that the teams that are responsible for getting to a specific,
short time-framed goal, can cooperate closely and have a better grip on
resources and requirements.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>qemu monitor cd change</title><link href="http://192.168.1.71:8000/2010/08/qemu-monitor-cd-change/" rel="alternate"></link><published>2010-08-30T21:38:00+02:00</published><updated>2010-08-30T21:38:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2010-08-30:/2010/08/qemu-monitor-cd-change/</id><summary type="html">&lt;p&gt;I've been playing around with kvm (which uses qemu) to try out other
operating systems and Linux distributions. Up until now, little progress
on that part (not because it is difficult, just little time) but there
are a few things worth mentioning. For this post, let's start with a
quicky &lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been playing around with kvm (which uses qemu) to try out other
operating systems and Linux distributions. Up until now, little progress
on that part (not because it is difficult, just little time) but there
are a few things worth mentioning. For this post, let's start with a
quicky on CD changes.&lt;/p&gt;
&lt;p&gt;qemu's integrated monitor is very nice and powerful. To go to the
monitor from inside the vnc session, press Ctrl+Alt+2 (to go back, use
Ctrl+Alt+1). Then you can query for attached hardware, add new devices,
remove others, cpu's, etc. Something I found necessary was to switch
CD/DVD images. With &lt;code&gt;info block&lt;/code&gt; I found the device. I then ran
&lt;code&gt;eject ide1-cd0&lt;/code&gt; followed by &lt;code&gt;change ide1-cd0 /path/to/new/image&lt;/code&gt; et
voila, new CD available.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>HP webcam on Linux</title><link href="http://192.168.1.71:8000/2010/08/hp-webcam-on-linux/" rel="alternate"></link><published>2010-08-13T18:18:00+02:00</published><updated>2010-08-13T18:18:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2010-08-13:/2010/08/hp-webcam-on-linux/</id><summary type="html">&lt;p&gt;Okay, getting the HP webcam running on Linux wasn't hard at all. Enable
Video For Linux (CONFIG_VIDEO_DEV) which can be found in the Linux
kernel configuration at Device Drivers, Multimedia Support. Then, select
Video capture adapters and inside that menu, select V4L USB devices and
then USB Video Class (UVC &lt;/p&gt;</summary><content type="html">&lt;p&gt;Okay, getting the HP webcam running on Linux wasn't hard at all. Enable
Video For Linux (CONFIG_VIDEO_DEV) which can be found in the Linux
kernel configuration at Device Drivers, Multimedia Support. Then, select
Video capture adapters and inside that menu, select V4L USB devices and
then USB Video Class (UVC). Once installed, reboot (or load the kernel
module) and ensure that your user is in the video group. You'll then be
able to activate the webcam (for instance, using &lt;strong&gt;mplayer tv://&lt;/strong&gt; or
using skype).&lt;/p&gt;
&lt;p&gt;The integrated microphone is also no problem. It is by default muted, so
using &lt;strong&gt;alsamixer&lt;/strong&gt;, press F4 (to get to the capture menu) and unmute
the channels + enable capturing (press spacebar).&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>New laptop, time to play</title><link href="http://192.168.1.71:8000/2010/08/new-laptop-time-to-play/" rel="alternate"></link><published>2010-08-13T01:33:00+02:00</published><updated>2010-08-13T01:33:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2010-08-13:/2010/08/new-laptop-time-to-play/</id><summary type="html">&lt;p&gt;I gave myself a nice treat and bought a new laptop. After some
consideration, I decided to go with the HP Pavilion DV7 3150EB. Years
ago, I didn't take an HP laptop as the reviews were not that satisfying.
However, it looks as if that is past. So I first &lt;/p&gt;</summary><content type="html">&lt;p&gt;I gave myself a nice treat and bought a new laptop. After some
consideration, I decided to go with the HP Pavilion DV7 3150EB. Years
ago, I didn't take an HP laptop as the reviews were not that satisfying.
However, it looks as if that is past. So I first took a stab at the
&lt;a href="http://www.gentoo.org/doc/en/gentoo-x86+raid+lvm2-quickinstall.xml"&gt;Gentoo Quickinstall for LVM2 and
RAID&lt;/a&gt;
and gave myself a (software) RAID-1 system with everything except / and
/boot using LVM2. To my satisfaction, following the guide was a breeze
and it worked out just fine.&lt;/p&gt;
&lt;p&gt;The real hurdle, that I just won, was to get the wireless up and running
on WPA2. I noticed earlier (before I bought the laptop) that getting the
Broadcom 43255 wifi (Broadcom Corporation Device 4357 (rev 01)) might be
a challenge. Well, the open-source b43 driver didn't detect the wifi
card, but the (closed-source) Broadcom STA driver (as supported by
Broadcom itself) does. To install it on Gentoo, it was as easy as
unmasking &lt;em&gt;broadcom-sta&lt;/em&gt; and installing it. It worked immediately, but
not for WPA/WPA2 networks (and I am not going to put my wireless in
non-WPA2 mode). Luckily, it was easy to discover that it was
&lt;em&gt;wpa_supplicant&lt;/em&gt; itself that was giving the card a hard time as non-WPA
networks worked flawlessly. A quick stab at the &lt;code&gt;wpa_supplicant.conf&lt;/code&gt;
file gave me the final success I needed: &lt;em&gt;ap_scan=2&lt;/em&gt; did the trick.&lt;/p&gt;
&lt;p&gt;Tomorrow: getting the webcam working...&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>Executing, but only when you're home</title><link href="http://192.168.1.71:8000/2010/01/executing-but-only-when-youre-home/" rel="alternate"></link><published>2010-01-18T23:48:00+01:00</published><updated>2010-01-18T23:48:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2010-01-18:/2010/01/executing-but-only-when-youre-home/</id><summary type="html">&lt;p&gt;Sometimes you want to execute a particular command, but only when you're
at home. Examples would be running fetchmail (or fetchnews) through
cron, but you don't want this to run when you're in the train, connected
to the Internet through GPRS...&lt;/p&gt;
&lt;p&gt;My idea here would be to create a script &lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes you want to execute a particular command, but only when you're
at home. Examples would be running fetchmail (or fetchnews) through
cron, but you don't want this to run when you're in the train, connected
to the Internet through GPRS...&lt;/p&gt;
&lt;p&gt;My idea here would be to create a script (say "athome.sh") which returns
0 if you're at home, and 1 otherwise. The key of the script is that the
MAC address of your (default) gateway is unique.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nv"&gt;GW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;/sbin/ip route &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;/default/ {print $3}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;MGW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;/sbin/arp -e &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;GW&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{print $3}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;MGW&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;00:11:22:33:44:55&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;With this script, you can then run &lt;code&gt;athome.sh &amp;amp;&amp;amp; fetchmail&lt;/code&gt;. If you
aren't home, &lt;code&gt;athome.sh&lt;/code&gt; will return 1 and the fetchmail command will
never be executed. When you are, the command returns 0 and fetchmail is
launched.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>Online image gallery</title><link href="http://192.168.1.71:8000/2009/10/online-image-gallery/" rel="alternate"></link><published>2009-10-05T21:48:00+02:00</published><updated>2009-10-05T21:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2009-10-05:/2009/10/online-image-gallery/</id><content type="html">&lt;p&gt;If you're not up to the various free image gallery sites, you might want
to try out &lt;a href="http://www.zenphoto.org/"&gt;ZenPhoto&lt;/a&gt;. Quite powerful, easy to
use and well themeable. Requires PHP / MySQL.&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>hex2passwd, a password generator</title><link href="http://192.168.1.71:8000/2008/09/hex2passwd-a-password-generator/" rel="alternate"></link><published>2008-09-25T19:34:00+02:00</published><updated>2008-09-25T19:34:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2008-09-25:/2008/09/hex2passwd-a-password-generator/</id><summary type="html">&lt;p&gt;I know that repeatable password generators are less secure than random
character generators. After all, if you want a strong password, you can
simply perform &lt;strong&gt;head -c 8 /dev/urandom | mimencode&lt;/strong&gt; to obtain a nice,
random password string.&lt;/p&gt;
&lt;p&gt;However, in certain cases you might want to generate passwords given a &lt;/p&gt;</summary><content type="html">&lt;p&gt;I know that repeatable password generators are less secure than random
character generators. After all, if you want a strong password, you can
simply perform &lt;strong&gt;head -c 8 /dev/urandom | mimencode&lt;/strong&gt; to obtain a nice,
random password string.&lt;/p&gt;
&lt;p&gt;However, in certain cases you might want to generate passwords given a
particular entry which always returns the same password. For instance,
for low-profile web sites. Most people use mneumonics (such as username
reversed and appended with domainname abbreviation to give an example)
but mneumonics can be quite insecure, especially if you use a mneumonic
that, once someone sees one of your passwords, he can deduce all
passwords.&lt;/p&gt;
&lt;p&gt;An example would be the above-given algorithm, which yields for the
following sites:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bugs.gentoo.org, user foobar, password raboofbgo
forums.gentoo.org, user bleh, password helbfgo
www.sourceforge.net, user mynick, password kcinymwsn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I'm sure you can find the password for other sites I would show you, so
this kind of passwords are not that secure.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href="http://dev.gentoo.org/~swift/tools-hex2passwd.html"&gt;hex2passwd&lt;/a&gt;,
a tool which generates (the same) password for the same input over and
over again. With the tool you can make your mneumonic a bit more secure
as it uses hashfunctions to create a pseudorandom sequence and a
character mapping to convert the hash result into a possible password.&lt;/p&gt;
&lt;p&gt;An example for the above sites / mneumonic would yield:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;For bugs.gentoo.org, user foobar
$ echo raboofbgo | sha1sum | hex2passwd -n 8
XqXgOYce
For forums.gentoo.org, user bleh
$ echo helbfgo | sha1sum | hex2passwd -n 8
l8U.tdzg
For www.sourceforge.net, user mynick
$ echo kcinymwsn | sha1sum | hex2passwd -n 8
70z4Bu3k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, the tool offers some more flexibility, such as choosing your
own character maps or scrambling the maps before you use them. In any
case, if you think such a tool is useful for you as well, don't hesitate
to download, compile and install it - it's a simple C program, probably
too ugly to show ;-)&lt;/p&gt;</content><category term="Free Software"></category></entry><entry><title>Playing with gqview</title><link href="http://192.168.1.71:8000/2008/08/playing-with-gqview/" rel="alternate"></link><published>2008-08-18T15:48:00+02:00</published><updated>2008-08-18T15:48:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:192.168.1.71,2008-08-18:/2008/08/playing-with-gqview/</id><summary type="html">&lt;p&gt;Some time ago I received a digital camera; however, due to diskspace
shortage I need to clean up my home directory. One of the directories
that eats most of my sectors is one where I store all my pictures.&lt;/p&gt;
&lt;p&gt;I know I have a lot of duplicate pictures, pictures deduced &lt;/p&gt;</summary><content type="html">&lt;p&gt;Some time ago I received a digital camera; however, due to diskspace
shortage I need to clean up my home directory. One of the directories
that eats most of my sectors is one where I store all my pictures.&lt;/p&gt;
&lt;p&gt;I know I have a lot of duplicate pictures, pictures deduced from master
pictures (lower resolution, some editing) and similar pictures (same
scene taken 4 or 5 times with different camera settings, hoping to get
at least one good shot) but managing them wasn't easy.&lt;/p&gt;
&lt;p&gt;I now played a bit with &lt;a href="http://gqview.sourceforge.net/"&gt;gqview&lt;/a&gt; and
this tool seems to provide some features I find very interesting; one of
them is the "find duplicates" where you can even search for pictures
with "similar" content and I must say that it does work. Of course,
nothing is perfect, but I've managed to clean up the picture directory
so it works for me.&lt;/p&gt;</content><category term="Free Software"></category></entry></feed>