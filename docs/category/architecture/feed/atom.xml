<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Simplicity is a form of art... - Architecture</title><link href="https://blog.siphos.be/" rel="alternate"></link><link href="https://blog.siphos.be/category/architecture/feed/atom.xml" rel="self"></link><id>https://blog.siphos.be/</id><updated>2022-01-13T09:00:00+01:00</updated><entry><title>Ownership and responsibilities for infrastructure services</title><link href="https://blog.siphos.be/2022/01/ownership-and-responsibilities-for-infrastructure-services/" rel="alternate"></link><published>2022-01-13T09:00:00+01:00</published><updated>2022-01-13T09:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2022-01-13:/2022/01/ownership-and-responsibilities-for-infrastructure-services/</id><content type="html">&lt;p&gt;In a perfect world, using infrastructure or technology services would be
seamless, without impact, without risks. It would auto-update, tailor to
the user needs, detect when new features are necessary, adapt, etc. But
while this is undoubtedly what vendors are saying their product delivers,
the truth is way, waaaay different.&lt;/p&gt;
&lt;p&gt;Managing infrastructure services implies that the company or organization
needs to organize itself to deal with all aspects of supporting a service.
What are these aspects? Well, let's go through those that are top-of-mind
for me...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operational support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you have a service running, then you need to ensure operational
support is in place in case there are issues. Be it to resolve a
malfunction, a security issue, or a performance degradation - you need
(human) resources to ensure that the service remains running adequately.&lt;/p&gt;
&lt;p&gt;In many organizations, this is handled in a three- or sometimes 
even four-level support structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;First line&lt;/em&gt; is generally a call center that procedurally validates if
  an issue is service-bound, or if they can assist the user to correctly
  or better use the service. &lt;em&gt;First line&lt;/em&gt; often does not require any
  knowledge of the customer base nor target infrastructure, and is strongly
  procedure-oriented. They do not have operational duties on the services
  themselves, and are an important part to weed out unstructured or invalid
  service requests. They then escalate the issues to &lt;em&gt;second line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Second line&lt;/em&gt; is an organization that has knowledge on the customer
  base and the services themselves. They are also often the last line
  that has a wide view of all services within the company, as subsequent
  support levels are more specialized. &lt;em&gt;Second line&lt;/em&gt; has the ability to
  take actions on the services themselves (like restarting services)
  if this is agreed upon in the past with the main stakeholders, and when
  this is executed in a controlled manner. If &lt;em&gt;second line&lt;/em&gt; isn't able to
  resolve an issue, it moves to &lt;em&gt;third line&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Third line&lt;/em&gt; support is generally the team that is operationally involved
  in the service itself. If the problem lays with a customer portal for
  instance, then &lt;em&gt;third line support&lt;/em&gt; is often the team that maintains the
  customer portal. They know the service and its usage in detail, and are
  in many organizations the last line of support.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some organizations even continue this layering, with a fourth line of
support being a technical expert in a particular component used by the
service. If the customer portal has problems, and it is within the database,
and the DBAs of the customer portal team do not have the knowledge to
resolve it, they might escalate further to a dedicated DBA team (which
is not assigned to any particular business service but specifically
organized to be experts in database and database administration).&lt;/p&gt;
&lt;p&gt;Not all companies have a technology-oriented support team though, and
many companies will consider this as part of 3rd line support as well,
if not just to be more aligned with market terminology on support structures.
Still, organizing and optimizing this third line of support is often
something that infrastructure service support is heavily involved in.&lt;/p&gt;
&lt;p&gt;Regardless of the structure approached by the organization, these teams
will need the knowledge and supporting tools and procedures to do their job.
You need people that can develop support procedures, create simplified
automation (for second line to execute), and continuously update that
information. And if a vendor is involved, then the support line will need to
have knowledge on how to approach the vendor: what are the procedures and
processes for raising incidents, what is the priority queue like? Does
the vendor have certain SLAs that the team should know about?&lt;/p&gt;
&lt;p&gt;The several layers of support will need continuous training, even if it
is just refreshing past information. It is also wise to involve these
support lines in information sharing, like when you know there is a growth
in database reliance in the business services, or when you know many
databases are being migrated from one technology to another. Second line
for instance might be able to use that information, together with their
cross-organizational knowledge, to better triage issues.&lt;/p&gt;
&lt;p&gt;Finally, this support structure is often tied to service level agreements that
the company makes either internally, or with its customers. Hence, the support
must be guaranteed within those SLAs. That implies that the support must be
driven through multiple people, as the organization has to provide support
during off-hours, during holidays, during absence... heck, even during global
pandemics. I use the "3 FTE per technology" principle: you need three people
with enough knowledge to support the technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintenance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As mentioned in the support part, teams exist that are responsible for
the service itself, including patching and updates. This is part of the
maintenance requirement on services, and infrastructure services are not
different. Perhaps even more so than business services, infrastructure
services have a wider impact if they are hit with a bug or with
performance degradation, as many business services rely on the infrastructure
to be up and running, highly available, well-performing, and secure.&lt;/p&gt;
&lt;p&gt;Maintenance tasks for services include, alongside the participation in the
operational support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Executing security- and stability patch validation and roll-out (updates)&lt;/li&gt;
&lt;li&gt;Proactively assessing the state of the service to see if improvements
  or mitigations are needed, before these result in actual issues&lt;/li&gt;
&lt;li&gt;Ensuring sufficient capacity is available for now and in the immediate
  future&lt;/li&gt;
&lt;li&gt;Resolving performance issues, be it by increasing resources, moving
  services to different locations or hosts, fine-tuning configuration
  parameters, offloading workload to different services, etc.&lt;/li&gt;
&lt;li&gt;Executing planned maintenance activities, which can include upgrades
  to higher versions. When the service cannot be transparently upgraded,
  this will involve a thorough alignment with all stakeholders as part of
  the change management processes.&lt;/li&gt;
&lt;li&gt;In case of larger, wide-spanning incidents (or even disasters), the teams
  play an active role in the orchestrated recovery together with all other
  teams.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Designing and architecting the integrations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To ensure that the services are well supported and can be maintained in
an efficient manner, there is often a strong focus on the proper design of
the infrastructure services and their role in the architecture. This
design does not just include pointing out which components exist where,
but also how the service integrates within the larger landscape:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are administration tasks executed? How do administrators authenticate
  themselves?&lt;/li&gt;
&lt;li&gt;Where are the monitoring metrics found and stored? Do you use trend analysis,
  and in which system?&lt;/li&gt;
&lt;li&gt;How are the assets related to the service tracked? What is the lifecycle for
  the individual assets, and how does that impact the maintainability of the
  service?&lt;/li&gt;
&lt;li&gt;What is the best way to use the service, and what will not be allowed?&lt;/li&gt;
&lt;li&gt;How are backups taken, and what does a restore procedure look like? Do you
  support both in-place restores, side restores, etc.&lt;/li&gt;
&lt;li&gt;What underlying resource requirements exist? Do you require certain
  tiered storage, or network performance requirements? If you allow your
  service to be instantiated by others (rather than maintain a platform
  yourself), what are the target resources that you allow? Are there
  minimal resource requirements?&lt;/li&gt;
&lt;li&gt;How do users interact with the service? Do they access it directly,
  or do you require intermediate gateways (like reverse proxies)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, design and architecture go beyond integrations. I focus strongly on
integrations here as it is a part of design and architecture that has strong
dependencies and relations with other teams and technologies. To work out
the integration side of a service, you can't do this autonomously without
assistance from the other service teams. Of course, if those teams have
everything well documented and architected, and easy to consume, then the
team responsible for the design and architecture can do a large part of the
work independently, but having a final validation or confirmation is always
beneficial.&lt;/p&gt;
&lt;p&gt;One of the values of a proper integration design is a higher quality of the
service delivery. If integrations are missed, you'll find that a service 
might not be ready to be activated in production, and then suddenly there
is a stronger focus on timely delivery than on quality. Situations where
firewall rules need to be quickly pushed and opened up because a project
failed to assess their integrations, resulting in security risks, is
sadly enough all too common.&lt;/p&gt;
&lt;p&gt;Larger organizations will often have architects and designers within the teams
or directorates to support this endeavor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secure setup and deployment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given that some integrations might result in heightened risks, and
infrastructure services are often widely used or consumed, the organization
will need to make sure that the services are designed to be secure.&lt;/p&gt;
&lt;p&gt;Security of a service is more than just ensuring it is up-to-date. You
will also need to make sure it is configured correctly (as misconfigurations
are a frequent occurrence of security incidents), that the authentication
and authorizations are properly designed (and where needed or possible,
using multi-factor authentication), that the deployment considers the
placement and interactions (firewalls, zoning, etc.), that the service
provides functional (or perhaps even physical) segregation, that the
data governance is appropriate and aligned with regulatory and company
requirements, that the service is continuously validated by the security
tooling available (patch indications, vulnerability management, ...), etc.&lt;/p&gt;
&lt;p&gt;As services also evolve when they are alive, secure setup and deployment
is not a one-off (but the initial thoughts and designs are not to be
underestimated): the teams will need to assess the impact of new
insights (like security notifications, vulnerabilities, global changes
by the organization, new threats in the wider IT world) which implies
that the team has a continuous security and risk focus.&lt;/p&gt;
&lt;p&gt;In many cases, there is even a coding and development component with
a service. Not many services can be installed and deployed without requiring
any integration scripting or even in-depth coding (such as custom plugins).
And when there is coding, there is the need for secure coding: following
a Secure Development LifeCycle (SDLC) approach to get assurance about
the secure state of the developed code.&lt;/p&gt;
&lt;p&gt;When the deployment uses infrastructure-as-code methods, follows a
GitOps approach, or similar, then there should also be sufficient attention
to the secure setup of these pipelines and the platforms on which they
run. The code (or configurations) hosted should also follow appropriate
security guidelines&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robust and reliable services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Designing for a trustworthy, secure environment is one thing. The other
major focus area for infrastructure services is the availability,
robustness, and resilience of the service. While not all services
require to be up and running 24/7, nowadays it is hard to imagine
many services to still have significant downtimes.&lt;/p&gt;
&lt;p&gt;So, you often need to architecture and design for a resilient service,
which can take a beating if it has to. Organize load- and stress testing.
Organize disruptive testing if needed, and learn from the results to build
a better service. Design for a service that you can do technical maintenance
on without disrupting the customers themselves as much as possible - but
don't overshoot.&lt;/p&gt;
&lt;p&gt;If the service is set up in multiple locations, make sure that there is
independence between these locations (often across different regions) so
that failures in one region do not affect the other regions. Consider a setup
as used in many public cloud environments: high availability across availability
zones in the same region, and regional independence while allowing for
cross-region usage scenarios for the customers.&lt;/p&gt;
&lt;p&gt;Assess what can go wrong, and either try to update the architectures and
designs to be more resilient against these failures, or establish procedures
and processes to quickly recover. A common focus area here is to recover
from disasters (using so-called Disaster Recovery Procedures), and there 
are plenty of disasters to assess: data center failures, large Internet
outages, ransomware or other cyberattacks, worldwide epidemic outbreaks,
etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quality assurance at all stages&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The organization has to be able to provide fixes for defects and problems that
were raised, or to preemptively fix issues before they reach the customers. That
doesn't mean that the organization needs to be able to develop the fixes itself:
especially with off-the-shelf products the development is done by the
independent software vendor (ISV). However, the organization does have the
responsibility to track and put their weight on this so that the issues are
indeed properly resolved (and in case of a third-party product, preferably
through a fix that is applied to the main product, and not a one-off for that
particular company). Of course, if the service is developed in-house, then the
development of the fixes has to be guaranteed by the organization as well.&lt;/p&gt;
&lt;p&gt;To be able to provide secure and reliable services, it is vital to have
good change management processes and tools in place so that you can
approach the various stages of quality assurance before reaching production.
In &lt;a href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/"&gt;The pleasures of having
DTAP&lt;/a&gt; I mention
the benefits of having four environments for the various stages of a development
lifecycle (development, testing, acceptance, and production) and that is
perfectly applicable to infrastructure services as well, even when the
environments for infrastructure services might be isolated from those of the
more business-oriented development stages: you want to make sure that the
business-oriented development has production-grade services for its processes,
and not the intermediate and possibly less reliable in-development infrastructure
services.&lt;/p&gt;
&lt;p&gt;Throughout these environments, testing can (should) be introduced to provide
guidance to the engineers and developers to improve on the service. Testing can
take several forms: unit testing within the frameworks, integration testing of
code in larger environments, integration testing of new products in those
environments, sanity testing with simple use cases to ensure nothing blows
up (figuratively... hopefully), regression tests to ensure no fixed defects
creep back in, acceptance tests for specific features or use cases, load
testing to ensure the product stays up under the expected loads (and the
individual components or services have the right performance profile), stress testing
to ensure the product is resilient against higher loads or bursts, destructive
tests to see the product behaves as expected when unauthorized usage is
performed, security testing and penetration testing to provide assurance
on the secure state of the product deployment, etc.&lt;/p&gt;
&lt;p&gt;In many cases, the QA part is driven by organizational requirements, and not all
products require the same intensity on the tests and assurance levels. But, by
introducing the right automation, it is far easier to include multiple test
scenarios and test types in your pipeline.&lt;/p&gt;
&lt;p&gt;Products then pass through the various stages according to the change
management principles and processes that are in effect in the organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Strategy and roadmap&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The team responsible for an infrastructure service will also need to consider
the service in the long term: is the current technology (or set of products)
still state-of-the-art, mature, and following market practices? Or does the team
consider the technology to be relatively outdated and in need of an update? When
would the right time be to address this update?&lt;/p&gt;
&lt;p&gt;Perhaps the currently used technology is nearing its end-of-sale, end-of-support
(EOS), or even end-of-life (EOL). In that case, the team has to be ready to
address these lifecycle stages accordingly, be it through migrations, or
refactoring of current usages. Perhaps the teams find that it is more sensible
to get an extended support contract in place, or that they have the ability to
take the support (including code development) completely internally. Whatever
the choice is, it has to be made clear for the wider organization, and the
support team has to be ready to take on its commitments.&lt;/p&gt;
&lt;p&gt;There is a distinction between the service (or better yet, "capability") that is
offered, versus the products and technologies that support and realize it.
Capabilities will be needed by the organization for a long, long time, whereas
the products that realize these capabilities can have much shorter timeframes.&lt;/p&gt;
&lt;p&gt;The team will need to consider alternatives for the products, and see when those
alternatives make sense to address and implement. Perhaps the organization might
benefit from multiple implementations using different technologies because they
serve different usage scenarios, and the costs of keeping multiple technologies
in the air is less than the value it provides. Or perhaps an alternative needs
to be quickly realizable in case of a sudden exit scenario of the current
technology (or vendor). In that case, while the alternative doesn't need to be
up and running, the team must be able to address the change in due time.&lt;/p&gt;
&lt;p&gt;The need to have a proper roadmap on the capability and products that are being
used also reflects in the relationship that that team has with the vendor. For
strategically important products, an organization might even want to participate
in that vendor's Customer Advisory Board (CAB) or equivalent programs. The
team should have the time and resources to collaborate with that vendor to build
a partnership, participate in the conferences and other events, as that provides
input to the team on the progress and future of that product. Those insights are
primordial to properly design and organize an internal roadmap.&lt;/p&gt;
&lt;p&gt;Once the internal roadmap has taken shape, then the responsible teams are
involved in supporting the organization through updates and upgrades, by
communicating the need for these upfront (so that the internal customers can
plan around it), and to track the progress so there are no lingering risks for
the wide organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More elaborate (internal) customer support&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The support for the infrastructure service often has to extend beyond the
operational support that I started with in this article. Teams that provide
certain capabilities within the organization don't do this as a vendor, but as
part of that organization. So when an internal customer needs help in migrating
away from a service, the team is still involved in this process.&lt;/p&gt;
&lt;p&gt;Hence, more elaborate support such as migrations (both towards a new technology,
service or capability, as well as migration away from it), finding
suitable alternatives that are not properly handled by the capability (and thus
might be best provided by a different team), ... helps in building out the trust
that the wider organization has in IT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cost, licenses, and contractual obligations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many services have certain contractual obligations associated with them, often
known as the Terms and Conditions of the contract and product usage.
Infrastructure teams will need to make sure that these T&amp;amp;Cs are known and that
the organization adheres to them.&lt;/p&gt;
&lt;p&gt;The cost of an infrastructure service usage also needs to be correctly devised
and accounted for. Teams have to make sure the product usage remains within the
allocated licenses (or, if there is no capping in place, that the usage is
sufficiently constrained that the organization does not get any surprises), and
is often involved in defining a chargeback towards the rest of the organization.&lt;/p&gt;
&lt;p&gt;I tend to make a distinction between showback (show the organization how much a
service costs to the company or usage group) and chargeback (a governance-driven
or tax-driven requirement for charging usage to the organization), as the latter
is more a company decision on how to approach this, whereas the showback is the
actual, factual cost. Showback is needed to support conscious decisions on next
steps or consumption patterns, whereas chargeback might be necessary for
tax reasons in larger corporations where IT is considered as part of a different
legal entity.&lt;/p&gt;
&lt;p&gt;Addressing cost, licenses, and T&amp;amp;Cs is not to be underestimated. Many vendors
make this very difficult, as that allows for many interpretations during license
audits that can give a nice bonus to the vendor if he can show that his
interpretation is more appropriate than how you thought that the contract or
license was structured.&lt;/p&gt;
&lt;p&gt;The cost of a product or technology is not just the cost of the purchase, and
even that cost is still somewhat variable: many things are open for negotiation,
and you also don't need to tackle this directly with the vendor, as there is a
large market of middle parties that facilitate purchasing technologies. These
can often provide better rates as they can bundle purchases of multiple
customers and thus negotiate better deals with the main vendor.&lt;/p&gt;
&lt;p&gt;The cost also depends on the support contract associated with it, as well as
the costs of other depending technologies (such as capacity requirements) that come
from its implementation. This is often neglected in SaaS purchases: even though
you have correctly negotiated a good price for the SaaS service, you might be
jeopardizing your internet connectivity and need to upgrade the bandwidth, the
anti-DDoS service, your firewall capabilities, etc. because the consumption of
that SaaS service is significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The responsibilities for managing and tracking infrastructure services are large
and not to be underestimated. It is not a matter of deploying a new service and
assuming everybody can deal with it, nor are all responsibilities equally
visible to the end-user.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1481535279861280769"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;
</content><category term="Architecture"></category><category term="RACI"></category><category term="responsibilities"></category></entry><entry><title>The pleasures of having DTAP</title><link href="https://blog.siphos.be/2021/12/the-pleasures-of-having-DTAP/" rel="alternate"></link><published>2021-12-30T12:00:00+01:00</published><updated>2021-12-30T12:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-12-30:/2021/12/the-pleasures-of-having-DTAP/</id><summary type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;No, not Diphtheria, Tetanus, and Pertussis (vaccine), but &lt;em&gt;Development,
Test, Acceptance, and Production (DTAP)&lt;/em&gt;: different environments that,
together with a well-working release management process, provide a way to
get higher quality and reduced risks in production. DTAP is an important
cornerstone for a larger infrastructure architecture as it provides
environments that are tailored to the needs of many stakeholders.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What are these four environments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's go over the four environments one by one with a small introduction
to their purpose. I'll cover more specific use cases further down in this
post.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Development&lt;/em&gt; environment is a functionally complete environment on
which the development of products or code is done. It should have the same
technologies in place and very similar setups (deployments) so that
developers are not facing a too different environment. Too much difference
might imply different behavior, which is contra-productive. The environment
is accessible by developers and testers, and is mainly development-oriented.&lt;/p&gt;
&lt;p&gt;Products or code that are being developed will be visible and used in this
environment. Developers and engineers hardly have any threshold to reach for
deploying or modifying code here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Testing&lt;/em&gt; environment is used when the development has reached a phase
where the product or code has passed a minimum of quality. Unit tests
succeed, the code builds fine, and the developer has indicated that the
code or product is ready for wider testing (hence the name).&lt;/p&gt;
&lt;p&gt;A testing environment generally applies a multitude of tests, most of
them (hopefully) automated, but with a strong dependency on testers
(also known as Quality Assurance engineers or QA engineers) to find issues.&lt;/p&gt;
&lt;p&gt;The automated tests focus on integrations, regression testing, security
testing, etc., and provide insights into the new builds or products which the
development team can take up and iterate over to improve the product.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Acceptance&lt;/em&gt; environment is a production-like environment, not
just for one product, but for the entire business or business unit: the same
setup, the same infrastructure, the same foundations, the same application
portfolio, the same integrations, etc. This environment intends to 
validate that the product is fully ready to be released. It is often
also abbreviated as the &lt;em&gt;User Acceptance Testing (UAT)&lt;/em&gt; environment.&lt;/p&gt;
&lt;p&gt;While the testing environment is generally approached by QA engineers, the
acceptance environment should be more tailored to business testers. When
developers or engineers introduce a feature, the stakeholders that
requested that feature use the acceptance environment to accept if the
coming release fulfills their request or not.&lt;/p&gt;
&lt;p&gt;As the environment is production-like in its entirety (and not just for
a single product), it is a prime target for executing performance tests
as well. Cross-product dependencies and processes (like migrations) are
validated here too.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Production&lt;/em&gt; environment is the environment in which the product or
code "goes live", where the customers use it. That does not mean that
a product put in production is immediately accessible for the customers:
there is still a difference between deployment (bring to production),
activation (enable usage), and release (use by customers).&lt;/p&gt;
&lt;p&gt;While DTAP is well-known in larger organizations, there are some challenges
or misconceptions that I would like to point out, and which I discuss
further down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environments are more than just the systems where the code is deployed to.
  Each environment has production services associated with it.&lt;/li&gt;
&lt;li&gt;A prime challenge to implementing DTAP is the cost associated with it. But
  it does not need to be as expensive as you think, and DTAP implementations
  often have a positive business case.&lt;/li&gt;
&lt;li&gt;Agile methodologists might find DTAP to be old-style. They are correct
  that many implementations are prohibitive towards fast deployment and
  release strategies, but that isn't because DTAP is conceptually wrong.&lt;/li&gt;
&lt;li&gt;Not all environments need the same data. On the contrary, a proper DTAP
  design likely uses separate datasets in each environment to deal with 
  the security and regulatory requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conceptual environments still require production services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The purpose of each environment is strongly tied to the 'phase' in which
the code or the product resides in the development lifecycle. That means
that these environments have a strong focus on that product or code, and
not on the services that are needed.&lt;/p&gt;
&lt;p&gt;Indeed, a development environment also entails services that are already
'in production', like a usable workstation, development services like
code repositories and build systems, ticketing services, and more. A testing
environment requires test automation engines, regression test frameworks,
security tools, and more. All these services are production-ready - and they
often have their own DTAP environments as well.&lt;/p&gt;
&lt;p&gt;It is a common misconception that a development-oriented system or service
has a lower SLA or lower risk profile than production, and infrastructure
architecture should make clear that there is a difference between the
systems that host the products or code under review and the systems that
facilitate the functionality needed within the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Static cost is a major inhibitor for implementing DTAP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaller companies or organizations might be hesitant to introduce DTAP
environments as it might be cost-prohibitive. While it is true
that, from a 'static' view, a DTAP environment costs more than a
production-only environment, you need to consider the impact of implementing
DTAP in the processes.&lt;/p&gt;
&lt;p&gt;The main purpose of DTAP is not to make your CFO angry, but to improve the
quality of your production environment (and usage). Ask yourself: how costly
is it when your production systems go down, or when your customers complain
and you need to fix things... Do you update code directly on the server(s)?
What if a security patch is rolled out and suddenly prevents your
customer-facing application from working?&lt;/p&gt;
&lt;p&gt;While DTAP mentions four environments, some companies settle with three, and
others with five or more. Perhaps your testing and acceptance are done by the
same people, and you do not have many automated testing facilities at hand.
Splitting your pre-production environments into multiple environments doesn't
make sense yet, and you might first want to focus on improving your testing
maturity in general.&lt;/p&gt;
&lt;p&gt;If you want to make the case for DTAP, consider the use cases or scenarios that
have visibly disrupted your business and how/where the environments would have
helped. In many cases, you'll notice that there is a positive business case
for a step-wise move towards DTAP.&lt;/p&gt;
&lt;p&gt;Furthermore, a proper design of these environments will facilitate an
economical view towards DTAP. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can use pay-as-you-use environments (as is commonly the case in the 
  public cloud) which you only activate when you do your testing. If you have
  a 24/7 customer-facing service in production, that doesn't mean that your
  acceptance environment has to be 24/7. Yes, it should be as much
  production-like as possible, but if it isn't doing anything outside
  business hours, then you don't need it running outside business hours.&lt;/li&gt;
&lt;li&gt;Commercial products often have distinct terms and conditions for
  non-production usage. You can have databases in production with a
  premium, gold service level agreement, while having the same database in
  the other environments with a low, bronze service level agreement
  towards that vendor: cheaper, but technically the same.&lt;/li&gt;
&lt;li&gt;Abstraction and virtualization technologies allow for better control
  of the resources that are being used. For instance, you can have
  an acceptance environment that is only at 20% of the resources
  of production for day-to-day validation, and then increase its resources
  to 100% for load testing periods. If these environments are not in a
  pay-as-you-use model, shifting resources from one environment to
  another allows for controlling the costs.&lt;/li&gt;
&lt;li&gt;Security controls in these environments might be different as well,
  assuming that these environments have different data needs: if you
  use fictitious data in development and testing, and anonymized data
  in acceptance, then the investments on, say, data leakage controls
  might be different for these environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, while DTAP is at first glance a costly approach, the actual case is
that it is positive for the company.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DTAP is not inefficient, but some implementations are&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a world where "Release fast, release often" is the norm, having a rigid
DTAP strategy might be contra-productive. However, this is more an
implementation concern than a conceptual one. If I consider the downsides
that &lt;a href="https://medium.com/the-liberators/want-to-be-agile-drop-your-dtap-pipeline-7dcb496fe9e3"&gt;Christiaan Verwijs&lt;/a&gt;
mentions in his post, I feel that I can safely state that this is not due
to the lifecycle of the code or product, but rather the choices that a
company has made while assessing and implementing a release strategy.&lt;/p&gt;
&lt;p&gt;There is no need to bundle and make bulk releases with DTAP. You can
perfectly design an environment where DevOps teams can release easily
to production. More so, the bulk release strategy is frequently the result
of an application design constraint, not a deployment constraint.&lt;/p&gt;
&lt;p&gt;Development methodologies and DTAP environments do need to be tailored
to each other. The purpose of DTAP is to facilitate the quality of products
and code, and thus should be tailored towards efficient and qualitative
development processes. In many environments, DTAP is synonymous with
"infrastructure operations" and that's a wrong approach. 
Operations-oriented teams (like &lt;em&gt;Site Reliability Engineers (SREs)&lt;/em&gt;),
development-oriented teams, and DevOps teams all have the same benefits from
DTAP.&lt;/p&gt;
&lt;p&gt;Some might state that an acceptance environment is no longer suitable in
the modern age, as they can deploy products and code to production without
losing the benefits of the acceptance environment. With blue/green
deployments or canary releases, you can enable business testers and
stakeholders to validate new features or code before releasing it
to the wider public.&lt;/p&gt;
&lt;p&gt;To accomplish this properly, however, the platform that is used will
balance resources accordingly, and you're conceptually implementing an
(albeit temporary) acceptance environment in an automated way. This is
an implementation choice and has to be balanced against the requirements
that the organization has.&lt;/p&gt;
&lt;p&gt;For instance, if you work with sensitive data, you might not be allowed
to use this data during testing. In Europe, the &lt;em&gt;General Data Protection
Regulation (GDPR)&lt;/em&gt; is a strong regulatory requirement for dealing with
sensitive data. It isn't a playbook though: companies need to evaluate
how and where data is used, and perhaps the balance made by the company
allows, if not with explicit consent, to use data unmodified for
acceptance testing. But if that isn't the case and your acceptance tests
need to use sanitized data, then having separate environments is likely
more sensible (although different implementations exist that allow
for anonymization in production as well - they're, however, not as easy
to implement).&lt;/p&gt;
&lt;p&gt;Plus, DTAP does not imply that production is doing everything in a single
unit of work: Deploy, Activate and Release. You can still perfectly position
those tasks in production while having an explicit acceptance environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Separate datasets in each environment make sense&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For regulated companies and organizations, security officers might want
to use the DTAP distinction to focus on data minimization strategies as well.
As mentioned before, the GDPR is a strong regulatory requirement whose
alignment can be facilitated by a well-designed DTAP approach.&lt;/p&gt;
&lt;p&gt;You can use fictitious data in development and testing, with development
using datasets that developers use for validating the specific functionality
they are working on (and preferably share and put alongside the code
and products), whereas testing uses a coherent but still fictitious
dataset. I use "coherent" here as an indication that the data should
be functionally correct and integer: a (fictitious) person record in the
customer database in the testing environment should be mapped to the
(fictitious) calls or other interactions that are stored in the support
database (also in the testing environment) and the (fictitious) portfolio
that this (fictitious) person has in the product database (in the testing
environment).&lt;/p&gt;
&lt;p&gt;Don't underestimate how powerful, but also how challenging a good fictitious
dataset is.&lt;/p&gt;
&lt;p&gt;For acceptance testing, perhaps the company decided that anonymized data
is to be used. Or it uses pseudonymized data (which is a weaker form)
with additional technical controls to prevent leakage and attacks (including
inference) that try to deduce the origin of the data.&lt;/p&gt;
&lt;p&gt;Again, these are choices by the company or organization, which need to be
taken with the risk and business stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DTAP is a sensible approach for improving quality in products and code.
While it isn't the holy grail for quality assurance, it has solid
foundations that many larger companies and organizations feel comfortable
with. The implementation details make or break how well-adjusted the DTAP
approach is for modern development processes and regulatory requirements.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1476505537047109635"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="DTAP"></category><category term="environments"></category><category term="zoning"></category><category term="development"></category><category term="test"></category><category term="acceptance"></category><category term="production"></category></entry><entry><title>Creating an enterprise open source policy</title><link href="https://blog.siphos.be/2021/11/creating-an-enterprise-open-source-policy/" rel="alternate"></link><published>2021-11-20T15:00:00+01:00</published><updated>2021-11-20T15:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-20:/2021/11/creating-an-enterprise-open-source-policy/</id><summary type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Nowadays it is impossible to ignore, or even prevent open source from being
active within the enterprise world. Even if a company only wants to use
commercially backed solutions, many - if not most - of these are built with, and
are using open source software.&lt;/p&gt;
&lt;p&gt;However, open source is more than just a code sourcing possibility. By having a
good statement within the company on how it wants to deal with open source, what
it wants to support, etc. engineers and developers can have a better
understanding of what they can do to support their business further.&lt;/p&gt;
&lt;p&gt;In many cases, companies will draft up an &lt;em&gt;open source policy&lt;/em&gt;, and in this post
I want to share some practices I've learned on how to draft such a policy.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Assess the current situation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When drafting a policy, make sure you know what the current situation already
is. Especially when the policy might be very restrictive, you might be facing a
huge backlash from the organization if the policy is not reflecting the reality.
If that is the case, and the policy still needs to go through, proper
communication and grooming will be needed (and of course, the "upper management
hammer" can help out as well).&lt;/p&gt;
&lt;p&gt;Often, higher management is not aware of the current situation either. They
might think that open source is hardly in use. Presenting them with facts and
figures not only makes it more understandable, it will also support the need for
a decent open source policy.&lt;/p&gt;
&lt;p&gt;When you have a good view on the current usage, you can use that to track where
you want to go to. For instance, if your company wants to adopt open source more
actively, and pursue open source contributions, you might want to report on the
currently detected contributions, and use that for follow-up later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get HR and compliance involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before you embark on the journey of developing a decent open source policy, make
sure you have management support on this, as well as people from HR and your
compliance department (unless your policy will be extremely restrictive, but
let's hope that is not the case).&lt;/p&gt;
&lt;p&gt;You will need (legal &amp;amp;) compliance involved in order to draft and assess the
impact of internal developers and engineers working on open source projects, as
well as the same people working on open source projects in their free time. Both
are different use cases but have to be assessed regardless.&lt;/p&gt;
&lt;p&gt;HR is generally involved at a later stage, so they know how the company wants to
deal with open source development. This could be useful for recruitment, but
also for HR to understand what the policy is about in case of issues.&lt;/p&gt;
&lt;p&gt;An important consideration to assess is how the company, and the contractual
obligations that the employees have, deals with intellectual property. In some
companies, the contract allows for the employees to retain the intellectual
property rights for their creations outside of company projects. However, that
is not always the case, and in certain sectors intellectual property might be
assumed to be owned by the company whenever the creation is something in which
the company is active. And that might be considered very broadly (such as
anything IT related for employees of an IT company).&lt;/p&gt;
&lt;p&gt;The open source policy that you develop should know what the contractual
stipulations say, and clarify for engineers and developers how the company
considers the intellectual property ownership. This is important, as it defines
who can decide to contribute something to open source.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Understand and simplify license requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many of the decisions that the open source policy has to clarify will be related
to the open source licenses in use. Moreover, it might even be relevant to
define what open source is to begin with.&lt;/p&gt;
&lt;p&gt;A good source to use is the &lt;a href="https://opensource.org/osd"&gt;Open Source Definition&lt;/a&gt;
as published and maintained by the &lt;a href="https://opensource.org/"&gt;Open Source Initiative
(OSI)&lt;/a&gt;. Another definition is the one by the &lt;a href="https://www.fsf.org/"&gt;Free
Software Foundation&lt;/a&gt; titled "&lt;a href="https://www.fsf.org/about/what-is-free-software"&gt;What is free software and
why is it so important for society&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;The license is the agreement that the owner of the software puts out that
declares how users can use that software. Most, if not all software that a
company uses, will have a license - open source or not. But most commercial
software titles have specific licenses that you need to go through for each
specific product, as the licenses are not reused. In the open source world,
licenses are reused so that end users do not need to go through product-specific
terms.&lt;/p&gt;
&lt;p&gt;The OSI organization has a list of &lt;a href="https://opensource.org/licenses"&gt;approved
licenses&lt;/a&gt;. However, even amongst these
licenses, you will find different types of licenses out there. While they are
commonly grouped into &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;copyleft&lt;/a&gt; and
&lt;a href="https://fossa.com/blog/all-about-permissive-licenses/"&gt;permissive&lt;/a&gt; open source
licenses, there are two main categories within the copyleft licenses that you
need to understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strong copyleft licenses that require making all source code available upon
  distribution, or sometimes even disclosure of the application base&lt;/li&gt;
&lt;li&gt;"scoped" copyleft licenses that require making only the source code available of the
  modules or libraries that use the open source license (especially if you
  modified them) without impacting the entire application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the term "strong copyleft" is something that I think is somewhat generally
accepted (such as in the Snyk article "&lt;a href="https://snyk.io/learn/open-source-licenses/"&gt;Open Source Licenses: Types and
Comparison&lt;/a&gt;" or in &lt;a href="https://en.wikipedia.org/wiki/Copyleft"&gt;Wikipedia's
article&lt;/a&gt;), I do not like to use its
opposite "weak" term, as the licenses themselves do not reduce the open source
identity from the code. Instead, they make sure the scope of the license is
towards a particular base (such as a library) and not the complete application
that uses the license.&lt;/p&gt;
&lt;p&gt;Hence, open source policies might want to focus on those three license types for
each of the use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;permissive licenses, like Apache License 2.0 or MIT&lt;/li&gt;
&lt;li&gt;scoped copyleft licenses, like LGPL or EPL-2.0&lt;/li&gt;
&lt;li&gt;strong copyleft licenses, like GPL or AGPL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Differentiate on the different open source use cases&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several use cases that the policy will need to tackle. These are, in
no particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using off-the-shelf, ready-to-use open source products&lt;/li&gt;
&lt;li&gt;Using off-the-shelf libraries and modules for development&lt;/li&gt;
&lt;li&gt;Using open source code&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for company purposes&lt;/li&gt;
&lt;li&gt;Contributing to open source projects for personal/private purposes&lt;/li&gt;
&lt;li&gt;Launching and maintaining open source projects from the company&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these use cases might have their specific focuses. Combine that with the
license categories listed earlier, and you can start assessing how to deal with
these situations.&lt;/p&gt;
&lt;p&gt;For instance, you might want to have a policy that generally boils down to the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When using off-the-shelf, ready-to-use open source products, all types of
  products are allowed, assuming the organization remains able to support the
  technologies adopted. Furthermore, the products have to be known by the
  inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using libraries or modules in development projects, only open source
  products with permissive or scoped copyleft licenses can be used. Furthermore,
  the libraries or modules have to be well managed (kept up-to-date) and known
  by the inventory and asset tooling used by the company.&lt;/li&gt;
&lt;li&gt;When using open source code, only code that is published with a permissive
  license can be used. At all times, a reference towards the original author
  has to be retained.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for company purposes, approval has
  to be given by the hierarchical manager of the team. Contributions have to be
  tagged appropriately as originating from the company (e.g. using the company
  e-mail address as author). Furthermore, employees are not allowed to
  contribute code or intellectual property that is deemed a competitive
  advantage for the company.&lt;/li&gt;
&lt;li&gt;When contributing to open source projects for personal/private purposes,
  employees are prohibited to use code from the company or to do contributions
  using their company's e-mail address. However, the company does not claim
  ownership on the contributions an employee does outside the company's projects
  and hours.&lt;/li&gt;
&lt;li&gt;When creating new projects or publishing internal projects as open source,
  sufficient support for the project has to be granted from the company, and the
  publications are preferentially done within the same development services
  (like version control) under management of the company. This ensures
  consistency and control over the company's assets and liability. Projects have
  to use a permissive license (and perhaps even a single, particular license).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, if the company actively pursues an open source first strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Off-the-shelf, ready-to-use open source products are preferred over
  propriatary products. Internal support teams must be able to deal with
  general maintenance and updates. The use of commercially backed products is
  not mandatory, but might help when there is a need for acquiring short-term
  support (such as through independent consultants).&lt;/li&gt;
&lt;li&gt;Development projects must use projects that use permissive or scoped copyleft
  licenses for the libraries and dependencies of that project. Only when the
  development project itself uses a strong copyleft license are dependencies
  with (the same) strong copyleft license allowed. Approval to use a strong
  copyleft license is left to the management board.&lt;/li&gt;
&lt;li&gt;Engineers and developers retain full intellectual property rights to their
  contributions. However, a Contributor License Agreement (CLA) is used to grant
  the company the rights to use and distribute the contributions under the
  license mentioned, as well as initiate or participate in legal actions related
  to the contributed code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Clarify what is allowed to be contributed and what not&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above example I already indicated a "do not contribute code that is
deemed a competitive advantage" statement. While it would be common sense,
companies will need to clarify this (if they follow this principle) in their
policies so they are not liable for problems later on.&lt;/p&gt;
&lt;p&gt;A competitive advantage primarily focuses on a company's crown jewels, but can
be extended with code or other intellectual property (like architectural
information, documentation, etc.) that refers to indirect advantageous
solutions. If a company is a strong data-driven company that gains massive
insights from data, it might refuse to share its artificial intelligence related
code.&lt;/p&gt;
&lt;p&gt;There are other principles that might decide if code is contributed or not. For
instance, the company might only want to contribute code that has received all
the checks and controls to ensure it is secure, it is effective and efficient,
and is understandable and well-written. After all, when such contributions are
made in name of the company, the quality of that code reflects upon the company
as well.&lt;/p&gt;
&lt;p&gt;I greatly suggest to include examples in the open source policy to clarify or
support certain statements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assess the maturity of an open source product&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When supporting the use of open source products, the policy will also have to
decide which open source products can be used and which ones can't. Now, it is
it possible to create an exhaustive list (as that would defeat the purpose of an
open source policy). Instead, I recommend to clarify how stakeholders can assess
if an open source product can be used or not.&lt;/p&gt;
&lt;p&gt;Personally, I consider this from a "maturity" point of view. Open source
products that are mature are less likely to become a liability within a larger
company, whereas products that only have a single maintained (like my own
&lt;a href="https://github.com/sjvermeu/cvechecker"&gt;cvechecker&lt;/a&gt; project) are not to be used
without understanding the consequences.&lt;/p&gt;
&lt;p&gt;So, what is a mature open source project? There are online resources that could
help you out (like the Qualipso-originated &lt;a href="https://en.wikipedia.org/wiki/OpenSource_Maturity_Model"&gt;Open Source Maturity Model
(OSMM)&lt;/a&gt;), but
personally I tend to look at the following principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The project has an active development, with more than 5 active contributors in
  the last three months.&lt;/li&gt;
&lt;li&gt;The project is visibly used by several other projects or products.&lt;/li&gt;
&lt;li&gt;The project has well-maintained documentation, both for developers and for
  users. This can very well be a decent wiki site.&lt;/li&gt;
&lt;li&gt;The project has an active support community, with not only an issue system,
  but also interactive services like forums, IRC, Slack, Discord, etc.&lt;/li&gt;
&lt;li&gt;The project supports more than one major version in parallel, and has a clear
  lifecycle for its support (such as "major version is supported up to at least
  1 year after the next major version is released").&lt;/li&gt;
&lt;li&gt;The project publishes its artefacts in a controlled and secure manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A policy is just the beginning, not the end&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As always, there will be situations where a company wants to allow a one-off
case to deviate from the policy. Hence, make clear how deviations can be
targeted.&lt;/p&gt;
&lt;p&gt;For instance, you might want to position an architecture review board to support
deviations from the license usage. When you do, make sure that this governance
body knows how to deal with such deviations - understanding what licenses are,
what the impact might be towards the organization, etc.&lt;/p&gt;
&lt;p&gt;Furthermore, once the policy is ready to be made available, make sure you have
support for that policy in the organization, as well as supporting tools and
processes.&lt;/p&gt;
&lt;p&gt;You might want to include an internal community to support open source/free
software endeavors. This community can help other stakeholders with the
assessment of a product's maturity, or with the license identification.&lt;/p&gt;
&lt;p&gt;You might want to make sure you can track license usage in projects and
deployments. For software development projects, there are plenty of commercial
and free services that scan and present license usage (and other details) for a
project. Inventory and asset management utilities often also include
identification of detected software. Validate that you can report on open source
usage if the demand comes up, and that you can support development and
engineering teams in ensuring open source usage is in line with the company's
expectations.&lt;/p&gt;
&lt;p&gt;The company might want to dedicate resources in additional leakage detection and
prevention measures for the open source contributions. While the company might
already have code scanning techniques in place in their on-premise version
control system, it might be interesting to extend this service to the public
services (like &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt; and
&lt;a href="https://about.gitlab.com/"&gt;GitLab&lt;/a&gt;). And with that, I do not want to imply
using the same tools and integrations, but more on a functional level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finishing off&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few companies, and most governmental organizations, publish their open source
policies online. The &lt;a href="https://todogroup.org/"&gt;TODO Group&lt;/a&gt; has graceously drafted
a &lt;a href="https://github.com/todogroup/policies"&gt;list of examples and templates&lt;/a&gt; to
use. They might be a good resource to use when drafting up your own.&lt;/p&gt;
&lt;p&gt;Having a clear and understandable open source policy simplifies discussions, and
with the appropriate support within the organization it might jumpstart
initiatives even further. Assuming the policy is sufficiently supportive of open
source, having it published might eliminate the fear of engineers and developers
to suggest certain open source projects.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1462043477835976705"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="opensource"></category><category term="enterprise"></category><category term="legal"></category><category term="compliance"></category></entry><entry><title>Hybrid cloud can be very complex</title><link href="https://blog.siphos.be/2021/11/hybrid-cloud-can-be-very-complex/" rel="alternate"></link><published>2021-11-08T20:00:00+01:00</published><updated>2021-11-08T20:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-11-08:/2021/11/hybrid-cloud-can-be-very-complex/</id><summary type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I am not an advocate for hybrid cloud architectures. Or at least, not the
definition for hybrid cloud that assumes one (cloud or on premise) environment
is just an extension of another (cloud or on premise) environment. While such
architectures seem to be simple and fruitful - you can easily add some capacity
in the other environment to handle burst load - they are a complex beast to
tame.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Hybrid cloud complexity starts with the definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing that I've already learned is not to use "hybrid cloud" without
defining what I mean. And if somebody else uses it (or a research article), I
will frantically try to get a definition on what the person or article implies
with the term.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://azure.microsoft.com/en-us/overview/what-is-hybrid-cloud-computing/"&gt;Microsoft&lt;/a&gt;
for instance defines hybrid cloud as "a computing environment that combines an
on-premises datacenter with a public cloud, allowing data and applications to
be shared between them."&lt;/p&gt;
&lt;p&gt;This definition isn't unambiguous. What does Microsoft mean with "sharing"? If
I expose an application and/or its data through APIs that are shielded and
independently managed in each environment, and allow for interaction between
those APIs, does that still entail a hybrid cloud architecture? Because if that
is the case, then I want to know what other cloud interaction architectures
Microsoft thinks exist besides hybrid cloud.&lt;/p&gt;
&lt;p&gt;I do think that the intention of Microsoft's definition is that the cloud
hosting environments are considered as "similar enough" to the on premise
environment, and managed in the same way (some cloud specifics
notwithstanding), as inspired by their claim that hybrid cloud allow
"businesses [to] use the cloud to instantly scale capacity up or down to handle
excess capacity" and that organizations using hybrid cloud architectures "are
able to use many of the same security mreasures that they use in their existing
on-premises infrastructure".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gartner.com/en/information-technology/glossary/hybrid-cloud-computing"&gt;Gartner&lt;/a&gt;
defines hybrid cloud computing as "policy-based and coordinated service
provisioning, use and management across a mixture of internal and external
cloud services." That doesn't narrow things down, and in &lt;a href="https://www.gartner.com/document/3956442"&gt;(paywalled) research
articles&lt;/a&gt;, Gartner does express that
"hybrid cloud is a vague term that does not allow enough granularity for
implementation planning by cloud and infrastructure professionals".&lt;/p&gt;
&lt;p&gt;&lt;a href="https://go.forrester.com/blogs/13-08-02-cloud_management_in_a_hybrid_cloud_world/"&gt;Forrester&lt;/a&gt;
follows a definition that is very generic, as "a cloud service connected to any
other corporate resource" makes it a hybrid cloud service. Regardless of how it
is infrastructurally or application-wise integrated.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ibm.com/cloud/learn/hybrid-cloud"&gt;IBM&lt;/a&gt; declares hybrid cloud as
"integrates public cloud services, private cloud services and on-premises
infrastructure and provides orchestration, management and application
portability across all three. The result is a single, unified and flexible
distributed computing environment [...]"&lt;/p&gt;
&lt;p&gt;This definition does seem to imply an architecture that considers all hosting
environments as infrastructurally equal (as they see the sum of all
environments as a single, unified environment).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud"&gt;Wikipedia&lt;/a&gt;
mentions that hybrid cloud "is a composition of a public cloud and a private
environment [...] that remain distinct entities but are bound together,
offering the benefits of multiple deployment models." Such a definition leaves
the implementation details open, as it boils down to any architecture where you
mix such hosting environments.&lt;/p&gt;
&lt;p&gt;For me, a &lt;strong&gt;hybrid cloud on infrastructure level&lt;/strong&gt; implies that the different
environments are similarly or equally managed, using the same processes,
principles and most often even tooling, and that application teams have little
impact on where their application (or application components) are hosted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The promise of hybrid cloud towards business&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When business decision makers hear (or are confronted with) hybrid cloud, they
are often told that it a perfect way to deal with capacity management. Whereas
a pure on-premise deployment model requires you to purchase and deploy enough
capacity to deal with your maximum workload (and even more, if you need to
consider disaster recovery situations), a hybrid cloud could simply "add more
resources as needed, without requiring the application to be refactored for
cloud-native or cloud hosting".&lt;/p&gt;
&lt;p&gt;Let's make this more tangible with an example: a simple ticket sales service,
which consists out of a website (frontend) and an API (which is backend-alike).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales service overview" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-overview.png"&gt;&lt;/p&gt;
&lt;p&gt;The company that manages this ticket sales application is currently fully
on-premise, with a simple deployment model where the front and backend are
hosted on a web application hosting cluster (which could be a Kubernetes
cluster), and the backend also uses a database.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level infrastructure" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-hlinfra.png"&gt;&lt;/p&gt;
&lt;p&gt;Ticket sales are seasonally bound, and often ticket sales platforms are
services offered to the specific events. Suppose a major event wants to sell
its tickets through this ticket sale application, and you are afraid that the
website part will not be able to deal with the load, then you could use a
hybrid cloud setup to enable bursting on the front-end side.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales high level bursting" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-burstfrontend.png"&gt;&lt;/p&gt;
&lt;p&gt;Of course, this is just one of many target architectures that might solve the
capacity challenge, and there is no reason to believe the API itself wouldn't
be overloaded as well. But let's stick to this simple example.&lt;/p&gt;
&lt;p&gt;From a business perspective, this all sounds very fun and promising. There
seems to be no initial investment needed, and the capacity of the cloud is
limitless, not?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network investments needed for such hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, as always, the devil is in the details. If we were to pursue this
architecture, we need to have the public cloud and the on premise environment
properly connected. You don't want to use regular Internet access, because the
intention is to see these environments as a single, unified environment, and
you don't open up your internal systems directly to the Internet either do you?&lt;/p&gt;
&lt;p&gt;So you need to architect the public cloud usage to be as private as possible,
and then connect that environment with your on premise network, preferably
through a high speed private link. Sure, you can use VPN over internet, but
with a private link you have more guarantees on the latency for instance, and
for many cloud environments such private link interactions are also bringing
benefits for data ingress/egress (cheaper for you). They also generally have
better SLAs (although larger environments will have very high internet-related
SLAs) and do not require the same security protection measures (like anti-DDoS
protection).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ticket sales network connectivity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-connectivity.png"&gt;&lt;/p&gt;
&lt;p&gt;In the design, we assume that the end users still first go through your on
premise environment, as the perimeter protections you have in place for
instance still need to apply. Perimeter protections are not just simple
firewall capabilities. It includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anti-DDoS measures&lt;/li&gt;
&lt;li&gt;context gathering for, and applying coarse-grained access controls&lt;/li&gt;
&lt;li&gt;intrusion detection and prevention&lt;/li&gt;
&lt;li&gt;anti-malware protection&lt;/li&gt;
&lt;li&gt;application attack prevention&lt;/li&gt;
&lt;li&gt;network traffic filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A perimeter is meant to act as a first line of defense. However, when
integrating networks from external sites, you still need some protection
measures applied (shown as external site protection in the diagram), as you are
handing off some ownership to other parties and thus want to have some
safeguards in place.&lt;/p&gt;
&lt;p&gt;Because we still have all traffic through the perimeter, burst loads can still
jeopardize the service offering itself. If the perimeter, internet line, or the
load balancer that spreads the load across the frontends is saturated, then
your service will go down. The hybrid cloud setup used in this example wont
help out here.&lt;/p&gt;
&lt;p&gt;Second, the high speed private link will need to be able to deal with not only
the load of the user to the frontend (and back), but also between the frontend
and backend. And if you were to support bursting the backend application to the
public cloud as well, then it needs to deal with the load between this
application and the database.&lt;/p&gt;
&lt;p&gt;The link is also often not something you set up yourself between you and the
public cloud. You will need intermediate parties to support this, as often this
first requires you to have private links to certain larger networks, and then
have this larger network set up a private link to the point of presence where
you want to 'attach' to that public cloud environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Management complexity rises with hybrid cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The investments do not stop just at the network connectivity. You will need to
look into managing the web application (such as deployment and releases),
servers (bootstrapping, updating, maintaining), and other network areas.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hybrid cloud management complexity" src="https://blog.siphos.be/images/202111/hybridcloud-ticketsales-management.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's start with the web application management. Your existing management
systems will now need to deal with the public cloud as well. Your application
needs to be deployed on multiple clusters, and you will need to reconfigure the
load balancers in front to know where these clusters are. Unlike the
pre-installed environments on premise, public cloud is more dynamic (you want
to use it for bursting after all), so the target IP addresses might change (or
you set up fixed IP addresses, but that costs money even when you don't use
them).&lt;/p&gt;
&lt;p&gt;You will need to deal with deployments that can succeed left and fail right, or
vice-versa. While this is not impossible to deal with, the current way of
working might not support that yet because, you know, you never had to deal
with it.&lt;/p&gt;
&lt;p&gt;What about tracking performance and user experience? Your application
management suite might not know about public cloud setups yet, and once
included, it might find that there is latency impact. But can you just use this
APM suite in the public cloud? Perhaps your company has a site license which
does not include other locations. Or it requires per-node licenses, and require
each node to be assigned a license for 30 days at least. In case of burst
situations, you might only have these systems up for a few hours, and with the
next action, these will be new nodes with new licenses.&lt;/p&gt;
&lt;p&gt;Also on the server management level you might find many obstacles. Your on
premise system might use a certain hypervisor integration (e.g. using VMware
vCenter API) which you don't have in the public cloud. So you need to adapt the
management system anyway, which means you need to develop your management
systems to create a hybrid cloud, rather than reap the benefits of hybrid cloud
directly.&lt;/p&gt;
&lt;p&gt;Your servers might use many control systems that have the same licensing issues
as mentioned earlier. Or they are latency-bound, causing either reachability
issues, or requiring you to adapt the infrastructure architecture of these
management systems to be aware of the public cloud.&lt;/p&gt;
&lt;p&gt;Also on network management level it isn't just about connectivity. Your
firewall management might not see the public cloud firewalls automatically (or
doesn't support it), or your current network design doesn't allow for the
bursting of network environments (subnets) in a sufficient dynamic manner.&lt;/p&gt;
&lt;p&gt;The more you consider this hybrid cloud situation, the more you find out that
you will need to revisit all management, support and control systems for this
setup. And you know what is hard to do in an IT environment? Reassessing &lt;em&gt;all&lt;/em&gt;
management, support and control systems. You are effectively redesigning your
entire IT environment, and that is exactly what the promise of "hybrid cloud"
wanted to take away. Or at least, the promise that is done to certain decision
makers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendors know that it is complex (and are happy because of it)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Most of the IT vendors that are related to the management, support and control
of your infrastructure, will say that management in hybrid cloud is hard: &lt;a href="https://www.redhat.com/en/blog/operating-hybrid-architecture-and-managing-complexity"&gt;Red
Hat&lt;/a&gt;
for instance mentions in its container-related article that "Saying that
Kubernetes makes it possible to build a cross-environment management layer
doesn't mean it's easy". &lt;a href="https://blogs.arubanetworks.com/solutions/easing-the-complexity-of-hybrid-cloud/"&gt;Aruba
Networks&lt;/a&gt;
(part of PHE) mentions that "the major drawback is complex management of these
platforms". Consulting firms concur as well, with for instance David Linthicum,
Chief Cloud Strategy Officer of Deloitte Consulting, mentioning in &lt;a href="https://techbeacon.com/enterprise-it/4-things-you-need-know-about-managing-complex-hybrid-clouds"&gt;4 things
you need to know about managing complex hybrid
clouds&lt;/a&gt;
that "hybrid clouds are usually complex, hard to build, and hard to manage".&lt;/p&gt;
&lt;p&gt;Of course, IT vendors will be happy to tell you that it is hard (and for once I
concur), because they will then follow up with how their shiny tool supports
hybrid cloud much better than your existing ones. IT vendors know that an
infrastructural hybrid cloud means a redesign of (many parts of) your IT
architecture, so there is big money involved.&lt;/p&gt;
&lt;p&gt;Hence, it is important that hybrid cloud endeavours are assessed completely,
and that your business decision makers hold off with the decision until the
full scope of this exercise is known (or at least, you have a decent ballpark
estimate on the impact, not just financially, but also time-to-market). &lt;/p&gt;
&lt;p&gt;So, what are all the areas you need to consider? That's difficult to state, but
hopefully the list below can help you out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How are your servers bootstrapped (initial deployment)? Can this interact
  with the cloud APIs to do the same in the cloud, or will you need to adapt
  your processes?&lt;/li&gt;
&lt;li&gt;Once your server is bootstrapped, how do you add software, libraries and
  other artefacts to it?&lt;/li&gt;
&lt;li&gt;What security services do you need on your systems? Anti-malware? Behavior
  analytics? Intrusion detection and prevention? Privileged access management
  utilities? &lt;/li&gt;
&lt;li&gt;How do your engineers and administrators follow-up on the systems? Monitoring
  approaches? Application performance management? Trace capabilities? Logging?&lt;/li&gt;
&lt;li&gt;Are there any control systems in place that manage the infrastructure? Are
  these control systems latency-sensitive?&lt;/li&gt;
&lt;li&gt;How do you deal with applicative releases? If you have CI/CD infrastructure
  in place, how would it deal with a burst environment? Does it have any
  dynamical detection capabilities?&lt;/li&gt;
&lt;li&gt;Can your support systems deal with more ephemeral infrastructure (capacity that
  is added for a few hours and then removed again)?&lt;/li&gt;
&lt;li&gt;Can your processes deal with ephemeral infrastructure?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Perhaps your current environment is already capable of dealing with such hybrid
clouds. While the situations I am confronted with at work support my view that
we need to apply a different 'hybrid' approach than the 'seamless
infrastructure' one (I like to see the environments progress more
independently, gradually move towards a &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;zero
trust&lt;/a&gt; model), I do
believe that such hybrid cloud setups can work in certain situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions and feedback&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do come across an intention declaration to move to a hybrid cloud (which
follows the infrastructural 'seamless' setup as implied in this post), make
sure you inform the stakeholders of the consequences. Show that a majority of
management, support and control systels are not designed nor capable of dealing
with this bursting out-of-the-box, and that this will require a significant IT
investment which might not be visible to the decision maker currently.&lt;/p&gt;
&lt;p&gt;If you have the time and resources, try to already build up the arguments for
it by focusing on your management, support and control systems, validating how
ready they would be, and what types of investments would be needed. Compare
this with a setup where the infrastructure side of the hybrid cloud still uses
separate environments, and where you manage each environment using the
strengths of that environment.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1457745672304840709"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="hybrid"></category><category term="cloud"></category></entry><entry><title>Transparent encryption is not a silver bullet</title><link href="https://blog.siphos.be/2021/10/transparent-encryption-is-not-a-silver-bullet/" rel="alternate"></link><published>2021-10-19T08:20:00+02:00</published><updated>2021-10-19T08:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-19:/2021/10/transparent-encryption-is-not-a-silver-bullet/</id><summary type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Transparent encryption is relatively easy to implement, but without
understanding what it actually means or why you are implementing it, you will
probably make the assumption that this will prevent the data from being
accessed by unauthorized users. Nothing can be further from the truth.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Listing the threats to protect against&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's first list the threats you want to protect against. It is beneficial that
these threats are also scored in the organization for their likelihood of
occurrence and effect, so that you can optimize and prioritize the measures
appropriately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data leakage through theft or loss of storage media&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (OS level)&lt;/li&gt;
&lt;li&gt;Data leakage through unauthorized data access (middleware/database level)&lt;/li&gt;
&lt;li&gt;Data leakage through application vulnerability (including injection attacks)&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through data-in-transit interception&lt;/li&gt;
&lt;li&gt;Loss of confidentiality through local privilege escalation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all the "data leakage" threats are also about loss of confidentiality,
and any loss of confidentiality can also result in data leakage, I made the
distinction in name as the data intercepted through that threat is generally
not as 'bulky' as the others.&lt;/p&gt;
&lt;p&gt;To visualize the threats, consider the situation of an application that has a
database as its backend. The application is hosted on a different system than
the database. In the diagram, the blue color indicates an application-specific
focus. This does not mean it isn't infrastructure oriented anymore, but more
that it can't be transparently implemented without the application supporting
it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application and database interaction" src="https://blog.siphos.be/images/202110/te-accesspatterns.png"&gt;&lt;/p&gt;
&lt;p&gt;There are eight roles listed (well, technically seven roles but let's keep it simple and make "physical access" also a role), ranging from the application user to the physical access:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;application user&lt;/em&gt; interacts with the application itself, for instance
  from a browser to the web application.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;application administrator&lt;/em&gt; also interacts with the application, but has more privileges. The user might also have access to the system on which the application itself resides (but that isn't further modelled here).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;network poweruser&lt;/em&gt; is a user that has access to the network traffic
  between the client and application, as well as to the network traffic between
  the application and the database. Depending on the privileges of the users,
  these powerusers can be administrators on systems that reside in the same
  network.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware user&lt;/em&gt; is a role that has access to the application
  data in the database directly (so not (only) through the application). This
  can commonly be a supporting function in the organization.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;database / middleware administrator&lt;/em&gt; is the administrator of the
  database engine (or other middleware component that is used).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system administrator&lt;/em&gt; is the administrator for the server on which the
  database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;system user&lt;/em&gt; is an unprivileged user that has access to the server on
  which the database is hosted.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;physical access&lt;/em&gt; is a role that has physical access to the server and
  storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, while the example is easiest to understand with a database system, be
aware that there exist many other middleware services that manage data (like
queueing systems) and the same threats and measurements apply to them as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent encryption is a physical medium data protection measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Transparent encryption, such as through LUKS (with DM-Crypt) on Linux, will
encrypt the data on the disks, while still presenting the data unencrypted to
the users. All users. Its purpose hence is not to prevent unauthorized users
from accessing the data directly, but to prevent the storage media to expose
the data if the media is leaked or lost.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Disk Encryption" src="https://blog.siphos.be/images/202110/te-tde.png"&gt;&lt;/p&gt;
&lt;p&gt;In the diagram, you notice that the transparent disk encryption only takes
effect between the server and its storage. Hence, the only 'inappropriate'
access that it is mitigating is the physical access to the server storage. Note
that physical access to the server itself is still an important attack vector
that isn't completely mitigated here - attackers with physical access to
servers will not have a too hard time to find an entrypoint to the system.
Advanced attackers might even be able to capture the data from memory without
being detected.&lt;/p&gt;
&lt;p&gt;Transparent disk encryption is very sensible when dealing with removable media
(like USB sticks), especially if they contain any (possible) confidential data
and the method for transparent encryption is supported on all systems where you
are going to use the removable media. In larger enterprises, it also makes sense
to apply as well when multiple teams or even companies have physical access and
could attempt to maliciously access the systems.&lt;/p&gt;
&lt;p&gt;For server disks or SAN storage for instance, this has to be balanced against
the downsides of the encryption. You can do disk encryption from the storage
array for instance, but this might impact the array's capability for
deduplication and compression. If your data centers are highly secured, and you
do not allow the storage media to leave the premises without being properly
wiped or destroyed, then such transparent encryption imo has little value.&lt;/p&gt;
&lt;p&gt;Of course, when you have systems hosted in third party locations, then you do
have a higher risk that the media are being removed or stolen, especially if
those locations are accessed by many others, and your own space isn't
physically further protected. So while a company-controlled data center with
tight access requirements, policies and controls that no media leaves the
premises and what not could easily evaluate to not apply transparent disk
encryption, using a public cloud service or a non-private colocation facility
should assess encryption capabilities on disk (and higher).&lt;/p&gt;
&lt;p&gt;Furthermore, a properly configured database system will not expose its data to
unauthorized users to start with, so the &lt;em&gt;system user&lt;/em&gt; role should not have
access to the data. But once you have local access to a system, there is always
the threat that a privilege escalation bug is triggered that allows the
(previously lower privileged) user to access protected files.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transparent database encryption isn't that much better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies (or more general middleware) offer transparent
encryption themselves. In this case, the actual database files on the system
are encrypted by the database engine, but the database users still see the data
as it is unencrypted.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transparent Database Encryption" src="https://blog.siphos.be/images/202110/te-tdbe.png"&gt;&lt;/p&gt;
&lt;p&gt;Here again, it is important to know what you are protecting yourself from.
Transparent database/middleware encryption does prevent the non-middleware
administrators from directly viewing the data through the files. However,
system administrators generally have the means to become the database (or
middleware) administrator, so while the threat is not direct, it is still
indirectly there.&lt;/p&gt;
&lt;p&gt;The threat of privilege escalation on the system level is partially mitigated.
While a full system compromise will lead to the system user getting system
administrator privileges, partial compromise (such as receiving access to the
data files, but not to the encryption key itself, or not being able to
impersonate users but just access data) will be mitigated by the transparent
database encryption.&lt;/p&gt;
&lt;p&gt;Important to see here is that the threats related to the physical access are
also mostly mitigated by the transparent database encryption, with the
exception that database-only encryption might result in the encryption key
being leaked if it is situated on the system storage.&lt;/p&gt;
&lt;p&gt;Most of the threats however are still not mitigated: network interception (if
it doesn't use a properly configured TLS channel), admin access, database user
access, application admin and application users (through application
vulnerability) can still get access to all that data. The only focus these
measures have is data loss through physical access.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Database or middleware supported, application-driven encryption is somewhat better&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some database technologies support out-of-the-box data encryption through the
appropriate stored procedures or similar. In this case, the application itself
is designed to use these encryption methods from the database (or middleware),
and often holds the main encryption key itself (rather than in the database).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Database or middleware supported data encryption" src="https://blog.siphos.be/images/202110/te-dmsde.png"&gt;&lt;/p&gt;
&lt;p&gt;While this prevents some of the attack vectors (for instance, some attacks
against the application will not result in getting a context that is able to
decrypt the data) and mitigates the attack vectors related to direct database
user access, there are still plenty of issues here.&lt;/p&gt;
&lt;p&gt;System administrators and database administrators are still able to control the
encryption/decryption process. Sure, it becomes harder and requires more
thought and expertise (like modifying the stored procedures to also store the
key or the data in a different table for them to access), but it remains possible.&lt;/p&gt;
&lt;p&gt;Because of the attack complexity, this measure is one that starts to meet
certain expectations. And because the database or middleware is still
responsible for the encryption/decryption part, it can still use its knowledge
of the data for things like performance tuning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application-managed data encryption is a highly appreciated measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With application-managed data encryption, the application itself will encrypt
and decrypt the data even before it is sent over to the database or middleware.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Application-managed data encryption" src="https://blog.siphos.be/images/202110/te-amde.png"&gt;&lt;/p&gt;
&lt;p&gt;With this measure, many of the threats are mitigated. Even network interception
is partially prevented, as the network interception now is only still possible
to obtain data between the client and the application, and not between the
application and database. Also, all roles that are not application-related will
no longer be able to get to the data.&lt;/p&gt;
&lt;p&gt;Personally, I think that application-managed data encryption is always
preferred over the database- or middleware supported encryption methods.
Not only does it remove many threats, it is also much more portable, as you do
not need a database or middleware that supports it (and thus have to include
logic for that in the application).&lt;/p&gt;
&lt;p&gt;Of course, applications will need to ensure that they can still use the
functionalities of the database and middleware appropriately. If you store
names in the database in an encrypted fashion, it is no longer possible to do a
select on its content appropriately.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Client-managed data encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest level of protection against the threats listed, but of course also
the most impactful and challenging to implement, is to use client-managed data
encryption.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Client-managed data encryption" src="https://blog.siphos.be/images/202110/te-cmde.png"&gt;&lt;/p&gt;
&lt;p&gt;A web application might for instance have a (properly designed) encryption
method brought to the browser (e.g. using javascript), allowing the end user to
have sensitive data be encrypted even before it is transmitted over the
network.&lt;/p&gt;
&lt;p&gt;In that case, none of the attack vectors will be able to obtain the data. Of
course, there are plenty of other attack vectors (protecting web applications
is an art by itself), but for those we covered, client-managed encryption does
tick many of the boxes.&lt;/p&gt;
&lt;p&gt;However, client-managed data encryption is also very complex to do securely
while still being able to fully support the users. Most applications that
employ this focus already on sensitive material (like password managers) and
use end user provided information to generate the encryption keys. You need to
be able to deal with stale versions (old javascript libraries), multitude of
browsers (if it is browser-based), vulnerabilities within browsers themselves
and the web application, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network encryption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Network encryption (as in the use of TLS encrypted communications) only focuses
on the confidentiality and integrity of the communication, in our example
towards the network poweruser that might be using network interception.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Network encryption" src="https://blog.siphos.be/images/202110/te-ne.png"&gt;&lt;/p&gt;
&lt;p&gt;While the majority of other threats are still applicable, I do want to point
out that network encryption is an important measure against other threats. For
instance, with network encryption, attackers cannot easily inject code or data
in existing flows. In case of the client-managed data encryption approach for
instance, the use of network encryption is paramount, as otherwise an 'in the
middle' attacker can just remove the client-side encryption part of the code
that is transmitted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I hope that this article provides better insights in when transparent
encryption is sensible, and when not. With the above assessment, it should be
obvious that transparent (and thus without any application support) encryption
methods do not cover all the threats out there, and it is likely that your
company already has other means to cover the threats that it does handle.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Full overview" src="https://blog.siphos.be/images/202110/te-full.png"&gt;&lt;/p&gt;
&lt;p&gt;The above image shows all the different encryption levels and where in the
application, database and system interactions they are situated.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1450345580778110980"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="encryption"></category><category term="transparent"></category><category term="luks"></category><category term="dm-crypt"></category></entry><entry><title>Evaluating the zero trust hype</title><link href="https://blog.siphos.be/2021/10/evaluating-the-zero-trust-hype/" rel="alternate"></link><published>2021-10-05T00:00:00+02:00</published><updated>2021-10-05T00:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-10-05:/2021/10/evaluating-the-zero-trust-hype/</id><summary type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Security vendors are touting the benefits of "zero trust" as the new way to
approach security and security-conscious architecturing. But while there are
principles within the zero trust mindset that came up in the last dozen years,
most of the content in zero trust discussions is tied to age-old security
propositions.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the zero trust hype, two sources are driving (or aggregating) most of the
content that exists for zero trust: &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;NIST's Zero Trust Architecture
publication&lt;/a&gt; (report
800-207) and &lt;a href="https://cloud.google.com/beyondcorp/"&gt;Google's BeyondCorp Zero Trust Enterprise
Security&lt;/a&gt; resources.&lt;/p&gt;
&lt;p&gt;The NIST publication is a "dry" consolidation of what zero trust entails, and
focuses on the architecture and design principles for a zero trust environment.
It defines a zero trust architecture as an architecture that "assumes there is
no implicit trust granted to assets or users accounts based solely on their
physical or network location". &lt;/p&gt;
&lt;p&gt;The principles that it applies are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All data sources and computing services are considered resources&lt;/li&gt;
&lt;li&gt;All communication is secured regardless of network location&lt;/li&gt;
&lt;li&gt;Access to individual enterprise resources is granted on a per-session basis&lt;/li&gt;
&lt;li&gt;Access to resources is determined by dynamic policy [...] and may include
  other behavioral and environmental attributes&lt;/li&gt;
&lt;li&gt;The enterprise monitors and measures the integrity and security posture of all
  owned and associated assets&lt;/li&gt;
&lt;li&gt;All resource authentication and authorization are dynamic and strictly
  enforced before access is allowed&lt;/li&gt;
&lt;li&gt;The enterprise collects as much information as possible about the current
  state of assets, network infrastructure, and communications, and uses it to
  improve its security posture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within the publication, a common view is used to explain zero trust and the
components that take an active role within the architecture. This view is
happily shared by vendors to show where in the zero trust architecture their
component(s) are positioned.&lt;/p&gt;
&lt;p&gt;&lt;img alt="NIST core view on zero trust" src="https://blog.siphos.be/images/202110/zerotrust-core.png"&gt;&lt;/p&gt;
&lt;p&gt;The publication further evaluates a few possible architectural approaches (or
patterns if you will) for zero trust, with specific focus on the network side.
It ends with a chapter on migrating to a zero trust architecture.&lt;/p&gt;
&lt;p&gt;The Google resources through its BeyondCorp publication are more loosely written
and have a stronger focus on the cultural and principle aspects of zero trust.
One could see these publications more as an introduction to the value that zero
trust provides to a company and its users, with the focus on exposing services
everywhere, providing dynamic access controls through proxy services, and
eliminating classical patterns like using Virtual Private Networks (VPN) to bind
everything together.&lt;/p&gt;
&lt;p&gt;The main motivation beyond the zero trust principles in Google's publication is
to eliminate the perimeter-style protection where all controls are on the
perimeter, after which users have nearly free rein across the internally
exposed infrastructure.&lt;/p&gt;
&lt;p&gt;The principles it applies are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Access to services must not be determined by the network from which you
  connect&lt;/li&gt;
&lt;li&gt;Access to services is granted based on contextual factors from the user and
  their device&lt;/li&gt;
&lt;li&gt;Access to services must be authenticated, authorized, and encrypted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While these two main resources embody the bulk of what zero trust is, it does
not determine it completely. Many vendors and consultancy firms have
their view of zero trust, which largely coincides with the above, but often
has specific attention points or even foundations that are not part of the
previously mentioned resources.&lt;/p&gt;
&lt;p&gt;The term "zero trust" implies a "trust nothing and nobody" approach to
architecture and design, which you can fill in and apply everywhere. Of course,
you eventually will need to apply some level of trust somewhere, and how this is
done can depend on so many factors that it is unlikely that we will ever settle
down in the zero trust hype on what is and isn't proper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Focus areas in zero trust&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While evaluating zero trust, I read through many other resources out there.
Besides the paywalled analyst resources from Gartner and Forrester, it also
included resources from vendors to learn how they see zero trust evolve.&lt;/p&gt;
&lt;p&gt;In most of these resources, there are commonalities that everybody seems to
agree on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approach authentication and authorization at all layers in the stack: device,
  operating system, network, communication path (next-hop), communication
  session, application, etc.&lt;/li&gt;
&lt;li&gt;Enforce high maturity in asset management and inventory management. Asset
  management is more than just devices (it also entails applications, cloud
  services, etc.) and you should not only focus on those you own, but also those
  that are associated with your architecture (such as Bring Your Own Device
  (BYOD) assets)&lt;/li&gt;
&lt;li&gt;Ensure data classification and data management are applied and continuously
  evaluated and updated.&lt;/li&gt;
&lt;li&gt;Contain workloads within sufficiently small logical bounds. This could be
  through micro-segmentation (but that is not the sole method out there).&lt;/li&gt;
&lt;li&gt;Expose services globally (as in, globally reachable), but that does not
  imply that all services are accessible by each and every one.&lt;/li&gt;
&lt;li&gt;Use dynamic access policies and policy enforcement. Dynamic includes
  context-based accesses (access decisions are taken by more than just the
  authentication side of things) as well as authorizations that can change as
  new insights are passed on (such as threat intelligence).&lt;/li&gt;
&lt;li&gt;Perform continuous monitoring, including behavioral assessments.&lt;/li&gt;
&lt;li&gt;Encrypt everything (or more soundly put, cryptographically protect resources
  at all layers of the stack).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="https://www.cisa.gov"&gt;Cybersecurity and Infrastructure Security Agency&lt;/a&gt; has
recently also released the first draft of its &lt;a href="https://www.cisa.gov/publication/zero-trust-maturity-model"&gt;Zero Trust Maturity
Model&lt;/a&gt; that
companies can use to evaluate their posture against the zero trust principles.
It is strongly based upon the NIST explanation of zero trust, with attention to
five pillars (identity, device, network/environment, application workload, and
data) and three foundations (visibility and analytics, automation and
orchestration, and governance). Again, we observe some interpretation of what
zero trust could entail, in this particular case how the US government would
like to approach this towards its agencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why zero trust isn't exactly new&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attentive readers will already understand that most of the principles or focus
areas in zero trust are not new. Let's take a few of the core components and
principles and see how novel these are.&lt;/p&gt;
&lt;p&gt;One of the core components in the zero trust architecture is a policy
enforcement methodology, one that detaches enforcement from declaration.
Separating the mechanism from a policy isn't new. &lt;a href="https://ieeexplore.ieee.org/document/502679"&gt;Decentralized trust
management&lt;/a&gt;, published in 1996,
attempted to implement the necessary abstractions for it. The &lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xacml"&gt;Extensible Access
Control Markup
Language&lt;/a&gt;,
published by OASIS in 2003, is an open standard for integrating the different policy
components.&lt;/p&gt;
&lt;p&gt;The ability to perform authentication at all levels of a stack is also not new.
We can execute device authentication using the &lt;a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module"&gt;Trusted Platform
Module&lt;/a&gt; for instance,
whose first publication was in 2009. The use of certificates for authenticating
websites is common since &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security"&gt;SSL v3 came
about&lt;/a&gt; in 1996.
Authenticating end users through passwords is as old as IT itself, and
multi-factor authentication has had plenty of research since 2005. It is very
popular nowadays since the introduction of the &lt;a href="https://datatracker.ietf.org/doc/html/rfc6238"&gt;Time-based One-time Password
(T-OTP)&lt;/a&gt; as published in 2011.&lt;/p&gt;
&lt;p&gt;Even the use of user profiling for security analytics isn't novel. In 2004, the
paper on &lt;a href="https://ieeexplore.ieee.org/abstract/document/1386699"&gt;User profiling for computer
security&lt;/a&gt; was the start
of what became a very active market in cybersecurity nowadays: User Entity and
Behavior Analytics (UEBA).&lt;/p&gt;
&lt;p&gt;The dismissal of the perimeter-only security architecture seems to be the most
specific 'new' principle, although the foundations for security have long been
to not just consider security from a network point of view: starting with the
layered architecture and requirement tracking by Peter G. Neumann's &lt;a href="http://www.csl.sri.com/users/neumann/survivability.pdf"&gt;Practical
Architectures for Survivable Systems and Networks&lt;/a&gt;
published in 2000, we have seen the market take up more and more traction on
securing the different layers and assessing security not just based on the
perimeter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Personal observations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Zero trust is energizing the cybersecurity ecosystem, allowing both active
research and commercial evolutions/improvements. With the further
digitization of our environment, the significant increase in exposed services (think
IoT), and users that are always online, companies should indeed ensure that their
services (both external-facing and internal ones) are secure. The
increase in attention through the "zero trust" hype is positive, but should not
be considered completely new. Instead, it is an aggregation of already existing
best practices and designs.&lt;/p&gt;
&lt;p&gt;The lack of a common architecture (despite NISTs efforts) is to be expected, as
each company, organization or government has a different architecture and
vision. This, of course, means that decision-makers will need to understand that
"zero trust" is not a pattern to apply blindly. Vendors will attempt to
influence businesses, but without a good understanding of the current
environment and understanding the direction a company wants to go, these will
just be tools. And as the saying goes, "A fool with a tool is still a fool".&lt;/p&gt;
&lt;p&gt;Many companies will already have started on their journey to "zero trust"
without having it named as such. Layered security, security in depth, and other
statements already contribute to the zero trust approach. If you want to
approach zero trust, it is wise to consider where you are at already, and what
main principles you want to address next. You can call it "zero trust" or your
"zero trust strategy" to get attention, but beware of external influences that
might want to inject complexity because you called it "zero trust". The benefit
is not in attaining a zero trust compliant architecture, but in ensuring the
company has a good security posture, including the flexibility to adjust as the
environment evolves.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1445380710706073613"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="zero trust"></category><category term="security"></category><category term="enterprise"></category><category term="network security"></category></entry><entry><title>Scale is a cloud threat</title><link href="https://blog.siphos.be/2021/09/scale-is-a-cloud-threat/" rel="alternate"></link><published>2021-09-28T17:00:00+02:00</published><updated>2021-09-28T17:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-28:/2021/09/scale-is-a-cloud-threat/</id><summary type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Not that long ago, a vulnerability was found in &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/"&gt;Microsoft Azure Cosmos
DB&lt;/a&gt;, a NoSQL SaaS database
within the Microsoft Azure cloud. The vulnerability, which is dubbed
&lt;a href="https://chaosdb.wiz.io/"&gt;ChaosDB&lt;/a&gt; by the &lt;a href="https://twitter.com/wiz_io"&gt;Wiz Research
Team&lt;/a&gt;, uses a vulnerability or misconfiguration in
the &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-jupyter-notebooks"&gt;Jupyter Notebook
feature&lt;/a&gt;
within Cosmos DB. This vulnerability allowed an attacker to gain access to
other's Cosmos DB credentials. Not long thereafter, a second vulnerability
dubbed
&lt;a href="https://www.wiz.io/blog/omigod-critical-vulnerabilities-in-omi-azure"&gt;OMIGOD&lt;/a&gt;
showed that cloud security is not as simple as some vendors like you to believe.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are a good example of how scale is a cloud threat. Companies
that do not have enough experience with public cloud might not assume this in
their threat models.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Perimeter controls and isolation domains&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before tackling the scale of a cloud service, let's consider an on premise
service. Services that run on premise for a company are often built up
specifically for that company, and have no relationship with other customers of
the same service. Taking the NoSQL example, companies can perfectly run NoSQL
database services on premise that have no internet presence. Moreover, these
services are also often not directly exposed to the internet.&lt;/p&gt;
&lt;p&gt;Running services within your own premises reduces the likelihood that attackers
exploit vulnerabilities of that service. Attackers that are not particularly
eyeballing your company might not know you have that service on premise. Even if
they do know, having proper protections in place should prevent direct access to
those services.&lt;/p&gt;
&lt;p&gt;&lt;img alt="On premise services" src="https://blog.siphos.be/images/202109/cloud-scale-on-premise.png"&gt;&lt;/p&gt;
&lt;p&gt;Some situations do require services to expose themselves to the internet. This
exposure increases the &lt;em&gt;attack surface&lt;/em&gt; for the service significantly.
However, these services are still part of a rather isolated deployment that I
call an &lt;strong&gt;isolation domain&lt;/strong&gt;: a logical aggregation of services that share
one or more integrations and interactions, broadening the scope of
potential vulnerabilities and misconfigurations.&lt;/p&gt;
&lt;p&gt;Separate isolation domains imply that vulnerabilities or misconfigurations
that rely on information from the domain cannot spread. This is not the same as
separate deployments or environments, as those often do share certain
integrations. For instance, all NoSQL databases within a company might use that
company's identity provider for federated authentication. But NoSQL databases
exposed to the internet from two completely different companies are often in
separate isolation domains.&lt;/p&gt;
&lt;p&gt;The Cosmos DB vulnerability exploited the fact that all Cosmos DB deployments
are part of the same isolation domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perimeter and isolation domain challenges for public cloud&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Public cloud platform services, like Cosmos DB, are often lacking these two
attributes: they have different perimeter protections in place, and share the
same isolation domain.&lt;/p&gt;
&lt;p&gt;I do not want to imply that public cloud providers do not provide perimeter
protections against their services. They most definitely do, but the scope of
the perimeter is different from what a company would apply. Whereas a company
gains some measurable security by hiding services or ensuring those services are
not reachable from unauthorized contexts, public cloud platform services need to
be easily accessible for the public cloud to become successful. Security
paradigms like &lt;a href="https://www.nist.gov/publications/zero-trust-architecture"&gt;Zero
Trust&lt;/a&gt; are needed to
raise the security posture of these services. Companies that are building
solutions within the public cloud will find that this requires a different
mindset, and that these environments are not comparable with the traditional
on-premise designs.&lt;/p&gt;
&lt;p&gt;For the Cosmos DB vulnerability, the FAQ mentions that instances that are not
internet facing are still somewhat impacted (as the credentials could have been
leaked) but accessing the database (by using the credentials) will not be
possible without additional vulnerabilities or misconfigurations being
addressed. This is comparable to an administrator password leakage for your
properly isolated on-premise database: while your database might not be
immediately accessible by attackers, you're still going to change the password
as soon as possible to prevent it from being used in later attacks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Public cloud platform services" src="https://blog.siphos.be/images/202109/cloud-scale-public-cloud.png"&gt;&lt;/p&gt;
&lt;p&gt;The isolation domain is a bigger hurdle to take though, as this is almost always
by design. Platform services always share interactions or integrations across
all the customers of the public cloud. Even though you have your own logical
deployment (or even ask for your own physical deployment), the main interface to
access your service is shared. The service you use to authenticate users or
systems is shared (even when it will eventually use federated authentication to
your own identity provider, the initial service is still the same).&lt;/p&gt;
&lt;p&gt;This shared isolation domain makes each public cloud service a fantastic target
for attackers (and luckily also security researchers). Exploits might not just
reveal data or insights from one customer, but from thousands of customers all
over the world. And the bigger the cloud provider, the bigger the impact.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shared control planes also imply sharing the isolation domain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This problem of using a shared isolation domain is not restricted to public
cloud platform services only. Even on premise deployments that use a public
control plane are taking part in the same isolation domain as all other
customers of the same service.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Control plane also implies sharing isolation domains" src="https://blog.siphos.be/images/202109/cloud-scale-control-plane.png"&gt;&lt;/p&gt;
&lt;p&gt;Suppose you set up a big data platform on premise, but use your vendor's SaaS
service as a control plane to manage this big data platform. This SaaS service
is also used by the other customers of that vendor, so your deployment is part
of the same isolation domain.&lt;/p&gt;
&lt;p&gt;While such setups have benefits (such as using the same control plane for
multiple deployments across different environments and even hosting setups, and
not having to manage and maintain the control services yourself) they do
increase the risk exposure in a not dissimilar fashion from the pure public
cloud services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to tackle these concerns&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Knowing about these increased risks (reachability/exposure, and the shared
isolation domain) is the foremost important part that this article wants to
address. Once these risks are considered, companies can start taking
precautions. I've mentioned the zero trust model as a way to address the
reachability/exposure risk. To address the shared isolation domain, reducing the
impact of a successful exploit can be done through proper architecture and
design that uses the "it is not if, but when" principle for cyberattacks.&lt;/p&gt;
&lt;p&gt;For instance, the data within the databases can use application-level encryption
(meaning the encryption is not done by or through the database, but by the
front-end application that interacts with the database) to reduce the impact of
data leakage through such vulnerabilities. Proper data governance processes
should also be in place to remove any data that is no longer needed on that
database. Active security validations on log data should exist to detect
deviating access patterns, and access controls should be in place to prevent
unauthorized access even from succesfully authenticated users or systems.&lt;/p&gt;
&lt;p&gt;In the Cosmos DB case, the vulnerability was possible through a selectable
feature: deployments that do not have the Jupyter Notebook feature active would
not leak the credentials. Hence, proper configuration management of services and
disabling features that are not going to be used is paramount for cloud
services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If architects are sufficiently aware of the added risks of public cloud
services, they can properly balance these risks against the benefits of the
public cloud, and make appropriate adjustments to the architecture and design of
the solutions. The main challenge here is to make sure this awareness is raised,
and that this awareness is not only reaching the architects, but also the
engineers and other stakeholders. If not, architects risk that they will be seen
as "innovation inhibitors" if they would recommend changes and improvements to
tackle these risks.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1442867880639401989"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="cloud"></category><category term="vulnerability"></category></entry><entry><title>Naming conventions</title><link href="https://blog.siphos.be/2021/09/naming-conventions/" rel="alternate"></link><published>2021-09-15T19:00:00+02:00</published><updated>2021-09-15T19:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-15:/2021/09/naming-conventions/</id><summary type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Naming conventions. Picking the right naming convention is easy if you are all
by yourself, but hard when you need to agree upon the conventions in a larger
group. Everybody has an opinion on naming conventions, and once you decide
on it, you do expect everybody to follow through on it.&lt;/p&gt;
&lt;p&gt;Let's consider why naming conventions are (not) important and consider a few
examples to help in creating a good naming convention yourself.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Naming conventions imply standardization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you settle on a naming convention, you're effectively putting some
standardization in place which you expect everybody to follow, and which should
also cover 100% of the cases. So, when assessing a possible naming convention,
first identify what standards you need to enforce and are future proof.&lt;/p&gt;
&lt;p&gt;Say you are addressing database object naming conventions. Are you able to
enforce this at all times? You might want to start tables with &lt;code&gt;tbl_&lt;/code&gt; and views
with &lt;code&gt;vw_&lt;/code&gt;, but when you are dealing with ISV software, they generally do not
allow such freedom on 'their' database definitions. Your DBAs thus will learn
to deal with setups that are more flexible anyway.&lt;/p&gt;
&lt;p&gt;Using a naming convention for internal development is of course still a
possible path to pursue. But in that case, you will need to look at the
requirements from the development teams (and related stakeholders).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standardization does not imply naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The inverse isn't true: even though you might have certain standards in place,
it doesn't mean that the object names need to reflect the standards. If your
company standardizes on two operating systems (like Red Hat Enterprise Linux
and Microsoft Windows), it doesn't mean that server names have to include an
identifier that maps to Linux or Windows.&lt;/p&gt;
&lt;p&gt;I personally often fall into this trap - I see standards, so I want to see them
fixed in the naming convention because that allows better control over
following the standards. But naming conventions aren't about control, they are
about exposing identifiable information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure it for readability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trying to add too much information in a naming convention makes it
more complex for users to deal with. You might be able to read and understand
the naming convention immediately upon seeing it, but are all the other
stakeholders equally invested in understanding the naming conventions? &lt;/p&gt;
&lt;p&gt;Say that you have a hostname that looks like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppchypkc05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While I can tell you that this name comes from the following convention, it
might be overdoing things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;s&lt;/strong&gt; to identify it is a server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is a physical server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p&lt;/strong&gt; to identify it is hosted in a production environment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c&lt;/strong&gt; to identify it is a cattle-alike managed server&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hypk&lt;/strong&gt; to identify the ownership (in this case, hypervisor usage, KVM)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c05&lt;/strong&gt; to identify it is the fifth cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;m01&lt;/strong&gt; to identify it is the first master node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reg1&lt;/strong&gt; to identify the first region (location)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if you still want to include this information, using separators might make
this more obvious. For instance, for the given name, I would suggest splitting
this as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sppc-hypk-c05m01.reg1.internal.company.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first two parts are then global naming convention requirements, with the
first set being about the type of system whereas the second is about ownership,
and the third is then a naming convention specific to that owner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assets that follow a certain naming convention provide information about that
asset that a reader can immediately assume, without having to do additional
lookups. The intention here is that you want to define important information
that many stakeholders will need immediately to support their work (and thus
their efficiency). Insights that are useful for a select set of stakeholders
might not be suitable for a naming convention (or at least not a global one).&lt;/p&gt;
&lt;p&gt;You should consider every stakeholder that comes in contact with the name of
the asset, and how that stakeholder would obtain the information they need. If
you have a central, easily accessible configuration management system, it might
be possible to have many structured insights exposed through that interface,
but is that useful when you are dealing with lists of assets?&lt;/p&gt;
&lt;p&gt;Suppose you do not include the host class for hostnames, with the host class
being what class of system the host is (server, workstation, router, firewall,
appliance, ...). Does your SOC team need this insight every time they are going
through events? Does your helpdesk need that information? What about the
resource managers?&lt;/p&gt;
&lt;p&gt;If all these stakeholders do need that information over and over again, it
might be sensible to include it in the naming convention. If, however, only a
few stakeholders need that information, you might want to expose that easily
through different means. For instance, resource managers might be able to easily
join that information with the asset management system information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Choose what information NOT to expose easily&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes, you want to have some information about objects easily available,
but not for everybody. It might be information that can be abused for nefarious
purposes. In that case, you want this information to be shielded and only
offered to authenticated and authorized users. For instance, if you use separate
accounts for administering systems, you might not want to add information about
what type of admin account it is, as account enumeration might reveal too much
immediately and provide attackers with better insights.&lt;/p&gt;
&lt;p&gt;So, rather than having &lt;code&gt;ken_adadmin&lt;/code&gt; for Ken's Active Directory administration
account, stick to a nonsensible account identification like &lt;code&gt;ua1503&lt;/code&gt; (user
account 1503). Stakeholders that need information about accounts, in this case,
can still notice it is a user account rather than a system or machine account
and will need to query the central repositories for more information (such as
AD to get information about the user - and don't forget to add sensitive users
to, for instance, the &lt;code&gt;Protected Users&lt;/code&gt; group in AD).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use layered naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With "global naming convention" I am suggesting the ability to add naming
conventions for specific purposes, but leave that open in general. A server
name could, for instance, require an indication of the environment (production or
not) and the fact that it is a server (and not a workstation), but leave a part
of the name open for the administrators. The administrators can then add their
local naming convention to it.&lt;/p&gt;
&lt;p&gt;An active directory group, for instance, might have a standard global naming
convention (usually the start of the group name) and leave the second part
open, whereas specific teams can then use that part to add in their local naming
convention. Groups that are used for NAS access might then use a naming
convention to identify which NAS share and which privileges are assigned,
whereas a group that is used for remote access support can use VPN naming
conventions.&lt;/p&gt;
&lt;p&gt;The University of Wisconsin has their &lt;a href="https://kb.wisc.edu/iam/page.php?id=30600"&gt;Campus Active Directory - Naming
Convention&lt;/a&gt; published online, and
the workstation and server object part is a good example of this: while the
objects in AD have to follow a global naming convention (because Active
Directory is often core to an organization) it leaves some room for local
department policies to assign their own requirements:
&lt;code&gt;&amp;lt;department&amp;gt;&amp;lt;objectfunction&amp;gt;-&amp;lt;suffix&amp;gt;&lt;/code&gt; only has the first two fields
standardized globally, with the &lt;code&gt;&amp;lt;suffix&amp;gt;&lt;/code&gt; field left open (but within certain
length constraints).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consider the full name for your naming conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do want to add information in a naming convention, do not consider
this purely on a single object type, but at the full name. A hostname by itself
is just a hostname, but when you consider the fully qualified hostname (thus
including domain names) you know that certain information points can be put in
the domain name rather than the hostname. The people over at &lt;a href="https://www.serverdensity.com/"&gt;Server
Density&lt;/a&gt; have a post titled "&lt;a href="https://blog.serverdensity.com/server-naming-conventions-and-best-practices/"&gt;Server Naming
Conventions and Best
Practices&lt;/a&gt;"
where they describe that the data center location (for the server) is a
subdomain.&lt;/p&gt;
&lt;p&gt;Another example is for databases, where you not only have a table, but also the
database in which the table is located. Hence, ownership of that table can
easily be considered on the database level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learn from mistakes or missing conventions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you approach naming conventions, you will make mistakes. But before making
mistakes yourself, try looking out for public failures that might have been due
to (bad or missing) naming conventions. Now, most public root cause analysis
reports do not go in-depth on the matter completely, but they do provide some
insights we might want to learn from.&lt;/p&gt;
&lt;p&gt;For instance, the incident that AWS had on February 28th, 2017, has a &lt;a href="https://aws.amazon.com/message/41926/"&gt;Summary of
the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1)
Region&lt;/a&gt;. While there is no immediate
indication about the naming conventions used (mainly that a wrong command input
impacted more servers than it should), we could ask ourselves if the functional
purpose of the servers was included in the name (or, if not in the name, if it
was added in other labeling information that the playbook should use). The
analysis does reveal that AWS moved on to implement partitions (which they call
cells), and the cell name will likely become part of the naming convention (or
other identifiers).&lt;/p&gt;
&lt;p&gt;Also internally, it is important to go over the major incidents and their
root causes, and see if the naming conventions of the company are appropriate
or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Still need examples?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While most commercial companies will not expose their own naming conventions
(as there is no value for them to receive, and it exposes information that
malicious users might abuse), many governmental agencies and educational
institutions do have this information publicly available, given their
organization public nature. Hence, searching for "naming convention" on &lt;code&gt;*.gov&lt;/code&gt;
and &lt;code&gt;*.edu&lt;/code&gt; already reveals many examples.&lt;/p&gt;
&lt;p&gt;Personally, I am still a stickler for naming conventions, but I am slowly
accepting that some information might be better exposed elsewhere.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1438175688444596227"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="naming"></category></entry><entry><title>Location view of infrastructure</title><link href="https://blog.siphos.be/2021/09/location-view-of-infrastructure/" rel="alternate"></link><published>2021-09-07T18:00:00+02:00</published><updated>2021-09-07T18:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-07:/2021/09/location-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In this last post on the infrastructure domain, I cover the fifth and final
viewpoint that is important for an infrastructure domain representation, and
that is the &lt;em&gt;location view&lt;/em&gt;. As mentioned in previous posts, the viewpoints I
think are most representative of the infrastructure domain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/"&gt;process view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;service view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;component view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;location view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like with the component view, the location view is a layered approach. While I
initially wanted to call it the network view, "location" might be a broader
term that matches the content better. Still, it's not a perfect name, but the
name is less important than the content, not?&lt;/p&gt;


&lt;p&gt;&lt;img alt="Location view representation" src="https://blog.siphos.be/images/202109/location-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through the layers from bottom to top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Easiest to represent: geographic location and facilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The geographic location is the least IT-specific view out there, as it
represents where everything is in the world. These views are popular not only to
scope projects better (like data center locations) but also to support getting
important infrastructural metrics.&lt;/p&gt;
&lt;p&gt;WAN latency, for instance, is limited by the distance (you can't outsmart
physics), and by knowing the path between two points, you can calculate the
throughput and latency (such as through the &lt;a href="https://wintelguy.com/wanlat.html"&gt;Wintelguy WAN Latency
Estimator&lt;/a&gt;). When designing redundant network
connections between separate locations, you might depend on multiple line
providers. To ensure there are no chokepoints where both providers have their
lines go through the same location, you can ask for the fiber path details to
validate this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A10's DDoS Threat Intelligence
map" src="https://blog.siphos.be/images/202109/a10-ddos-threat-intelligence.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://www.a10networks.com/"&gt;A10 Networks&lt;/a&gt;, through their &lt;a href="https://www.a10networks.com/products/network-security-services/threat-intelligence-service/"&gt;DDoS
Threat Intelligence
Service&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Using geographic locations also facilitates understanding by other stakeholders,
even if it has a less technological impact on the case at hand. For instance,
while undergoing active DDoS attacks, a geographic representation of where they
come from helps to get more understanding from management, even though on
network-level you're more interested in the Autonomous System (AS) networks that
are involved. Those are very large groups of networks that cover the main
global-wide routing of data.&lt;/p&gt;
&lt;p&gt;If we drill down from a geographic location, the next view is the
facility-related one. Here, the view focuses on building design and
infrastructure (such as HVAC and power distribution in data rooms or data
centers), as well as the location of individual devices (floor plans, rack
spaces). Facility views help not just with initial network designs where you
want to onboard a new headquarter location, but also with capacity management
within data centers (identifying hotspots), dealing with wireless networks and
their impact on the surroundings, cable management for networks, ensuring the
resilience of infrastructure services and more.&lt;/p&gt;
&lt;p&gt;A decent facility view is very helpful when dealing with operational technology
environments (IoT), and can be dynamically generated. A while ago, I was at a
conference where they showed people's movement based on the data received from
their smartphones. They used the view to see which areas were too crowded (it
was pre-COVID times), as well as to see if there are sudden movements that might
indicate problems or threats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Foundations for networks: connectivity, underlay, and virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next three layers in the location view focus on the foundations for a
company's network. They are strongly IT oriented with the main stakeholders
being the telco- and infrastructure related teams and roles. Unlike the higher
level viewpoints, the foundations require more thought in their design as errors
are harder to correct.&lt;/p&gt;
&lt;p&gt;The connectivity focuses on the cabling and other connections made between
devices. This includes backplane-related connectivity, something that is
relevant when using enclosures or pre-engineered systems. Connectivity and
cabling seem rudimentary, but are critical for the proper functioning of a
network. Remember the science report about possible faster-than-light
neutrino's? Well, a &lt;a href="http://blogs.nature.com/news/2012/02/faster-than-light-neutrino-measurement-has-two-possible-errors.html"&gt;faulty connection was partially to
blame&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The network underlay is the network view that network engineers have on their
network. For small environments, the network view from the engineering point of
view might be the same as the view from the application side, but for larger
environments, I often see a distinction between the two. And when that occurs,
the underlay view is less of a concern for application engineers and business
stakeholders (unless of course there are major issues with the underlay design),
but that does not make it less important. People are often not aware of how our
electricity net works, but if it fails, we're all affected. Similarly, if the
network underlay is badly designed, the higher networks will see troubles too.&lt;/p&gt;
&lt;p&gt;The network virtualization stack is a technology component that supports
building virtual networks on top of the underlay environment. So while the
underlay is like the foundation on top of which all networks are hosted, the
virtualization makes this possible. In that sense, it is similar to the
hypervisor level on the component view (and perhaps is less of a location view
than a component one, although network virtualization technologies do require a
common understanding of the full network to function properly).&lt;/p&gt;
&lt;p&gt;Companies use different virtualization technologies and concepts. A simple
network virtualization technology is the VLAN (Virtual LAN), which presents
itself as a single broadcast domain to the participating systems, even though
these systems might not be physically connected to the same switch or switch
environment. It is even possible to stretch VLANs across wide areas.&lt;/p&gt;
&lt;p&gt;But virtualization can go further. Technologies such as Cisco ACI or VMware NSX
don't just focus on the LAN level, but also virtualize the network on the
addressing and routing part. And with Network Functions Virtualization (NFV), we
also include firewalls and traffic control. However, do not consider NFV to just
be the next phase beyond Software Defined Networks (SDN), as NFV and SDN are
different beasts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;View from application and system side: topology and protocols&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest levels in the location view focus on the network as it is seen by
the business applications and systems that a company hosts.&lt;/p&gt;
&lt;p&gt;The network topology is the view on segregation, segments/subnets, and the
network functions that take part in the overall environment (such as DNS/DHCP/IP
address management, firewall functionality, proxies, and other gateways). This
is the view that is probably going to change the most, as it constantly evolves
based on business demand and IT evolutions. Topology views are not just
one-of-a-kind: depending on the scope you want to address, multiple views might
be needed to convey the message you want to tell.&lt;/p&gt;
&lt;p&gt;One type of view within the topology is the &lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning
view&lt;/a&gt; which I've
covered before, and which is very expressive towards the other stakeholders: it
covers the entire company's environment while abstracting enough of the details
so that it remains understandable.&lt;/p&gt;
&lt;p&gt;If we were to zoom in further on the network topology, you get into the specific
interactions that are made between systems, which are standardized in protocols.
But while the network (and application) protocols are often very standardized,
they are also very challenging to understand.&lt;/p&gt;
&lt;p&gt;The main challenge is that there are so many protocols out there, with so many
options and implementation choices, that you need to be an expert to
troubleshoot issues if they arise. Web applications aren't just disclosed over
the HTTP protocol: you have channel encryption (TLS), might be using WebSockets,
or even the QUIC protocol. And if you think you understand HTTP, do you
understand HTTP/2?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The past few posts (with a few historical ones) make up what I consider being 
the infrastructure domain, and how to structurally approach changes within. Of
course, these are not the only views out there, and based on the project ahead,
different viewpoints might come up. But for a holistic view of the
infrastructure domain, I think these five cover it well.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1435271642507264000"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="location"></category><category term="network"></category><category term="virtualization"></category><category term="protocol"></category></entry><entry><title>Process view of infrastructure</title><link href="https://blog.siphos.be/2021/09/process-view-of-infrastructure/" rel="alternate"></link><published>2021-09-01T11:20:00+02:00</published><updated>2021-09-01T11:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-09-01:/2021/09/process-view-of-infrastructure/</id><summary type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my &lt;a href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/"&gt;previous post&lt;/a&gt;,
I started with the five different views that would support a good view of
what infrastructure would be. I believe these views (component, location,
process, service, and zoning) cover the breadth of the domain. The post also
described the component view a bit more and linked to previous posts I made (one
for &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;services&lt;/a&gt;, another for
&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;zoning&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The one I want to tackle here is the most elaborate one, also the most
enterprise-ish, and one that always is a balance on how much time and
effort to put into it (as an architect), as well as hoping that the processes
are sufficiently standardized in a flexible manner so that you don't need
to cover everything again and again in each project.&lt;/p&gt;
&lt;p&gt;So, let's talk about processes...&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Six process groups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of process frameworks out there. I've covered many of these
in a previous article (&lt;a href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/"&gt;What is the infrastructure domain?&lt;/a&gt;), with &lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL&lt;/a&gt;
and &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT&lt;/a&gt; being my main resources.&lt;/p&gt;
&lt;p&gt;Companies often select a mature framework to align their IT on. After all, why
invent everything over and over again if a popular framework with all its
resources exists. But just like how companies like to use commercially available
resources, they also like to adjust and nudge it left and right to fit
the organization better. And that's fine.&lt;/p&gt;
&lt;p&gt;I'm going to give a spin to it and combine processes with non-functionals, as
infrastructure is often about non-functionals. That doesn't mean that frameworks
like ITIL or CObIT are not good, but explaining them easily is sometimes a
challenge, and I'm not versed enough in the intricacies of these frameworks
to use them as a starting point, rather as a reference.&lt;/p&gt;
&lt;p&gt;The six groups that I feel cover the infrastructure domain sufficiently are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt;, which is about the company and the organization
  used within the company.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Consumers &amp;amp; Suppliers&lt;/em&gt;, which is about the interaction (and supporting needs)
  with the consumers of our services, as well as with the providers.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Research &amp;amp; Development&lt;/em&gt;, which is about preparing updates on the services
  and architecture&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Risk &amp;amp; Security&lt;/em&gt;, which enables risk reduction strategies and facilitates
  secure integrations of services&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Custodianship&lt;/em&gt;, which is about facilitating maintenance and support
  improvements that are more environment-wide&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Engineering &amp;amp; Operations&lt;/em&gt;, which focuses on the operational control and
  support of the services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In some ugly visualization, these groups (and the processes or non-functionals
that are within) can be represented as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Six process groups for infrastructure" src="https://blog.siphos.be/images/202109/six-process-groups.png"&gt;&lt;/p&gt;
&lt;p&gt;Let's go through each of the groups in a bit more detail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Governance and Organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first group is titled &lt;em&gt;Governance &amp;amp; Organization&lt;/em&gt; and covers the company
specifics. It has five processes or non-functionals in it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strategy and Innovation&lt;/li&gt;
&lt;li&gt;Enterprise Architecture&lt;/li&gt;
&lt;li&gt;Organizational Efficiency&lt;/li&gt;
&lt;li&gt;Separation of Concerns&lt;/li&gt;
&lt;li&gt;Formalization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;strategy&lt;/em&gt; of a company is an important starting point for larger changes,
as architects need to make sure that the changes they want to guide, coach, or
support are aligned with the company strategy. Most companies will have a
hierarchy of strategies, with the main strategy being translated to specific
strategies or strategic objectives, which are then further formalized down.&lt;/p&gt;
&lt;p&gt;Alongside supporting the company strategy in general, domain architects might
need to plan the strategy for their domain as well (as I had to do for the
infrastructure domain). This, again, is a translation of the overall strategy,
showing how infrastructure will support the strategic objectives.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;innovation&lt;/em&gt; part, the infrastructure domain needs to make clear how to
support innovative ideas and suggestions in the organization. Often,
the infrastructure and operations field is considered to be protective, and
might be perceived as obstructing innovative ideas. That's not a correct view -
while the operations field often has a more conservative view of the
infrastructure to make sure the production environment is available and secure,
it also has a viewpoint on how to deal with innovation. For instance, the
delivery of sandbox environments in which innovations can play a role, or
prototype environments that have limited, secured integration possibilities
toward the rest of the environment.&lt;/p&gt;
&lt;p&gt;Innovation is not only limited to technological innovation. Innovative ideas on
governance and organization (such as when agile development practices were
being formulated) are also important to track, as they influence many
other processes and non-functional attributes.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;enterprise architecture&lt;/em&gt; part covers abstracting and guiding the
organization, the business, the product development, etc. in a coherent and
well-documented manner. It lays the foundations for an effective and efficient,
collaborative, large organization. Domain architects ensure that their domain
architecture is related to the enterprise architecture (and contributes to the
enterprise architecture directly).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Organizational efficiency&lt;/em&gt; focuses more on how the organization functions.
Does the organization use DevOps teams, for instance, or is it more traditional
in the development versus operations part? Does the organization have vertical
technology-oriented teams, or more horizontal, solution-driven teams, or a
mixture? Knowing how the company organizes its IT is important to properly
structure and present changes. Domain architects also provide input towards
reorganizations, as they can easily define how such shifts in responsibilities
will affect the organization and its efficiency.&lt;/p&gt;
&lt;p&gt;With the &lt;em&gt;segregation of duties&lt;/em&gt;, I focus on which roles can be shared and which
ones can't. Knowing which segregation applies in the organization (such as
the different security officer focus, the segregation between risk officers and
audit officers, the exclusivity between domain administrators (in Active
Directory terms) and regular server administrators) is an important driver
for architecting. Architects can suggest introducing additional segregations or
suggest methods for removing the need to segregate these duties (as often those
are inspired by historical events and assessed on older capabilities). I've
mentioned DevOps before, and those who supported a DevOps transition will know
about the historical segregation between development and operations.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;formalization&lt;/em&gt; is about how to have formal evidence of decisions.
While this is often just part of a company's 'governance', I focus on it
specifically, as it is always balancing efficiency and effectiveness. When
formal evidence is expected by the organization, it is wise to keep track of why
this is. Often, formalization can be optimized further without reducing the
benefits or impacting the requirements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consumers and Suppliers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whereas the first group focused on the company itself, the group on consumers
and suppliers focuses on the users of the infrastructure services that are
offered (which often, but not always, are internal customers) and the suppliers
for the various services the organization consumes.&lt;/p&gt;
&lt;p&gt;It covers the following four processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost &amp;amp; Licensing&lt;/li&gt;
&lt;li&gt;Portfolio&lt;/li&gt;
&lt;li&gt;Agreements &amp;amp; Support&lt;/li&gt;
&lt;li&gt;Incidents &amp;amp; Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;cost and licensing&lt;/em&gt; part is often a large time-consumer. I include
chargeback and showback here as well, although I must be very clear that actual
costs and chargeback-reported costs are not the same. For the services
infrastructure offers internally, knowing the costs (showback) and charging the
costs through to the internal customers (chargeback) are hard processes to
tackle, requiring intensive thoughts on what is and isn't allowed, how the
company looks at the services, etc.&lt;/p&gt;
&lt;p&gt;While looking at the suppliers, an important part is to understand and optimize
the licensing. Each product and service that you consume costs money one way or
another. I've had the "pleasure" of being an Oracle license manager (as in,
responsible within my company for tracking, reporting, and optimizing the costs
associated with all Oracle products we consumed) for a while, and being part of
the Microsoft license management team (with responsibility for data center
oriented products). Knowing the license requirements, the terms and conditions,
the contractual obligations (and deviations that a company negotiated), etc. is
very useful.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;portfolio&lt;/em&gt; is about knowing what you consume (from vendors) and offer
internally, and how you intend to track and evolve it. For infrastructure
services, for instance, you want to make sure you have a decent catalog (which
is part of the portfolio) that your internal customers can consume. Designing
the catalog is an important first step in assessing and deriving the domain
architecture if it isn't available yet.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;agreements and support&lt;/em&gt; I look not just at the contractual agreements
related to service consumption (as in, the terms and conditions related to the
licensing), but also towards agreements on areas such as support (Can we call
the vendor any time of the day? How much time does the vendor contractually
have before they 'pick up the phone'? Is the service agreement conforming to
market expectations?). The same is true for the service agreements offered
internally - something that is best aligned with the portfolio.&lt;/p&gt;
&lt;p&gt;Deriving decent service level agreements (SLA), and ensuring they can be
tracked and asserted, can be important if the vendor isn't all that
trustworthy, as well as to show your internal customers that you care about
reaching and keeping the SLAs.&lt;/p&gt;
&lt;p&gt;Support is also about how to reach and interact with the support organization.
From an infrastructure point of view, that isn't always as easy as it sounds.
Some vendors require specific applications to interact with their support
organization, and your company might not allow those applications. Or, certain
metrics need to be sent out to a cloud service, but that cloud service isn't
easily identified as being secure and compliant enough with the regulations you
have to cover.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;incidents and problems&lt;/em&gt; item covers the standard incident and problem
resolution processes, and how these are handled in the organization. It is
about standardizing what incidents are, how to react to them, how to derive
problems from the observations, prioritizing work related to incidents and
problems, and more. A decent incident and problem tracking solution is a must,
but the solution itself is just part of the setup. A good tool does not give
you an efficient and effective organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research &amp;amp; Development&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next group is about evolving the service offerings. I consider the following
four processes and non-functionals as part of this group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Design&lt;/li&gt;
&lt;li&gt;Product Lifecycle&lt;/li&gt;
&lt;li&gt;Quality Control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With &lt;em&gt;architecture&lt;/em&gt;, I focus on the solution architecture, and how the solutions
interact with the domain architecture. The domain architecture is generally
part of the enterprise architecture, whereas the solution architecture is
something that regularly needs updates based on the changes that are being
planned. Most of this architecture is done by the system architects with
the support or coaching of the domain architect.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;design&lt;/em&gt; is the next phase of a development cycle and is more detailed
than the solution architecture. Designs are often handled by the
engineering teams themselves, with the support of the system architects. For
domain architects, knowing where the designs are and to read them/understand
them is vital for a good collaboration between architects and engineering teams.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;product life-cycle&lt;/em&gt; focuses on the entire life-cycle of a product, starting
with innovative ideas and research, prototypes, towards supporting the
development of the product, and even after deployment towards end-of-life
support/tracking, or in case of bought products the end-of-sale,
end-of-premier-support, extended support, custom support, and whatnot.&lt;/p&gt;
&lt;p&gt;Balancing the product life-cycles against each other is a common occurrence for
architects and product owners, as it is always a puzzle about when to make which
changes, release what versions, etc. If you don't track the life-cycle of a
product continuously, you might face situations where you need to
purchase older products because your reinvestment wasn't planned yet, but
capacity limits require an increase anyway.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;quality control&lt;/em&gt; is about ensuring the quality of the products is 
according to expectations. This includes support for different environments
(pre-production), specific quality testing (which I'll discuss later as well),
supporting QA teams (if you have those), and the processes for reporting
defects, etc. It also includes quality assurance on products purchased from
third parties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Risk &amp;amp; Security&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A major part of my work is to assess the risk exposure and ensuring a secure
and reliable infrastructure. Hence, it shouldn't come as a surprise that it is
an entire group by itself.&lt;/p&gt;
&lt;p&gt;While security is a large domain (with lots of focus on processes and
assurance), the following processes and non-functionals are strongly
represented in the infrastructure domain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Crypto&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorization&lt;/li&gt;
&lt;li&gt;Privacy&lt;/li&gt;
&lt;li&gt;Access Control&lt;/li&gt;
&lt;li&gt;Audit &amp;amp; Compliance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;em&gt;crypto&lt;/em&gt;, a major challenge is not only to ensure cryptographic services or
protocols are used where it makes sense (or better said, not used where it
makes sense) but also to understand the intricate details of the cryptographic
services, knowing what service is used for which purpose, etc. I could make an
entire article purely to discuss the sense and nonsense of transparent
encryption on file systems or databases...&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;authentication&lt;/em&gt; processes (and the closely related identity processes) are
to support assurance on the identity of a user, process, system, device, or
any other type of subject. Knowing how the authentication is handled, which
authentication protocols are used, the landscape in case of federated
authentication, etc. can take up several days to know. Authentication is no
longer based on user IDs and passwords. We have OpenID Connect, SAML, Kerberos,
TACACS, RADIUS, NTLM, and more, which all have their quirks. And those are just
the protocols to handle authentication: they don't talk about user management,
the processes you will need to support in case of account abuse, etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Authorization&lt;/em&gt;, while often combined with the authentication phase, is about
knowing what a (freshly authenticated) identity may do (authorized). Here, we
have the challenges of coarse-grained authorization versus fine-grained
authorizations, dynamic authorizations, transient authorizations, or
authorizations that are inherited from others. Often, architects will need to
design the authorization granularity and approach based on the organizational
and security requirements.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;privacy&lt;/em&gt; controls are about ensuring confidential or strictly confidential
data (if those are the terms used by the organization) are properly protected.
Data can be anonymized, pseudonymized, redacted, tokenized, encrypted, and/or
de-identified. Architects should know which control is possible where, which
services can be used, what the impact is of the controls, as well as what the
organizational data requirements are.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;access control&lt;/em&gt; part is closely related to the authorization part. In
effect, it should be what enforces the authorizations. Access control is a wide
domain with many, many products and services working with (or against
apparently) each other. Especially in more modern architectures where zero
trust plays a role, you'll notice that access control is a challenging beast,
with dynamic and contextual controls becoming primary services rather than the
standard, relatively static (role-based) access controls.&lt;/p&gt;
&lt;p&gt;The last part is the &lt;em&gt;audit and compliance&lt;/em&gt;, where audit focuses on obtaining
traceability of all events (what has happened where, when, by whom), whereas
compliance looks at assuring current state and processes are according to
expectations. Compliance can be about assuring adherence to the organizational
processes and standards, but also that a system's configuration is accurate and
still in effect. So, yes, it is more than what a "compliance" department would
focus on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custodianship&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In custodianship, I group processes and non-functionals that often play a more
active role after having a successful deployment, or after a project is
finished. While these do not imply that they are to be implemented by different
teams (that's the organizational efficiency which has to decide on this), I
notice that they are challenging to keep up properly for a large organization.&lt;/p&gt;
&lt;p&gt;In it, I cover the following processes and non-functionals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Governance&lt;/li&gt;
&lt;li&gt;Rationalization&lt;/li&gt;
&lt;li&gt;Reporting &amp;amp; Insights&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Data governance&lt;/em&gt; is about defining and tracking data, data flows, data
definitions, as well as their purpose. You need proper data governance to know
which privacy measures to apply, as one of its measures is the retention of the
data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Rationalization&lt;/em&gt; is the effort to rationalize existing infrastructure usage
and services. While major rationalization exercises are company-wide
initiatives, there are many benefits to achieve with small, incremental
rationalization exercises. Most of the time, rationalization exercises are
about cost reduction, but that doesn't always need to be the case. Of course,
eventually, everything is about finances nowadays.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;reporting and insights&lt;/em&gt;, I consider the means to report on various areas
(such as capacity, cost, performance, and SLA breaches), as well as gain
insights from the data at hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Engineering &amp;amp; Operations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final group covers the disciplines that I see on the engineering and
operations side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Orchestration&lt;/li&gt;
&lt;li&gt;Testing&lt;/li&gt;
&lt;li&gt;Change Management&lt;/li&gt;
&lt;li&gt;Operational Control&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;configuration&lt;/em&gt; part is to ensure that the systems and services are
properly configured and that the life-cycle of the configuration items is
guaranteed as well. &lt;/p&gt;
&lt;p&gt;With &lt;em&gt;orchestration&lt;/em&gt;, the focus is on ensuring larger environments are optimally
used and the appropriate abstractions are in place. Kubernetes is a good
example of an orchestration that requires close attention and support. But
others exist as well, such as those in the Hadoop ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt; focuses on the various testing strategies that enable us to trust the
final product and services and help in ensuring no regressions are
creeping in. Testing on the infrastructure side is about load and performance
testing, smoke testing, regression testing, destructive testing, etc.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;change management&lt;/em&gt; process is about properly staging the changes,
communicating the changes, following up on the changes, etc. It is not just
about preparing a deployment and then hitting a button: you want to validate
that the change is successful, track the performance to ensure nothing is acting
strangely, etc.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;operational control&lt;/em&gt;, I consider the systems that drive the operational
systems autonomously. Self-driving and self-healing are the two non-functionals
I embed under this. Many cluster management systems are part of this, and
designing for self-driving and self-healing infrastructure comes up more and
more with modern systems.&lt;/p&gt;
&lt;p&gt;Finally, &lt;em&gt;monitoring&lt;/em&gt; covers tracking the telemetry of the systems, the logs
that are generated, and derive the right insights from it. I initially wanted
to call it "Observation" as that seems to be the term that comes up more and
more (monitoring too much resembles watching telemetry for thresholds, whereas
observation goes much beyond that) but monitoring seems to resound most amongst
users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A huge list that covers most of the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This list of processes and non-functional attributes covers most, if not all,
requirements that are related to the infrastructure domain. The component
view that I mentioned in a previous post, for instance, is part of the
architecture and design processes in the Research &amp;amp; Development group.&lt;/p&gt;
&lt;p&gt;However, because they are still processes and non-functionals, they can often
seem to be less tangible. Sure, you have to manage the costs, but how do you do
that? What processes do you have in place to manage cost insights? How do you
deliver these insights? What tooling is used to support the organization? What
tooling is mandatory to use (like license management tools from larger vendors)?
Detailing this and making the right choices is part of being an architect.&lt;/p&gt;
&lt;p&gt;In the next post, I will look at the location view. Unlike the process view,
which is often shared with other IT domains, the location view is something
that is often more exclusive for the infrastructure domain.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1432996739846397957"&gt;discussion on
Twitter&lt;/a&gt;.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="process"></category></entry><entry><title>Component view of infrastructure</title><link href="https://blog.siphos.be/2021/08/component-view-of-infrastructure/" rel="alternate"></link><published>2021-08-27T21:10:00+02:00</published><updated>2021-08-27T21:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-08-27:/2021/08/component-view-of-infrastructure/</id><summary type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT architects try to use views and viewpoints to convey the target architecture
to the various stakeholders. Each stakeholder has their own interests in the
architecture and wants to see their requirements fulfilled. A core
role of the architect is to understand these requirements and make sure the
requirements are met, and to balance all the different requirements.&lt;/p&gt;
&lt;p&gt;Architecture languages or meta-models often put significant focus on these
views. Archimate has a large annex on &lt;a href="https://pubs.opengroup.org/architecture/archimate3-doc/apdxc.html#_Toc10045495"&gt;Example
Viewpoints&lt;/a&gt;
just for this purpose. However, unless the organization is widely accustomed to
enterprise architecture views, it is unlikely that the views themselves are the
final product: being able to translate those views into pretty slides and
presentations is still an important task for architects when they need to
present their findings to non-architecture roles.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Infrastructure domain in viewpoints&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While searching for a way to describe the infrastructure domain,
I tend to align with certain viewpoints as well, as it allows architects
to decompose a complex situation into more manageable parts. So the question
is no longer "how do I show what the infrastructure domain is", but rather
"what different viewpoints do I need to cover the scope of (and
explanation on) the infrastructure domain".&lt;/p&gt;
&lt;p&gt;I currently settle on five views:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;component view&lt;/em&gt;, which covers the vertical stack of an IT infrastructure
  component.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;location view&lt;/em&gt;, which is the horizontal stack for IT infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;process view&lt;/em&gt;, which covers the general enterprise requirements for IT
  infrastructure&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;service view&lt;/em&gt;, which provides insights into what functional offerings are
  provided (and for which I posted a current view a short while ago, titled "&lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An
  IT services overview&lt;/a&gt;")&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;zoning view&lt;/em&gt;, which represents the IT environment landscape. A few years
  ago, I covered this as well in "&lt;a href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/"&gt;Structuring infrastructural
  deployments&lt;/a&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these views are related to each other, but represent insights that are
particularly useful for certain discussions or representations. Some viewpoints
are even details for another. For instance, the &lt;em&gt;zoning view&lt;/em&gt; is a view that
provides more detail on a particular layer in the &lt;em&gt;location view&lt;/em&gt;. A simple
relationship between the above five views is the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between the five infrastructure views" src="https://blog.siphos.be/images/202108/five-infra-views.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, this isn't a proper meta-model, just a representation. It starts
with what the infrastructure domain has to accomplish (process view),
which defines the services the domain has to support. These services
comprise several components, and these are deployed in various
zones across the organization. The zone overview is part of the more
elaborate location views.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Components are a good introduction to infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While a good coverage of the infrastructure domain would start with the
process view, I think it is not always the easiest. Not all stakeholders
are fully acquainted with processes and what they entail, and I feel it
might be easier to start with a more tangible view, i.e. a component
view.&lt;/p&gt;
&lt;p&gt;For instance, when explaining what IT infrastructure is to an outsider
(say, a family member that isn't active in the IT world), I often start with
a component view (often using a cellphone as a starting example), then going
about the massive amount of components that need to be managed, hence the need
for proper processes. After elaborating a bit on the various processes involved,
we can then go to a service overview, to then move on to the hosting of all
those services in a structured and reliable environment (zoning), with the
various challenges related to locations.&lt;/p&gt;
&lt;p&gt;So, what is the component view that I reuse a lot? It is basically
the vertical stack that most hosting-related services use to explain where
their product is situated:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layered view on a component level" src="https://blog.siphos.be/images/202108/component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;If you start with a cellphone view, then you can easily describe the hardware,
operating system, application, and data layers in the view. You can mention that
the hardware is an expensive one-time investment that the user hopes to use for
a few years (so you can explain &lt;em&gt;capital expenditures (CapEx)&lt;/em&gt; and &lt;em&gt;operational
expenditures (OpEx)&lt;/em&gt;. The latter can be a cloud service that the
user synchronizes its data to, like Apple iCloud or Google Drive).&lt;/p&gt;
&lt;p&gt;The distinction between operating system and application, and its impact on
the users, can also be explained easily: operating system upgrades are
heavier, and users often want to choose when this occurs, as operating system
upgrades are not always fully backward compatible. Or, the user's hardware isn't
supported on the next operating system (e.g. upgrading Apple iOS 12 to iOS 13,
or Android 10 to Android 11). Applications, on the other hand, are often
automatically updated and are less intrusive. However, because there are
many applications, managing the application landscape can be more daunting than
the operating system one.&lt;/p&gt;
&lt;p&gt;Then we can move on to the scaling challenges that an organization has to
face, which will gradually build up more insights into the component layers. For
instance, if a company is developing and maintaining a mobile application, it
wants to test its new releases on different operating system
versions.  But it would not be sensible to have each developer walk around with
six phones because they need to test the application on iOS 12, iOS 13, iOS 14,
Android 9, Android 10, and Android 11. Instead, testing could be done on
emulators (which can be considered hypervisors, albeit often not that exhaustive
in features).&lt;/p&gt;
&lt;p&gt;This introduces concepts of optimizing resources for cost, but also the
benefits of having these services available 'at distance' (remote access
to the emulation environments) as well as first steps in virtualization.
You can state that this emulation is something the user can do on their
laptops, but that in enterprise environments this is done with either
cloud services or on the enterprise servers, as that facilitates collaboration
with team members, and simplifies managing these assets when the teams get
larger or smaller. And these servers, well, they too are virtualized for
resource optimization.&lt;/p&gt;
&lt;p&gt;We can also discuss the data layer, and the challenge that a regular user
has when their phone is near its limits (e.g. storage is full), the options the
user has (add SD card if the phone supports it, or use cloud storage services),
and compare that with larger enterprises where data hosting is often either
centralized or abstracted, so that systems are not bound to the limits of their
device's storage capacity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views enable scalability and cost insights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The layered view on components, of course, is a meta-view rather than an actual
one: it shows how a stack can be built up, but the actual benefit is when
you look at the component view of a solution.&lt;/p&gt;
&lt;p&gt;For instance, if we were to assess a Kubernetes cluster, it could be represented
as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kubernetes component view" src="https://blog.siphos.be/images/202108/k8s-component-view.png"&gt;&lt;/p&gt;
&lt;p&gt;Going bottom-up on this view, we can identify (and thus elaborate on) the
various layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the hardware level, we see four physical servers (named sppc01 to sppc04).
  These servers are of a particular brand and have 32 Gb of memory each (which
  isn't a lot, the cluster is rather small).&lt;/li&gt;
&lt;li&gt;KVM is used as the hypervisor. The hypervisor combines the four physical
  servers in a single cluster.&lt;/li&gt;
&lt;li&gt;KVM then provides eight virtual systems (named svpc01 to svpc08) from the
  cluster. The first three are used for the Kubernetes control plane, the others
  are the worker nodes. Note that it is recommended to host the nodes of the
  control plane on different physical machines so that a failure on one physical
  machine doesn't jeopardize the cluster availability. This can be configured on
  the hypervisor, but that is outside the scope of this article.&lt;/li&gt;
&lt;li&gt;The physical servers use a hardened Gentoo Linux operating system using the
  musl C library, whereas the virtual servers use a regular Gentoo Linux
  installation as their operating system.&lt;/li&gt;
&lt;li&gt;The orchestration layer is Kubernetes itself, using the CRI-O container
  runtime as middleware.&lt;/li&gt;
&lt;li&gt;The applications depicted are those of the Kubernetes ecosystem, with the main
  control plane applications and worker node applications listed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we were to host an application inside the Kubernetes cluster, it would
be deployed on the worker nodes. The logical design of a Kubernetes cluster
is not something to be represented in a component view (that's more for
the location view, as there we will talk about the topology of services).&lt;/p&gt;
&lt;p&gt;With such component views, we can have some insights into the costs. Of course,
this is just a simple Kubernetes cluster, and built with pure open-source
software, so the costs are going to be on the hardware side (and the resources
they consume). In larger enterprises, however, the hypervisor is often a
commercially backed one like Hyper-V (Microsoft) and vSphere (VMware), which
have their specific licensing terms (which could be the number of machines or
even CPUs).  Also, enterprises often use a commercially backed Kubernetes, like
Rancher or OpenShift (Red Hat, part of IBM), which often have per-node licensing
terms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Component views are just the beginning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I use a component view to explain what infrastructure is about,
it is merely the beginning. It provides a rudimentary layered view, which most
people can easily relate to. Content-wise, it is reasonably understandable (or
easy enough to explain) for people that aren't IT savvy, and is something that
you can easily find a lot of material for online.&lt;/p&gt;
&lt;p&gt;If we delve into the processes of (or related to) infrastructure, it becomes
more challenging to keep the readers/listeners with you. Processes can (will)
often be very abstract, and going into the details of each process is a lengthy
endeavor. I'll cover that in a later post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback? Comments?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A few days ago I've dropped Disqus as comment engine from my blog site, mainly
for concerns about my visitor's security, as well as the advertisements that it
embedded. I want my blog to be simple and straightforward, so I decided to not
have any other third-party services with it for now.&lt;/p&gt;
&lt;p&gt;So, if you have feedback or comments, don't hesitate to &lt;a href="mailto:sven.vermeulen@siphos.be"&gt;drop me an
email&lt;/a&gt;, or join the &lt;a href="https://twitter.com/infrainsight/status/1431332634370711552"&gt;discussion on
Twitter&lt;/a&gt;&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="component"></category><category term="viewpoint"></category></entry><entry><title>Disaster recovery in the public cloud</title><link href="https://blog.siphos.be/2021/07/disaster-recovery-in-the-public-cloud/" rel="alternate"></link><published>2021-07-30T20:00:00+02:00</published><updated>2021-07-30T20:00:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-30:/2021/07/disaster-recovery-in-the-public-cloud/</id><summary type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;The public cloud is a different beast than an on-premise environment, and that
also reflects itself on how we (should) look at the processes that are
actively steering infrastructure designs and architecture. One of these
is the business continuity, severe incident handling, and the
hopefully-never-to-occur disaster recovery. When building up procedures
for handling disasters (&lt;a href="https://en.wikipedia.org/wiki/Disaster_recovery"&gt;DRP = Disaster Recovery Procedure or Disaster 
Recover Planning&lt;/a&gt;),
it is important to keep in mind what these are about.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;What is a disaster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters are major incidents that have wide-ranging consequences to the
regular operations of the business. What entails a disaster can be different
between organizations, although they are commonly tied to the size of the
infrastructure and the organizational and infrastructural maturity. I'll get
back to the size dependency later when covering public cloud.&lt;/p&gt;
&lt;p&gt;A small organization that only has a few systems can declare a
disaster when all those systems are unreachable because their network
provider's line is interrupted. A larger organization probably has
redundancy of the network in place to mitigate that threat. And even
without the redundancy, organizations might just not depend that much
on those services.&lt;/p&gt;
&lt;p&gt;The larger the environment becomes though, the more the business depends
on the well-being of the services. And while I can only hope that high
availability, resiliency and appropriate redundancy are taken into account
as well, there are always threats that could jeopardize the availability
of services.&lt;/p&gt;
&lt;p&gt;When the problem at hand is specific to one or a manageable set of services,
then taking appropriate action to remediate that threat is generally not a
disaster. It can be a severe incident, but in general it is taken up
by the organization as an incident with a sufficiently small yet
efficient and well organized coordination: the teams involved are 
low in numbers, and the coordination can be done accurately.&lt;/p&gt;
&lt;p&gt;However, when the problem is significant or has a very wide scope, then
depending on the standard incident coordination will be insufficient. You
need to coordinate across too many teams, make sure communication is done
correctly, business is continuously involved/consulted, and most of all - 
you want to make sure that the organization doesn't independently try
to resolve issues when they don't have a full view on the situation
themselves.&lt;/p&gt;
&lt;p&gt;The latter is a major reason in my opinion why a DRP is so important
to have (the plan/procedure, not an actual disaster). If there is no
proper, well-aligned plan of action, teams will try to get in touch
with other teams, polluting communication and only getting incomplete
information. They might take action that other teams should know about
(but won't) or are heavily impacted by (e.g. because they are at that
time trying to do activities themselves). It can make the situation
much worse.&lt;/p&gt;
&lt;p&gt;Because we have to make a distinction between incident management
and disaster management, an organization has to formally declare
a problem as a disaster, and communicate that singular fact ("we
are now in disaster mode") so that all teams know how to respond: 
according to the Disaster Recovery Plan (DRP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disasters are not just 'force majeure'&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Disasters aren't extraordinary events or circumstances beyond the
control of the organization. Depending on the business needs, you
might very well take precautionary actions against situations you've
never encountered before and will never encounter. We've recently had
a disastrous weather in Belgium (and other countries in Western Europe)
with floods happening in large areas. But that doesn't mean that
for an organization a flood event will trigger a disaster declaration
within a company (the disastrous weather was a disaster from a
human side, with several dozen deaths and millions of damage, so it
feels somewhat incorrect to consider the threat from a theoretical
standpoint here).&lt;/p&gt;
&lt;p&gt;If you're located in a flood-sensitive environment, you can still take
precautionary actions and assess what to do in case of a flood event. 
Depending on the actions taken, a flood event (threat) will not manifest
into availability concerns, data and infrastructure destruction, people
unavailability, etc. It is only when the threat itself turns into an
unforeseen (or non-remediated) situation that we speak of a disaster.&lt;/p&gt;
&lt;p&gt;This is why disasters depend on organizations, and how risk averse
the organization (and business) is. Some businesses might not want to
take precautionary actions against situations that in the past only
occur once every 100 years, especially if the investment they would
have to do is too significant compared to the losses they might have.&lt;/p&gt;
&lt;p&gt;Common disaster threats (sometimes also called catastrophic events)
that I'm used to evaluate from an infrastructure point of view, with a
company that has four strategic data centers, multiple headquarter
locations and a high risk averse setting (considering the financial
market it plays in) are cyberattacks, local but significant infrastructure
disruptions (data center failures or destruction), people-oriented
threats (targetting key personnel), critical provider outages,
disgruntled employees, and so forth. Searching for risk matrices
online can give you some interesting pointers, such as the European
Commission's &lt;a href="https://ec.europa.eu/echo/sites/default/files/swd_2017_176_overview_of_risks_2.pdf"&gt;Overview of Natural and Man-made Disaster Risks the
European Union may
face&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Public cloud related events&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In case of public cloud, the catastrophic events that might occur are
different, and it would be wrong to just consider the same events and
with the same action plan. A prime example, and the one I really want
people to focus on, is regional outages.&lt;/p&gt;
&lt;p&gt;If your current company considers region-wide failures (for
instance because you have two data centers but within the same
region) more from a reactive point of view rather than preventive
(e.g. the DRP in case of region-wide failures is to approach
the reconstruction within the region whenever possible, rather
than fail over to a different region), it might feel the same about
public cloud region failures.&lt;/p&gt;
&lt;p&gt;That would be wrong though. Whereas it is likely that a region-wide
failure for a company is not going to happen in its lifetime, a public
cloud provider is so much more massive in size, that the likelihood
of region-wide failures is higher. If you do a quick search for
region-wide failures in AWS or Azure, you'll find plenty of examples.
And while the failures themselves might be considered 'incidents' from
the public cloud provider point of view, they might be disasters for
the companies/customers that rely on them.&lt;/p&gt;
&lt;p&gt;For me, tackling disaster recovery when dealing with public cloud strongly
focuses on region failures and (coordinated) recovery from region failures.
Beyond region failures, I also strongly recommend to look into the dependencies
that the public cloud consumption has with services outside of the public cloud.
Some of these dependencies might also play a role in certain catastrophic
events. Say that you depend on Azure AD for your authentication and
authorization, and Microsoft is suddenly facing not just a world-wide
Azure AD outage, but also has to explain to you that they cannot restore its
data.&lt;/p&gt;
&lt;p&gt;Preparing for disasters is about preparing for multiple possible catastrophic
events, and in case of public cloud, you're required to think at massive scales.
And that includes designing for region-wide failures as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Impact of public cloud disasters to the organization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Generally, if your organization has a decent maturity in dealing with disaster
recovery planning, they will be using Service Level Agreements with the
business to help decide what to do in case of disasters. Important
non-functionals that are included here are RTO (Recovery Time Objective), RPO
(Recovery Point Objective), and MTD (Maximum Tolerable Downtime). There are
others possibly listed in the SLA as well, but let me focus on these three.
If you want to learn more about contigency planning in general, I recommend
to go through the &lt;a href="https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf"&gt;NIST Special Publication 800-34 Rev.1, "Contingency Planning
Guide for Federal Informatino
Systems"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the RTO, we represent the time it is allowed to take to recover a service
to the point that it functions again. This might include with reduced capacity
or performance degradation. The RTO can be expressed in hours, days, weeks
or other arbitrary value. It is a disaster-oriented value, not availability!
As long as no disaster is declared, the RTO is not considered.&lt;/p&gt;
&lt;p&gt;The RPO identifies how much data loss is acceptable by the business in case of
a disaster. Again, this is disaster-oriented: the business can very well take
extra-ordinary steps to ensure full transactional consistency outside of 
disaster situations, yet allow for a "previous day" RPO in case of a disaster.&lt;/p&gt;
&lt;p&gt;The MTD is most often declared not on a single service, but at business service
level, and explains how long unavailability of that service is tolerated before
it seriously impacts the survivability of the business. It is related to the
RTO, as most services will have an RTO that is more strict/lower value than the
overall MTD, whereas the MTD is near non-negotiable.&lt;/p&gt;
&lt;p&gt;Now, what does public cloud disasters have to do with this? Well, in theory
nothing, as this model and process of capturing business requirements is quite
sensible and maps out perfectly as well. However (and here's the culprit),
an organization that sets up new services on a frequent basis might get
accustomed to certain values, and these values might not be as easy to approach
in a public cloud environment. Furthermore, the organization might not be
accustomed to different disaster scenario's for the SLA: having different sets
of RTO/RPO depending on the category of disaster.&lt;/p&gt;
&lt;p&gt;Let's get back to the region-wide disasters. A company might have decided not
to have region-wide proactive measures in place, and fixate their SLA
non-functionals on local disasters: a data center failure is considered a threat
that still requires proactive measures, whereas regional failures are treated
differently.  The organization decides to only have one SLA set defined, and
includes RTO and RPO values based on the current, local threat matrix. They
might decide that a majority of applications or services has a RPO of "last
transaction", meaning the data has to be fully restored at the latest situation
in case of a disaster.&lt;/p&gt;
&lt;p&gt;This generally implies synchronous replication as an infrastructural
solution. If the organization is used to having this method available (for
instance through SAN replication, cluster technologies, database recovery,
etc.) then they won't sweat at all if the next dozen services all require
the same RPO.&lt;/p&gt;
&lt;p&gt;But now comes the public cloud, and a strong point of attention is region-wide
failures. Doing synchronous replication across regions is not a proper tactic
(as it implies significant performance degradation) and especially not sensible
to do at the same scale as with local replication (e.g. between availability
zones in the same region). Now you have to tell your business that this RPO
value is not easily attainable in the public cloud. The public cloud, which
solves all the wonders in the world. The public cloud, which has more maturity
on operations than your own company. Yet you can't deliver the same SLA?&lt;/p&gt;
&lt;p&gt;Apples and pears. The disasters are different, so your offering might be
different. Of course, you should explain that your 'on premise' disaster
scenarios do not include region-wide failures, and that if you include
the same scenarios for 'on premise' that that RPO value would not be
attainable on premise either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The public cloud provides many capabilities, and has to deal with a
significantly larger environment than companies are used to. This also means
that disasters that are considered 'extremely unlikely' are now 'likely' (given
the massive scale of the public cloud), and that the threats you have to
consider while dealing with disaster recovery have to be re-visited for public
cloud enabled scenarios.&lt;/p&gt;
&lt;p&gt;My recommendation is to tackle the disaster-oriented non-functional requirements
by categorizing the disasters and having different requirements based on the
disaster at hand. Mature your cloud endeavours so that regional outages
are not a problem anymore (moving them away from the 'disaster' board), and 
properly map all dependencies you have through the public cloud exercises so
that you can build up a good view on what possible threats exist that would
require a well-coordinated approach to tackle the event.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="DRP"></category></entry><entry><title>What is the infrastructure domain?</title><link href="https://blog.siphos.be/2021/07/what-is-the-infrastructure-domain/" rel="alternate"></link><published>2021-07-19T15:20:00+02:00</published><updated>2021-07-19T15:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-19:/2021/07/what-is-the-infrastructure-domain/</id><summary type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my job as domain architect for "infrastructure", I often come across
stakeholders that have no common understanding of what infrastructure means in
an enterprise architecture. Since then, I am trying to figure out a way to
easily explain it - to find a common, generic view on what infrastructure
entails. If successful, I could use this common view to provide context on the
many, many IT projects that are going around.&lt;/p&gt;


&lt;p&gt;Of course, I do not want to approach this solely from my own viewpoint. There
are plenty of reference architectures and frameworks out there that could assist
in this. However, I still have the feeling that they are either too complex to
use for non-architect stakeholders, too simple to expose the domain properly, or
just don't fit the situation that I am currently faced with. And that's OK,
many of these frameworks are intended for architects, and from those frameworks
I can borrow insights left and right to use for a simple visualization, a
landscaping of some sort.&lt;/p&gt;
&lt;p&gt;So, let's first look at the frameworks and references out there. Some remarks
though that might be important to understand the post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When I use "the infrastructure domain", I reflect on how I see it. Within the
  company that I work for, there is some guidance on what the scope is of the
  infratructure domain, and that of course strongly influences how I look at
  "the infrastructure domain". But keep in mind that this is still somewhat
  organization- or company oriented. YMMV.&lt;/li&gt;
&lt;li&gt;While I am lucky enough to have received the time and opportunity to learn
  about enterprise architecture and architecture frameworks (and even got a
  masters degree for it), I also learned that I know nothing. Enterprises are
  complex, enterprise architecture is not a single framework or approach, and
  the enterprise architecture landscape is continuously evolving. So it is very
  well possible that I am missing something (and I will gladly learn from
  feedback on this).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Open Group Architecture Framework (TOGAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you would ask for one common framework out there for enterprise architecture,
then &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/"&gt;TOGAF&lt;/a&gt; is
probably it. It is a very exhaustive framework that focuses on various aspects
of enterprise architecture: the architecture development methodology, techniques
to use, the content of an architecture view (like metamodel descriptions), and the
capabilities that a mature organization should pursue.&lt;/p&gt;
&lt;p&gt;A core part of TOGAF is the &lt;em&gt;Architecture Development Method&lt;/em&gt; cycle, which has
several phases, including phases that are close to the infrastructure domain:
"Technology Architecture (D)" as well as areas of "Opportunities and Solutions
(E)" and "Information Systems Architectures (C)". Infrastructure is more than
'just' technology, but the core of it does fit within the technology part.&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF Cycle" src="https://blog.siphos.be/images/202107/togaf-adm-cycle.png"&gt;
&lt;em&gt;The ADM cycle, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap04.html"&gt;The Open Group, TOGAF 9.2, Chapter 4&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With TOGAF, you can support a full enterprise architecture view from the
strategy and vision up to the smallest change and its governance. However, the
key word here is &lt;em&gt;support&lt;/em&gt;, as TOGAF will not really give you much food for
simply representing the scope of infrastructure.&lt;/p&gt;
&lt;p&gt;I'm not saying it isn't a good framework, on the contrary. Especially with
&lt;a href="http://www.opengroup.org/archimate-forum"&gt;ArchiMate&lt;/a&gt; as modeling language (also
from The Open Group), using TOGAF and its meta model is a good way to facilitate
a mature architecture practice and enterprise-wide view within the
organization. But just like how application architecture and design requires a
bit more muscle than &lt;a href="https://pubs.opengroup.org/architecture/togaf9-doc/arch/chap30.html"&gt;TOGAF's
metamodel&lt;/a&gt;
supports, the same is true for infrastructure.&lt;/p&gt;
&lt;p&gt;There are also plenty of other enterprise frameworks out there that can easily
be mapped to TOGAF. Most of these focus mainly on the layering (business,
information, application, technology), processes (requirement management and the
like) and viewpoints (how to visualize certain insights) and, if you're fluent
in TOGAF, you can easily understand these other frameworks as well. I will not
be going through those in detail, but I also do not want to insinuate that they
are not valid anymore if you compare them with TOGAF: TOGAF is very extensive
and has a huge market adoption, but sometimes an extensive and exhaustive
framework isn't what a company needs...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TOGAF Technical Reference Model / Integrated Information Infrastructure
Reference Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As TOGAF is extremely extensive, it has parts that can be used to reference or
visualize infrastructure a bit better. In TOGAF 9.1, we had the &lt;em&gt;TOGAF Technical
Reference Model (TRM)&lt;/em&gt; and &lt;em&gt;TOGAF Integrated Information Infrastructure
Reference Model (III-RM)&lt;/em&gt; where you might feel that this is closer to what I
am seeking (for instance,
&lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;III-RM&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="TOGAF III-RM" src="https://blog.siphos.be/images/202107/togaf-iii-rm.png"&gt;
&lt;em&gt;Focus of the III-RM, taken from &lt;a href="https://pubs.opengroup.org/architecture/togaf91-doc/arch/chap44.html"&gt;The Open Group, TOGAF 9.1, Chapter 44&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While it does become a bit more tangible, TOGAF does not expand much on this
reference model. Instead, it is more meant as a starting point for organizations
to develop their own reference models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Information Technology Infrastructure Library (ITIL)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.axelos.com/best-practice-solutions/itil"&gt;ITIL 4&lt;/a&gt; is another very
common framework, this time owned by AXELOS Limited. The focus of ITIL is on
process support, with many processes (sorry,
'&lt;a href="https://wiki.en.it-processmaps.com/index.php/ITIL_4"&gt;practices&lt;/a&gt;' as they are
called now) being very much close to what I consider to be part of the
infrastructure domain. The practices give a good overview of 'what things to
think about' when dealing with the infrastructure domain. Now, ITIL is not
exclusive to the infrastructure domain, and the company that I work for
currently considers many of these practices as processes that need to be tackled
across all domains.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ITIL Practices" src="https://blog.siphos.be/images/202107/itil-practices.jpg"&gt;
&lt;em&gt;ITIL 4 Practices, taken from &lt;a href="https://valueinsights.ch/-the-itil-4-practices-overview/"&gt;Value Insights, The ITIL 4 Practices Overview&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Still, I do want to see some of the ITIL practices reflected in the generic
infrastructure view as they are often influencing the infrastructure domain and
the projects within. The ITIL practices make it possible to explain that it
isn't just about downloading and installing software or quickly buying an
appliance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference Model of Open Distributed Processing (RM-ODP)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.rm-odp.net/"&gt;RM-ODP standard&lt;/a&gt; has a strong focus on distributed
processing (hence the name), which is a big part of what the infrastructure
domain is about. If we ignore the workplace environment for a bit, and focus on
hosting of applications, the majority of today's hosting initiatives are on
distributed platforms.&lt;/p&gt;
&lt;p&gt;&lt;img alt="RM-ODP Five Viewpoints" src="https://blog.siphos.be/images/202107/rm-odp.png"&gt;
&lt;em&gt;Five viewpoints of RM-ODP, taken from &lt;a href="https://sparxsystems.com/products/3rdparty/odp/index.html"&gt;MDG Technology for ODP - UML for ODP, SparxSystems&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within RM-ODP guidance is given on how to handle requirement management, how to
document the processing semantics, how to identify the components, integrations
and limitations of the systems, and how to select the most appropriate
technology. The intention of RM-ODP is to be as precise and formal as possible,
and leave no room for interpretation. To accomplish that, RM-ODP uses an object
model approach.&lt;/p&gt;
&lt;p&gt;Unlike the more business and information system oriented frameworks, RM-ODP has
a strong focus on infrastructure. Its viewpoints include engineering,
computational and technology for instance. The challenge that rises here
however is that it sticks to the more engineering oriented abstractions, which
make perfect sense for people and stakeholders involved in the infrastructure
domain, but is often Chinese for others.&lt;/p&gt;
&lt;p&gt;Personally, I consider RM-ODP to be a domain-specific standard strongly
associated with the infrastructure domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Department of Defense Architecture Framework (DoDAF)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dodcio.defense.gov/library/dod-architecture-framework/"&gt;DoDAF&lt;/a&gt;
 is an architecture framework that has a strong focus on the definition
and visualization of the different viewpoints. It is less tangible than RM-ODP,
instead focusing on view definitions: what and how should something be
presented to the stakeholders. The intention of DoDAF is that organizations
develop and use an enterprise architecture that supports and uses the
viewpoints that DoDAF prescribes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="DoDAF viewpoints" src="https://blog.siphos.be/images/202107/dodaf-viewpoints.png"&gt;
&lt;em&gt;DoDAF viewpoints, taken from &lt;a href="https://www.visual-paradigm.com/guide/enterprise-architecture/what-is-dodaf-framework/"&gt;"What is DoDAF Framework", Visual Paradigm&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unlike broad scale architecture frameworks that look at an enterprise in its
entirety, my impression is that DoDAF is more towards product architecture.
That makes DoDAF more interesting for solution architectures, which often
require to be more detailed and thus hit closer to home when looking at the
infrastructure domain. However, it is not something I can 'borrow' from to
easily explain what infrastructure is about.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nato.int/cps/en/natohq/topics_157575.htm"&gt;NATO's Architecture Framework (NAF)&lt;/a&gt;
seems to play witin the same realm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sherwood Applied Business Security Architecture (SABSA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The SABSA framework and methodology has a strong security focus, but covers the
requirements from full enterprise view up to the solutions themselves. One of
the benefits of SABSA is inherent to this security-orientation: you really need
to know and understand how things work before you can show that they are
secure. Hence, SABSA is a quite complete framework and methodology.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SABSA Metamodel" src="https://blog.siphos.be/images/202107/sabsa-metamodel.png"&gt;
&lt;em&gt;SABSA metamodel, taken from &lt;a href="https://sabsa.org/the-chief-architects-blog-a-brief-history-of-sabsa-21-years-old-this-year/"&gt;"A Brief History of SABSA: Part 1", sabsa.org&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important focus area in SABSA is the integration between services, which is
something I am frequently confronted with at work. Yet unlike the more solution
driven frameworks, SABSA retains its business-oriented top-down approach, which
places it alongside the TOGAF one in my opinion. Moreover, we can apply TOGAF's
development method while using SABSA to receive more direct requirements and
design focus.&lt;/p&gt;
&lt;p&gt;Its risk and enabler orientation offers a help to not only explain how things
are set up, but also why. Especially in the sector I am currently active in
(financial sector) having a risk-based, security-conscious approach is a good
fit. The supporting list of attributes, metrics, security services, etc. allow
for defining more complete architectures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Control Objectives for IT (CObIT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a similar area as ITIL, the &lt;a href="https://www.isaca.org/resources/cobit"&gt;CObIT
framework&lt;/a&gt; focuses less on a complete
enterprise architecture framework and more on processes, process maturity, and
alignment of the processes within the organization. I am personally a fan of
CObIT as it is a more tangible framework, with more clear deliverables and
requirements, compared to others. But like with most frameworks, it has
received numerous updates to fit in continuously growing environments and
cultures which makes it heavy to use.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CObIT Core Model" src="https://blog.siphos.be/images/202107/cobit-core-model.jpg"&gt;
&lt;em&gt;The CObIT 2019 Core Model, taken from "&lt;a href="https://www.isaca.org/resources/news-and-trends/industry-news/2020/using-cobit-2019-to-plan-and-execute-an-organization-transformation-strategy"&gt;Using CObIT 2019 to plan and execute
an organization transformation strategy,
ISACA.org&lt;/a&gt;"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The framework is less about the content of infrastructure and technology, and
more about how to assess, develop, build, maintain, operate and control
whatever service is taken up. However, there are references to infrastructure
(especially when dealing with non-functionals) or controls that are actively
situated in infrastructure (such as backup/restore, disaster recovery, etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IT for IT (IT4IT)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Open Group has a similar framework like CObIT, called
&lt;a href="https://www.opengroup.org/it4it"&gt;IT4IT&lt;/a&gt;. It does have a reference architecture
view that attempts to group certain deliverables/services together to create a
holistic view on what IT should offer. But unlike the larger enterprise
frameworks it focuses strongly on service delivery and its dependencies.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IT4IT Reference Architecture" src="https://blog.siphos.be/images/202107/it4it-reference-architecture.png"&gt;
&lt;em&gt;IT4IT Reference Architecture, taken from &lt;a href="https://www.opengroup.org/it4it-forum"&gt;The Open Group IT4IT
Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Within the IT4IT reference architecture, a grouping is attempted that maps on a
value stream, starting from a strategy over the deployment up to the detection
and correction facilities. This value stream orientation is common across
frameworks, but often feels like the value is always to "add more", "deliver
more". In my opinion, rationalization exercises, decommissioning and
custodianship is too much hidden. Sure, it is part of the change management
processes and operational maintenance, but those are extremely valuable areas
that are not expressively visible in these frameworks. Compare that to the
attention that risk and security receives: while security consciousness should
be included at all phases of the value stream, security is always explicitly
mentioned.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vendor-specific visualizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Several vendors have their own visualization methodology that represents more
specific requirements from the domain(s) in which those vendors are active.
These are generally domain-specific visualizations, even with a vendor-specific
view. Such methodologies are nice to use when dealing with specific viewpoints,
but I do not believe these should be considered "architecture" frameworks. They
don't deal with requirement management, strategy alignment, and often lack
functional and non-functional insights. Still, they are a must to know in the
infrastructure areas.&lt;/p&gt;
&lt;p&gt;If you are active in Amazon AWS for instance, then you've undoubtedly come
across drawings like the one visible in "&lt;a href="https://aws.amazon.com/blogs/architecture/wordpress-best-practices-on-aws/"&gt;Wordpress: Best Practices on
AWS&lt;/a&gt;".
These drawings provide a deployment viewpoint that lists the main interactions
between AWS services.&lt;/p&gt;
&lt;p&gt;When you are more network oriented, then you've been immersed in Cisco's network
diagrams, like the one visible in "&lt;a href="https://www.cisco.com/c/en/us/td/docs/solutions/Verticals/PCI_Retail/roc.html"&gt;Verizon Business Assessment for: Cisco PCI
Solution for
Retail&lt;/a&gt;".
These network diagrams again focus on the deployment viewpoint of the network
devices and their main interactions.&lt;/p&gt;
&lt;p&gt;There are probably plenty more of these specific visualizations, but the two
mentioned above are most visible to me currently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of frameworks out there to learn from, and some of these can be
used to find ways of explaining what the infrastructure domain is about.
However, they are often all very complete and require an architectural mindset
to start from, which is not obvious when trying to convey something to outside
or indirect stakeholders.&lt;/p&gt;
&lt;p&gt;Few frameworks have a reference that is directly consumable by non-architect
stakeholders. The most tangible ones seem to be related to the IT processes, but
those still require an IT mindset to interpret.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="pattern"></category></entry><entry><title>Organizing service documentation</title><link href="https://blog.siphos.be/2021/07/organizing-service-documentation/" rel="alternate"></link><published>2021-07-08T09:20:00+02:00</published><updated>2021-07-08T09:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-07-08:/2021/07/organizing-service-documentation/</id><summary type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As I mentioned in &lt;a href="https://blog.siphos.be/2021/06/an-it-services-overview/"&gt;An IT services overview&lt;/a&gt;
I try to keep track of the architecture and designs of the IT services and
solutions in a way that I feel helps me keep in touch with all the various
services and solutions out there. Similar to how system administrators try to
find a balance while working on documentation (which is often considered a
chore) and using a structure that is sufficiently simple and standard for the
organization to benefit from, architects should try to keep track of
architecturally relevant information as well.&lt;/p&gt;
&lt;p&gt;So in this post, I'm going to explain a bit more on how I approach documenting
service and solution insights for architectural relevance.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Why I tend to document some of it myself&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Within the company I currently work for, not all architecture and designs are
handled by a central repository, but that doesn't mean there is no architecture
and design available. They are more commonly handled through separate documents,
online project sites and the like. If we had a common way of documenting
everything in the same tool using the same processes and the same taxonomy, it
wouldn't make sense to document things myself... unless even then I would find
that I am missing some information.&lt;/p&gt;
&lt;p&gt;It all started when I tried to keep track of past decisions for a service or
solution. Decisions on architecture boards, on risk forums, on department
steering committees and what not. &lt;em&gt;Historical insights&lt;/em&gt; I call it, and it often
provides a good sense of why a solution or service came up, what the challenges
were, which principles were used, etc.&lt;/p&gt;
&lt;p&gt;Once I started tracking the historical decisions and topics, I quickly moved on
to a common structure: an entry page with the most common information about the
service or solution, and then subpages for the following categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Administration (processes, authorizations, procedures)&lt;/li&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;li&gt;Authorizationn&lt;/li&gt;
&lt;li&gt;Auditing and logging&lt;/li&gt;
&lt;li&gt;Configuration management&lt;/li&gt;
&lt;li&gt;Cost management&lt;/li&gt;
&lt;li&gt;Cryptography and privacy&lt;/li&gt;
&lt;li&gt;Data management (data handling, definitions, governance, lineage,
  backup/restore, ...)&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;Design and development (incl. naming convention)&lt;/li&gt;
&lt;li&gt;Figures&lt;/li&gt;
&lt;li&gt;High availability and disaster recovery&lt;/li&gt;
&lt;li&gt;History&lt;/li&gt;
&lt;li&gt;Operations (actor groups, source systems &amp;amp; interactions/external interfacing)&lt;/li&gt;
&lt;li&gt;Organization (management, organisational structure within the company, etc.)&lt;/li&gt;
&lt;li&gt;Performance management&lt;/li&gt;
&lt;li&gt;Patterns&lt;/li&gt;
&lt;li&gt;Processes&lt;/li&gt;
&lt;li&gt;Quality assurance &amp;amp; reporting&lt;/li&gt;
&lt;li&gt;Roadmap and restrictions (incl. lifecycle)&lt;/li&gt;
&lt;li&gt;Risks and technical debt&lt;/li&gt;
&lt;li&gt;Runtime information&lt;/li&gt;
&lt;li&gt;Terminology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, I won't go in depth about all the different categories listed. Perhaps some
areas warrant closer scrutiny in later posts, but for now the names of the
categories should be sufficiently self-explanatory.&lt;/p&gt;
&lt;p&gt;If there is an internal service (website or similar) that covers the
details properly, then I will of course not duplicate that information. Instead,
I will add a link to that resource and perhaps just document how to interpret
the information on that resource.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The entry page&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The entry page of a service or solution always looks the same. It starts off
with a few attributes associated with the service:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The taxonomy used within the company&lt;/li&gt;
&lt;li&gt;The main point of contact(s)&lt;/li&gt;
&lt;li&gt;The backlog where the responsible team tracks its progress and evolution&lt;/li&gt;
&lt;li&gt;A link to the main documentation resources&lt;/li&gt;
&lt;li&gt;The internal working name&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;taxonomy&lt;/em&gt; is something I strongly appreciate in my current company. It is
a uniform identifier associated with the product, service or solution, and is
used for several of the operational processes the company has. This taxonomy
comes up in things like chargeback, service level agreements, responsibility
overviews, data classifications, enterprise architectural overviews, etc.&lt;/p&gt;
&lt;p&gt;For instance, a managed macbook (asset) might have a taxonomy identifier of
&lt;code&gt;mmac&lt;/code&gt;, or we have a service for exchanging internal company data identified as
&lt;code&gt;cd70&lt;/code&gt; (it doesn't need to have an identifier that "reads" properly). Of course,
people don't just run around shouting the identifiers, but when we go through
the information available at the company, this identifier is often the primary
key so to speak to find the information.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;main points of contacts&lt;/em&gt;, I usually just document the person that is
my go-to person to get some quick insights. The full list of all contacts (like
product owner, product manager, system architect, business analyst, release
manager, etc.) is managed in a central tool (and uses the taxonomy to quickly
find the right person), so I just have the few names that I quickly need listed
here.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;backlog&lt;/em&gt; is something I recently added to support any questions on "when
will we have feature XYZ". In the past, I had to contact the people to get this
information, but that often became cumbersome, especially when the team uses a
proper tool for tracking the work on the service.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;main documentation&lt;/em&gt; is often the most important part. It is a link to the
documentation that the team provides for end users, architects or other roles.
Some teams still have their information on a NAS, others in a document library
on SharePoint, others use a wiki, and there are teams that use a structured
document management system. Working for a big company has its consequences...&lt;/p&gt;
&lt;p&gt;Finally, the &lt;em&gt;internal working name&lt;/em&gt; is the name that a service or solution
receives the most. For infrastructure services, this is often the name of the
product from the time the product entered the organization. While the vendor has
switched the name of the product numerous times since, the old name sticks. For
instance, while I will document IBM's cloud offering as "IBMCloud" (its current
name) I will list its working name as "Bluemix" because that's how the company
internally often refers to it.&lt;/p&gt;
&lt;p&gt;After the basic attributes, I have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a few paragraphs for &lt;em&gt;description&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;a diagram or architecture view to give a &lt;em&gt;high level design&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;most important questions&lt;/em&gt; surrounding the service or solution&lt;/li&gt;
&lt;li&gt;some &lt;em&gt;tips and titbits&lt;/em&gt; for myself&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The high level design is often a view that I maintain myself, which uses the
&lt;a href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/"&gt;abstraction&lt;/a&gt; that I
mentioned earlier in my blog. It is not covering everything, but to a sufficient
level that I can quickly understand the context of the service or solution and
how it is generally implemented.&lt;/p&gt;
&lt;p&gt;The most important questions are mostly a refresher for questions that pop up
during discussions. For instance, for an API gateway, common questions might be
"What are the security controls that it enforces" or "Does the API GW perform
schema validation on JSON structures?".&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The history of a service&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below the entry page, the various categories come up. As I mentioned, it all
started with the historical insights on the service. By having a chronological
overview of all decisions and related material per service, I can quickly find
the most important information in many cases.&lt;/p&gt;
&lt;p&gt;Want to know the latest architecture for a service? Let's look in the history
when the last architectural review was, and at which decision body or board it
came up. Once found, I just need to go to the meeting minutes or case details to
find it.&lt;/p&gt;
&lt;p&gt;Want to know why the decision was taken to allow a non-standard integration?
Take a look at the history where this decision was taken, and consult its
meeting minutes.&lt;/p&gt;
&lt;p&gt;Need to ask for a continuance for something but you're just new in the team and
don't know why or how it was approved in the past? No worries, I'll share with
you the history of the service, where you can find the information, and coach
you a bit through our organization.&lt;/p&gt;
&lt;p&gt;Having the history for services and solutions available has been a massive
efficiency gain for myself and my immediate stakeholders. Of course, I would
have loved if the organization tracked this themselves, but as long as they
don't (especially since organization changes more often than technology) I will
put time and effort to track it myself (at least for the services and solutions
that I am interested in).&lt;/p&gt;
&lt;p&gt;The historical information I track myself is not a copy/paste of the meeting
minutes of those entries. I try to use a single paragraph explaination, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ARB  2020/12/05     &amp;quot;Switch of PostgreSQL authentication provider to PAM+sssd&amp;quot;
    Approval to switch the authentication provider of the PostgreSQL
    database assets from the internal authentication to a PAM-supported
    method, and use the RHEL sssd solution as a facilitator. Link with
    Active Directory.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ARB&lt;/code&gt; is the name of the decision body (here it could be the &lt;em&gt;Architecture
Review Board&lt;/em&gt;) and tells me where I can find more details if needed. I don't
really bother with trying to add actual links, because that takes time and often
the links become invalid after we switch from one solution to another.&lt;/p&gt;
&lt;p&gt;Since then, I also started adding information related to the service that isn't
just decision body oriented:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Incident    2021/06/08  &amp;quot;Fastly major outage&amp;quot;
    A major outage occurred on Fastly, a widely used cloud edge provider,
    between 09:47 UTC and 12:35 UTC. This impacted service ABC and DEF.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Incidents can be internal or external, and if they are internal I'll document
the incident and root cause analysis numbers associated with the incident as
well.&lt;/p&gt;
&lt;p&gt;It also doesn't need to be about problems. It can be announcements from the
vendor as well, as long as the announcement is or can be impactful for my work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How complete is this overview&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My overview is far, far, far from complete. It is also not my intention to make
it a complete overview, but instead use it as a quick reference when needed.
Services that are commonly discussed (for instance because they have wide
implications on other domain's architectures) are documented more in depth than
services that are barely influential to my meetings and projects. And that
doesn't mean that the services themselves are not important.&lt;/p&gt;
&lt;p&gt;Furthermore, the only areas that I truly want to have up-to-date, is the entry
page and the history. For all the other information I always hope to be able to
link to existing documentation that is kept up-to-date by the responsible teams.&lt;/p&gt;
&lt;p&gt;But in case the information isn't available, using the same structure for noting
down what insights that I gather helps out tremendously.&lt;/p&gt;
&lt;p&gt;I also don't want my overview to become a critical asset. It is protected from
prying eyes (as there is occasionally confidential information inside) and I am
coaching the organization to take up a lot of the information gathering and
documentation in a structured way. For instance, for managing EOL information,
we are publishing this in a standard way for all internal stakeholders to see
(and report on). The roadmap and strategy for the services within the domain are
now being standardized within the backlog tool as well, so that everybody can
clearly document when they expect to work on something, when certain investments
are needed, etc.&lt;/p&gt;
&lt;p&gt;In the past, architects often had to scramble all that information together
(hence one of my categories on &lt;em&gt;Roadmap&lt;/em&gt;) whereas we can now use the backlog
tools of the teams themselves to report on it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which tool to use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Personally, I use a wiki-alike service for this, so that I can search through
the pages, move and merge information, use tagging and labels and what not. I
also think that, unless the company already has a central tool for this, a well
maintained wiki with good practices and agreements on how to use it would do
wonders.&lt;/p&gt;
&lt;p&gt;I've been playing around in my spare time with several wiki technologies.
&lt;a href="https://www.dokuwiki.org/dokuwiki"&gt;Dokuwiki&lt;/a&gt; is still one of my favorites due
to its simplicity, whereas &lt;a href="https://www.mediawiki.org/wiki/MediaWiki"&gt;MediaWiki&lt;/a&gt;
is one of my go-to's for when the organization really wants to pursue a scalable
and flexible organization-wide wiki. However, considering that I try to
structure the information in a hierarchical way, I am planning to play around
with &lt;a href="https://www.bookstackapp.com/"&gt;BookStack&lt;/a&gt; a bit more.&lt;/p&gt;
&lt;p&gt;But while having a good tool is important, it isn't the critical part of
documenting information. Good documentation, in my opinion, comes from a good
structure and a coherent way of working. If you do it yourself, then of course
it is coherent, but it takes time and effort to maintain it. If you collaborate
on it, you have to make sure everybody follows the same practices and agreements
- so don't make them too complex.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="documentation"></category><category term="structure"></category><category term="wiki"></category></entry><entry><title>Not sure if TOSCA will grow further</title><link href="https://blog.siphos.be/2021/06/not-sure-if-TOSCA-will-grow-further/" rel="alternate"></link><published>2021-06-30T14:30:00+02:00</published><updated>2021-06-30T14:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-30:/2021/06/not-sure-if-TOSCA-will-grow-further/</id><summary type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;TOSCA is an OASIS open standard, and is an abbreviation for &lt;em&gt;Topology and
Orchestration Specification for Cloud Applications&lt;/em&gt;. It provides a
domain-specific language to describe how an application should be deployed
in the cloud (the topology), which and how many resources it needs, as well
as tasks to run when certain events occur (the orchestration). When I
initially came across this standard, I was (and still am) interested
in how far this goes. The promise of declaring an application (and even
bundling the necessary application artefacts) within a single asset and
then using this asset to deploy on whatever cloud is very appealing to
an architect. Especially in organizations that have a multi-cloud
strategy.&lt;/p&gt;


&lt;p&gt;But while I do see some adoption of TOSCA, I get the feeling that it is
struggling with its position against the various infrastructure-as-code
(IaC) frameworks that are out there. While many of these frameworks do
not inherently support the abstraction that TOSCA has, it is not all that
hard to apply similar principles and use those frameworks to facilitate
multi-cloud deployments.&lt;/p&gt;
&lt;p&gt;Before considering the infrastructural value of TOSCA further, let's
see what TOSCA is about first.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplifying and abstracting cloud deployments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TOSCA offers a model where you can declare how an application should be
hosted in the cloud, or in a cloud-native environment (like a Kubernetes
cluster). For instance, you might want to describe a certain document
management system, which has a web application front-end deployed on 
a farm of web application servers with a load balancer in front of it,
a backend processing system, a database system and a document storage
system. With TOSCA, you can define these structural elements with their
resource requirements.&lt;/p&gt;
&lt;p&gt;For instance, for the database system, we could declare that it has to
be a PostgreSQL database system with a certain administration password,
and within the database system we define two databases with their
own user roles:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;topology_template:
  ...
  node_templates:
    db_server:
      type: tosca.nodes.Compute
      ...
    postgresql:
      type: tosca.nodes.DBMS.PostgreSQL
      properties:
        root_password: &amp;quot;...&amp;quot;
      requirements:
        host: db_server
    db_filemeta:
      type: tosca.nodes.Database.PostgreSQL
      properties:
        name: db_filemeta
        user: fmusr
        password: &amp;quot;...&amp;quot;
      artifacts:
        db_content:
          file: files/file_server_metadata.txt
          type: tosca.artifacts.File
      requirements:
        - host: postgresql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The parameters can, and should be further parameterized. TOSCA supports
declaring inputs that are provided upon deployment so you can safely
publicize the TOSCA structure without putting passwords in there
for instance. Furthermore, TOSCA allows you to add scripts to execute
when a resource is created, which is a common requirement for database
systems.&lt;/p&gt;
&lt;p&gt;But that's not all. Within TOSCA, you then further define the relationship
between the different systems (nodes), including connectivity requirements.
Connections can then be further aligned with virtual networks to model
the network design of the application.&lt;/p&gt;
&lt;p&gt;One of the major benefits of TOSCA is that it also provides abstraction on
the requirements. While the above example explicitly pushes for a PostgreSQL
database hosted on a specific compute server, we could also declare that we
need a database management system, or for the network part we need firewall
capabilities. The TOSCA interpreter, when mapping the model to the target
cloud environment, can then either suggest or pick the technology service
itself. The TOSCA model can then have different actions depending on the
selected technology. For the database for instance, it would have different
deployment scripts.&lt;/p&gt;
&lt;p&gt;The last major benefit that I would like to point out are the workflow
and policy capabilities of TOSCA. You can declare how for instance a 
backup process should look like, or how to cleanly stop and start the
application. You can even model how a rolling upgrade of the application
or database could be handled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is not just theoretical&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Standards can often be purely theoretical, with one or a few reference
implementations out there. That is not the case with TOSCA. While reading
up on TOSCA, it became clear that it has a strong focus on Network
Functions Virtualization (NFV), a term used to denote the shift
from appliance-driven networking capabilities towards an environment
that has a multitude of solutions running in virtual environments, and
where the infrastructure adopts to this virtualized situation with
(for network) virtual routers, firewalls, etc. Another standards body,
namely the European Telecommunications Standards Institute (ETSI), seems
to be the driving force behind the NFV architecture.&lt;/p&gt;
&lt;p&gt;TOSCA has a simple profile for NFV, which aligns with ETSI's NFV and 
ensures that TOSCA parsers that support this profile can easily be used
to set up and manage solutions in virtualized environments (and thus
also cloud environments). The amount of online information about TOSCA
with respect to the NFV side is large, although still in my opinion
strongly vendor-oriented (products that support TOSCA) and standards-oriented
(talks about how well it fits). On TOSCA's &lt;a href="https://www.oasis-open.org/tosca-implementation-stories/"&gt;implementation stories&lt;/a&gt;
page, two of the three non-vendor stories are within the telco industry.&lt;/p&gt;
&lt;p&gt;There are a few vendors that heavily promote
TOSCA: &lt;a href="https://cloudify.co/"&gt;Cloudify&lt;/a&gt; and &lt;a href="https://designer.otc-service.com"&gt;Ubicity&lt;/a&gt;
offer multi-cloud orchestrators that are TOSCA-based. Many vendors, 
including the incumbent network technology vendors like Cisco and Nokia,
embrace TOSCA NFV. But most information from practical TOSCA usage out
there is in open source solutions. The list of &lt;a href="https://github.com/oasis-open/tosca-community-contributions/wiki/Known-TOSCA-Implementations"&gt;known TOSCA implementations&lt;/a&gt;
mentions plenty of open source products. One of the solutions that I
am considering of playing around with is &lt;a href="https://turandot.puccini.cloud/"&gt;turandot&lt;/a&gt;,
which uses TOSCA to compose and orchestrate Kubernetes workloads.&lt;/p&gt;
&lt;p&gt;As an infrastructure architect, TOSCA could be a nice way of putting
initial designs into practice: after designing solutions in a language
like ArchiMate, which is in general not 'executable', the next step could
be to move the deployment specifications into TOSCA and have the next
phases of the project use and enhance the TOSCA definition. But that
easily brings me to what I consider to be shortcomings of the current
situation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inhibitors for growth potential&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are a number of issues I have with the current state of TOSCA.&lt;/p&gt;
&lt;p&gt;TOSCA's ecosystem &lt;em&gt;seems&lt;/em&gt; to be lacking sufficient visualization support.
I did come across &lt;a href="https://projects.eclipse.org/projects/soa.winery"&gt;Winery&lt;/a&gt;
but that seems to be it. I would really like to see a solution that reads
TOSCA and generates an architectural overview. For instance, for the
example I started this blog with, something like the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visualization of a deployment" src="https://blog.siphos.be/images/202106/tosca-archimate.png"&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, my impression is that TOSCA is strongly and mostly Infrastructure
as a Service (IaaS) oriented. The company I currently work for strongly
focuses on platform services, managed cloud services rather than the
more traditional infrastructure services where we would still have to do
the blunt of the management ourselves. Can TOSCA still play a role
in solutions that are fully using platform services? Perhaps the answer
here is "no", as those managed services are often very cloud-vendor specific,
but that isn't always the case, and can often also be tackled using the
abstraction and implementation specifics that TOSCA supports.&lt;/p&gt;
&lt;p&gt;I also have to rely too much on impressions. While the TOSCA ecosystem
has plenty of open source solutions out there, I find it hard to get
tangible examples: TOSCA definitions of larger-scale definitions that
not only show an initial setup, but are actively maintained to show
maintenance evolution of the solution. If TOSCA is so great for vendors
to have a cloud-independent approach, why do I find it hard to find
vendors that expose TOSCA files? If the adoption of TOSCA stops
at the standards bodies and too few vendors, then it is not likely
to grow much further.&lt;/p&gt;
&lt;p&gt;TOSCA orchestration engines often are in direct competition with
general IaC orchestration like Terraform. Cloudify has a post that
&lt;a href="https://cloudify.co/blog/terraform-vs-cloudify/"&gt;compares Terraform with their solution&lt;/a&gt;
but doesn't look into how Terraform is generally used in CI/CD
processes that join Terraform with the other services that create a
decent end-to-end orchestration for cloud deployments. For Kubernetes,
it competes with Helm and the likes - not fully, as TOSCA has other 
benefits of course, but if you compare how quickly Helm is taking
the lead in Kubernetes you can understand the struggle that TOSCA in
my opinion has.&lt;/p&gt;
&lt;p&gt;Another inhibitor is TOSCA's name. If you search for information on
TOSCA, you need to exclude &lt;a href="https://www.tricentis.com/resources/tricentis-tosca-overview/"&gt;Tricentis'&lt;/a&gt;
continuous testing platform, the &lt;a href="https://en.wikipedia.org/wiki/Tosca"&gt;1900's Opera&lt;/a&gt;,
and several other projects, films, and other entities that use the same
name. You'll need to explicitly mention OASIS and/or cloud as well if
you want to find decent articles about TOSCA, knowing well that there
can be pages out there that are missed because of it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I appreciate the value TOSCA brings, I feel that it might not grow
to its fullest potential. I hope to be wrong of course, and I would
like to see big vendors publish their reference architecture TOSCA material
so that large-scale solutions are shown to be manageable using TOSCA and
that solution architects do not need to reinvent the wheel over and
over again, as well as link architecture with the more operations
side of things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To learn more about TOSCA, there are a few resources that I would recommend
here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=tosca"&gt;OASIS TOSCA Technical Committee&lt;/a&gt;
  has a number of resources linked. The &lt;a href="https://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.3/os/TOSCA-Simple-Profile-YAML-v1.3-os.pdf"&gt;TOSCA Simple Profile in YAML Version 1.3&lt;/a&gt;
  PDF is a good read which gradually explains the structure of a TOSCA YAML
  file with plenty of examples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.etsi.org/technologies/nfv"&gt;Network Functions Virtualisation (NFV)&lt;/a&gt;
  is the ETSI site on NFV. Given the focus on NFV that I find around when
  looking at TOSCA (and is even referenced on this page) understanding what
  NFV is about clarifies a bit more how valuable TOSCA is/can be in this
  environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NHYRESmE6uA"&gt;OCB: AMA on TOSCA the Topology and Orchestration Specification for Cloud Applications - Tal Liron&lt;/a&gt;
  is an hour-long briefing that covers TOSCA not only in theory but also applies
  it in practice, and covers some of the new features that are coming up.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="TOSCA"></category><category term="OASIS"></category><category term="topology"></category><category term="orchestration"></category><category term="infrastructure"></category><category term="IaC"></category><category term="NFV"></category></entry><entry><title>Integrating or customizing SaaS within your own cloud environment</title><link href="https://blog.siphos.be/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/" rel="alternate"></link><published>2021-06-23T15:10:00+02:00</published><updated>2021-06-23T15:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-23:/2021/06/integrating-or-customizing-SaaS-within-your-own-cloud-environment/</id><summary type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Software as a Service (SaaS) solutions are often a quick way to get new
capabilities into an organization’s portfolio. Smaller SaaS solutions are
simple, web-based solutions which barely integrate with the organization’s
other solutions, besides the identity and access management (which is often
handled by federated authentication).&lt;/p&gt;
&lt;p&gt;More complex or intermediate solutions require more integration focus, and
a whole new market of Integration Platform as a Service (iPaaS) solutions
came up to facilitate cross-cloud integrations. But even without the iPaaS
offerings, integrations are often a mandatory part to leverage the benefits
of the newly activated SaaS solution.&lt;/p&gt;
&lt;p&gt;In this post I want to bring some thoughts on the integrations that might be
needed to support customizing a SaaS solution.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Integrations versus customizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of ways to integrate solutions in a larger ecosystem. Most
integrations focus on data integration (file transfers mostly) and service
integration (using APIs or Enterprise Service Bus solutions), although many
other creative methods are used to facilitate the integration of a SaaS
within the organization’s portfolio.&lt;/p&gt;
&lt;p&gt;This creativity, in my opinion, often transforms into customization of a SaaS
solution rather than an integration approach. SaaS services are being extended
with new, customized functionality, but in a way that we’re no longer thinking
about integrating this customization with the SaaS, but injecting the SaaS
with closely tied services. And SaaS providers are often happy to support
this, as it binds the customer to their solution.&lt;/p&gt;
&lt;p&gt;Now, customizations are not integrations, and integrations are not
customizations. If you customize a SaaS offering, then you still need an
integration between the custom development and the SaaS offering. Sometimes
this integration is as simple as uploading the customization into the SaaS
and the SaaS does the rest. Or the customization is a completely separate
application service, which integrates over managed APIs with the SaaS. Or you
use an intermediate solution that bridges the two solutions.&lt;/p&gt;
&lt;p&gt;While many integrations are possible, I feel that there are a few integration
approaches that are in most cases just wrong. One of these is linking the
SaaS within your own private solution (network or cloud).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don’t just extend a SaaS environment into your own&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I don’t believe that it is wise to just extend a SaaS environment into your
own, even when the SaaS provider enables this. Services like VPC peering,
which can be used to link your VPC with the SaaS provider’s VPC, are easy
ways to do so, but applying it without adjusting the architecture for it
makes your long-term maintainability and security more challenging. How
do you ensure that the SaaS does not abuse this link? How do you ensure
that you don’t accidentally leak information to the SaaS? How do you ensure
high availability and resiliency is retained?&lt;/p&gt;
&lt;p&gt;In traditional architectures, we would have these provider’s locations
considered as a separate network, and introduce the appropriate controls
(like those offered by application firewalls, reverse proxies, etc.) in
between the business application and the third party. This would often be
facilitated by the network operations teams, and governed through more
centralized environments.&lt;/p&gt;
&lt;p&gt;In cloud environments, architectures can be completely different, and
individual applications might pick integration architectures that are more
fruitful for them, without considering the larger environment. If the DevOps
teams that manage the solution architecture are mature, then they too will
consider the various non-functionals that play a role with such integrations.
But if the experience is lacking, setting up direct extensions towards the
SaaS might seem to be a quick and valid solution.&lt;/p&gt;
&lt;p&gt;Some areas that I would focus on when such integrations are requested, are
(in no particular order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impact of the integration to the deployment architecture of the solution,
  considering the availability zones and region concepts used within the
  solution&lt;/li&gt;
&lt;li&gt;Authentication of the integration at various levels (not just
  authentication based on the identity being used)&lt;/li&gt;
&lt;li&gt;Isolation of the integration, ensuring that no other parties are impacted
  by the integration&lt;/li&gt;
&lt;li&gt;Filtering capabilities on network and application level&lt;/li&gt;
&lt;li&gt;Logging and metrics to get visibility on the integration, its usage, the
  volumes sent over, etc.&lt;/li&gt;
&lt;li&gt;Resilience in case of temporary failures (be it through buffering
  mechanisms, or asynchronous integrations)&lt;/li&gt;
&lt;li&gt;Registration of the integration in the enterprise architecture, so that
  assessments, vendor relationship, and other processes are aware of the
  integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When properly tackled, then services like AWS PrivateLink of course do
have a role to play. But it isn’t to just link one solution with another
and be done with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granting SaaS providers limited access to your resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another approach that I see happening is to grant a SaaS provider
administrative access to your own resources. Just like with extending SaaS
environments, I feel that this is not something to apply by default and has
to be carefully assessed. For some SaaS solutions, this is part of their
selling proposal, and is something you know up front. But not all SaaS
solutions are equally obvious in this requirement.&lt;/p&gt;
&lt;p&gt;Some organizations might not have their cloud architecture, account structure
and the like designed to enable SaaS providers to get administrative access
to (some) resources. If the current architectures focus on highly integrated
accounts and solutions, then granting a SaaS administrative access might
jeopardize the security and stability of your overall architecture.&lt;/p&gt;
&lt;p&gt;Furthermore, granting a third party access to your resources also has
implications on accountability. If a SaaS has access to storage within your
account, it could accidentally manipulate data it shouldn’t have access to,
upload another customer’s data on your storage (or vice-versa), or due to a
cyber incident upload toxic data to your account. The provider might also
inadvertently access the data in an economically unfriendly way.&lt;/p&gt;
&lt;p&gt;Using such access patterns should be carefully designed. While you can
often not implement specific IT or security measures on the solution design,
it might be possible to use separate accounts for instance, focusing on
the integration between your ‘core’ solutions and this intermediate one
to ensure a secure and resilient setup, while optimizing cost management
for this intermediate account. You can even consider putting such accounts
under a different tree in the organization structure and apply restrictive
policies such as through AWS’ Service Control Policies (SCP).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Creating solutions that link with third parties requires thought and
design. Cloud providers make it a lot easier to change and apply connections
and integrations, but that does not make the architectural work that
precedes it less obvious - on the contrary.&lt;/p&gt;
&lt;p&gt;Customizations with SaaS providers still need to be carefully assessed
and integrated, with attention on the non-functionals such as resilience,
availability, security and the like.&lt;/p&gt;
&lt;p&gt;If the SaaS provider needs access to your own resources, carefully assess
how fine-grained this can be implemented and how the accountability is
assigned. See if an intermediate account can be used where both you and
the SaaS provider have administrative access to, while keeping the rest
of the organization’s data and solutions elsewhere.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="cloud"></category><category term="SaaS"></category><category term="integration"></category><category term="customization"></category></entry><entry><title>An IT services overview</title><link href="https://blog.siphos.be/2021/06/an-it-services-overview/" rel="alternate"></link><published>2021-06-14T17:30:00+02:00</published><updated>2021-06-14T17:30:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-14:/2021/06/an-it-services-overview/</id><summary type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My current role within the company I work for is “domain architect”, part
of the enterprise architects teams. The domain I am accountable for is 
“infrastructure”, which can be seen as a very broad one. Now, I’ve been
maintaining an overview of our IT services before I reached that role, 
mainly from an elaborate interest in the subject, as well as to optimize
my efficiency further.&lt;/p&gt;
&lt;p&gt;Becoming a domain architect allows me to use the insights I’ve since
gathered to try and give appropriate advice, but also now requires me to
maintain a domain architecture. This structure is going to be the starting
point of it, although it is not the true all and end all of what I would
consider a domain architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;A single picture doesn’t say it all&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To start off with my overview, I had a need to structure the hundreds of
technology services that I want to keep an eye on in a way that I can 
quickly find it back, as well as present to other stakeholders what 
infrastructure services are about. This structure, while not perfect, 
currently looks like in the figure below. Occasionally, I move one or
more service groups left or right, but the main intention is just to
have a structure available.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overview of the IT services" src="https://blog.siphos.be/images/202106/it_service_overview.png"&gt;&lt;/p&gt;
&lt;p&gt;Figures like these often come about in mapping exercises, or capability models.
A capability model that I recently skimmed through is the
&lt;a href="https://www.if4it.com/SYNTHESIZED/MODELS/ENTERPRISE/enterprise_capability_model.html"&gt;IF4IT Enterprise Capability Model&lt;/a&gt;.
I stumbled upon this model after searching for some reference architectures
on approaching IT services, including a paper titled
&lt;a href="https://www.researchgate.net/publication/238620971_IT_Services_Reference_Catalog"&gt;IT Services Reference Catalog&lt;/a&gt;
by Nelson Gama, Maria do Mar Rosa, and Miguel Mira da Silva.&lt;/p&gt;
&lt;p&gt;Capability models, or service overviews like the one I presented, do not fit
each and every organization well. When comparing the view I maintain with
others (or the different capability and service references out there), I
notice two main distinctions: grouping, and granularity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Certain capabilities might be grouped one way in one reference, and use a
  totally different grouping in another. A database system might be part of
  a “Databases” group in one, a “Data Management” group in another, or even
  “Information Management” in a third. Often, this grouping also reveals the
  granularity that the author wants to pursue.&lt;br&gt;
  Grouping allows for keeping things organized and easier to explain, but has
  no structural importance. Of course, a well-chosen grouping also allows you
  to tie principles and other architectural concerts to the groups themselves,
  and not services in particular. But that still falls under the explainability
  part.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The granularity is more about how specific a grouping is. In the example
  above, “Information Management” is the most coarse-grained grouping, whereas
  “Databases” might be a very specific one. Granularity can convey more insights
  in the importance of services, although it can also be due to historical
  reasons, or because an overview started from one specific service and expanded
  later. In that case, it is very likely that the specific service and its
  dependencies are more elaborately documented.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the figure I maintain, the grouping is often based both on the extensiveness 
of a group (if a group contains far too many services, I might want to see if I
can split up the group) as well as historical and organizational choices. For
instance, if the organization has a clear split between network oriented
teams and server oriented teams, then the overview will most likely convey
the same message, as we want to have the overview interpretable by many
stakeholders - and those stakeholders are often aware of the organizational
distinctions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services versus solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I try to keep track of the evolutions of &lt;em&gt;services&lt;/em&gt; and &lt;em&gt;solutions&lt;/em&gt; within this
overview. Now, the definition of a “service” versus “solution” does warrant
a bit more explanation, as it can have multiple meanings. I even use “service”
for different purposes depending on the context.&lt;/p&gt;
&lt;p&gt;For domain architecture, I consider an “&lt;em&gt;infrastructure service&lt;/em&gt;” as a product
that realizes or supports an IT capability. It is strongly product oriented
(even when it is internally developed, or a cloud service, or an appliance)
and makes a distinction between products that are often very closely related.
For instance, Oracle DB is an infrastructure service, as is the Oracle
Enterprise Manager. The Oracle DB is a product that realizes a “relational
database” capability, whereas OEM is a “central infrastructure management”
capability.&lt;/p&gt;
&lt;p&gt;The reason I create distinct notes for these is because they have different
life cycles, might have different organizational responsible teams, different
setups, etc. Hence, components (parts of products) I generally do not consider
as separate, although there are definitely examples where it makes sense to
consider certain components separate from the products in which they are
provided.&lt;/p&gt;
&lt;p&gt;The several hundred infrastructure services that the company is rich in are
all documented under this overview.&lt;/p&gt;
&lt;p&gt;Alongside these infrastructure services I also maintain a solution overview.
The grouping is exactly the same as the infrastructure services, but the
intention of solutions is more from a full internal offering point of view.&lt;/p&gt;
&lt;p&gt;Within &lt;em&gt;solution architectures&lt;/em&gt;, I tend to focus on the choices that the
company made and the design that follows it. Many solutions are considered
‘platforms’ on top of which internal development, third party hosting or
other capabilities are provided. Within the solution, I describe how the
various infrastructure services interact and work together to make the
solution reality.&lt;/p&gt;
&lt;p&gt;A good example here is the mainframe platform. Whereas the mainframe itself
is definitely an infrastructure service, how we internally organize the
workload and the runtimes (such as the LPARs), how it integrates with the
security services, database services, enterprise job scheduling, etc. is
documented in the solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Not all my domain though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not all services and solutions that I track are part of ‘my’ domain though.
For instance, at my company, we make a distinction between the
infrastructure-that-is-mostly-hosting, and
infrastructure-that-is-mostly-workplace. My focus is on the ‘mostly hosting’
orientation, whereas I have a colleague domain architect responsible for
the ‘mostly workplace’ side of things.&lt;/p&gt;
&lt;p&gt;But that’s just about accountability. Not knowing how the other solutions
and services function, how they are set up, etc. would make my job harder,
so tracking their progress and architecture is effort that pays off.&lt;/p&gt;
&lt;p&gt;In a later post I’ll explain what I document about services and solutions
and how I do it when I have some time to spend.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="overview"></category><category term="service"></category><category term="landscape"></category><category term="catalog"></category><category term="capability"></category></entry><entry><title>Virtualization vs abstraction</title><link href="https://blog.siphos.be/2021/06/virtualization-vs-abstraction/" rel="alternate"></link><published>2021-06-03T10:10:00+02:00</published><updated>2021-06-03T10:10:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2021-06-03:/2021/06/virtualization-vs-abstraction/</id><summary type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;When an organization has an extensively large, and heterogeneous
infrastructure, infrastructure architects will attempt to make itless
complex and chaotic by introducing and maintaining a certain degree of
standardization. While many might consider standardization as a
rationalization (standardizing on a single database technology, single
vendor for hardware, etc.), rationalization is only one of the many ways
in which standards can simplify such a degree of complexity.&lt;/p&gt;
&lt;p&gt;In this post, I'd like to point out two other, very common ways to
standardize the IT environment, without really considering a
rationalization: abstraction and virtualization.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Abstraction: common and simplified interfaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The term "abstraction" has slightly different connotations based on the
context in which the term is used. Generally speaking, an abstraction
provides a less detailed view on an object and shows the intrinsic qualities
upon which one looks at that object. Let's say we have a PostgreSQL database
and a MariaDB database. An abstract view on it could find that it has a lot
of commonalities, such as tabular representation of data, a network-facing
interface through which their database clients can interact with the
database, etc.&lt;/p&gt;
&lt;p&gt;We then further generalize this abstraction to come to the generalized
"relational database management system" concept. Furthermore, rather than
focusing on the database-specific languages of the PostgreSQL database and
the MariaDB database (i.e. the commands that database clients send to the
database), we abstract the details that are not shared across the two, and
create a more common set of commands that both databases support.&lt;/p&gt;
&lt;p&gt;Once you standardize on this common set of commands, you get more freedom in
exchanging one database technology for the other. This is exactly what
happened several dozen years ago, and resulted in the SQL standard
(ISO/IEC 9075). This standard is a language that, if all your relational
database technologies support it, allows you - as an organization - to work
with a multitude of database technologies while still having a more efficient
and standardized way of dealing with it.&lt;/p&gt;
&lt;p&gt;Now, the SQL language standard is one example. IT is filled with many other
examples, some more formally defined as standards than others. Let's look at
a more recent example, within the area of application containerization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRI and OCI are abstraction implementations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When the Docker project, now supported through the Docker company, started
with enabling application containerization in a more efficient way, it leaned
upon the capabilities that the Linux kernel offered on hiding information and
isolating resources (namespaces and control groups) and expanded on it to
make it user friendly. It was an immediate hit, and has since then resulted
in a very competitive market.&lt;/p&gt;
&lt;p&gt;With Docker, applications could be put in more isolated environments and run
in parallel on the same system, without these applications seeing the other
ones. Each application has its own, private view on the system. With these
containers, the most important service that is still shared is the kernel,
with the kernel offering only those services to the containers that it can
keep isolated.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Container runtime abstraction" src="https://blog.siphos.be/images/202106/container-runtimes.jpg"&gt;
&lt;em&gt;Source: &lt;a href="https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/"&gt;https://merlijn.sebrechts.be/blog/2020-01-docker-podman-kata-cri-o/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, while Docker can be easily attributed to bringing this to the wider
public, other initiatives quickly followed suit. Multiple container
technologies were coming to life, and started to bid for a place in the
containerization market. To be able to compete here, many of these attempted
to use the same interfaces (be it system calls, commands or other) as Docker
used, so the users can more easily switch. But while trying to copy and
implement the same interfaces is a possible venue, it is still strongly
controlled by the evolution that Docker is taking.&lt;/p&gt;
&lt;p&gt;Since then, larger projects like Kubernetes have started introducing an
abstraction between the container runtime (which implements the actual
containerization) and the container definitions and management (which uses
the containerization). Within Kubernetes for instance, this is through the
Common Runtime Interface (CRI), and the Open Container Interface (OCI) is
used to link the container runtime management with the underlying container
technologies.&lt;/p&gt;
&lt;p&gt;Introducing such an abstraction is a common way to establish a bit more
foothold in the market. Rather than trying to copy the market leader
verbatim, you create an intermediate layer, with immediate implementation
for the market leader as well, but with the promise that anyone that uses
the intermediate layer will be less tied to a single vendor or project: it
abstracts that vendor or project specifics away and shows mainly the
intrinsic qualities needed.&lt;/p&gt;
&lt;p&gt;If that abstraction is successful, other implementations for this abstraction
layer can easily come in and replace the previous technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstraction is not virtualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The introduction of abstraction layers, abstract technologies or abstract
languages should not be misunderstood for virtualization. Abstraction does
not hide or differently represent the resources beneath. It does not represent
itself as something else, but merely leaves out details that might make
interactions with the different technologies more complex.&lt;/p&gt;
&lt;p&gt;Virtualization on the other hand takes a different view. Rather than removing
the specific details, it represents a resource as something that it isn't
(or isn't completely). Hypervisors like KVM create a virtual hardware view,
and translates whatever calls towards the virtual hardware into calls to the
actual hardware - sometimes to the same type of hardware, but often towards
the CPU or resources that simulate the virtualized hardware further.&lt;/p&gt;
&lt;p&gt;Abstraction is a bit like classification, and defining how to work with a
resource through the agreed upon interfaces for that class. If you plug in
a USB device like a USB stick or USB keyboard or mouse, operating systems
will be able to interact with it regardless of its vendor and product,
because it uses the abstraction offered by the device classes: the USB mass
storage device class for the USB stick, or the USB human interface device
class for the keyboard and mouse. It abstracts away the complexity of
dealing with multiple implementations, but the devices themselves still
need to be classified as such.&lt;/p&gt;
&lt;p&gt;On hypervisors, you can create a virtual USB stick which in reality is just
a file on the hypervisor host or on a network file share. The hypervisor
virtualizes the view towards this file as if it is a USB stick, but in reality
there is no USB involved at all. Again, this doesn't have to be the case,
the hypervisor might as well enable virtualization of the USB device and
still eventually interact with an actual USB device.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VLANs are virtualized networks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another example of a virtualization is network virtualization through the
use of VLANs. In a virtual local area network (VLAN), all systems that
interact with this VLAN will see each other on the network as if they are
part of the same broadcast domain. Well, they are part of the same broadcast
domain. But if you look at the physical network implementation, this does not
imply that all these systems are attached to the same switch, and that no
routers are put in between to facilitate the communication.&lt;/p&gt;
&lt;p&gt;In larger enterprises, the use of VLANs is ubiquitous. Network virtualization
enables the telco teams and departments to optimize the actual physical
network without continuously impacting the configurations and deployments of
the services that use the network. Teams can create hundreds or thousands of
such VLANs while keeping the actual hardware investments under control, and
even be able to change and manage the network without impacting services.&lt;/p&gt;
&lt;p&gt;This benefit is strongly tied to virtualization, as we see the same in
hardware virtualization for server and workstation resources. By offering
virtualized systems, the underlying hardware can be changed, replaced or
switched without impact on the actual software that is running within the
virtualized environment. Well, mostly without impact, because not all
virtualization technologies or implementations are creating a full
virtualized view - sometimes shortcuts are created to improve efficiency
and performance. But in general, it works Just Fine (tm).&lt;/p&gt;
&lt;p&gt;Resource optimization and consolidation is easily accomplished when using
virtualization. You need far fewer switches in a virtualized network, and
you need far fewer servers for a virtualized server farm. But, it does come
at a cost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Virtualization introduces different complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you introduce a virtualization layer, be it for network, storage,
hardware or application runtimes, you introduce a layer that needs to be
actively managed. Abstraction is often much less resource intensive, as it
is a way to simplify the view on the actual resources while still being 100%
aligned with those underlying resources. Virtualization means that you need
to manage the virtualized resources, and keep track of how these resources
map to the actual underlying resources.&lt;/p&gt;
&lt;p&gt;&lt;img alt="vSphere services" src="https://blog.siphos.be/images/202106/vsphere.png"&gt;
&lt;em&gt;Source: &lt;a href="https://virtualgyaan.com/vmkernel-components-and-functionality/"&gt;https://virtualgyaan.com/vmkernel-components-and-functionality/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's look at virtualized hardware for servers. On top of it, you have to
run and maintain the hypervisor, which represents the virtual hardware to
the operating systems. Within those (virtually running) operating systems,
you have a virtual view on resources: CPU, memory, etc. The sum of all
(virtual) CPUs is often not the same as the sum of all (actual) CPUs
(depending on configuration of course), and in larger environments the
virtual operating systems might not even be running on the same hardware
as they did a few hours ago, even though the system has not been restarted.&lt;/p&gt;
&lt;p&gt;Doing performance analysis implies looking at the resources within (virtual)
as well as mapped on the actual resources, which might not be of the same
type. A virtual GPU representation might be mapped to an actual GPU (and if
you want performance, I hope it is) but doesn't have to be. I've done
investigations on a virtual Trusted Platform Module (TPM) within a virtual
system running on a server that didn't have a TPM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assessing which standardization to approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I'm confronted with an increase in IT complexity, I will often be
looking at a certain degree of standardization to facilitate this in the
organization. But what type of standardization to approach depends strongly
on the situation.&lt;/p&gt;
&lt;p&gt;Standardization by rationalization is often triggered by cost optimization
or knowledge optimization. An organization that has ten different relational
database technologies in use could benefit of a rationalization in the number
of technologies to support. However, unless there is also sufficient
abstraction put in place, this rationalization can be intensive. Another
rationalization example could be on public cloud, where an organization
chooses to only focus on a single or two cloud providers but not more.&lt;/p&gt;
&lt;p&gt;While rationalization is easy to understand and explain, it does have adverse
consequences: you miss the benefits of whatever you're rationalized away,
and unless another type of standardization is put in place, it will be hard
to switch later on if the rationalization was ill-advised or picked the
wrong targets to rationalize towards.&lt;/p&gt;
&lt;p&gt;Standardization by abstraction focuses more on simplification. You are
introducing technologies that might have better interoperability through
this abstraction, but this can only be successful if the abstraction is
comprehensive enough to still use the underlying resources in an optimal
manner.&lt;/p&gt;
&lt;p&gt;My own observation on abstraction is that it is not commonly accepted by
engineers and developers at face value. It requires much more communication
and explanation than rationalization, which is often easy to put under "cost
pressure". Abstraction focuses on efficiency in a different way, and thus
requires different communication. At the company I currently work for,
we've introduced the Open Service Broker (OSB) API as an abstraction for
service instantiation and service catalogs, and after even more than a
year, including management support, it is still a common endeavor to
explain and motivate why we chose this.&lt;/p&gt;
&lt;p&gt;Virtualization creates a highly efficient environment and supports resource
optimizations that aren't possible in other ways. Its benefits are much easier
to explain to the organization (and to management), but has a downside that
is often neglected: it introduces complexity. Hence, virtualization should
only be pursued if you can manage the complexity, and that it isn't much
worse than the complexity you want to remove. Virtualization requires
organizational support which is more platform-oriented (and thus might be
further away from the immediate 'business value' IT often has to explain),
in effect creating a new type of technology within the ever increasing
catalog of IT services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Software-defined infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While virtualization has been going around in IT for quite some time (before
I was born), a new kid on the block is becoming very popular: software-defined
infrastructure. The likes of Software Defined Network (SDN), Compute (SDC) and
Storage (SDS) are already common or becoming common. Other implementations,
like the Software Defined Perimeter, are getting jabbed by vendors as well.&lt;/p&gt;
&lt;p&gt;Now, SDI is not its own type of standardization. It is a way of managing
resources through code, and thus is a way of abstracting the infrastructure.
But unlike using a technology-agnostic abstraction, it pulls you into a
vendor-defined abstraction. That has its pros and cons, and as an architect
it is important to consider how to approach infrastructure-as-code, as SDI
implementations are not the only way to accomplish this.&lt;/p&gt;
&lt;p&gt;Furthermore, SDI does not imply virtualization. Certainly, if a technology
is virtualized, then SDI will also easily interact with it, and help you
define and manage the virtualized infrastructure as well as its underlay
infrastructure. But virtualization isn't a prerequisite for SDI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you're confronted with chaos and complexity, don't immediately start
removing technologies under the purview of "rationalization". Consider your
options on abstraction and virtualization, but be aware of the pros and cons
of each.&lt;/p&gt;</content><category term="Architecture"></category><category term="architecture"></category><category term="virtualization"></category><category term="abstraction"></category></entry><entry><title>Abstracting infrastructure complexity</title><link href="https://blog.siphos.be/2020/12/abstracting-infrastructure-complexity/" rel="alternate"></link><published>2020-12-25T23:00:00+01:00</published><updated>2020-12-25T23:00:00+01:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-12-25:/2020/12/abstracting-infrastructure-complexity/</id><summary type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;IT is complex. Some even consider it to be more magic than reality. And with
the ongoing evolutions and inventions, the complexity is not really going
away. Sure, some IT areas are becoming easier to understand, but that is often
offset with new areas being explored.&lt;/p&gt;
&lt;p&gt;Companies and organizations that have a sizeable IT footprint generally see an
increase in their infrastructure, regardless of how many rationalization
initiatives that are started. Personally, I find it challenging, in a fun
way, to keep up with the onslaught of new technologies and services that are
onboarded in the infrastructure landscape that I'm responsible for.&lt;/p&gt;
&lt;p&gt;But just understanding a technology isn't enough to deal with its position in
the larger environment.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Complexity is a challenging beast&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If someone were to attempt drawing out how the IT infrastructure of a larger
IT environment looks like in reality, it would soon become very, very large and
challenging to explain. Perhaps not chaotic, but definitely complicated.&lt;/p&gt;
&lt;p&gt;One of the challenges is the amount of "something" that is out there. That can
be the amount of devices you have, the amount of servers in the network, the
amount of flows going through firewalls or gateways, the amount of processes
running on a server, the amount of workstations and end user devices in use,
the amount of containers running in the container platform, the amount of cloud
platform instances that are active... &lt;/p&gt;
&lt;p&gt;The "something" can even be less tangible than the previous examples such as
the amount of projects that are being worked on in parallel or the amount of
changes that are being prepared. However, that complexity is not one I'll deal
with in this post.&lt;/p&gt;
&lt;p&gt;Another challenge is the virtualized nature of IT infrastructure, which has
a huge benefit for the organization and simplifies infrastructure services
for its own consumers, but does make it more, well, complicated to deal with.&lt;/p&gt;
&lt;p&gt;Virtual networks (vlans), virtual systems (hypervisors), virtual firewalls,
virtual applications (with support for streaming desktop applications to the
end user device without having the applications installed on that device),
virtual storage environments, etc. are all wonderful technologies which allow
for much more optimized resource usage, but does introduce a higher complexity
of the infastructure at large.&lt;/p&gt;
&lt;p&gt;To make sense of such larger structures, we start making abstractions of what
we see, structuring it in a way that we can more easily explain, assess or analyze
the environment and support changes properly. These abstract views do reflect
reality, but only to a certain extend. Not every question that can be asked can
be answered satisfactory with the same abstract view, but when it can, it is very
effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstracting service complexity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my day-to-day job, I'm responsible for the infrastructure of a reasonably
large environment. With "responsible" I don't want to imply that I'm the one
and only party involved of course - responsibilities are across a range of
people and roles. I am accountable for the long-term strategy on
infrastructure and the high-level infrastructure architecture and its offerings,
but how that plays out is a collaborative aspect.&lt;/p&gt;
&lt;p&gt;Because of this role, I do want to keep a close eye on all the services that
we offer from infrastructure side of things. And hence, I am often confronted
with the complexity mentioned earlier. To resolve this, I try to look at all
infastructure services in an abstract way, and document it in the same way so
that services are more easily explained.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An Archimate based view on the abstractions listed" src="https://blog.siphos.be/images/202012/abstracting-infrastructure-complexity-kvm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 - A possible visualization of the abstraction model, here in Archimate&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The abstraction I apply is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with &lt;em&gt;components&lt;/em&gt;, building blocks that are used and which refer
  to a single product or technology out there. A specific Java product can
  be considered such a component, because by itself it hardly has any value.&lt;/li&gt;
&lt;li&gt;Components are put together to create a &lt;em&gt;solution&lt;/em&gt;. This is something that
  is intended to provide value to the organization at large, and is the level
  at which something is documented, has an organizational entity responsible
  for it, etc. Solutions are not yet instantiated though. An example of a
  solution could be a Kafka-based pub/sub solution, or an OpenLDAP-based
  directory solution.&lt;/li&gt;
&lt;li&gt;Solutions are used to create &lt;em&gt;services&lt;/em&gt;. A service is something that has
  an SLA attached to it. In most cases, the same solution is used to create
  multiple services. We can think of the Kafka-based pub/sub solution that
  has three services in the organization: a regular non-production one,
  a regular production one, and a highly available production service.&lt;/li&gt;
&lt;li&gt;Services are supported through one or more &lt;em&gt;clusters&lt;/em&gt;. These are a
  way for teams to organize resources in support of a service. Some services
  might be supported by multiple clusters, for instance spread across
  different data centers. An OpenLDAP-based service might be supported by
  a single OpenLDAP cluster with native synchronization support spread across
  two data centers, or by two OpenLDAP clusters with a different
  synchronization mechanism between the two clusters.&lt;/li&gt;
&lt;li&gt;Clusters exist out of one or more &lt;em&gt;instances&lt;/em&gt;. These are the actual deployed
  technology processes that enable the cluster. In an OpenLDAP cluster, you
  could have two master processes (&lt;code&gt;slapd&lt;/code&gt; processes) running, which are the
  instances within the cluster.&lt;/li&gt;
&lt;li&gt;On top of the clusters, we enable &lt;em&gt;containers&lt;/em&gt; (I call those containers, but
  they don't have anything to do with container technology like Docker containers).
  The containers are what the consumers are actually interested in. That could
  be an organization unit in an LDAP structure, a database within an RDBMS, 
  a set of topics within a Kafka cluster, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are the basic abstractions I apply for most of the technologies, allowing
me to easily make a good view on the environment. Let's look at a few examples
here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: Virtualization of Wintel systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a large, virtualized environment, you generally have a specific hypervisor
software being used: be it RHV (Red Hat Virtualization) based upon
KVM, Microsoft HyperV, VMWare vSphere or something else - the technology used
is generally well known. That's one of the components being used, but that is
far from the only component.&lt;/p&gt;
&lt;p&gt;To better manage the virtualized environment the administration teams might
use an orchestration engine like Ansible, Puppet or Saltstack. They might also
have a component in use for automatically managing certificates and what not.&lt;/p&gt;
&lt;p&gt;All these components are needed to build a full virtualization solution. For
me, as an architect, knowing which components are used is useful for things
like lifecycle management (which components are EOL, which components can be
easily replaced with a different one versus components that are more lock-in
oriented, etc.) or inventory management (which component is deployed where,
which version is used), which supports things like vulnerability management
(if we can map components to their Common Platform Enumeration (CPE) then
we can easily see which vulnerabilities are reported through the Common
Vulnerabilities and Exposure (CVE) reports).&lt;/p&gt;
&lt;p&gt;The interaction between all these components creates a sensible solution,
which is the virtualization solution. At this level, I'm mostly interested
in the solution roadmap, the responsibilities and documentation associated
with it, the costs, maturity of the offering within the organization, etc.
It is also on the solution level that most architectural designs are made,
and the best practices (and malpractices) are documented.&lt;/p&gt;
&lt;p&gt;The virtualization solution itself is then instantiated within the
organization to create one or more services. These could be different
services based on the environment (a lab/sandbox virtualization service
with low to no SLA, a non-production one with standard SLA, a non-production
one with specific disaster recovery requirements, a production one with
standard SLA (and standard disaster recovery requirements), a high-performance
production one, etc.&lt;/p&gt;
&lt;p&gt;These services are mostly important for other architects, project leads
or other stakeholders that are going to make active use of the virtualization
services - the different services (which one could document as "service
plans") make it more obvious on what the offering is, and what differentiation
is supported.&lt;/p&gt;
&lt;p&gt;Let's consider a production, standard SLA virtualization service. The
system administrators of the virtualization environment might enable this
service across multiple clusters. This could be for several reasons: this
could be due to limits (maximum number of hosts per cluster), or because
of particular resource requirements (different CPU architecture requirements
- yes even with virtualization this is still a thing), or to make
things manageable for the administrators in general.&lt;/p&gt;
&lt;p&gt;While knowing which cluster an application is on is, in general, not
that important, it can be very important when there are problems, or when
limits are being reached. As an architect, I'm definitely interested in
knowing why multiple clusters are made (what is the reasoning behind it) as
it gives a good view on what the administrators are generally dealing with.&lt;/p&gt;
&lt;p&gt;Within a cluster (to support the virtualization) you'll find multiple hosts.
Often, a cluster is sized to be able to deal with one or two host fall-outs
so that the virtual machines (which are hosted on the cluster - these are
the "containers" that I spoke of) can be migrated to another host with only
a short downtime as a consequence (if their main host crashed) or no downtime
at all (if it is scheduled maintenance of the host). These hosts are the
instances of the cluster.&lt;/p&gt;
&lt;p&gt;By using this abstraction, I can "map" the virtualization environment in
a way that I have a good enough view, without proclaiming to be anything
more than an informed architect, on this setup to support my own work,
and to be able to advice management on major investment requirements,
challenges, strategic evolutions and more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More than just documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the above method is used for documenting the environment in which
I work (and which works well for the size of the environment I have to deal
with), it can be used for simplifying management of the technologies as
well. This level of abstraction can easily be used in environments that
push self-servicing forward.&lt;/p&gt;
&lt;p&gt;Let's take the &lt;a href="https://www.openservicebrokerapi.org/"&gt;Open Service Broker API&lt;/a&gt;
as an example. This is an API that defines how to expose (infrastructure)
services to consumers that can easily create (provision) and destroy 
(deprovision) their own services. Brokers that support the API will
then automatically handle the service management. This model can easily
be put in to support the previous abstraction.&lt;/p&gt;
&lt;p&gt;Take the virtualization environment again. If we want to enable self-servicing
on a virtualized environment, we can think of an offering where internal customers
can create new virtual machines (provision) either based on a company-vetted
template, or through an image (like with virtual appliances). The team that
manages the virtualization environment has a number of services, which they
describe in the service plans exposed by the API. An internal customer, when
privisioning a virtual machine, is thus creating a "container" for the right
service (based on their selected service plan) and on the right cluster
(based upon the parameters that the internal customer passes along with the
creation of its machine).&lt;/p&gt;
&lt;p&gt;We can do the same with databases: a certain database solution (say PostgreSQL)
has its own offerings (exposed through service plans linked to the service), and
internal customers can create their own database ("container") on the right
cluster through this API.&lt;/p&gt;
&lt;p&gt;I personally have a few scripts that I use at home myself to quickly set
up a certain technology, using the above abstraction level as the foundation.
Rather than having to try and remember how to set up a multi-master OpenLDAP
service, or a replicated Kafka setup, I have scripts that create this based
upon this abstraction: the script parameters always use the service, cluster,
instance and container terminology and underlyingly map this to the
technology-specific approach.&lt;/p&gt;
&lt;p&gt;It is my intention to also promote this abstraction usage within my
work environment, as I believe it allows us to more easily explain what
all the infrastructure is used for, but also to more easily get new employees
known to our environment. But even if that isn't reached, the abstraction is
a huge help for me to assess and understand the multitude of technologies
that are out there, be it our mainframe setup, the SAN offerings, the
network switching setup, the databases, messaging services, cloud
landing zones, firewall setups, container platforms and more.&lt;/p&gt;</content><category term="Architecture"></category><category term="infrastructure"></category><category term="archimate"></category></entry><entry><title>Working on infra strategy</title><link href="https://blog.siphos.be/2020/10/working-on-infra-strategy/" rel="alternate"></link><published>2020-10-04T13:20:00+02:00</published><updated>2020-10-04T13:20:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2020-10-04:/2020/10/working-on-infra-strategy/</id><summary type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;After a long hiatus, I'm ready to take up blogging again on my public blog.
With my day job becoming more intensive and my side-job taking the remainder
of the time, I've since quit my work on the Gentoo project. I am in process
of releasing a new edition of the SELinux System Administration book, so I'll
probably discuss that more later.&lt;/p&gt;
&lt;p&gt;Today, I want to write about a task I had to do this year as brand new domain
architect for infrastructure.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Transitioning to domain architect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I have been an infrastructure architect for quite some time already, my
focus then was always on specific areas within infrastructure (databases,
scheduling, big data), or on general infrastructure projects or challenges
(infrastructure zoning concept, region-wide disaster recovery analysis and
design). As one of my ex-colleagues and mentors put it, as infrastructure
architects you are allowed to piss on each other's area: you can (and perhaps
should) challenge the vision and projects of others, of course in a
professional way.&lt;/p&gt;
&lt;p&gt;I heeded the advice of this person, and was able to get a much better grip on
all our infrastructure services, their designs and challenges. I mentioned
earlier on that my day job became more intensive: it was not just the direct
responsibilities that I had that became more challenging, my principle to learn
and keep track of all infrastructure evolutions were a large part of it as
well. This pays off, as feedback and advice within the architecture review
boards is more to the point, more tied to the situation.&lt;/p&gt;
&lt;p&gt;Furthermore, as an architect, I still try to get my hands dirty on everything
bouncing around. When I was focusing on big data, I learned Spark and
pySpark, I revisited my Python knowledge, and used this for specific cases
(like using Python to create reports rather than Excel) to make sure I get a
general feel of what engineers and developers have to work with. When my focus
was on databases, I tried to get acquainted with DBA tasks. When we were
launching our container initiative, I set up and used Kubernetes myself (back
then this was also to see if SELinux is working properly with Kubernetes and
during the installation).&lt;/p&gt;
&lt;p&gt;While this does not make me anything near what our engineers and experts are
doing, I feel it gives me enough knowledge to be able to talk and discuss
topics with these colleagues without being that "ivory tower" architect,
and better understand (to a certain level) what they are going through when
new initiatives or solutions are thrown at them.&lt;/p&gt;
&lt;p&gt;End of 2019, the company decided that a reorganization was due, not only on
department and directorate level, but also on the IT and Enterprise
Architecture level. One of the areas that improved was to make sure the
infrastructure in general was also covered and supported by the EA team.
Part of that move, two of my infrastructure architect colleagues and
myself joined the EA team. One colleague is appointed to tackle a strategic
theme, another is now domain architect for workplace/workforce,
and I got the task of covering the infrastructure domain. Well, it is called
infrastructure, but focus on the infrastructure related to hosting of
applications and services: cloud hosting, data center, network, compute,
private cloud, container platform, mainframe, integration services,
middleware, etc. Another large part of what I consider "infrastructure" is
part of the workplace domain, which my colleague is pushing forward.&lt;/p&gt;
&lt;p&gt;While I was still handing over my previous workload, coaching the new colleague
that got thrown in to make sure both him and the teams involved are not left
with a gap, the various domain enterprise architects got a first task: draft
up the strategy for the domain… and don't wait too long ;-)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tackling a domain strategy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, I've drafted infrastructural strategies quite a few times already,
although those have most focus on the technology side. The domain view
goes beyond just the technological means: to be able to have a well-founded
strategy, I also have to tackle the resources and human side of things, the
ability of the organization to deal with (yet another?) transformation, the
processes involved, etc.&lt;/p&gt;
&lt;p&gt;Unlike the more specific area focus I had in the past, where the number of
direct stakeholders is limited, the infrastructure domain I now support has
many more direct stakeholders involved. If I count the product managers, system
architects, product owners (yes we are trying the Scaled Agile approach in our
organization) and the managerial roles within the domain, I have 148 people to
involve, spread across 7 Agile Release Trains with different directorate
steering. The consumers of the infrastructure services (which are more part of
business delivery services rather than on IT level) are even much larger than
that, and are the most important ones (but also more difficult) to get in touch
with.&lt;/p&gt;
&lt;p&gt;Rather than just asking what the main evolutions are in the several areas of
the domains, I approached this more according to practices I read in books like
"Good Strategy, Bad Strategy" by Richard Rumelt. I started with interviews of
all the stakeholders to get to learn what their challenges and problems are.
I wanted our strategy to tackle the issues at hand, not focus on technological
choices. Based on these interviews, I grouped the issues and challenges to see
what are the primary causes of these issues.&lt;/p&gt;
&lt;p&gt;Then I devised what action domains I need to focus on in the strategy. An
action domain was an area that more clearly describes the challenges ahead:
while I had close to 200 challenges observed from the interviews, I can assign
the huge majority of them to one of two action domains: if we tackle these
domains then we are helping the organization in most of their challenges.
After validating that these action domains are indeed covering the needs of
the organization, I started working on the principles how to tackle these
issues.&lt;/p&gt;
&lt;p&gt;Within the principles I want to steer the evolution within the infrastructure
domain, without already focusing on the tangible projects to accomplish that.
The principles should map to both larger projects (which I wanted to describe
in the strategy as well) as well as smaller or more continuity-related
projects. I eventually settled with four principles:
  - one principle covering how to transform the environment,
  - one principle covering what we offer (and thus also what we won't be
    offering anymore),
  - one principle which extends our scope with a major area that our internal
    customers are demanding, and
  - one principle describing how we will design our services&lt;/p&gt;
&lt;p&gt;Four principles are easy enough to remember for all involved, and if they are
described well, they are steering enough for the organization to take up in
their solutions. But with principles alone the strategy is not tangible enough
for everyone, and many choices to be made are not codified within those
principles. The next step was to draw out the vision for  infrastructure, based
upon current knowledge and the principles above, and show the major areas of
work that lays ahead, as well as give guidance on what these areas should
evolve to.&lt;/p&gt;
&lt;p&gt;I settled for eight vision statements, each worked out further with high level
guidance, as well as impact information: how will this impact the organization?
Do we need specific knowledge or other profiles that we miss? Is this a vision
that instills a cultural change (which often implies a slower adoption and the
need for more support)? What are the financial consequences? What will happen
if we do not pursue this vision?&lt;/p&gt;
&lt;p&gt;Within each vision, I collaborated with the various system architects and other
stakeholders to draft out epics, changes that support the vision and are ready
to be taken up in the Scaled Agile approach of the organization. The epics that
would be due soon were fully expanded, with a lean business case (attempt) and
phasing. Epics that are scheduled later (the strategy is a 5-year plan) are
mainly paraphrased as expanding those right now provides little value.&lt;/p&gt;
&lt;p&gt;While the epics themselves are not fully described in the strategy (the visions
give the rough approach), drafting these out is a way to verify if the vision
statements are feasible and correct, and is a way to check if the organization
understands and supports the vision.&lt;/p&gt;
&lt;p&gt;From the moment I got the request to the final draft of the strategy note,
around 2 months have passed. The first draft was slideware and showed the
intentions towards management (who wanted feedback within a few weeks after
the request), after which the strategy was codified in a large document, and
brought for approval on the appropriate boards.&lt;/p&gt;
&lt;p&gt;That was only the first hurdle though. Next was to communicate this strategy
further…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication and involvement are key&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The strategic document was almost finalized when COVID-19 struck. The company
moved to working at home, and the way of working changed a lot. This also
impacted how to approach the communication of the strategy and trying to get
involvement of people. Rather than physically explaining the strategy, watching
the body language of the people to see if they understand and support it or
not, I was facing digital meetings where we did not yet have video.
Furthermore, the organization was moving towards a more distributed approach
with smaller teams (higher agility) with fewer means of bringing out
information to larger groups.&lt;/p&gt;
&lt;p&gt;I selected a few larger meetings (such as those where all product managers and
system architects are present) to present and discuss the strategy, but also
started making webinars on this so that interested people could get informed
about it. I even decided to have two webinars: a very short one (3 minutes)
which focuses on the principles alone (and quickly summarizes the vision
statements), and an average one (20-ish minutes) which covers the principles
and vision statements.&lt;/p&gt;
&lt;p&gt;I also made recordings of the full explanations (e.g. those to management
team), which take 1 hour, but did not move those towards a webinar (due to
time pressure). Of course, I also published the strategy document itself for
everyone, as well as the slides that accompany it.&lt;/p&gt;
&lt;p&gt;One of the next steps is to translate the strategy further towards the
specific agile release trains, drafting up specific roadmaps, etc. This will
also allow me to communicate and explain the strategy further. Right now, this
is where we are at - and while I am happy with the strategy content, I do feel
that the communication part received too little attention from myself, and is
something I need to continue to put focus on.&lt;/p&gt;
&lt;p&gt;If a strategy is not absorbed by the organization, it fails as a strategy. And
if you do not have sufficient collaboration on the strategy after it was
'finalized' (not just communication but collaboration) then the organization
cannot absorb it. I also understand that the infrastructure strategy isn't
the only one guiding the organization: each domain has a strategy, and
while the domain architects do try to get the strategies aligned (or at least
not contradictory to each other), it is still not a single, company-wide
strategy.&lt;/p&gt;
&lt;p&gt;Right now, colleagues are working on consolidating the various strategies on
architectural level, while the agile organization is using the strategies to
formulate their specific solution visions (and for a handful of solutions I'm
also directly involved).&lt;/p&gt;
&lt;p&gt;We'll see how it pans out.&lt;/p&gt;
&lt;p&gt;So, do you think this is a sensible approach I took? How did you tackle
communication and collaboration of such initiatives during COVID-19 measures? &lt;/p&gt;</content><category term="Architecture"></category></entry><entry><title>Project prioritization</title><link href="https://blog.siphos.be/2017/07/project-prioritization/" rel="alternate"></link><published>2017-07-18T20:40:00+02:00</published><updated>2017-07-18T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-07-18:/2017/07/project-prioritization/</id><summary type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;sub&gt;This is a long read, skip to “Prioritizing the projects and changes” for the
approach details...&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;Organizations and companies generally have an IT workload (dare I say,
backlog?) which needs to be properly assessed, prioritized and taken up.
Sometimes, the IT team(s) get an amount of budget and HR resources to "do their
thing", while others need to continuously ask for approval to launch a new
project or instantiate a change.&lt;/p&gt;
&lt;p&gt;Sizeable organizations even require engineering and development effort on IT
projects which are not readily available: specialized teams exist, but they are
governance-wise assigned to projects. And as everyone thinks their project is
the top-most priority one, many will be disappointed when they hear there are
no resources available for their pet project.&lt;/p&gt;
&lt;p&gt;So... how should organizations prioritize such projects?&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Structure your workload, the SAFe approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first exercise you want to implement is to structure the workload, ideas or
projects. Some changes are small, others are large. Some are disruptive, others
are evolutionary. Trying to prioritize all different types of ideas and changes
in the same way is not feasible.&lt;/p&gt;
&lt;p&gt;Structuring workload is a common approach. Changes are grouped in projects,
projects grouped in programs, programs grouped in strategic tracks. Lately,
with the rise in Agile projects, a similar layering approach is suggested in
the form of SAFe.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="http://www.scaledagileframework.com/"&gt;Scaled Agile Framework&lt;/a&gt; a structure is suggested that uses, as a
top-level approach, value streams. These are strategically aligned steps that
an organization wants to use to build solutions that provide a continuous flow
of value to a customer (which can be internal or external). For instance, for a
financial service organization, a value stream could focus on 'Risk Management
and Analytics'.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SAFe full framework" src="https://blog.siphos.be/images/201707/safe-full.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;SAFe full framework overview, picture courtesy of www.scaledagileframework.com&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;The value streams are supported through solution trains, which implement
particular solutions. This could be a final product for a customer (fitting in
a particular value stream) or a set of systems which enable capabilities for a
value stream. It is at this level, imo, that the benefits exercises from IT
portfolio management and benefits realization management research plays its
role (more about that later). For instance, a solution train could focus on an
'Advanced Analytics Platform'.&lt;/p&gt;
&lt;p&gt;Within a solution train, agile release trains provide continuous delivery for
the various components or services needed within one or more solutions. Here,
the necessary solutions are continuously delivered in support of the solution
trains. At this level, focus is given on the culture within the organization
(think DevOps), and the relatively short-lived delivery delivery periods. This
is the level where I see 'projects' come into play.&lt;/p&gt;
&lt;p&gt;Finally, you have the individual teams working on deliverables supporting a
particular project.&lt;/p&gt;
&lt;p&gt;SAFe is just one of the many methods for organization and development/delivery
management. It is a good blueprint to look into, although I fear that larger
organizations will find it challenging to dedicate resources in a manageable
way. For instance, how to deal with specific expertise across solutions which
you can't dedicate to a single solution at a time? What if your organization
only has two telco experts to support dozens of projects? Keep that in mind,
I'll come back to that later...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get non-content information about the value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next to the structuring of the workload, you need to obtain information about
the solutions that you want to implement (keeping with the SAFe terminology).
And bear in mind that seemingly dull things such as ensuring your firewalls are
up to date are also deliverables within a larger ecosystem. Now, with
information about the solutions, I don't mean the content-wise information, but
instead focus on other areas.&lt;/p&gt;
&lt;p&gt;Way back, in 1952, Harry Markowitz introduced &lt;a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory"&gt;Modern portfolio theory&lt;/a&gt; as a
mathematical framework for assembling a portfolio of assets such that the
expected return is maximized for a given level of risk (quoted from Wikipedia).
This was later used in an IT portfolio approach by McFarlan in his &lt;a href="https://hbr.org/1981/09/portfolio-approach-to-information-systems"&gt;Portfolio
Approach to Information Systems&lt;/a&gt; article, published in September 1981.&lt;/p&gt;
&lt;p&gt;There it was already introduced that risk and return shouldn't be looked at
from an individual project viewpoint, but how it contributes to the overall
risk and return. A balance, if you wish. His article attempts to categorize
projects based on risk profiles on various areas. Personally, I see the
suggested categorization more as a way of supporting workload assessments (how
many mandays of work will this be), but I digress.&lt;/p&gt;
&lt;p&gt;Since then, other publications came up which tried to document frameworks and
methodologies that facilitate project portfolio prioritization and management.
The focus often boils down to value or benefits realization. In &lt;a href="https://books.google.be/books/about/The_Information_Paradox.html?id=mk60QgAACAAJ&amp;amp;redir_esc=y&amp;amp;hl=en"&gt;The
Information Paradox&lt;/a&gt; John Thorp comes up with a benefits realization
approach, which enables organizations to better define and track benefits
realization - although it again boils down on larger transformation exercises
rather than the lower-level backlogs. The realm of &lt;a href="https://en.wikipedia.org/wiki/IT_portfolio_management"&gt;IT portfolio management&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Benefits_realisation_management"&gt;Benefits realization management&lt;/a&gt; gives interesting pointers as to
the lecture part of prioritizing projects.&lt;/p&gt;
&lt;p&gt;Still, although one can hardly state the resources are incorrect, a common
question is how to make this tangible. Personally, I tend to view the above on
the value stream level and solution train level.  Here, we have a strong
alignment with benefits and value for customers, and we can leverage the ideas
of past research.&lt;/p&gt;
&lt;p&gt;The information needed at this level often boils down to strategic insights and
business benefits, coarse-grained resource assessments, with an important focus
on quality of the resources. For instance, a solution delivery might take up
500 days of work (rough estimation) but will also require significant back-end
development resources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Handling value streams and solutions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we implement this on the highest level in the structure, it should be
conceivable that the overview of the value streams (a dozen or so) and
solutions (a handful per value stream) is manageable, and something that at an
executive level is feasible to work with. These are the larger efforts for
structuring and making strategic alignment. Formal methods for prioritization
are generally not implemented or described.&lt;/p&gt;
&lt;p&gt;In my company, there are exercises that are aligning with SAFe, but it isn't
company-wide. Still, there is a structure in place that (within IT) one could
map to value streams (with some twisting ;-) and, within value streams, there
are structures in place that one could map to the solution train exercises.&lt;/p&gt;
&lt;p&gt;We could assume that the enterprise knows about its resources (people, budget
...) and makes a high-level suggestion on how to distribute the resources in
the mid-term (such as the next 6 months to a year). This distribution is
challenged and worked out with the value stream owners. See also "lean
budgeting" in the SAFe approach for one way of dealing with this.&lt;/p&gt;
&lt;p&gt;There is no prioritization of value streams. The enterprise has already made
its decision on what it finds to be the important values and benefits and
decided those in value streams.&lt;/p&gt;
&lt;p&gt;Within a value stream, the owner works together with the customers (internal or
external) to position and bring out solutions. My experience here is that
prioritization is generally based on timings and expectations from the
customer. In case of resource contention, the most challenging decision to make
here is to put a solution down (meaning, not to pursue the delivery of a
solution), and such decisions are hardly taken.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prioritizing the projects and changes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the lower echelons of the project portfolio structure, we have the projects
and changes. Let's say that the levels here are projects (agile release trains)
and changes (team-level). Here, I tend to look at prioritization on project
level, and this is the level that has a more formal approach for
prioritization.&lt;/p&gt;
&lt;p&gt;Why? Because unlike the higher levels, where the prioritization is generally
quality-oriented on a manageable amount of streams and solutions, we have a
large quantity of projects and ideas. Hence, prioritization is more
quantity-oriented in which formal methods are more efficient to handle.&lt;/p&gt;
&lt;p&gt;The method that is used in my company uses scoring criteria on a per-project
level. This is not innovative per se, as past research has also revealed that
project categorization and mapping is a powerful approach for handling project
portfolio's. Just look for "categorizing priority projects it portfolio" in
Google and you'll find ample resources. Kendal's &lt;a href="https://www.amazon.com/Advanced-Project-Portfolio-Management-PMO/dp/1932159029"&gt;Advanced Project Portfolio
Management and the PMO&lt;/a&gt; (book) has several example project scoring
criteria's. But allow me to explain our approach.&lt;/p&gt;
&lt;p&gt;It basically is like so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each project selects three value drivers (list decided up front)&lt;/li&gt;
&lt;li&gt;For the value drivers, the projects check if they contribute to it slightly (low), moderately (medium) or fully (high)&lt;/li&gt;
&lt;li&gt;The value drivers have weights, as do the values. Sum the resulting products to get a priority score&lt;/li&gt;
&lt;li&gt;Have the priority score validated by a scoring team&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get to the details of it.&lt;/p&gt;
&lt;p&gt;For the IT projects within the infrastructure area (which is what I'm active
in), we have around 5 scoring criteria (value drivers) that are value-stream
agnostic, and then 3 to 5 scoring criteria that are value-stream specific. Each
scoring criteria has three potential values: low (2), medium (4) and high (9).
The numbers are the weights that are given to the value.&lt;/p&gt;
&lt;p&gt;A scoring criteria also has a weight. For instance, we have a scoring criteria
on efficiency (read: business case) which has a weight of 15, so a score of
medium within that criteria gives a total value of 60 (4 times 15). The
potential values here are based on the "return on investment" value, with low
being a return less than 2 years, medium within a year, and high within a few
months (don't hold me on the actual values, but you get the idea).&lt;/p&gt;
&lt;p&gt;The sum of all values gives a priority score. Now, hold your horses, because
we're not done yet. There is a scoring rule that says a project can only be
scored by at most 3 scoring criteria. Hence, project owners need to see what
scoring areas their project is mostly visible in, and use those scoring
criteria. This rule supports the notion that people don't bring around ideas
that will fix world hunger and make a cure for cancer, but specific, well
scoped ideas (the former are generally huge projects, while the latter requires
much less resources).&lt;/p&gt;
&lt;p&gt;OK, so you have a score - is that your priority? No. As a project always falls
within a particular value stream, we have a "scoring team" for each value
stream which does a number of things. First, it checks if your project really
belongs in the right value stream (but that's generally implied) and has a
deliverable that fits the solution or target within that stream. Projects that
don't give any value or aren't asked by customers are eliminated.&lt;/p&gt;
&lt;p&gt;Next, the team validates if the scoring that was used is correct: did you
select the right values (low, medium or high) matching the methodology for said
criteria? If not, then the score is adjusted.&lt;/p&gt;
&lt;p&gt;Finally, the team validates if the resulting score is perceived to be OK or
not. Sometimes, ideas just don't map correctly on scoring criteria, and even
though a project has a huge strategic importance or deliverable it might score
low. In those cases, the scoring team can adjust the score manually. However,
this is more of a fail-safe (due to the methodology) rather than the norm.
About one in 20 projects gets its score adjusted. If too many adjustments come
up, the scoring team will suggest a change in methodology to rectify the
situation.&lt;/p&gt;
&lt;p&gt;With the score obtained and validated by the scoring team, the project is given
a "go" to move to the project governance. It is the portfolio manager that then
uses the scores to see when a project can start.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Providing levers to management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, these scoring criteria are not established from a random number generator.
An initial suggestion was made on the scoring criteria, and their associated
weights, to the higher levels within the organization (read: the people in
charge of the prioritization and challenging of value streams and solutions).&lt;/p&gt;
&lt;p&gt;The same people are those that approve the weights on the scoring criteria. If
management (as this is often the level at which this is decided) feels that
business case is, overall, more important than risk reduction, then they will
be able to put a higher value in the business case scoring than in the risk
reduction.&lt;/p&gt;
&lt;p&gt;The only constraint is that the total value of all scoring criteria must be
fixed. So an increase on one scoring criteria implies a reduction on at least
one other scoring criteria. Also, changing the weights (or even the scoring
criteria themselves) cannot be done frequently. There is some inertia in
project prioritization: not the implementation (because that is a matter of
following through) but the support it will get in the organization itself.&lt;/p&gt;
&lt;p&gt;Management can then use external benchmarks and other sources to gauge the
level that an organization is at, and then - if needed - adjust the scoring
weights to fit their needs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resource allocation in teams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Portfolio managers use the scores assigned to the projects to drive their
decisions as to when (and which) projects to launch. The trivial approach is to
always pick the projects with the highest scores. But that's not all.&lt;/p&gt;
&lt;p&gt;Projects can have dependencies on other projects. If these dependencies are
"hard" and non-negotiable, then the upstream project (the one being dependent
on) inherits the priority of the downstream project (the one depending on the
first) if the downstream project has a higher priority. Soft dependencies
however need to validate if they can (or have to) wait, or can implement
workarounds if needed.&lt;/p&gt;
&lt;p&gt;Projects also have specific resource requirements. A project might have a high
priority, but if it requires expertise (say DBA knowledge) which is unavailable
(because those resources are already assigned to other ongoing projects) then
the project will need to wait (once resources are fully allocated and the
projects are started, then they need to finish - another reason why projects
have a narrow scope and an established timeframe).&lt;/p&gt;
&lt;p&gt;For engineers, operators, developers and other roles, this approach allows them
to see which workload is more important versus others. When their scope is
always within a single value stream, then the mentioned method is sufficient.
But what if a resource has two projects, each of a different value stream? As
each value stream has its own scoring criteria it can use (and weight), one
value stream could systematically have higher scores than others...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mixing and matching multiple value streams&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To allow projects to be somewhat comparable in priority values, an additional
rule has been made in the scoring methodology: value streams must have a
comparable amount of scoring criteria (value drivers), and the total value of
all criteria must be fixed (as was already mentioned before). So if there are
four scoring criteria and the total value is fixed at 20, then one value stream
can have its criteria at (5,3,8,4) while another has it at (5,5,5,5).&lt;/p&gt;
&lt;p&gt;This is still not fully adequate, as one value stream could use a single
criteria with the maximum amount (20,0,0,0). However, we elected not to put in
an additional constraint, and have management work things out if the situation
ever comes out. Luckily, even managers are just human and they tend to follow
the notion of well-balanced value drivers.&lt;/p&gt;
&lt;p&gt;The result is that two projects will have priority values that are currently
sufficiently comparable to allow cross-value-stream experts to be exchangeable
without monopolizing these important resources to a single value stream
portfolio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Current state&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The scoring methodology has been around for a few years already. Initially, it
had fixed scoring criteria used by three value streams (out of seven, the other
ones did not use the same methodology), but this year we switched to support
both value stream agnostic criteria (like in the past) as well as value stream
specific ones.&lt;/p&gt;
&lt;p&gt;The methodology is furthest progressed in one value stream (with focus of around
1000 projects) and is being taken up by two others (they are still looking at
what their stream-specific criteria are before switching).&lt;/p&gt;</content><category term="Architecture"></category><category term="pmo"></category><category term="strategy"></category><category term="SAFe"></category><category term="prioritization"></category><category term="project"></category></entry><entry><title>Structuring infrastructural deployments</title><link href="https://blog.siphos.be/2017/06/structuring-infrastructural-deployments/" rel="alternate"></link><published>2017-06-07T20:40:00+02:00</published><updated>2017-06-07T20:40:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2017-06-07:/2017/06/structuring-infrastructural-deployments/</id><summary type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Many organizations struggle with the all-time increase in IP address
allocation and the accompanying need for segmentation. In the past, governing
the segments within the organization means keeping close control over the
service deployments, firewall rules, etc.&lt;/p&gt;
&lt;p&gt;Lately, the idea of micro-segmentation, supported through software-defined
networking solutions, seems to defy the need for a segmentation governance.
However, I think that that is a very short-sighted sales proposition. Even
with micro-segmentation, or even pure point-to-point / peer2peer communication
flow control, you'll still be needing a high level overview of the services
within your scope.&lt;/p&gt;
&lt;p&gt;In this blog post, I'll give some insights in how we are approaching this in
the company I work for. In short, it starts with requirements gathering,
creating labels to assign to deployments, creating groups based on one or two
labels in a layered approach, and finally fixating the resulting schema and
start mapping guidance documents (policies) toward the presented architecture.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;As always, start with the requirements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From an infrastructure architect point of view, creating structure is one way
of dealing with the onslaught in complexity that is prevalent within the wider
organizational architecture. By creating a framework in which infrastructural
services can be positioned, architects and other stakeholders (such as
information security officers, process managers, service delivery owners, project
and team leaders ...) can support the wide organization in its endeavor of
becoming or remaining competitive.&lt;/p&gt;
&lt;p&gt;Structure can be provided through various viewpoints. As such, while creating
such framework, the initial intention is not to start drawing borders or
creating a complex graph. Instead, look at attributes that one would assign
to an infrastructural service, and treat those as labels. Create a nice
portfolio of attributes which will help guide the development of such framework.&lt;/p&gt;
&lt;p&gt;The following list gives some ideas in labels or attributes that one can use.
But be creative, and use experienced people in devising the "true" list of
attributes that fits the needs of your organization. Be sure to describe them
properly and unambiguously - the list here is just an example, as are the
descriptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;tenant&lt;/strong&gt; identifies the organizational aggregation of business units which are
  sufficiently similar in areas such as policies (same policies in use),
  governance (decision bodies or approval structure), charging, etc. It
  could be a hierarchical aspect (such as organization) as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;location&lt;/strong&gt; provides insight in the physical (if applicable) location of the
  service. This could be an actual building name, but can also be structured
  depending on the size of the environment. If it is structured, make sure to
  devise a structure up front. Consider things such as regions, countries,
  cities, data centers, etc. A special case location value could be the
  jurisdiction, if that is something that concerns the organization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;service type&lt;/strong&gt; tells you what kind of service an asset is. It can be a
  workstation, a server/host, server/guest, network device, virtual or
  physical appliance, sensor, tablet, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trust level&lt;/strong&gt; provides information on how controlled and trusted the service
  is. Consider the differences between unmanaged (no patching, no users doing
  any maintenance), open (one or more admins, but no active controlled
  maintenance), controlled (basic maintenance and monitoring, but with still
  administrative access by others), managed (actively maintained, no privileged
  access without strict control), hardened (actively maintained, additional
  security measures taken) and kiosk (actively maintained, additional security
  measures taken and limited, well-known interfacing).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compliance set&lt;/strong&gt; identifies specific compliance-related attributes, such as the
  PCI-DSS compliancy level that a system has to adhere to.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consumer group&lt;/strong&gt; informs about the main consumer group, active on the service.
  This could be an identification of the relationship that consumer group has
  with the organization (anonymous, customer, provider, partner, employee, ...)
  or the actual name of the consumer group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;architectural purpose&lt;/strong&gt; gives insight in the purpose of the service in
  infrastructural terms. Is it a client system, a gateway, a mid-tier system,
  a processing system, a data management system, a batch server, a reporting
  system, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt; could be interpreted as to the company purpose of the system. Is it for
  commercial purposes (such as customer-facing software), corporate functions
  (company management), development, infrastructure/operations ...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;production status&lt;/strong&gt; provides information about the production state of a
  service. Is it a production service, or a pre-production (final testing before
  going to production), staging (aggregation of multiple changes) or development
  environment?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the final set of labels, the next step is to aggregate results to create
a high-level view of the environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating a layered structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chances are high that you'll end up with several attributes, and many of these
will have multiple possible values. What we don't want is to end in an
N-dimensional infrastructure architecture overview. Sure, it sounds sexy to do
so, but you want to show the infrastructure architecture to several stakeholders
in your organization. And projecting an N-dimensional structure on a
2-dimensional slide is not only challenging - you'll possibly create a projection
which leaves out important details or make it hard to interpret.&lt;/p&gt;
&lt;p&gt;Instead, we looked at a &lt;em&gt;layered approach&lt;/em&gt;, with each layer handling one or two
requirements. The top layer represents the requirement which the organization
seems to see as the most defining attribute. It is the attribute where, if its
value changes, most of its architecture changes (and thus the impact of a service
relocation is the largest).&lt;/p&gt;
&lt;p&gt;Suppose for instance that the domain attribute is seen as the most defining one:
the organization has strict rules about placing corporate services and commercial
services in separate environments, or the security officers want to see the
commercial services, which are well exposed to many end users, be in a separate
environment from corporate services. Or perhaps the company offers commercial
services for multiple tenants, and as such wants several separate "commercial
services" environments while having a single corporate service domain.&lt;/p&gt;
&lt;p&gt;In this case, part of the infrastructure architecture overview on the top level
could look like so (hypothetical example):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Top level view" src="https://blog.siphos.be/images/201706/07-1-toplevelview.png"&gt;&lt;/p&gt;
&lt;p&gt;This also shows that, next to the corporate and commercial interests of the
organization, a strong development support focus is prevalent as well. This
of course depends on the type of organization or company and how significant
in-house development is, but in this example it is seen as a major decisive
factor for service positioning.&lt;/p&gt;
&lt;p&gt;These top-level blocks (depicted as locations, for those of you using Archimate)
are what we call "&lt;strong&gt;zones&lt;/strong&gt;". These are not networks, but clearly bounded areas in
which multiple services are positioned, and for which particular handling rules
exist. These rules are generally written down in policies and standards - more
about that later.&lt;/p&gt;
&lt;p&gt;Inside each of these zones, a substructure is made available as well, based on
another attribute. For instance, let's assume that this is the architectural
purpose. This could be because the company has a requirement on segregating
workstations and other client-oriented zones from the application hosting related
ones. Security-wise, the company might have a principle where mid-tier services
(API and presentation layer exposures) are separate from processing services,
and where data is located in a separate zone to ensure specific data access or
more optimal infrastructure services.&lt;/p&gt;
&lt;p&gt;This zoning result could then be depicted as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Detailed top-level view" src="https://blog.siphos.be/images/201706/07-1-detailedtoplevel.png"&gt;&lt;/p&gt;
&lt;p&gt;From this viewpoint, we can also deduce that this company provides separate
workstation services: corporate workstation services (most likely managed
workstations with focus on application disclosure, end user computing, etc.)
and development workstations (most likely controlled workstations but with more
open privileged access for the developer).&lt;/p&gt;
&lt;p&gt;By making this separation explicit, the organization makes it clear that the
development workstations will have a different position, and even a different
access profile toward other services within the company.&lt;/p&gt;
&lt;p&gt;We're not done yet. For instance, on the mid-tier level, we could look at the
consumer group of the services:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Mid-tier explained" src="https://blog.siphos.be/images/201706/07-1-midtier.png"&gt;&lt;/p&gt;
&lt;p&gt;This separation can be established due to security reasons (isolating services
that are exposed to anonymous users from customer services or even partner
services), but one can also envision this to be from a management point of
view (availability requirements can differ, capacity management is more
uncertain for anonymous-facing services than authenticated, etc.)&lt;/p&gt;
&lt;p&gt;Going one layer down, we use a production status attribute as the defining
requirement:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anonymous user detail" src="https://blog.siphos.be/images/201706/07-1-anonymousdetail.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point, our company decided that the defined layers are sufficiently
established and make for a good overview. We used different defining properties
than the ones displayed above (again, find a good balance that fits the company
or organization that you're focusing on), but found that the ones we used were
mostly involved in existing policies and principles, while the other ones are
not that decisive for infrastructure architectural purposes. &lt;/p&gt;
&lt;p&gt;For instance, the tenant might not be selected as a deciding attribute, because
there will be larger tenants and smaller tenants (which could make the resulting
zone set very convoluted) or because some commercial services are offered toward
multiple tenants and the organizations' strategy would be to move toward
multi-tenant services rather than multiple deployments.&lt;/p&gt;
&lt;p&gt;Now, in the zoning structure there is still another layer, which from an
infrastructure architecture point is less about rules and guidelines and more
about manageability from an organizational point of view. For instance, in the
above example, a SAP deployment for HR purposes (which is obviously a corporate
service) might have its Enterprise Portal service in the &lt;code&gt;Corporate Services&lt;/code&gt; &amp;gt; 
&lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone. However, another service such as
an on-premise SharePoint deployment for group collaboration might be in &lt;code&gt;Corporate
Services&lt;/code&gt; &amp;gt; &lt;code&gt;Mid-tier&lt;/code&gt; &amp;gt; &lt;code&gt;Group Employees&lt;/code&gt; &amp;gt; &lt;code&gt;Production&lt;/code&gt; zone as well. Yet both
services are supported through different teams.&lt;/p&gt;
&lt;p&gt;This "final" layer thus enables grouping of services based on the supporting
team (again, this is an example), which is organizationally aligned with the
business units of the company, and potentially further isolation of services
based on other attributes which are not defining for all services. For instance,
the company might have a policy that services with a certain business impact
assessment score must be in isolated segments with no other deployments within
the same segment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about management services&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, the above picture is missing some of the (in my opinion) most important
services: infrastructure support and management services. These services do not
shine in functional offerings (which many non-IT people generally look at) but
are needed for non-functional requirements: manageability, cost control,
security (if security can be defined as a non-functional - let's not discuss
that right now).&lt;/p&gt;
&lt;p&gt;Let's first consider &lt;em&gt;interfaces&lt;/em&gt; - gateways and other services which are
positioned between zones or the "outside world". In the past, we would speak of
a demilitarized zone (DMZ). In more recent publications, one can find this as
an interface zone, or a set of Zone Interface Points (ZIPs) for accessing and
interacting with the services within a zone.&lt;/p&gt;
&lt;p&gt;In many cases, several of these interface points and gateways are used in the
organization to support a number of non-functional requirements. They can be
used for intelligent load balancing, providing virtual patching capabilities,
validating content against malware before passing it on to the actual services,
etc.&lt;/p&gt;
&lt;p&gt;Depending on the top level zone, different gateways might be needed (i.e.
different requirements). Interfaces for commercial services will have a strong
focus on security and manageability. Those for the corporate services might be
more integration-oriented, and have different data leakage requirements than
those for commercial services.&lt;/p&gt;
&lt;p&gt;Also, inside such an interface zone, one can imagine a substructure to take
place as well: egress interfaces (for communication that is exiting the zone),
ingress interfaces (for communication that is entering the zone) and internal
interfaces (used for routing between the subzones within the zone).&lt;/p&gt;
&lt;p&gt;Yet, there will also be requirements which are company-wide. Hence, one could
envision a structure where there is a company-wide interface zone (with
mandatory requirements regardless of the zone that they support) as well as a
zone-specific interface zone (with the mandatory requirements specific to that
zone).&lt;/p&gt;
&lt;p&gt;Before I show a picture of this, let's consider &lt;em&gt;management services&lt;/em&gt;. Unlike
interfaces, these services are more oriented toward the operational management
of the infrastructure. Software deployment, configuration management, identity
&amp;amp; access management services, etc. Are services one can put under management
services.&lt;/p&gt;
&lt;p&gt;And like with interfaces, one can envision the need for both company-wide
management services, as well as zone-specific management services.&lt;/p&gt;
&lt;p&gt;This information brings us to a final picture, one that assists the
organization in providing a more manageable view on its deployment landscape.
It does not show the 3rd layer (i.e. production versus non-production
deployments) and only displays the second layer through specialization
information, which I've quickly made a few examples for (you don't want to make
such decisions in a few hours, like I did for this post).&lt;/p&gt;
&lt;p&gt;&lt;img alt="General overview" src="https://blog.siphos.be/images/201706/07-1-firstgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;If the organization took an alternative approach for structuring (different
requirements and grouping) the resulting diagram could look quite different:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alternative general overview" src="https://blog.siphos.be/images/201706/07-1-secondgeneral.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flows, flows and more flows&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the high-level picture ready, it is not a bad idea to look at how flows
are handled in such an architecture. As the interface layer is available on
both company-wide level as well as the next, flows will cross multiple zones.&lt;/p&gt;
&lt;p&gt;Consider the case of a corporate workstation connecting to a reporting server
(like a Cognos or PowerBI or whatever fancy tool is used), and this reporting
server is pulling data from a database system. Now, this database system is
positioned in the &lt;code&gt;Commercial&lt;/code&gt; zone, while the reporting server is in the
&lt;code&gt;Corporate&lt;/code&gt; zone. The flows could then look like so:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Flow example" src="https://blog.siphos.be/images/201706/07-1-flow.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;Note for the Archimate people: I'm sorry that I'm abusing the flow relation
here. I didn't want to create abstract services in the locations and then use
the "serves" or "used by" relation and then explaining readers that the arrows
are then inverse from what they imagine.&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;In this picture, the corporate workstation does not connect to the reporting
server directly. It goes through the internal interface layer for the corporate
zone. This internal interface layer can offer services such as reverse proxies
or intelligent load balancers. The idea here is that, if the organization
wants, it can introduce additional controls or supporting services in this
internal interface layer without impacting the system deployments themselves
much.&lt;/p&gt;
&lt;p&gt;But the true flow challenge is in the next one, where a processing system
connects to a data layer. Here, the processing server will first connect to the
egress interface for corporate, then through the company-wide internal
interface, toward the ingress interface of the commercial and then to the data
layer.&lt;/p&gt;
&lt;p&gt;Now, why three different interfaces, and what would be inside it?&lt;/p&gt;
&lt;p&gt;On the corporate level, the egress interface could be focusing on privacy
controls or data leakage controls. On the company-wide internal interface more
functional routing capabilities could be provided, while on the commercial
level the ingress could be a database activity monitoring (DAM) system such as
a database firewall to provide enhanced auditing and access controls.&lt;/p&gt;
&lt;p&gt;Does that mean that all flows need to have at least three gateways? No, this is
a functional picture. If the organization agrees, then one or more of these
interface levels can have a simple pass-through setup. It is well possible that
database connections only connect directly to a DAM service and that such flows
are allowed to immediately go through other interfaces.&lt;/p&gt;
&lt;p&gt;The importance thus is not to make flows more difficult to provide, but to
provide several areas where the organization can introduce controls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making policies and standards more visible&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the effects of having a better structure of the company-wide deployments
(i.e. a good zoning solution) is that one can start making policies more clear,
and potentially even simple to implement with supporting tools (such as
software defined network solutions).&lt;/p&gt;
&lt;p&gt;For instance, a company might want to protect its production data and establish
that it cannot be used for non-production use, but that there are no
restrictions for the other data environments. Another rule could be that
web-based access toward the mid-tier is only allowed through an interface.&lt;/p&gt;
&lt;p&gt;These are simple statements which, if a company has a good IP plan, are easy to
implement - one doesn't need zoning, although it helps. But it goes further
than access controls.&lt;/p&gt;
&lt;p&gt;For instance, the company might require corporate workstations to be under
heavy data leakage prevention and protection measures, while developer
workstations are more open (but don't have any production data access). This
not only reveals an access control, but also implies particular minimal
requirements (for the &lt;code&gt;Corporate&lt;/code&gt; &amp;gt; &lt;code&gt;Workstation&lt;/code&gt; zone) and services (for the
&lt;code&gt;Corporate&lt;/code&gt; interfaces).&lt;/p&gt;
&lt;p&gt;This zoning structure does not necessarily make any statements about the
location (assuming it isn't picked as one of the requirements in the
beginning). One can easily extend this to include cloud-based services or
services offered by third parties.&lt;/p&gt;
&lt;p&gt;Finally, it also supports making policies and standards more realistic. I often
see policies that make bold statements such as "all software deployments must
be done through the company software distribution tool", but the policies don't
consider development environments (production status) or unmanaged, open or
controlled deployments (trust level). When challenged, the policy owner might
shrug away the comment with "it's obvious that this policy does not apply to
our sandbox environment" or so.&lt;/p&gt;
&lt;p&gt;With a proper zoning structure, policies can establish the rules for the right
set of zones, and actually pin-point which zones are affected by a statement.
This is also important if a company has many, many policies. With a good zoning
structure, the policies can be assigned with meta-data so that affected roles
(such as project leaders, architects, solution engineers, etc.) can easily get
an overview of the policies that influence a given zone.&lt;/p&gt;
&lt;p&gt;For instance, if I want to position a new management service, I am less
concerned about workstation-specific policies. And if the management service is
specific for the development environment (such as a new version control system)
many corporate or commercially oriented policies don't apply either.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above approach for structuring an organization is documented here in a
high-level manner. It takes many assumptions or hypothetical decisions which
are to be tailored toward the company itself. In my company, a different zoning
structure is selected, taking into account that it is a financial service
provider with entities in multiple countries, handling several thousand of
systems and with an ongoing effort to include cloud providers within its
infrastructure architecture.&lt;/p&gt;
&lt;p&gt;Yet the approach itself is followed in an equal fashion. We looked at
requirements, created a layered structure, and finished the zoning schema. Once
the schema was established, the requirements for all the zones were written out
further, and a mapping of existing deployments (as-is) toward the new zoning
picture is on-going. For those thinking that it is just slideware right now -
it isn't. Some of the structures that come out of the zoning exercise are
already prevalent in the organization, and new environments (due to mergers and
acquisitions) are directed to this new situation.&lt;/p&gt;
&lt;p&gt;Still, we know we have a large exercise ahead before it is finished, but I
believe that it will benefit us greatly, not only from a security point of
view, but also clarity and manageability of the environment.&lt;/p&gt;</content><category term="Architecture"></category><category term="segmentation"></category><category term="zoning"></category><category term="deployments"></category><category term="landscape"></category></entry><entry><title>Switching focus at work</title><link href="https://blog.siphos.be/2015/09/switching-focus-at-work/" rel="alternate"></link><published>2015-09-20T13:29:00+02:00</published><updated>2015-09-20T13:29:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-09-20:/2015/09/switching-focus-at-work/</id><summary type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Since 2010, I was at work responsible for the infrastructure architecture of 
a couple of technological domains, namely databases and scheduling/workload 
automation. It brought me in contact with many vendors, many technologies
and most importantly, many teams within the organization. The focus domain
was challenging, as I had to deal with the strategy on how the organization,
which is a financial institution, will deal with databases and scheduling in
the long term.&lt;/p&gt;


&lt;p&gt;This means looking at the investments related to those domains, implementation
details, standards of use, features that we will or will not use, positioning
of products and so forth. To do this from an architecture point of view means
that I not only had to focus on the details of the technology and understand 
all their use, but also become a sort-of subject matter expert on those topics.
Luckily, I had (well, still have) great teams of DBAs (for the databases) and
batch teams (for the scheduling/workload automation) to keep things in the right
direction. &lt;/p&gt;
&lt;p&gt;I helped them with a (hopefully sufficiently) clear roadmap, investment track,
procurement, contract/terms and conditions for use, architectural decisions and
positioning and what not. And they helped me with understanding the various
components, learn about the best use of these, and of course implement the 
improvements that we collaboratively put on the roadmap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Times, they are changing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last week, I flipped over a page at work. Although I remain an IT architect
within the same architecture team, my focus shifts entirely. Instead of a fixed
domain, my focus is now more volatile. I leave behind the stability of 
organizationally anchored technology domains and go forward in a more tense
environment.&lt;/p&gt;
&lt;p&gt;Instead of looking at just two technology domains, I need to look at all of them,
and find the right balance between high flexibility demands (which might not want
to use current "standard" offerings) which come up from a very agile context, and
the almost non-negotionable requirements that are typical for financial institutions.&lt;/p&gt;
&lt;p&gt;The focus is also not primarily technology oriented anymore. I'll be part of an 
enterprise architecture team with direct business involvement and although my
main focus will be on the technology side, it'll also involve information
management, business processes and applications.&lt;/p&gt;
&lt;p&gt;The end goal is to set up a future-proof architecture in an agile, fast-moving
environment (contradictio in terminis ?) which has a main focus in data analytics
and information gathering/management. Yes, "big data", but more applied than what
some of the vendors try to sell us ;-)&lt;/p&gt;
&lt;p&gt;I'm currently finishing off the high-level design and use of a Hadoop platform,
and the next focus will be on a possible micro-service architecture using Docker.
I've been working on this Hadoop design for a while now (but then it was together
with my previous function at work) and given the evolving nature of Hadoop (and
the various services that surround it) I'm confident that it will not be the last
time I'm looking at it. &lt;/p&gt;
&lt;p&gt;Now let me hope I can keep things manageable ;-)&lt;/p&gt;</content><category term="Architecture"></category><category term="work"></category><category term="hadoop"></category><category term="docker"></category></entry><entry><title>Making the case for multi-instance support</title><link href="https://blog.siphos.be/2015/08/making-the-case-for-multi-instance-support/" rel="alternate"></link><published>2015-08-22T12:45:00+02:00</published><updated>2015-08-22T12:45:00+02:00</updated><author><name>Sven Vermeulen</name></author><id>tag:blog.siphos.be,2015-08-22:/2015/08/making-the-case-for-multi-instance-support/</id><summary type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;With the high attention that technologies such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;,
&lt;a href="https://coreos.com/blog/rocket/"&gt;Rocket&lt;/a&gt; and the like get (I recommend to look at 
&lt;a href="https://github.com/p8952/bocker"&gt;Bocker&lt;/a&gt; by Peter Wilmott as well ;-), I
still find it important that technologies are well capable of supporting a
multi-instance environment.&lt;/p&gt;
&lt;p&gt;Being able to run multiple instances makes for great consolidation. The system
can be optimized for the technology, access to the system limited to the admins
of said technology while still providing isolation between instances. For some
technologies, running on commodity hardware just doesn't cut it (not all 
software is written for such hardware platforms) and consolidation allows for
reducing (hardware/licensing) costs.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Examples of multi-instance technologies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A first example that I'm pretty familiar with is multi-instance database
deployments: Oracle DBs, SQL Servers, PostgreSQLs, etc. The consolidation
of databases while still keeping multiple instances around (instead of
consolidating into a single instance itself) is mainly for operational 
reasons (changes should not influence other database/schema's) or
technical reasons (different requirements in parameters, locales, etc.)&lt;/p&gt;
&lt;p&gt;Other examples are web servers (for web hosting companies), which next to
virtual host support (which is still part of a single instance) could
benefit from multi-instance deployments for security reasons (vulnerabilities
might be better contained then) as well as performance tuning. Same goes
for web application servers (such as TomCat deployments).&lt;/p&gt;
&lt;p&gt;But even other technologies like mail servers can benefit from multiple
instance deployments. Postfix has a &lt;a href="http://www.postfix.org/MULTI_INSTANCE_README.html"&gt;nice guide&lt;/a&gt;
on multi-instance deployments and also covers some of the use cases for it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages of multi-instance setups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The primary objective that most organizations have when dealing with multiple
instances is the consolidation to reduce cost. Especially expensive, 
propriatary software which is CPU licensed gains a lot from consolidation 
(and don't think a CPU is a CPU, each company
&lt;a href="http://www-01.ibm.com/software/passportadvantage/pvu_licensing_for_customers.html"&gt;has&lt;/a&gt;
&lt;a href="http://www.oracle.com/us/corporate/contracts/processor-core-factor-table-070634.pdf"&gt;its&lt;/a&gt; (PDF)
&lt;a href="go.microsoft.com/fwlink/?LinkID=229882"&gt;own&lt;/a&gt; (PDF) core weight table to
get the most money out of their customers).&lt;/p&gt;
&lt;p&gt;But beyond cost savings, using multi-instance deployments also provides for
resource sharing. A high-end server can be used to host the multiple instances,
with for instance SSD disks (or even flash cards), more memory, high-end CPUs,
high-speed network connnectivity and more. This improves performance considerably,
because most multi-instance technologies don't need all resources continuously.&lt;/p&gt;
&lt;p&gt;Another advantage, if properly designed, is that multi-instance capable software
can often leverage the multi-instance deployments for fast changes. A database
might be easily patched (remove vulnerabilities) by creating a second codebase
deployment, patching that codebase, and then migrating the database from one
instance to another. Although it often still requires downtime, it can be made
considerably less, and roll-back of such changes is very easy.&lt;/p&gt;
&lt;p&gt;A last advantage that I see is security. Instances can be running as different
runtime accounts, through different SELinux contexts, bound on different
interfaces or chrooted into different locations. This is not an advantage
compared to dedicated systems of course, but more an advantage compared
to full consolidation (everything in a single instance).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don't always focus on multi-instance setups though&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Multiple instances isn't a silver bullet. Some technologies are generally much
better when there is a single instance on a single operating system. Personally,
I find that such technologies should know better. If they are really designed to
be suboptimal in case of multi-instance deployments, then there is a design error.&lt;/p&gt;
&lt;p&gt;But when the advantages of multiple instances do not exist (no license cost,
hardware cost is low, etc.) then organizations might focus on single-instance
deployments, because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-instance deployments might require more users to access the system
  (especially when it is multi-tenant)&lt;/li&gt;
&lt;li&gt;operational activities might impact other instances (for instance updating 
  kernel parameters for one instance requires a reboot which affects other
  instances)&lt;/li&gt;
&lt;li&gt;the software might not be properly "multi-instance aware" and as such
  starts fighting for resources with its own sigbling instances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that properly designed architectures are well capable of using
virtualization (and in the future containerization) moving towards
single-instance deployments becomes more and more interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What should multi-instance software consider?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Software should, imo, always consider multi-instance deployments. Even
when the administrator decides to stick with a single instance, all that
that takes is that the software ends up with a "single instance" setup
(it is &lt;em&gt;much&lt;/em&gt; easier to support multiple instances and deploy a single one,
than to support single instances and deploy multiple ones).&lt;/p&gt;
&lt;p&gt;The first thing software should take into account is that it might (and
will) run with different runtime accounts - service accounts if you whish.
That means that the software should be well aware that file locations are
separate, and that these locations will have different access control settings
on them (if not just a different owner).&lt;/p&gt;
&lt;p&gt;So instead of using &lt;code&gt;/etc/foo&lt;/code&gt; as the mandatory location, consider supporting
&lt;code&gt;/etc/foo/instance1&lt;/code&gt;, &lt;code&gt;/etc/foo/instance2&lt;/code&gt; if full directories are needed, or
just have &lt;code&gt;/etc/foo1.conf&lt;/code&gt; and &lt;code&gt;/etc/foo2.conf&lt;/code&gt;. I prefer the directory approach,
because it makes management much easier. It then also makes sense that the log
location is &lt;code&gt;/var/log/foo/instance1&lt;/code&gt;, the data files are at &lt;code&gt;/var/lib/foo/instance1&lt;/code&gt;,
etc.&lt;/p&gt;
&lt;p&gt;The second is that, if a service is network-facing (which most of them
are), it must be able to either use multihomed systems easily (bind to
different interfaces) or use different ports. The latter is a challenge
I often come across with software - the way to configure the software to
deal with multiple deployments and multiple ports is often a lengthy
trial-and-error setup.&lt;/p&gt;
&lt;p&gt;What's so difficult with using a &lt;em&gt;base port&lt;/em&gt; setting, and document how the
other ports are derived from this base port. &lt;a href="http://neo4j.com/docs/stable/ha-setup-tutorial.html"&gt;Neo4J&lt;/a&gt;
needs 3 ports for its enterprise services (transactions, cluster management
and online backup), but they all need to be explicitly configured if you
want a multi-instance deployment. What if one could just set &lt;code&gt;baseport = 5001&lt;/code&gt;
with the software automatically selecting 5002 and 5003 as other ports (or 6001
and 7001). If the software in the future needs another port, there is no need
to update the configuration (assuming the administrator leaves sufficient room).&lt;/p&gt;
&lt;p&gt;Also consider the service scripts (&lt;code&gt;/etc/init.d&lt;/code&gt;) or similar (depending on the
init system used). Don't provide a single one which only deals with one instance.
Instead, consider supporting symlinked service scripts which automatically obtain
the right configuration from its name.&lt;/p&gt;
&lt;p&gt;For instance, a service script called &lt;code&gt;pgsql-inst1&lt;/code&gt; which is a symlink to
&lt;code&gt;/etc/init.d/postgresql&lt;/code&gt; could then look for its configuration in &lt;code&gt;/var/lib/postgresql/pgsql-inst1&lt;/code&gt;
(or &lt;code&gt;/etc/postgresql/pgsql-inst1&lt;/code&gt;). &lt;/p&gt;
&lt;p&gt;Just like supporting &lt;a href="http://blog.siphos.be/2013/05/the-linux-d-approach/"&gt;.d directories&lt;/a&gt;,
I consider multi-instance support an important non-functional requirement for software.&lt;/p&gt;</content><category term="Architecture"></category></entry></feed>